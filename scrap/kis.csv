The (black) art of runtime evaluation: Are we comparing algorithms or implementations?,Hans-Peter Kriegel (Ludwig Maximilian University of Munich);Erich Schubert (Ludwig Maximilian University of Munich);Arthur Zimek (University of Southern Denmark);,"1919135125,2011689237,242745652","Any paper proposing a new algorithm should come with an evaluation of efficiency and scalability (particularly when we are designing methods for “big data”). However, there are several (more or less serious) pitfalls in such evaluations. We would like to point the attention of the community to these pitfalls. We substantiate our points with extensive experiments, using clustering and outlier detection methods with and without index acceleration. We discuss what we can learn from evaluations, whether experiments are properly designed, and what kind of conclusions we should avoid. We close with some general recommendations but maintain that the design of fair and conclusive experiments will always remain a challenge for researchers and an integral part of the scientific endeavor.",2017,Knowledge and Information Systems volume 52 issue 2 pp 1-38,methodology;management science;data mining;simulation;computer science;
Learning extremely shared middle-level image representation for scene classification,Peng Tang (Huazhong University of Science and Technology);Jin Zhang (Huazhong University of Science and Technology);Xinggang Wang (Huazhong University of Science and Technology);Bin Feng (Huazhong University of Science and Technology);Fabio Roli (University of Cagliari);Wenyu Liu (Huazhong University of Science and Technology);,"2437702003,2635749973,2117300371,2242327880,2048719660,2143675271","Learning middle-level image representations is very important for the computer vision community, especially for scene classification tasks. Middle-level image representations currently available are not sparse enough to make training and testing times compatible with the increasing number of classes that users want to recognize. In this work, we propose a middle-level image representation based on the pattern that extremely shared among different classes to reduce both training and test time. The proposed learning algorithm first finds some class-specified patterns and then utilizes the lasso regularization to select the most discriminative patterns shared among different classes. The experimental results on some widely used scene classification benchmarks (15 Scenes, MIT-indoor 67, SUN 397) show that the fewest patterns are necessary to achieve very remarkable performance with reduced computation time.",2017,Knowledge and Information Systems volume 52 issue 2 pp 1-22,information system;theoretical computer science;computer vision;data mining;artificial intelligence;machine learning;computer science;
"FIU-Miner (a fast, integrated, and user-friendly system for data mining) and its applications",Tao Li (Florida International University);Chunqiu Zeng (Florida International University);Wubai Zhou (Florida International University);Wei Xue (Florida International University);Yue Huang (Nanjing University of Posts and Telecommunications);Zheng Liu (Nanjing University of Posts and Telecommunications);Qifeng Zhou (Xiamen University);Bin Xia (Florida International University);Qing Wang (Florida International University);Wentao Wang (Florida International University);Xiaolong Zhu (Florida International University);,"2472069284,2155150411,2227176090,2232451325,2508143780,2717712658,2680994159,2582681207,2564420227,2492850689,2513461430","The advent of Big Data era drives data analysts from different domains to use data mining techniques for data analysis. However, performing data analysis in a specific domain is not trivial; it often requires complex task configuration, onerous integration of algorithms, and efficient execution in distributed environments. Few efforts have been paid on developing effective tools to facilitate data analysts in conducting complex data analysis tasks. In this paper, we design and implement FIU-Miner, a Fast, Integrated, and User-friendly system to ease data analysis. FIU-Miner allows users to rapidly configure a complex data analysis task without writing a single line of code. It also helps users conveniently import and integrate different analysis programs. Further, it significantly balances resource utilization and task execution in heterogeneous environments. Case studies of real-world applications demonstrate the efficacy and effectiveness of our proposed system.",2017,Knowledge and Information Systems volume 52 issue 2 pp 411-443,data mining;database;real time computing;simulation;computer science;
Evaluating intelligent knowledge systems: experiences with a user-adaptive assistant agent,Pauline M. Berry (SRI International);Thierry Donneau-Golencer (SRI International);Khang Duong (SRI International);Melinda Gervasio (SRI International);Bart Peintner (SRI International);Neil Yorke-Smith (SRI International);,"2316636898,2055697538,2147739087,2040995121,2330902230,207663627","This article examines experiences in evaluating a user-adaptive personal assistant agent designed to assist a busy knowledge worker in time management. We examine the managerial and technical challenges of designing adequate evaluation and the tension of collecting adequate data without a fully functional, deployed system. The CALO project was a seminal multi-institution effort to develop a personalized cognitive assistant. It included a significant attempt to rigorously quantify learning capability, which this article discusses for the first time, and ultimately the project led to multiple spin-outs including Siri. Retrospection on negative and positive experiences over the 6 years of the project underscores best practice in evaluating user-adaptive systems. Lessons for knowledge system evaluation include: the interests of multiple stakeholders, early consideration of evaluation and deployment, layered evaluation at system and component levels, characteristics of technology and domains that determine the appropriateness of controlled evaluations, implications of ‘in-the-wild’ versus variations of ‘in-the-lab’ evaluation, and the effect of technology-enabled functionality and its impact upon existing tools and work practices. In the conclusion, we discuss—through the lessons illustrated from this case study of intelligent knowledge system evaluation—how development and infusion of innovative technology must be supported by adequate evaluation of its efficacy.",2017,Knowledge and Information Systems volume 52 issue 2 pp 1-31,time management;knowledge based systems;knowledge management;data mining;database;artificial intelligence;simulation;computer science;
Properties of information sets and information processing with an application to face recognition,"Farrukh Sayeed (College of Engineering, Trivandrum);Madasu Hanmandlu (Indian Institute of Technology Delhi);","2519991460,1967859982","This paper presents the properties of information sets that help derive local features from a face when partitioned into windows and devises the information rules from the generalized fuzzy rules for information processing that helps match the unknown test face with the known for authenticating a user. information set is constituted from the information values that result from representing the uncertainty in a type-1 fuzzy set by Hanman–Anirban entropy function. The information values are shown to be the products of information sources (gray levels) in a window and their membership function values. The Hanman filter (HF) is devised to modify the information values using a cosine function whereas the Hanman transform (HT) is devised to evaluate the information source values based on the information obtained on them. Three classifiers, namely the inner product classifier, normed error classifier, and Hanman classifier are formulated. The two feature types based on HF and HT are tested on the AT&T (ORL) database, which contains pose variations in the face images and two other face databases: Indian face Database (IIT Kanpur) and UMIST (Sheffield) using new as well as known classifiers like Euclidean distance- based, Bayesian, and support vector machine classifiers.",2017,Knowledge and Information Systems volume 52 issue 2 pp 1-23,national electrical code;inter process communication;naive bayes classifier;support vector machine;high frequency;information processing;data mining;database;pattern recognition;artificial intelligence;machine learning;computer science;
Prequential AUC: properties of the area under the ROC curve for data streams with concept drift,Dariusz Brzezinski (Poznań University of Technology);Jerzy Stefanowski (Poznań University of Technology);,"2328495631,142644238","Modern data-driven systems often require classifiers capable of dealing with streaming imbalanced data and concept changes. The assessment of learning algorithms in such scenarios is still a challenge, as existing online evaluation measures focus on efficiency, but are susceptible to class ratio changes over time. In case of static data, the area under the receiver operating characteristics curve, or simply AUC, is a popular measure for evaluating classifiers both on balanced and imbalanced class distributions. However, the characteristics of AUC calculated on time-changing data streams have not been studied. This paper analyzes the properties of our recent proposal, an incremental algorithm that uses a sorted tree structure with a sliding window to compute AUC with forgetting. The resulting evaluation measure, called prequential AUC, is studied in terms of: visualization over time, processing speed, differences compared to AUC calculated on blocks of examples, and consistency with AUC calculated traditionally. Simulation results show that the proposed measure is statistically consistent with AUC computed traditionally on streams without drift and comparably fast to existing evaluation procedures. Finally, experiments on real-world and synthetic data showcase characteristic properties of prequential AUC compared to classification accuracy, G-mean, Kappa, Kappa M, and recall when used to evaluate classifiers on imbalanced streams with various difficulty factors.",2017,Knowledge and Information Systems volume 52 issue 2 pp 1-32,concept drift;data stream mining;data mining;pattern recognition;machine learning;statistics;computer science;
A survey on location estimation techniques for events detected in Twitter,Ozer Ozdikis (Middle East Technical University);Halit Oğuztüzün (Middle East Technical University);Pinar Karagoz (Middle East Technical University);,"2036993898,274004769,2068042486","Detection of events using voluntarily generated content in microblogs has been the objective of numerous recent studies. One essential challenge tackled in these studies is estimating the locations of events. In this paper, we review the state-of-the-art location estimation techniques used in the localization of events detected in microblogs, particularly in Twitter, which is one of the most popular microblogging platforms worldwide. We analyze these techniques with respect to the targeted event type, granularity of estimated locations, location-related features selected as sources of spatial evidence, and the method used to make aggregate decisions based on the extracted evidence. We discuss the strengths and advantages of alternative solutions to various problems related to location estimation, as well as their preconditions and limitations. We examine the most widely used evaluation methods to analyze the accuracy of estimations and present the results reported in the literature. We also discuss our findings and highlight important research challenges that may need further attention.",2017,Knowledge and Information Systems volume 52 issue 2 pp 1-49,microblogging;spatial analysis;internet privacy;world wide web;data mining;statistics;computer science;
{\lambda }-LGP: an improved version of linear genetic programming evaluated in the Ant Trail problem,Léo Françoso Dal Piccol Sotto (Federal University of São Paulo);Vinícius Veloso de Melo (Federal University of São Paulo);Márcio Porto Basgalupp (Federal University of São Paulo);,"1516183398,2116369660,2025429714","The Ant Trail problem has been widely investigated as a benchmark for automatic design of algorithms. One must design the program of a virtual ant to collect all pieces of food located in different places of a map, which may have obstacles, in a predefined limit of steps. This is a challenging problem, but several evolutionary computation (EC) researchers have reported methods with good results. In this paper, we propose an EC method called \({\lambda }\)-linear genetic programming (\({\lambda }\)-LGP), a variation of the well-known linear genetic programming (LGP) algorithm. Starting with an LGP based only on effective macro- and micro-mutations, the \({\lambda }\)-LGP proposed in this work consists in extending how the individuals are chosen for reproduction. In this model, a number (\({\lambda }\)) of mutations is applied to each individual, trying to explore its neighboring fitness regions; such individual might be replaced by one of its children according to different criteria. Several configurations were tested over three different trails: the Santa Fe, the Los Altos Hill, and the John Muir. Results show a very significant improvement over LGP by using this proposed variation. Also, \({\lambda }\)-LGP outperformed not only LGP, but also other state-of-the-art methods from the literature.",2017,Knowledge and Information Systems volume 52 issue 2 pp 445-465,genetic programming;artificial intelligence;machine learning;algorithm;computer science;
A segment-based approach for large-scale ontology matching,Xingsi Xue (Fujian University of Technology);Jeng-Shyang Pan (Fujian University of Technology);,"2631677823,2672187774","The most ground approach to solve the ontology heterogeneous problem is to determine the semantically identical entities between them, so-called ontology matching. However, the correct and complete identification of semantic correspondences is difficult to achieve with the scale of the ontologies that are huge; thus, achieving good efficiency is the major challenge for large- scale ontology matching tasks. On the basis of our former work, in this paper, we further propose a scalable segment-based ontology matching framework to improve the efficiency of matching large-scale ontologies. In particular, our proposal first divides the source ontology into several disjoint segments through an ontology partition algorithm; each obtained source segment is then used to divide the target ontology by a concept relevance measure; finally, these similar ontology segments are matched in a time and aggregated into the final ontology alignment through a hybrid Evolutionary Algorithm. In the experiment, testing cases with different scales are used to test the performance of our proposal, and the comparison with the participants in OAEI 2014 shows the effectiveness of our approach.",2017,Knowledge and Information Systems volume 52 issue 2 pp 1-18,ontology based data integration;process ontology;ontology alignment;suggested upper merged ontology;information retrieval;data mining;database;computer science;
Predicting high-risk students using Internet access logs,Qing Zhou (Chongqing University);Wenjun Quan;Yu Zhong;Wei Xiao;Chao Mou (Chongqing University);Yong Wang (Chongqing University of Posts and Telecommunications);,"2709772246,2735114630,2734966012,2734342150,2658574768,2617054730",-,2017,Knowledge and Information Systems,-
Universal trajectories of scientific success,"Tanmoy Chakraborty (Indraprastha Institute of Information Technology);Subrata Nandi (National Institute of Technology, Durgapur);","2735944024,2186056713","Success of a scientific entity generally undergoes myriad vicissitudes, resulting in different patterns of success trajectories. Understanding and characterizing the rise and fall of scientific success is important not only from the perspective of designing new mathematical models but also to enhance the quality of various real-world systems such as scientific article search and recommendation systems. In this paper, we present a large-scale study of the subject by analyzing the success of two major scientific entities—papers and authors—in Computer Science and Physics. We quantify “success” in terms of citations and in the process discover six distinct success trajectories which are prevalent across multidisciplinary datasets. Our results reveal that these trajectories are not fully random, but are rather generated through a complex process. We further shed light on the behavior of these trajectories and unfold many interesting facets by asking fundamental questions—which trajectory is more successful, how significant and stable are these categories, what factors trigger the rise and fall of trajectories? A few of our findings sharply contradict the well-accepted beliefs on bibliographic research such as “Preferential Attachment”, “first-mover advantage”. We believe that this study will argue in favor of revising the existing metrics used for quantifying scientific success.",2017,Knowledge and Information Systems pp 1-23,management science;data mining;simulation;computer science;
Recommending packages with validity constraints to groups of users,Shuyao Qi (University of Hong Kong);Nikos Mamoulis (University of Hong Kong);Evaggelia Pitoura (University of Ioannina);Panayiotis Tsaparas (University of Ioannina);,"2146889459,18851973,2242762518,2234654910",-,2017,Knowledge and Information Systems,-
Link prediction in evolving heterogeneous networks using the NARX neural networks,Alper Ozcan (Istanbul Technical University);Sule Gunduz Oguducu (Istanbul Technical University);,"2573022725,1217369908",-,2017,Knowledge and Information Systems,-
On mining approximate and exact fault-tolerant frequent itemsets,Shengxin Liu;Chung Keung Poon;,"2736035652,2734896091",-,2017,Knowledge and Information Systems,-
Pythagorean fuzzy mathematical programming method for multi-attribute group decision making with Pythagorean fuzzy truth degrees,Shu-Ping Wan;Zhen Jin;Jiu-Ying Dong;,"2734719895,2735447912,2736097172",-,2017,Knowledge and Information Systems,fuzzy set operations;fuzzy classification;artificial intelligence;algorithm;mathematics;
Artificial bee colony-based support vector machines with feature selection and parameter optimization for rule extraction,R. J. Kuo (National Taiwan University of Science and Technology);S. B. Li Huang;F. E. Zulvia (National Taiwan University of Science and Technology);T. W. Liao (Louisiana State University);,"2732927546,2727520996,2729520160,2730895016","Support vector machine (SVM) is a state-of-art classification tool with good accuracy due to its ability to generate nonlinear model. However, the nonlinear models generated are typically regarded as incomprehensible black-box models. This lack of explanatory ability is a serious problem for practical SVM applications which require comprehensibility. Therefore, this study applies a C5 decision tree (DT) to extract rules from SVM result. In addition, a metaheuristic algorithm is employed for the feature selection. Both SVM and C5 DT require expensive computation. Applying these two algorithms simultaneously for high-dimensional data will increase the computational cost. This study applies artificial bee colony optimization (ABC) algorithm to select the important features. The proposed algorithm ABC–SVM–DT is applied to extract comprehensible rules from SVMs. The ABC algorithm is applied to implement feature selection and parameter optimization before SVM–DT. The proposed algorithm is evaluated using eight datasets to demonstrate the effectiveness of the proposed algorithm. The result shows that the classification accuracy and complexity of the final decision tree can be improved simultaneously by the proposed ABC–SVM–DT algorithm, compared with genetic algorithm and particle swarm optimization algorithm.",2017,Knowledge and Information Systems pp 1-22,meta optimization;fsa red algorithm;population based incremental learning;id3 algorithm;support vector machine;decision tree;biological classification;metaheuristic;feature selection;data mining;pattern recognition;machine learning;computer science;
A many-sorted theory proposal for information retrieval: axiomatization and semantics,Loutfi Zerarga;Yassine Djouadi (Paul Sabatier University);,"2632087415,1253556957","Logic-based models have been already proposed for information retrieval purpose. However, there is a need for new formalisms providing more generic frameworks. For this purpose, an information retrieval axiomatic theory is proposed in this paper, independently of any model. Our proposal which mainly relies on many-sorted logic allows to consider various sets in the domain of discourse that provides us a rich framework to model the different items such as documents, index terms, queries. The theory relies on a sound set of axioms driving the retrieval process as proof of theorems. As such the genericity consists of a main motivation; it will be proved that three classical information retrieval models, namely the Boolean model; the fuzzy-set-based extension of the Boolean model; and the vector space model, satisfy the proposed theory, establishing then its consistency. Beyond the genericity, the proposed approach may face concrete problems. Indeed, it is well known that the use of the classical settings of formal concept analysis theory for information retrieval does not allow disjunctions and negations in queries. For this purpose, this paper gives a characterization of these queries forms using appropriates theorems of the theory. Useful algebraic properties (i.e., isomorphisms) are then established for this end.",2017,Knowledge and Information Systems pp 1-27,divergence from randomness model;standard boolean model;boolean model;formal concept analysis;vector space model;axiomatic system;discrete mathematics;machine learning;algorithm;computer science;mathematics;
"Shall I post this now? Optimized, delay-based privacy protection in social networks",Javier Parra-Arnau (Polytechnic University of Catalonia);Félix Gómez Mármol (NEC);David Rebollo-Monedero (Polytechnic University of Catalonia);Jordi Forné (Polytechnic University of Catalonia);,"1238808118,2042122991,6258794,2287917430","Despite the several advantages commonly attributed to social networks such as easiness and immediacy to communicate with acquaintances and friends, significant privacy threats provoked by unexperienced or even irresponsible users recklessly publishing sensitive material are also noticeable. Yet, a different, but equally significant privacy risk might arise from social networks profiling the online activity of their users based on the timestamp of the interactions between the former and the latter. In order to thwart this last type of commonly neglected attacks, this paper proposes an optimized deferral mechanism for messages in online social networks. Such solution suggests intelligently delaying certain messages posted by end users in social networks in a way that the observed online activity profile generated by the attacker does not reveal any time-based sensitive information, while preserving the usability of the system. Experimental results as well as a proposed architecture implementing this approach demonstrate the suitability and feasibility of our mechanism.",2017,Knowledge and Information Systems volume 52 issue 1 pp 1-33,privacy software;internet privacy;world wide web;computer security;data mining;database;computer science;
Synchronization-based scalable subspace clustering of high-dimensional data,Junming Shao (University of Electronic Science and Technology of China);Xinzuo Wang (University of Electronic Science and Technology of China);Qinli Yang (University of Electronic Science and Technology of China);Claudia Plant (University of Vienna);Christian Böhm (Ludwig Maximilian University of Munich);,"2109747452,2557442774,2635724009,2590664453,2486446532","How to address the challenges of the “curse of dimensionality” and “scalability” in clustering simultaneously? In this paper, we propose arbitrarily oriented synchronized clusters (ORSC), a novel effective and efficient method for subspace clustering inspired by synchronization. Synchronization is a basic phenomenon prevalent in nature, capable of controlling even highly complex processes such as opinion formation in a group. Control of complex processes is achieved by simple operations based on interactions between objects. Relying on the weighted interaction model and iterative dynamic clustering, our approach ORSC (a) naturally detects correlation clusters in arbitrarily oriented subspaces, including arbitrarily shaped nonlinear correlation clusters. Our approach is (b) robust against noise and outliers. In contrast to previous methods, ORSC is (c) easy to parameterize, since there is no need to specify the subspace dimensionality or other difficult parameters. Instead, all interesting subspaces are detected in a fully automatic way. Finally, (d) ORSC outperforms most comparison methods in terms of runtime efficiency and is highly scalable to large and high-dimensional data sets. Extensive experiments have demonstrated the effectiveness and efficiency of our approach.",2017,Knowledge and Information Systems volume 52 issue 1 pp 83-111,clustering high dimensional data;synchronization;theoretical computer science;data mining;machine learning;computer science;
Wind speed parameters sensitivity analysis based on fractals and neuro-fuzzy selection technique,Vlastimir Nikolić (University of Niš);Vojislav V. Mitić (University of Niš);Ljubiša Kocić (University of Niš);Dalibor Petković (University of Niš);,"2065990792,2265995024,2144982287,2022855462","Fluctuation of wind speed affects wind energy systems since the potential wind power is proportional the cube of wind speed. Hence precise prediction of wind speed is very important to improve the performances of the systems. Due to unstable behavior of the wind speed above different terrains, in this study fractal characteristics of the wind speed series were analyzed. According to the self-similarity characteristic and the scale invariance, the fractal extrapolate interpolation prediction can be performed by extending the fractal characteristic from internal interval to external interval. Afterward neuro-fuzzy technique was applied to the fractal data because of high nonlinearity of the data. The neuro-fuzzy approach was used to detect the most important variables which affect the wind speed according to the fractal dimensions. The main goal was to investigate the influence of terrain roughness length and different heights of the wind speed on the wind speed prediction.",2017,Knowledge and Information Systems volume 52 issue 1 pp 255-265,roughness length;wind speed;neuro fuzzy;feature selection;control theory;machine learning;simulation;computer science;mathematics;
rFILTA: relevant and nonredundant view discovery from collections of clusterings via filtering and ranking,Yang Lei (University of Melbourne);Nguyen Xuan Vinh (University of Melbourne);Jeffrey Chan (RMIT University);James Bailey (University of Melbourne);,"2306093603,1982975784,2408228308,2131557737","Meta-clustering is a popular approach for finding multiple clusterings in the dataset, taking a large number of base clusterings as input for further user navigation and refinement. However, the effectiveness of meta-clustering is highly dependent on the distribution of the base clusterings and open challenges exist with regard to its stability and noise tolerance. In addition, the clustering views returned may not all be relevant, hence there is open challenge on how to rank those clustering views. In this paper we propose a simple and effective filtering algorithm that can be flexibly used in conjunction with any meta-clustering method. In addition, we propose an unsupervised method to rank the returned clustering views. We evaluate the framework (rFILTA) on both synthetic and real-world datasets, and see how its use can enhance the clustering view discovery for complex scenarios.",2017,Knowledge and Information Systems volume 52 issue 1 pp 179-219,flame clustering;brown clustering;canopy clustering algorithm;correlation clustering;constrained clustering;data stream clustering;cure data clustering algorithm;single linkage clustering;fuzzy clustering;clustering high dimensional data;cluster analysis;conceptual clustering;data mining;pattern recognition;machine learning;computer science;mathematics;
Identification of critical situations via Event Processing and Event Trust Analysis,Massimiliano Leone Itria;Melinda Kocsis-Magyar;Andrea Ceccarelli (University of Florence);Paolo Lollini (University of Florence);Gabriele Giunta;Andrea Bondavalli (University of Florence);,"2703620312,2652396291,2074639126,1989086351,2675667972,167784832","In crisis management systems, situational awareness is usually at the basis of guiding the intervention process, and it is required to rapidly process data acquired from information sources on the field such as sensors or even humans. Given the variety and heterogeneity of sources and the amount of information that can be collected, together with the urgency of taking decisions, such information needs to be rapidly collected, filtered and aggregated in a form that can be used in subsequent machine-assisted decision support processes. At the same time, uncertainties in the input data or approximations in the processing phase may lead to an incorrect interpretation of the real situation in progress, which may generate mismanagements and severe consequences. This paper presents an event processor for crisis management systems that combines heterogeneous input sources to detect a critical situation. Complex Event Processing technology is applied for correlating data and creating events that describe the critical situation. Anomaly detection techniques are then used to analyze such events and detect possible anomalies, i.e., events not pertaining to the identified critical situation. The devised event processor creates trusted events that describe a critical situation merging inputs from heterogeneous and potentially untrusted sources. A prototype of the solution has been implemented and exercised within the crisis management system developed during the Secure! project. The experimental validation activities performed make use of different input sources, such as Twitter and sensors deployed on field (a Doppler radar for people detection and accelerometers for vibrations detection). The objective of the experimental campaign is to show (i) the adequacy of the solution to rapidly process the information and describe the critical situation, and (ii) its capability in detecting anomalous events that could impair the accuracy of the description of the critical situation.",2017,Knowledge and Information Systems volume 52 issue 1 pp 147-178,complex event processing;anomaly detection;decision support system;computer security;data mining;database;machine learning;simulation;computer science;
Incremental dimension reduction of tensors with random index,Fredrik Sandin (Luleå University of Technology);Blerim Emruli (Luleå University of Technology);Magnus Sahlgren;,"1937767362,188713978,2709326584","Random indexing (RI) is a lightweight dimension reduction method, which is used, for example, to approximate vector semantic relationships in online natural language processing systems. Here we generalise RI to multidimensional arrays and therefore enable approximation of higher-order statistical relationships in data. The generalised method is a sparse implementation of random projections, which is the theoretical basis also for ordinary RI and other randomisation approaches to dimensionality reduction and data representation. We present numerical experiments which demonstrate that a multidimensional generalisation of RI is feasible, including comparisons with ordinary RI and principal component analysis. The RI method is well suited for online processing of data streams because relationship weights can be updated incrementally in a fixed-size distributed representation, and inner products can be approximated on the fly at low computational cost. An open source implementation of generalised RI is provided.",2017,Knowledge and Information Systems volume 52 issue 1 pp 267-290,dimensionality reduction;data structure;theoretical computer science;data mining;database;pattern recognition;artificial intelligence;machine learning;statistics;computer science;mathematics;
A review of task scheduling based on meta-heuristics approach in cloud computing,"Poonam Singh;Maitreyee Dutta;Naveen Aggarwal (Panjab University, Chandigarh);","2607283323,2574578225,2193375809","Heterogeneous distributed computing systems are the emerging for executing scientific and computationally intensive applications. Cloud computing in this context describes a paradigm to deliver the resource-like computing and storage on-demand basis using pay-per-use model. These resources are managed by data centers and dynamically provisioned to the users based on their availability, demand and quality parameters required to be satisfied. The task scheduling onto the distributed and virtual resources is a main concern which can affect the performance of the system. In the literature, a lot of work has been done by considering cost and makespan as the affecting parameters for scheduling the dependent tasks. Prior work has discussed the various challenges affecting the performance of dependent task scheduling but did not consider storage cost, failure rate-related challenges. This paper accomplishes a review of using meta-heuristics techniques for scheduling tasks in cloud computing. We presented the taxonomy and comparative review on these algorithms. Methodical analysis of task scheduling in cloud and grid computing is presented based on swarm intelligence and bio-inspired techniques. This work will enable the readers to decide suitable approach for suggesting better schemes for scheduling user’s application. Future research issues have also been suggested in this research work.",2017,Knowledge and Information Systems volume 52 issue 1 pp 1-51,fair share scheduling;two level scheduling;flow shop scheduling;utility computing;cloud computing;scheduling;dynamic priority scheduling;swarm intelligence;distributed computing;data mining;database;real time computing;simulation;computer science;
"Exploiting interactions of review text, hidden user communities and item groups, and time for collaborative filtering",Yinqing Xu (The Chinese University of Hong Kong);Qian Yu (The Chinese University of Hong Kong);Wai Lam (The Chinese University of Hong Kong);Tianyi Lin (The Chinese University of Hong Kong);,"2156201790,2559805143,2119595446,2654068848","Rich side information concerning users and items are valuable for collaborative filtering (CF) algorithms for recommendation. For example, rating score is often associated with a piece of review text, which is capable of providing valuable information to reveal the reasons why a user gives a certain rating. Moreover, the underlying community and group relationship buried in users and items are potentially useful for CF. In this paper, we develop a new model to tackle the CF problem which predicts user’s ratings on previously unrated items by effectively exploiting interactions among review texts as well as the hidden user community and item group information. We call this model CMR (co-clustering collaborative filtering model with review text). Specifically, we employ the co-clustering technique to model the user community and item group, and each community–group pair corresponds to a co-cluster, which is characterized by a rating distribution in exponential family and a topic distribution. We have conducted extensive experiments on 22 real-world datasets, and our proposed model CMR outperforms the state-of-the-art latent factor models. Furthermore, both the user’s preference and item profile are drifting over time. Dynamic modeling the temporal changes in user’s preference and item profiles are desirable for improving a recommendation system. We extend CMR and propose an enhanced model called TCMR to consider time information and exploit the temporal interactions among review texts and co-clusters of user communities and item groups. In this TCMR model, each community–group co-cluster is characterized by an additional beta distribution for time modeling. To evaluate our TCMR model, we have conducted another set of experiments on 22 larger datasets with wider time span. Our proposed model TCMR performs better than CMR and the standard time-aware recommendation model on the rating score prediction tasks. We also investigate the temporal effect on the user–item co-clusters.",2017,Knowledge and Information Systems volume 52 issue 1 pp 221-254,collaborative filtering;biclustering;data science;world wide web;data mining;database;artificial intelligence;machine learning;computer science;
A user parameter-free approach for mining robust sequential classification rules,Elias Egho;Marc Boullé (Orange S.A.);Nicolas Voisine;Fabrice Clérot (Orange S.A.);,"2691001130,743486302,2140895767,1874864218","Sequential data are generated in many domains of science and technology. Although many studies have been carried out for sequence classification in the past decade, the problem is still a challenge, particularly for pattern-based methods. We identify two important issues related to pattern-based sequence classification, which motivate the present work: the curse of parameter tuning and the instability of common interestingness measures. To alleviate these issues, we suggest a new approach and framework for mining sequential rule patterns for classification purpose. We introduce a space of rule pattern models and a prior distribution defined on this model space. From this model space, we define a Bayesian criterion for evaluating the interest of sequential patterns. We also develop a user parameter-free algorithm to efficiently mine sequential patterns from the model space. Extensive experiments show that (i) the new criterion identifies interesting and robust patterns, (ii) the direct use of the mined rules as new features in a classification process demonstrates higher inductive performance than the state-of-the-art sequential pattern-based classifiers.",2017,Knowledge and Information Systems volume 52 issue 1 pp 53-81,data mining;pattern recognition;machine learning;mathematics;
Active instance matching with pairwise constraints and its application to Chinese knowledge base construction,Weiming Lu (Zhejiang University);Hao Dai (Zhejiang University);Zhenyu Zhang (Zhejiang University);Chao Wu;Yueting Zhuang;,"2165953393,2528183596,2622044451,2726999830,2729951713",-,2017,Knowledge and Information Systems,data mining;pattern recognition;machine learning;
The largest empty circle with location constraints in spatial databases,Gilberto Gutiérrez (University of Chile);Juan R. López;José R. Paramá (University of A Coruña);Miguel R. Penabad (University of A Coruña);,"2135450875,2729652511,19588099,2048545476",-,2017,Knowledge and Information Systems,combinatorics;geometry;mathematical optimization;
Patterns and anomalies in k-cores of real-world graphs with applications,Kijung Shin (Carnegie Mellon University);Tina Eliassi-Rad (Rutgers University);Christos Faloutsos (Carnegie Mellon University);,"2226806500,218538652,2198983026",-,2017,Knowledge and Information Systems,-
An efficient path computing model for measuring semantic similarity using edge and density,Xinhua Zhu;Fei Li;Hongchao Chen;Qi Peng;,"2690904506,2600039126,2711194230,2557267518",-,2017,Knowledge and Information Systems,theoretical computer science;computer vision;machine learning;mathematics;
Learning patterns for discovering domain-oriented opinion words,Pantelis Agathangelou (National and Kapodistrian University of Athens);Ioannis Katakis (National and Kapodistrian University of Athens);Ioannis Koutoulakis (National and Kapodistrian University of Athens);Fotis Kokkoras (Technological Educational Institute of Larissa);Dimitrios Gunopulos (National and Kapodistrian University of Athens);,"2687695414,1921192703,2628154470,2652530675,2712250546","Sentiment analysis is a challenging task that attracted increasing interest during the last years. The availability of online data along with the business interest to keep up with consumer feedback generates a constant demand for online analysis of user-generated content. A key role to this task plays the utilization of domain-specific lexicons of opinion words that enables algorithms to classify short snippets of text into sentiment classes (positive, negative). This process is known as dictionary-based sentiment analysis. The related work tends to solve this lexicon identification problem by either exploiting a corpus and a thesaurus or by manually defining a set of patterns that will extract opinion words. In this work, we propose an unsupervised approach for discovering patterns that will extract domain-specific dictionary. Our approach (DidaxTo) utilizes opinion modifiers, sentiment consistency theories, polarity assignment graphs and pattern similarity metrics. The outcome is compared against lexicons extracted by the state-of-the-art approaches on a sentiment analysis task. Experiments on user reviews coming from a diverse set of products demonstrate the utility of the proposed method. An implementation of the proposed approach in an easy to use application for extracting opinion words from any domain and evaluate their quality is also presented.",2017,Knowledge and Information Systems pp 1-33,sentiment analysis;world wide web;information retrieval;data mining;database;machine learning;computer science;
Three iteratively reweighted least squares algorithms for L_1-norm principal component analysis,Young Woong Park (Southern Methodist University);Diego Klabjan (Northwestern University);,"2729179242,134296357","Principal component analysis (PCA) is often used to reduce the dimension of data by selecting a few orthonormal vectors that explain most of the variance structure of the data. \(L_1\) PCA uses the \(L_1\) norm to measure error, whereas the conventional PCA uses the \(L_2\) norm. For the \(L_1\) PCA problem minimizing the fitting error of the reconstructed data, we propose three algorithms based on iteratively reweighted least squares. We first develop an exact reweighted algorithm. Next, an approximate version is developed based on eigenpair approximation when the algorithm is near convergent. Finally, the approximate version is extended based on stochastic singular value decomposition. We provide convergence analyses, and compare their performance against benchmark algorithms in the literature. The computational experiment shows that the proposed algorithms consistently perform the best and the scalability is improved as we use eigenpair approximation and stochastic singular value decomposition.",2017,Knowledge and Information Systems pp 1-25,iteratively reweighted least squares;machine learning;mathematical optimization;statistics;mathematics;
Modeling recurring concepts in data streams: a graph-based framework,Zahra Ahmadi (University of Mainz);Stefan Kramer (University of Mainz);,"2505307014,2272282606","Classifying a stream of non-stationary data with recurrent drift is a challenging task and has been considered as an interesting problem in recent years. All of the existing approaches handling recurrent concepts maintain a pool of concepts/classifiers and use that pool for future classifications to reduce the error on classifying the instances from a recurring concept. However, the number of classifiers in the pool usually grows very fast as the accurate detection of an underlying concept is a challenging task in itself. Thus, there may be many concepts in the pool representing the same underlying concept. This paper proposes the GraphPool framework that refines the pool of concepts by applying a merging mechanism whenever necessary: after receiving a new batch of data, we extract a concept representation from the current batch considering the correlation among features. Then, we compare the current batch representation to the concept representations in the pool using a statistical multivariate likelihood test. If more than one concept is similar to the current batch, all the corresponding concepts will be merged. GraphPool not only keeps the concepts but also maintains the transition among concepts via a first-order Markov chain. The current state is maintained at all times and new instances are predicted based on that. Keeping these transitions helps to quickly recover from drifts in some real-world problems with periodic behavior. Comprehensive experimental results of the framework on synthetic and real-world data show the effectiveness of the framework in terms of performance and pool management.",2017,Knowledge and Information Systems pp 1-30,concept drift;data mining;artificial intelligence;machine learning;computer science;
Unsupervised Outlier Detection for Time Series by Entropy and Dynamic Time Warping,Seif-Eddine Benkabou (University of Lyon);Khalid Benabdeslem (University of Lyon);Bruno Canitia;,"2222404605,230725257,2402958505","In the last decade, outlier detection for temporal data has received much attention from data mining and machine learning communities. While other works have addressed this problem by two-way approaches (similarity and clustering), we propose in this paper an embedded technique dealing with both methods simultaneously. We reformulate the task of outlier detection as a weighted clustering problem based on entropy and dynamic time warping for time series. The outliers are then detected by an optimization problem of a new proposed cost function adapted to this kind of data. Finally, we provide some experimental results for validating our proposal and comparing it with other methods of detection.",2017,Knowledge and Information Systems pp 1-24,time series;anomaly detection;data mining;pattern recognition;machine learning;statistics;computer science;
Node reactivation model to intensify influence on network targets,Chien-Wei Chang (National Cheng Kung University);Mi-Yen Yeh (Academia Sinica);Kun-Ta Chuang (National Cheng Kung University);,"2231931080,2120443347,2124692862","In this paper, we study a novel problem of influence maximization on social networks: Given a period of promotion time, a set of target users and a network in which each node can be activated by its neighbors multiple times, we aim at determining the k most influential seeds to maximize the total frequency of activations received by these target users. The promising viral marketing paradigm on social network is different from the current research in two main aspects. First, instead of maximizing the message spread over the entire social network, we focus on the target market since the business vendors almost specify the group of target users before designing its marketing strategy. Second, the status of a user is no longer a binary indicator representing either active or inactive. In the new model, the user status turns to be an integer value reflecting the amount of influences delivered to that user. In this paper, we prove the NP-hard nature of this challenging problem. We further present several strategies, including an efficient heuristic algorithm based on the simulated annealing optimization concept and a greedy algorithm as the baseline, to select the initial k seeds in pursuit of resulting quality close to the optimal one. As demonstrated in the empirical study on real data, instead of only providing the flexibility of striking a compromise between the execution efficiency and the resulting quality, our proposed heuristic algorithm can achieve high efficiency and meanwhile can obtain the target acceptance frequency even better than the greedy result in some cases, demonstrating its prominent feasibility to resolve the challenging problem efficiently.",2017,Knowledge and Information Systems pp 1-24,simulated annealing;management science;data mining;simulation;computer science;
Exploiting reciprocity toward link prediction,Niladri Sett (Indian Institute of Technology Guwahati);Devesh (Indian Institute of Technology Guwahati);Sanasam Ranbir Singh (Indian Institute of Technology Guwahati);Sukumar Nandi (Indian Institute of Technology Guwahati);,"274351354,2692448770,2428983429,2150739198","This paper addresses link prediction problem in directed networks by exploiting reciprocative nature of human relationships. It first proposes a null model to present evidence that reciprocal links influence the process of “triad formation”. Motivated by this, reciprocal links are exploited to enhance link prediction performance in three ways: (a) a reciprocity-aware link weighting technique is proposed, and existing weighted link prediction methods are applied over the resultant weighted network; (b) new link prediction methods are proposed, which exploit reciprocity; and (c) existing and proposed methods are combined toward supervised prediction to enhance the prediction performance further. All experiments are carried out on two real directed network datasets.",2017,Knowledge and Information Systems pp 1-13,reciprocity;data mining;artificial intelligence;machine learning;mathematics;
Finding multiple stable clusterings,Juhua Hu (Simon Fraser University);Qi Qian (Alibaba Group);Jian Pei (Simon Fraser University);Rong Jin (Alibaba Group);Shenghuo Zhu (Alibaba Group);,"2160078699,2163753803,2126330539,2687806448,2096537073","Multi-clustering, which tries to find multiple independent ways to partition a data set into groups, has enjoyed many applications, such as customer relationship management, bioinformatics and healthcare informatics. This paper addresses two fundamental questions in multi-clustering: How to model quality of clusterings and how to find multiple stable clusterings (MSC). We introduce to multi-clustering the notion of clustering stability based on Laplacian eigengap, which was originally used by the regularized spectral learning method for similarity matrix learning. We mathematically prove that the larger the eigengap, the more stable the clustering. Furthermore, we propose a novel multi-clustering method MSC. An advantage of our method comparing to the state-of-the-art multi-clustering methods is that our method can provide users a feature subspace to understand each clustering solution. Another advantage is that MSC does not need users to specify the number of clusters and the number of alternative clusterings, which is usually difficult for users without any guidance. Our method can heuristically estimate the number of stable clusterings in a data set. We also discuss a practical way to make MSC applicable to large-scale data. We report an extensive empirical study that clearly demonstrates the effectiveness of our method.",2017,Knowledge and Information Systems volume 51 issue 3 pp 991-1021,data mining;machine learning;algorithm;mathematics;
Minimizing conservativity violations in ontology alignments: algorithms and evaluation,Alessandro Solimando;Ernesto Jiménez-Ruiz (University of Oxford);Giovanna Guerrini;,"2047061645,2129252886,2630581452","In order to enable interoperability between ontology-based systems, ontology matching techniques have been proposed. However, when the generated mappings lead to undesired logical consequences, their usefulness may be diminished. In this paper, we present an approach to detect and minimize the violations of the so-called conservativity principle where novel subsumption entailments between named concepts in one of the input ontologies are considered as unwanted. The practical applicability of the proposed approach is experimentally demonstrated on the datasets from the Ontology Alignment Evaluation Initiative.",2017,Knowledge and Information Systems volume 51 issue 3 pp 775-819,ontology based data integration;process ontology;ontology alignment;suggested upper merged ontology;information retrieval;data mining;database;computer science;
Parallel Construction of Wavelet Trees on Multicore Architectures,José Fuentes-Sepúlveda (University of Concepción);Erick Elejalde (University of Concepción);Leo Ferres (Universidad del Desarrollo);Diego Seco (University of Concepción);,"829010365,2288031225,2610031909,2115406492","The wavelet tree has become a very useful data structure to efficiently represent and query large volumes of data in many different domains, from bioinformatics to geographic information systems. One problem with wavelet trees is their construction time. In this paper, we introduce two algorithms that reduce the time complexity of a wavelet tree’s construction by taking advantage of nowadays ubiquitous multicore machines. Our first algorithm constructs all the levels of the wavelet in parallel with O(n) time and \(O(n\lg \sigma + \sigma \lg n)\) bits of working space, where n is the size of the input sequence and \(\sigma \) is the size of the alphabet. Our second algorithm constructs the wavelet tree in a domain decomposition fashion, using our first algorithm in each segment, reaching \(O(\lg n)\) time and \(O(n\lg \sigma + p\sigma \lg n/\lg \sigma )\) bits of extra space, where p is the number of available cores. Both algorithms are practical and report good speedup for large real datasets.",2017,Knowledge and Information Systems volume 51 issue 3 pp 1043-1066,succinct data structure;multi core processor;parallel algorithm;theoretical computer science;parallel computing;data mining;database;artificial intelligence;machine learning;algorithm;computer science;
Sequential pattern mining in databases with temporal uncertainty,Jiaqi Ge (Indiana University – Purdue University Indianapolis);Yuni Xia (Indiana University – Purdue University Indianapolis);Jian Wang (Nanjing University);Chandima Hewa Nadungodage (Indiana University – Purdue University Indianapolis);Sunil Prabhakar (Purdue University);,"2107884066,2334154954,2652111714,2035691258,2119765128","Temporally uncertain data widely exist in many real-world applications. Temporal uncertainty can be caused by various reasons such as conflicting or missing event timestamps, network latency, granularity mismatch, synchronization problems, device precision limitations, data aggregation. In this paper, we propose an efficient algorithm to mine sequential patterns from data with temporal uncertainty. We propose an uncertain model in which timestamps are modeled by random variables and then design a new approach to manage temporal uncertainty. We integrate it into the pattern-growth sequential pattern mining algorithm to discover probabilistic frequent sequential patterns. Extensive experiments on both synthetic and real datasets prove that the proposed algorithm is both efficient and scalable.",2017,Knowledge and Information Systems volume 51 issue 3 pp 821-850,sequential pattern mining;data mining;database;machine learning;computer science;
Can classification performance be predicted by complexity measures? A study using microarray data,Laura Moran-Fernandez (University of A Coruña);Verónica Bolón-Canedo (University of A Coruña);Amparo Alonso-Betanzos (University of A Coruña);,"2182508628,2001306915,1961547176","Data complexity analysis enables an understanding of whether classification performance could be affected, not by algorithm limitations, but by intrinsic data characteristics. Microarray datasets based on high numbers of gene expressions combined with small sample sizes represent a particular challenge for machine learning researchers. This type of data also has other particularities that may negatively affect the generalization capacity of classifiers, such as overlaps between classes and class imbalance. Making use of several complexity measures, we analyzed the intrinsic complexity of several microarray datasets with and without feature selection and then explored the connection with the empirical results obtained by four widely used classifiers. Experimental results for 21 binary and multiclass datasets demonstrate that a correlation exists between microarray data complexity and the classification error rates.",2017,Knowledge and Information Systems volume 51 issue 3 pp 1067-1090,gene chip analysis;microarray analysis techniques;biological classification;feature selection;data mining;pattern recognition;machine learning;computer science;
Fast affinity propagation clustering based on incomplete similarity matrix,Leilei Sun (Dalian University of Technology);Chonghui Guo (Dalian University of Technology);Chuanren Liu (Drexel University);Hui Xiong (Rutgers–Newark);,"2098357117,2719303095,2169554947,2153710278","Affinity propagation (AP) is a recently proposed clustering algorithm, which has been successful used in a lot of practical problems. Although effective in finding meaningful clustering solutions, a key disadvantage of AP is its efficiency, which has become the bottleneck when applying AP for large-scale problems. In the literature, most of the methods proposed to improve the efficiency of AP are based on implementing the message-passing on a sparse similarity matrix, while neither the decline in effectiveness nor the improvement in efficiency is theoretically analyzed. In this paper, we propose a two-stage fast affinity propagation (FastAP) algorithm. Different from previous work, the scale of the similarity matrix is first compressed by selecting only potential exemplars, then further reduced by sparseness according to k nearest neighbors. More importantly, we provide theoretical analysis, based on which the improvement of efficiency in our method is controllable with guaranteed clustering performance. In experiments, two synthetic data sets, seven publicly available data sets, and two real-world streaming data sets are used to evaluate the proposed method. The results demonstrate that FastAP can achieve comparable clustering performances with the original AP algorithm, while the computational efficiency has been improved with a several-fold speed-up on small data sets and a dozens-of-fold on larger-scale data sets.",2017,Knowledge and Information Systems volume 51 issue 3 pp 941-963,canopy clustering algorithm;correlation clustering;constrained clustering;data stream clustering;cure data clustering algorithm;affinity propagation;spectral clustering;fuzzy clustering;cluster analysis;data mining;pattern recognition;machine learning;computer science;mathematics;
FDHUP: Fast algorithm for mining discriminative high utility patterns,Jerry Chun-Wei Lin (Harbin Institute of Technology);Wensheng Gan (Harbin Institute of Technology);Philippe Fournier-Viger (Harbin Institute of Technology);Tzung-Pei Hong (National University of Kaohsiung);Han-Chieh Chao (Harbin Institute of Technology);,"2187636103,2168090839,165198604,2123991549,2476492724","Recently, high utility pattern mining (HUPM) has been extensively studied. Many approaches for HUPM have been proposed in recent years, but most of them aim at mining HUPs without any consideration for their frequency. This has the major drawback that any combination of a low utility item with a very high utility pattern is regarded as a HUP, even if this combination has low affinity and contains items that rarely co-occur. Thus, frequency should be a key criterion to select HUPs. To address this issue, and derive high utility interesting patterns (HUIPs) with strong frequency affinity, the HUIPM algorithm was proposed. However, it recursively constructs a series of conditional trees to produce candidates and then derive the HUIPs. This procedure is time-consuming and may lead to a combinatorial explosion when the minimum utility threshold is set relatively low. In this paper, an efficient algorithm named fast algorithm for mining discriminative high utility patterns (DHUPs) with strong frequency affinity (FDHUP) is proposed to efficiently discover DHUPs by considering both the utility and frequency affinity constraints. Two compact structures named EI-table and FU-tree and three pruning strategies are introduced in the proposed algorithm to reduce the search space, and efficiently and effectively discover DHUPs. An extensive experimental study shows that the proposed FDHUP algorithm considerably outperforms the state-of-the-art HUIPM algorithm in terms of execution time, memory consumption, and scalability.",2017,Knowledge and Information Systems volume 51 issue 3 pp 873-909,data mining;pattern recognition;machine learning;computer science;
Compressed double-array tries for string dictionaries supporting fast lookup,Shunsuke Kanda (University of Tokushima);Kazuhiro Morita (University of Tokushima);Masao Fuketa (University of Tokushima);,"2264336447,2171470526,2059996369","A string dictionary is a basic tool for storing a set of strings in many kinds of applications. Recently, many applications need space-efficient dictionaries to handle very large datasets. In this paper, we propose new compressed string dictionaries using improved double-array tries. The double-array trie is a data structure that can implement a string dictionary supporting extremely fast lookup of strings, but its space efficiency is low. We introduce approaches for improving the disadvantage. From experimental evaluations, our dictionaries can provide the fastest lookup compared to state-of-the-art compressed string dictionaries. Moreover, the space efficiency is competitive in many cases.",2017,Knowledge and Information Systems volume 51 issue 3 pp 1023-1042,string metric;trie;string;string searching algorithm;data management;theoretical computer science;database;machine learning;computer science;
Language identification of multilingual posts from Twitter: a case study,Ferran Pla (Polytechnic University of Valencia);Lluís-F. Hurtado (Polytechnic University of Valencia);,"2291484769,2168015868","This paper describes a method for handling multi-class and multi-label classification problems based on the support vector machine formalism. This method has been applied to the language identification problem in Twitter. The system evaluation was performed mainly on a Twitter data set developed in the TweetLID workshop. This data set contains bilingual tweets written in the most commonly used Iberian languages (i.e., Spanish, Portuguese, Catalan, Basque, and Galician) as well as the English language. We address the following problems: (1) social media texts. We propose a suitable tokenization that processes the peculiarities of Twitter; (2) multilingual tweets. Since a tweet can belong to more than one language, we need to use a multi-class and multi-label classifier; (3) similar languages. We study the main confusions among similar languages; and (4) unbalanced classes. We propose threshold-based strategy to favor classes with less data. We have also studied the use of Wikipedia and the addition of new tweets in order to increase the training data set. Additionally, we have tested our system on Bergsma corpus, a collection of tweets in nine languages, focusing on confusable languages using the Cyrillic, Arabic, and Devanagari alphabets. To our knowledge, we obtained the best results published on the TweetLID data set and results that are in line with the best results published on Bergsma data set.",2017,Knowledge and Information Systems volume 51 issue 3 pp 965-989,language identification;support vector machine;natural language processing;speech recognition;data mining;database;computer science;
Incremental reinforcement learning for multi-objective robotic tasks,Javier García;Roberto Iglesias (University of Santiago de Compostela);Miguel A. Rodríguez (University of Santiago de Compostela);Carlos V. Regueiro (University of A Coruña);,"2599664264,2471798508,2220567936,2304492444","Recently reinforcement learning has been widely applied to robotic tasks. However, most of these tasks hide more than one objective. In these cases, the construction of a reward function is a key and difficult issue. A typical solution is combining the multiple objectives into one single-objective reward function. However, quite often this formulation is far from being intuitive, and the learning process might converge to a behaviour far from what we need. Another alternative to face these multi-objective tasks is to use what is called transfer learning. In this case, the idea is to reuse the experience gained after the learning of an objective to learn a new one. Nevertheless, the transfer affects only to the learned policy, leaving out other gained information that might be relevant. In this paper, we propose a different approach to learn problems with more than one objective. In particular, we describe a two-stage approach. During the first stage, our algorithm will learn a policy compatible with a main goal at the same time that it gathers relevant information for a subsequent search process. Once this is done, a second stage will start, which consists of a cyclical process of small perturbations and stabilizations, and which tries to avoid degrading the performance of the system while it searches for a new valid policy but that also optimizes a sub-objective. We have applied our proposal for the learning of the biped walking. We have tested it on a humanoid robot, both on simulation and on a real robot.",2017,Knowledge and Information Systems volume 51 issue 3 pp 911-940,multi task learning;q learning;robot learning;multi objective optimization;error driven learning;reinforcement learning;artificial intelligence;machine learning;simulation;computer science;
Intelligent data analysis approaches to churn as a business problem: a survey,David L. García (Polytechnic University of Catalonia);Àngela Nebot (Polytechnic University of Catalonia);Alfredo Vellido (Polytechnic University of Catalonia);,"2252901791,2161728171,170241628","Globalization processes and market deregulation policies are rapidly changing the competitive environments of many economic sectors. The appearance of new competitors and technologies leads to an increase in competition and, with it, a growing preoccupation among service-providing companies with creating stronger customer bonds. In this context, anticipating the customer’s intention to abandon the provider, a phenomenon known as churn, becomes a competitive advantage. Such anticipation can be the result of the correct application of information-based knowledge extraction in the form of business analytics. In particular, the use of intelligent data analysis, or data mining, for the analysis of market surveyed information can be of great assistance to churn management. In this paper, we provide a detailed survey of recent applications of business analytics to churn, with a focus on computational intelligence methods. This is preceded by an in-depth discussion of churn within the context of customer continuity management. The survey is structured according to the stages identified as basic for the building of the predictive models of churn, as well as according to the different types of predictive methods employed and the business areas of their application.",2017,Knowledge and Information Systems volume 51 issue 3 pp 719-774,computational intelligence;knowledge management;data mining;artificial intelligence;computer science;
A semi-supervised approach to sentiment analysis using revised sentiment strength based on SentiWordNet,Farhan Hassan Khan (College of Electrical and Mechanical Engineering);Usman Qamar (College of Electrical and Mechanical Engineering);Saba Bashir (College of Electrical and Mechanical Engineering);,"2227675457,1801081672,2167905595","An immense amount of data is available with the advent of social media in the last decade. This data can be used for sentiment analysis and decision making. The data present on blogs, news/review sites, social networks, etc., are so enormous that manual labeling is not feasible and an automatic approach is required for its analysis. The sentiment of the masses can be understood by analyzing this large scale and opinion rich data. The major issues in the application of automated approaches are data unavailability, data sparsity, domain independence and inadequate performance. This research proposes a semi-supervised sentiment analysis approach that incorporates lexicon-based methodology with machine learning in order to improve sentiment analysis performance. Mathematical models such as information gain and cosine similarity are employed to revise the sentiment scores defined in SentiWordNet. This research also emphasizes on the importance of nouns and employs them as semantic features with other parts of speech. The evaluation of performance measures and comparison with state-of-the-art techniques proves that the proposed approach is superior.",2017,Knowledge and Information Systems volume 51 issue 3 pp 851-872,vector space model;kullback leibler divergence;support vector machine;sentiment analysis;information retrieval;data mining;database;pattern recognition;artificial intelligence;machine learning;computer science;
Linear Separability in Spatial Databases,Gilberto Gutiérrez (University of Chile);Pablo Pérez-Lantero;Claudio Torres;,"2135450875,2681881417,2534072023","Given two spatial point sets R and B in the plane, with cardinalities m and n, respectively, and stored in two separate R-trees, we propose an efficient algorithm to verify whether R and B are linearly separable. The sets R and B are linearly separable if there exists a line that splits the plane into to halfplanes, one containing all R and the other one containing all B. This is the first algorithm that answers the separability question in the context of the spatial data bases. That is, it considers as input big spatial data stored in secondary storage data structures (e.g., the R-tree) which are not allowed to be completely stored in the main memory of the computer to run a classic algorithm. The algorithms designed in this context aim to minimize as much as possible the number of blocks read from the secondary storage data structures to the main memory. Studied problems in this setting are the k-nearest neighbor problem and the spatial range query problem. Our algorithm explicitly exploits the geometric and spatial properties of the R-trees to access only the nodes relevant to decide the linear separability of the given sets. Our experimental results show the efficiency of the algorithm, since it accesses between the 0.34 and 2.79% of the nodes of the R-trees. We also analyze the asymptotic running time of the algorithm, showing that it runs in \(O(m\log m + n\log n)\) time in the worst case.",2017,Knowledge and Information Systems pp 1-28,r tree;theoretical computer science;discrete mathematics;data mining;database;machine learning;statistics;algorithm;computer science;mathematics;
Simulated annealing-based immunodominance algorithm for multi-objective optimization problems,Ruochen Liu (Xidian University);Jianxia Li (Xidian University);Xiaolin Song (Xidian University);Xin Yu (Xidian University);Licheng Jiao (Xidian University);,"2137004161,2609201162,2495256799,2640373762,2166558591","Based on the simulated annealing strategy and immunodominance in the artificial immune system, a simulated annealing-based immunodominance algorithm (SAIA) for multi-objective optimization (MOO) is proposed in this paper. In SAIA, all immunodominant antibodies are divided into two classes: the active antibodies and the hibernate antibodies at each temperature. Clonal proliferation and recombination are employed to enhance local search on those active antibodies while the hibernate antibodies have no function, but they could become active during the following temperature. Thus, all antibodies in the search space can be exploited effectively and sufficiently. Simulated annealing-based adaptive hypermutation, population pruning, and simulated annealing selection are proposed in SAIA to evolve and obtain a set of antibodies as the trade-off solutions. Complexity analysis of SAIA is also provided. The performance comparison of SAIA with some state-of-the-art MOO algorithms in solving 14 well-known multi-objective optimization problems (MOPs) including four many objectives test problems and twelve multi-objective 0/1 knapsack problems shows that SAIA is superior in converging to approximate Pareto front with a standout distribution.",2017,Knowledge and Information Systems pp 1-37,artificial immune system;adaptive simulated annealing;knapsack problem;multi objective optimization;simulated annealing;artificial intelligence;machine learning;mathematical optimization;computer science;mathematics;
Community-preserving anonymization of graphs,François Rousseau (École Polytechnique);Jordi Casas-Roma (Open University of Catalonia);Michalis Vazirgiannis (École Polytechnique);,"2599056255,147439060,1914497179","In this paper, we propose a novel edge modification technique that better preserves the communities of a graph while anonymizing it. By maintaining the core number sequence of a graph, its coreness, we retain most of the information contained in the network while allowing changes in the degree sequence, i. e. obfuscating the visible data an attacker has access to. We reach a better trade-off between data privacy and data utility than with existing methods by capitalizing on the slack between apparent degree (node degree) and true degree (node core number). Our extensive experiments on six diverse standard network datasets support this claim. Our framework compares our method to other that are used as proxies for privacy protection in the relevant literature. We demonstrate that our method leads to higher data utility preservation, especially in clustering, for the same levels of randomization and k-anonymity.",2017,Knowledge and Information Systems pp 1-29,privacy;social network;theoretical computer science;world wide web;data mining;database;computer science;mathematics;
Event Stream-Based Process Discovery using Abstract Representations,Sebastiaan J. van Zelst (Eindhoven University of Technology);Boudewijn F. van Dongen (Eindhoven University of Technology);Wil M. P. van der Aalst (Eindhoven University of Technology);,"2395246197,2064285910,270949118","The aim of process discovery, originating from the area of process mining, is to discover a process model based on business process execution data. A majority of process discovery techniques relies on an event log as an input. An event log is a static source of historical data capturing the execution of a business process. In this paper, we focus on process discovery relying on online streams of business process execution events. Learning process models from event streams poses both challenges and opportunities, i.e. we need to handle unlimited amounts of data using finite memory and, preferably, constant time. We propose a generic architecture that allows for adopting several classes of existing process discovery techniques in context of event streams. Moreover, we provide several instantiations of the architecture, accompanied by implementations in the process mining toolkit ProM (http://promtools.org). Using these instantiations, we evaluate several dimensions of stream-based process discovery. The evaluation shows that the proposed architecture allows us to lift process discovery to the streaming domain.",2017,Knowledge and Information Systems pp 1-29,process mining;business process discovery;data mining;database;real time computing;computer science;
A novel classifier ensemble approach for financial distress prediction,Deron Liang (National Central University);Chih-Fong Tsai (National Central University);An-Jie Dai (National Central University);William Eberle (Tennessee Technological University);,"2689815845,2106293631,2613306292,2112094044","Financial distress prediction is very important to financial institutions who must be able to make critical decisions regarding customer loans. Bankruptcy prediction and credit scoring are the two main aspects considered in financial distress prediction. To assist in this determination, thereby lowering the risk borne by the financial institution, it is necessary to develop effective prediction models for prediction of the likelihood of bankruptcy and estimation of credit risk. A number of financial distress prediction models have been constructed, which utilize various machine learning techniques, such as single classifiers and classifier ensembles, but improving the prediction accuracy is the major research issue. In addition, aside from improving the prediction accuracy, there have been very few studies that specifically consider lowering the Type I error. In practice, Type I errors need to receive careful consideration during model construction because they can affect the cost to the financial institution. In this study, we introduce a classifier ensemble approach designed to reduce the misclassification cost. The outputs produced by multiple classifiers are combined by utilizing the unanimous voting (UV) method to find the final prediction result. Experimental results obtained based on four relevant datasets show that our UV ensemble approach outperforms the baseline single classifiers and classifier ensembles. Specifically, the UV ensemble not only provides relatively good prediction accuracy and minimizes Type I/II errors, but also produces the smallest misclassification cost.",2017,Knowledge and Information Systems pp 1-26,type i and type ii errors;data mining;pattern recognition;machine learning;computer science;
Document-level sentiment classification using hybrid machine learning approach,"Abinash Tripathy (National Institute of Technology, Rourkela);Abhishek Anand (National Institute of Technology, Rourkela);Santanu Kumar Rath (National Institute of Technology, Rourkela);","2264980191,2612566238,2217373898","It is a practice that users or customers intend to share their comments or reviews about any product in different social networking sites. An analyst usually processes to reviews properly to obtain any meaningful information from it. Classification of sentiments associated with reviews is one of these processing steps. The reviews framed are often made in text format. While processing the text reviews, each word of the review is considered as a feature. Thus, selection of right kind of features needs to be carried out to select the best feature from the set of all features. In this paper, the machine learning algorithm, i.e., support vector machine, is used to select the best features from the training data. These features are then given input to artificial neural network method, to process further. Different performance evaluation parameters such as precision, recall, f-measure, accuracy have been considered to evaluate the performance of the proposed approach on two different datasets, i.e., IMDb dataset and polarity dataset.",2017,Knowledge and Information Systems volume 5 pp 1-27,online machine learning;data mining;pattern recognition;artificial intelligence;machine learning;computer science;
STEM: a suffix tree-based method for web data records extraction,Yixiang Fang (University of Hong Kong);Xiaoqin Xie (Harbin Engineering University);Xiaofeng Zhang (Harbin Institute of Technology);Reynold Cheng (University of Hong Kong);Zhiqiang Zhang (Harbin Engineering University);,"2277390951,2706943170,2678979916,2138267588,2654216231","To automatically extract data records from Web pages, the data record extraction algorithm is required to be robust and efficient. However, most of existing algorithms are not robust enough to cope with rich information or noisy data. In this paper, we propose a novel suffix tree-based extraction method (STEM) for this challenging task. First, we extract a sequence of identifiers from the tag paths of Web pages. Then, a suffix tree is built on top of this sequence and four refining filters are proposed to screen out data regions that might not contain data records. To evaluate model performance, we define an evaluation metric called pattern similarity and perform rigorous experiments on five real data sets. The promising experimental results have demonstrated that the proposed STEM is superior to the state-of-the-art algorithms like MDR, TPC and CTVS with respect to precision, recall and pattern similarity. Moreover, the time complexity of STEM is linear to the total number of HTML tags contained in Web pages, which indicates the potential applicability of STEM in a wide range of Web-scale data record extraction applications.",2017,Knowledge and Information Systems pp 1-27,compressed suffix array;information retrieval;data mining;database;computer science;
DeepAM: a heterogeneous deep learning framework for intelligent malware detection,Yanfang Ye (West Virginia University);Lingwei Chen (West Virginia University);Shifu Hou (West Virginia University);William Hardy (West Virginia University);Xin Li (West Virginia University);,"2150347379,2222016444,2668735387,2490344957,2704364736","With computers and the Internet being essential in everyday life, malware poses serious and evolving threats to their security, making the detection of malware of utmost concern. Accordingly, there have been many researches on intelligent malware detection by applying data mining and machine learning techniques. Though great results have been achieved with these methods, most of them are built on shallow learning architectures. Due to its superior ability in feature learning through multilayer deep architecture, deep learning is starting to be leveraged in industrial and academic research for different applications. In this paper, based on the Windows application programming interface calls extracted from the portable executable files, we study how a deep learning architecture can be designed for intelligent malware detection. We propose a heterogeneous deep learning framework composed of an AutoEncoder stacked up with multilayer restricted Boltzmann machines and a layer of associative memory to detect newly unknown malware. The proposed deep learning model performs as a greedy layer-wise training operation for unsupervised feature learning, followed by supervised parameter fine-tuning. Different from the existing works which only made use of the files with class labels (either malicious or benign) during the training phase, we utilize both labeled and unlabeled file samples to pre-train multiple layers in the heterogeneous deep learning framework from bottom to up for feature learning. A comprehensive experimental study on a real and large file collection from Comodo Cloud Security Center is performed to compare various malware detection approaches. Promising experimental results demonstrate that our proposed deep learning framework can further improve the overall performance in malware detection compared with traditional shallow learning methods, deep learning methods with homogeneous framework, and other existing anti-malware scanners. The proposed heterogeneous deep learning framework can also be readily applied to other malware detection tasks.",2017,Knowledge and Information Systems pp 1-21,autoencoder;boltzmann machine;active learning;feature learning;semi supervised learning;theoretical computer science;world wide web;data mining;database;artificial intelligence;machine learning;computer science;
Recent advances in feature selection and its applications,Yun Li (Nanjing University of Posts and Telecommunications);Tao Li (Nanjing University of Posts and Telecommunications);Huan Liu (Arizona State University);,"2694403938,2472069284,2122391114","Feature selection is one of the key problems for machine learning and data mining. In this review paper, a brief historical background of the field is given, followed by a selection of challenges which are of particular current interests, such as feature selection for high-dimensional small sample size data, large-scale data, and secure feature selection. Along with these challenges, some hot topics for feature selection have emerged, e.g., stable feature selection, multi-view feature selection, distributed feature selection, multi-label feature selection, online feature selection, and adversarial feature selection. Then, the recent advances of these topics are surveyed in this paper. For each topic, the existing problems are analyzed, and then, current solutions to these problems are presented and discussed. Besides the topics, some representative applications of feature selection are also introduced, such as applications in bioinformatics, social media, and multimedia retrieval.",2017,Knowledge and Information Systems pp 1-27,feature selection;data mining;pattern recognition;machine learning;computer science;
"Personalized trip recommendation for tourists based on user interests, points of interest visit durations and visit recency",Kwan Hui Lim (University of Melbourne);Jeffrey Chan (University of Melbourne);Christopher Leckie (University of Melbourne);Shanika Karunasekera (University of Melbourne);,"2080969542,2408228308,2111831791,1271724398","Tour recommendation and itinerary planning are challenging tasks for tourists, due to their need to select points of interest (POI) to visit in unfamiliar cities and to select POIs that align with their interest preferences and trip constraints. We propose an algorithm called PersTour for recommending personalized tours using POI popularity and user interest preferences, which are automatically derived from real-life travel sequences based on geo-tagged photographs. Our tour recommendation problem is modeled using a formulation of the Orienteering problem and considers user trip constraints such as time limits and the need to start and end at specific POIs. In our work, we also reflect levels of user interest based on visit durations and demonstrate how POI visit duration can be personalized using this time-based user interest. Furthermore, we demonstrate how PersTour can be further enhanced by: (i) a weighted updating of user interests based on the recency of their POI visits and (ii) an automatic weighting between POI popularity and user interests based on the tourist’s activity level. Using a Flickr dataset of ten cities, our experiments show the effectiveness of PersTour against various collaborative filtering and greedy-based baselines, in terms of tour popularity, interest, recall, precision and F\(_1\)-score. In particular, our results show the merits of using time-based user interest and personalized POI visit durations, compared to the current practice of using frequency-based user interest and average visit durations.",2017,Knowledge and Information Systems volume 51 pp 1-32,brand;personalization;social network;multimedia;world wide web;data mining;
Crowdsourcing for data management,"Valter Crescenzi (Roma Tre University);Alvaro A. A. Fernandes (School of Computer Science, University of Manchester);Paolo Merialdo (Worcester Polytechnic Institute);Norman W. Paton (School of Computer Science, University of Manchester);","1986707317,2136578401,262536330,2070167002","Crowdsourcing provides access to a pool of human workers who can contribute solutions to tasks that are challenging for computers. Proposals have been made for the use of crowdsourcing in a wide range of data management tasks, including data gathering, query processing, data integration, and cleaning. We provide a classification of key features of these proposals and survey results to date, identifying recurring themes and open issues.",2017,Knowledge and Information Systems pp 1-41,crowdsourcing software development;crowdsourcing;name resolution;data integration;data management;data science;data mining;database;computer science;
EFIM: a fast and memory efficient algorithm for high-utility itemset mining,Souleymane Zida (Université de Moncton);Philippe Fournier-Viger (Harbin Institute of Technology);Jerry Chun-Wei Lin (Harbin Institute of Technology);Cheng-Wei Wu (National Chiao Tung University);Vincent S. Tseng (Harbin Institute of Technology);,"2030554102,165198604,2187636103,2111809384,2662202843","In recent years, high-utility itemset mining has emerged as an important data mining task. However, it remains computationally expensive both in terms of runtime and memory consumption. It is thus an important challenge to design more efficient algorithms for this task. In this paper, we address this issue by proposing a novel algorithm named EFIM (EFficient high-utility Itemset Mining), which introduces several new ideas to more efficiently discover high-utility itemsets. EFIM relies on two new upper bounds named revised sub-tree utility and local utility to more effectively prune the search space. It also introduces a novel array-based utility counting technique named Fast Utility Counting to calculate these upper bounds in linear time and space. Moreover, to reduce the cost of database scans, EFIM proposes efficient database projection and transaction merging techniques named High-utility Database Projection and High-utility Transaction Merging (HTM), also performed in linear time. An extensive experimental study on various datasets shows that EFIM is in general two to three orders of magnitude faster than the state-of-art algorithms \(\hbox {d}^2\)HUP, HUI-Miner, HUP-Miner, FHM and UP-Growth+ on dense datasets and performs quite well on sparse datasets. Moreover, a key advantage of EFIM is its low memory consumption.",2017,Knowledge and Information Systems volume 51 issue 2 pp 595-625,data science;data mining;database;machine learning;computer science;
Markov logic networks for adverse drug event extraction from text,Sriraam Natarajan (Indiana University Bloomington);Vishal Bangera (Indiana University Bloomington);Tushar Khot (University of Wisconsin-Madison);Jose Picado (Oregon State University);Anurag Wazalwar (Indiana University Bloomington);Vitor Santos Costa (University of Porto);David Page (University of Wisconsin-Madison);Michael Caldwell (Marshfield Clinic);,"2655261187,2624078285,1838144096,2226491996,2602142073,2104007874,1975712082,2098265439","Adverse drug events (ADEs) are a major concern and point of emphasis for the medical profession, government, and society. A diverse set of techniques from epidemiology, statistics, and computer science are being proposed and studied for ADE discovery from observational health data (e.g., EHR and claims data), social network data (e.g., Google and Twitter posts), and other information sources. Methodologies are needed for evaluating, quantitatively measuring and comparing the ability of these various approaches to accurately discover ADEs. This work is motivated by the observation that text sources such as the Medline/Medinfo library provide a wealth of information on human health. Unfortunately, ADEs often result from unexpected interactions, and the connection between conditions and drugs is not explicit in these sources. Thus, in this work, we address the question of whether we can quantitatively estimate relationships between drugs and conditions from the medical literature. This paper proposes and studies a state-of-the-art NLP-based extraction of ADEs from text.",2017,Knowledge and Information Systems volume 51 issue 2 pp 435-457,statistical relational learning;data science;information retrieval;data mining;database;artificial intelligence;machine learning;statistics;computer science;
A survey of methods for time series change point detection,Samaneh Aminikhanghahi (Washington State University);Diane J. Cook (Washington State University);,"2227014466,2106796750","Change points are abrupt variations in time series data. Such abrupt changes may represent transitions that occur between states. Detection of change points is useful in modelling and prediction of time series and is found in application areas such as medical condition monitoring, climate change detection, speech and image analysis, and human activity analysis. This survey article enumerates, categorizes, and compares many of the methods that have been proposed to detect change points in time series. The methods examined include both supervised and unsupervised algorithms that have been introduced and evaluated. We introduce several criteria to compare the algorithms. Finally, we present some grand challenges for the community to consider.",2017,Knowledge and Information Systems volume 51 issue 2 pp 339-367,change detection;segmentation;time series;data mining;pattern recognition;machine learning;statistics;computer science;
A distributed approach for accelerating sparse matrix arithmetic operations for high-dimensional feature selection,Antonela Tommasel (National Scientific and Technical Research Council);Daniela Godoy (National Scientific and Technical Research Council);Alejandro Zunino (National Scientific and Technical Research Council);Cristian Mateos (National Scientific and Technical Research Council);,"48802306,2138287785,2155388350,2170358334","Matrix computations are both fundamental and ubiquitous in computational science, and as a result, they are frequently used in numerous disciplines of scientific computing and engineering. Due to the high computational complexity of matrix operations, which makes them critical to the performance of a large number of applications, their efficient execution in distributed environments becomes a crucial issue. This work proposes a novel approach for distributing sparse matrix arithmetic operations on computer clusters aiming at speeding-up the processing of high-dimensional matrices. The approach focuses on how to split such operations into independent parallel tasks by considering the intrinsic characteristics that distinguish each type of operation and the particular matrices involved. The approach was applied to the most commonly used arithmetic operations between matrices. The performance of the presented approach was evaluated considering a high-dimensional text feature selection approach and two real-world datasets. Experimental evaluation showed that the proposed approach helped to significantly reduce the computing times of big-scale matrix operations, when compared to serial and multi-thread implementations as well as several linear algebra software libraries.",2017,Knowledge and Information Systems volume 51 issue 2 pp 459-497,sparse matrix;feature selection;theoretical computer science;data mining;database;artificial intelligence;machine learning;algorithm;computer science;mathematics;
Constrained recommendations for query visualizations,Ibrahim A. Ibrahim (University of Queensland);Abdullah M. Albarrak (University of Queensland);Xue Li (University of Queensland);,"2564163972,1948003370,2239470812","The improvement of data storage and data acquisition techniques has led to huge accumulated data volumes in a variety of applications. International research enterprises such as the Human Genome and the Digital Sky Survey Projects are generating massive volumes of scientific data. A major challenge with these datasets is to glean insights from them to discover patterns or to originate relationships. The analysis of these massive, typically messy, and inconsistent volumes of data is indeed crucial and challenging in many application domains. Hence, the research community has introduced a number of visualizations tools to guide and help analysts in exploring the data space to extract potentially useful information. However, when working with high-dimensional datasets, identifying visualizations that show interesting variations and trends in data is not trivial: the analyst must manually specify a large number of visualizations, explore relationships among various attributes, and examine different subsets of data before discovering visualizations that are interesting or insightful. Though, exploring all possible visualizations involves complex challenges. It is a costly and time-consuming process especially when the dimensionality is high. Furthermore, the rapid growth of databases becomes multifaceted in their channels and dimensionality; thus, the transition from static analysis to real-time analytics represents a fundamental paradigm shift in the field of Big Data. Motivated by the above challenges, we propose an efficient framework called real-time scoring engine (RtSEngine) that assists analysts to limit the exploration of visualizations for a specified number of visualizations and/or certain execution time quote to recommend a set of visualizations that meet analysts’ budgets. To achieve that, RtSEngine incorporates our proposed approaches to prioritize and score attributes that form all possible visualizations in a dataset based on their statistical properties such as selectivity, data distribution, and number of distinct values. Then, RtSEngine recommends the visualizations created from the top-scored attributes. Moreover, we present visualizations cost-aware techniques that estimate the retrieval and computation costs of each visualization so that analysts may discard high-cost visualizations. We show and evaluate the effectiveness and efficiency of our proposed approaches, and asses the quality of visualizations and the overhead obtained by applying our techniques on both synthetic and real datasets.",2017,Knowledge and Information Systems volume 51 issue 2 pp 499-529,visual analytics;data science;world wide web;data mining;database;machine learning;computer science;
Decision-making and opinion formation in simple networks,Matan Leibovich (Open University);Inon Zuckerman (Ariel University);Avi Pfeffer (Charles River Laboratories);Ya’akov Gal (Ben-Gurion University of the Negev);,"2525756176,2206460754,2115076569,2136149760","In many networked decision-making settings, information about the world is distributed across multiple agents and agents’ success depends on their ability to aggregate and reason about their local information over time. This paper presents a computational model of information aggregation in such settings in which agents’ utilities depend on an unknown event. Agents initially receive a noisy signal about the event and take actions repeatedly while observing the actions of their neighbors in the network at each round. Such settings characterize many distributed systems such as sensor networks for intrusion detection and routing systems for Internet traffic. Using the model, we show that (1) agents converge in action and in knowledge for a general class of decision-making rules and for all network structures; (2) all networks converge to playing the same action regardless of the network structure; and (3) for particular network configurations, agents can converge to the correct action when using a well-defined class of myopic decision rules. These theoretical results are also supported by a new simulation-based open-source empirical test-bed for facilitating the study of information aggregation in general networks.",2017,Knowledge and Information Systems volume 51 issue 2 pp 691-718,multi agent system;data mining;artificial intelligence;machine learning;computer science;
Exceptionally monotone models—the rank correlation model class for Exceptional Model Mining,Lennart Downar;Wouter Duivesteijn (Ghent University);,"2242499878,134040164","Exceptional Model Mining strives to find coherent subgroups of the dataset where multiple target attributes interact in an unusual way. One instance of such an investigated form of interaction is Pearson’s correlation coefficient between two targets. EMM then finds subgroups with an exceptionally linear relation between the targets. In this paper, we enrich the EMM toolbox by developing the more general rank correlation model class. We find subgroups with an exceptionally monotone relation between the targets. Apart from catering for this richer set of relations, the rank correlation model class does not necessarily require the assumption of target normality, which is implicitly invoked in the Pearson’s correlation model class. Furthermore, it is less sensitive to outliers. We provide pseudocode for the employed algorithm and analyze its computational complexity, and experimentally illustrate what the rank correlation model class for EMM can find for you on six datasets from an eclectic variety of domains.",2017,Knowledge and Information Systems volume 51 issue 2 pp 369-394,econometrics;machine learning;statistics;mathematics;
Weighted-object ensemble clustering: methods and analysis,Yazhou Ren (University of Electronic Science and Technology of China);Carlotta Domeniconi (George Mason University);Guoji Zhang (South China University of Technology);Guoxian Yu (Southwest University);,"2234065118,45678088,2144418109,2126069887","Ensemble clustering has attracted increasing attention in recent years. Its goal is to combine multiple base clusterings into a single consensus clustering of increased quality. Most of the existing ensemble clustering methods treat each base clustering and each object as equally important, while some approaches make use of weights associated with clusters, or to clusterings, when assembling the different base clusterings. Boosting algorithms developed for classification have led to the idea of considering weighted objects during the clustering process. However, not much effort has been put toward incorporating weighted objects into the consensus process. To fill this gap, in this paper, we propose a framework called Weighted-Object Ensemble Clustering (WOEC). We first estimate how difficult it is to cluster an object by constructing the co-association matrix that summarizes the base clustering results, and we then embed the corresponding information as weights associated with objects. We propose three different consensus techniques to leverage the weighted objects. All three reduce the ensemble clustering problem to a graph partitioning one. We experimentally demonstrate the gain in performance that our WOEC methodology achieves with respect to state-of-the-art ensemble clustering methods, as well as its stability and robustness.",2017,Knowledge and Information Systems volume 51 issue 2 pp 661-689,k medians clustering;flame clustering;brown clustering;canopy clustering algorithm;dbscan;correlation clustering;constrained clustering;data stream clustering;cure data clustering algorithm;single linkage clustering;fuzzy clustering;graph partition;clustering high dimensional data;cluster analysis;consensus clustering;biclustering;conceptual clustering;data mining;pattern recognition;machine learning;statistics;computer science;mathematics;
Adaptive query relaxation and top-k result ranking over autonomous web databases,Xiangfu Meng (Liaoning Technical University);Xiaoyan Zhang (Liaoning Technical University);Yanhuan Tang (Liaoning Technical University);Chongchun Bi (Liaoning Technical University);,"2682580413,2606854178,2515856125,2489077250","Internet users may suffer the empty or too little answer problem when they post a strict query to the Web database. To address this problem, we develop a general framework to enable automatically query relaxation and top-k result ranking. Our framework consists of two processing steps. The first step is query relaxation. Based on the user original query, we speculate how much the user cares about each specified attribute by measuring its specified value distribution in the database. The rare distribution of the specified value of the attribute indicates the attribute may important for the user. According to the attribute importance, the original query is then rewritten as a relaxed query by expanding each query criterion range. The relaxed degree on each specified attribute is varied with the attribute weight adaptively. The most important attribute is relaxed with the minimum degree so that the answer returned by the relaxed query can be most relevant to the user original intention. The second step is top-k result ranking. In this step, we first generate user contextual preferences from query history and then use them to create a priori orders of tuples during the off-line pre-processing. Only a few representative orders are saved, each corresponding to a set of contexts. Then, these orders and associated contexts are used at querying time to expeditiously provide top-k relevant answers by using the top-k evaluation algorithm. Results of a preliminary user study demonstrate our query relaxation, and top-k result ranking methods can capture the users preferences effectively. The efficiency and effectiveness of our approach is also demonstrated.",2017,Knowledge and Information Systems volume 51 issue 2 pp 395-433,ranking;sargable;range query;boolean conjunctive query;online aggregation;web search query;web query classification;view;query by example;query expansion;query optimization;cluster analysis;information retrieval;data mining;database;machine learning;computer science;mathematics;
Measuring similarity of users with qualitative preferences for service selection,Hongbing Wang (Southeast University);Hualan Wang (Southeast University);Guibing Guo (Nanyang Technological University);Yangyu Tang (Southeast University);Jie Zhang (Nanyang Technological University);,"2101019922,2575974669,2132876648,2223837880,2422123818","Similarity measures are essential in many preference-based personalized applications such as collaborative recommendation and web service selection. However, previous studies have been mainly focused on the similarity measures for quantitative preference rather than those for qualitative preference, though the latter has attracted much attention recently. This paper aims to fill in this gap by proposing an intuitive similarity measure for conditional qualitative preference which is represented by CP-nets. In particular, we introduce two methods, a basic and a general similarity measures, corresponding to whether two CP-nets share similar structures and contents or not. Experimental results on two real-world data sets demonstrate that our similarity measure can not only correctly reflect the changes of users’ preferences, but also be effective in identifying similar users. In addition, only by adopting the K most important attributes, the computational cost can be greatly reduced while sufficiently high accuracy is preserved. Furthermore, we demonstrate the effectiveness of our method in complementing users’ preferences by aggregating those of similar users in a scenario where users’ preferences are incomplete.",2017,Knowledge and Information Systems volume 51 issue 2 pp 561-594,preference learning;knowledge management;world wide web;data mining;mathematics;
Indexed list-based high utility pattern mining with utility upper-bound reduction and pattern combination techniques,Heungmo Ryang (Sejong University);Unil Yun (Sejong University);,"1986327082,2025687059","High utility pattern mining has been studied as an essential topic in the field of pattern mining in order to satisfy requirements of many real-world applications that need to process non-binary databases including item importance such as market analysis. In this paper, we propose an efficient algorithm with a novel indexed list-based data structure for mining high utility patterns. Previous approaches first generate an enormous number of candidate patterns on the basis of overestimation methods in their mining processes and then identify actual high utility patterns from the candidates through an additional database scan, which leads to high computational overheads. Although several list-based algorithms to discover high utility patterns without candidate generation have been suggested in recent years, they require a large number of comparison operations. Our method facilitates efficient mining of high utility patterns with the proposed indexed list by effectively reducing the total number of such operations. Moreover, we develop two techniques based on this novel data structure to more enhance mining performance of the proposed method. Experimental results on real and synthetic datasets show that the proposed algorithm mines high utility patterns more efficiently than the state-of-the-art algorithms.",2017,Knowledge and Information Systems volume 51 issue 2 pp 627-659,data science;data mining;database;computer science;
Domain-agnostic discovery of similarities and concepts at scale,Olof Görnerup (Swedish Institute of Computer Science);Daniel Gillblad (Swedish Institute of Computer Science);Theodore Vasiloudis (Swedish Institute of Computer Science);,"218497630,313216600,2243462069","Appropriately defining and efficiently calculating similarities from large data sets are often essential in data mining, both for gaining understanding of data and generating processes and for building tractable representations. Given a set of objects and their correlations, we here rely on the premise that each object is characterized by its context, i.e., its correlations to the other objects. The similarity between two objects can then be expressed in terms of the similarity between their contexts. In this way, similarity pertains to the general notion that objects are similar if they are exchangeable in the data. We propose a scalable approach for calculating all relevant similarities among objects by relating them in a correlation graph that is transformed to a similarity graph. These graphs can express rich structural properties among objects. Specifically, we show that concepts—abstractions of objects—are constituted by groups of similar objects that can be discovered by clustering the objects in the similarity graph. These principles and methods are applicable in a wide range of fields and will be demonstrated here in three domains: computational linguistics, music, and molecular biology, where the numbers of objects and correlations range from small to very large.",2017,Knowledge and Information Systems volume 51 issue 2 pp 531-560,concept mining;discrete mathematics;data mining;machine learning;computer science;mathematics;
A valences-totaling model for English sentiment classification,"Vo Ngoc Phu (Duy Tan University);Vo Thi Ngoc Chau (Ho Chi Minh City University of Technology);Nguyen Duy Dat (Faculty of Information Technology, University Džemal Bijedić of Mostar);Vo Thi Ngoc Tran (Ho Chi Minh City University of Technology);Tuan A. Nguyen (Usman Institute of Technology);","2580325454,1211284406,2326257456,2231739181,2610822980","Sentiment classification plays an important role in everyday life, in political activities, activities of commodity production and commercial activities. Finding a time-effective and highly accurate solution to the classification of emotions is challenging. Today, there are many models (or methods) to classify the sentiment of documents. Sentiment classification has been studied for many years and is used widely in many different fields. We propose a new model, which is called the valences-totaling model (VTM), by using cosine measure (CM) to classify the sentiment of English documents. VTM is a new model for English sentiment classification. In this study, CM is a measure of similarity between two words and is used to calculate the valence (and polarity) of English semantic lexicons. We prove that CM is able to identify the sentiment valence and the sentiment polarity of the English sentiment lexicons online in combination with the Google search engine with AND operator and OR operator. VTM uses many English semantic lexicons. These English sentiment lexicons are calculated online and are based on the Internet. We present a full range of English sentences; thus, the emotion expressed in the English text is classified with more precision. Our new model is not dependent on a special domain and training data set—it is a domain-independent classifier. We test our new model on the Internet data in English. The calculated valence (and polarity) of English semantic words in this model is based on many documents on millions of English Web sites and English social networks.",2017,Knowledge and Information Systems pp 1-58,sentiment analysis;natural language processing;speech recognition;pattern recognition;computer science;
An algorithmic framework for frequent intraday pattern recognition and exploitation in forex market,Nikitas Goumatianos (Aalborg University);Ioannis T. Christou (University of Wisconsin-Madison);Peter Lindgren (Aarhus University);Ramjee Prasad (Aalborg University);,"71519574,660025571,2144832106,2220394469","We present a knowledge discovery-based framework that is capable of discovering, analyzing and exploiting new intraday price patterns in forex markets, beyond the well-known chart formations of technical analysis. We present a novel pattern recognition algorithm for Pattern Matching, that we successfully used to construct more than 16,000 new intraday price patterns. After processing and analysis, we extracted 3518 chart formations that are capable of predicting the short-term direction of prices. In our experiments, we used forex time series from 8 paired-currencies in various time frames. The system computes the probabilities of events such as “within next 5 periods, price will increase more than 20 pips”. Results show that the system is capable of finding patterns whose output signals (tested on unseen data) have predictive accuracy which varies between 60 and 85% depending on the type of pattern. We test the usefulness of the discovered patterns, via implementation of an expert system using a straightforward strategy based on the direction and the accuracy of the pattern predictions. We compare our method against three standard trading techniques plus a “random trader,” and we also test against the results presented in two recently published studies. Our framework performs very well against all systems we directly compare , and also, against all other published results.",2017,Knowledge and Information Systems pp 1-38,foreign exchange market;data mining;artificial intelligence;machine learning;computer science;
Crowd labeling latent Dirichlet allocation,"Luca Pion-Tonachini (University of California, San Diego);Scott Makeig (University of California, San Diego);Ken Kreutz-Delgado (University of California, San Diego);","249769124,206477137,350196202","Large, unlabeled datasets are abundant nowadays, but getting labels for those datasets can be expensive and time-consuming. Crowd labeling is a crowdsourcing approach for gathering such labels from workers whose suggestions are not always accurate. While a variety of algorithms exist for this purpose, we present crowd labeling latent Dirichlet allocation (CL-LDA), a generalization of latent Dirichlet allocation that can solve a more general set of crowd labeling problems. We show that it performs as well as other methods and at times better on a variety of simulated and actual datasets while treating each label as compositional rather than indicating a discrete class. In addition, prior knowledge of workers’ abilities can be incorporated into the model through a structured Bayesian framework. We then apply CL-LDA to the EEG independent component labeling dataset, using its generalizations to further explore the utility of the algorithm. We discuss prospects for creating classifiers from the generated labels.",2017,Knowledge and Information Systems pp 1-17,generative model;latent dirichlet allocation;bayesian probability;data mining;pattern recognition;machine learning;computer science;mathematics;
Mining collective knowledge: inferring functional labels from online review for business,Feifan Fan (Peking University);Wayne Xin Zhao (Renmin University of China);Ji-Rong Wen (Renmin University of China);Ge Xu (Minjiang University);Edward Y. Chang (HTC);,"2664866877,2307999729,2593770520,2655491663,2108840372","With the increasing popularity of online e-commerce services, a large volume of online reviews have been constantly generated by users. In this paper, we propose to study the problem of inferring functional labels using online review text. Functional labels summarize and highlight the main characteristics of a business, which can serve as bridges between the consumption needs and the service functions. We consider two kinds of semantic similarities: lexical similarity and embedding similarity, which characterize the relatedness in two different perspectives. To measure the lexical similarity, we use the classic probabilistic ranking formula, i.e., BM25; to measure the embedding similarity, we propose an extended embedding model which can incorporate weak supervised information derived from review text. These two kinds of similarities compensate each other and capture the semantic relatedness in a more comprehensive way. We construct a test collection consisting of four different domains based on a Yelp dataset and consider multiple baseline methods for comparison. Extensive experiments have shown that the proposed methods are very effective.",2017,Knowledge and Information Systems pp 1-25,data science;world wide web;data mining;database;machine learning;computer science;
CITIESData: a smart city data management framework,Xiufeng Liu (Delhi Technological University);Alfred Heller (Technical University of Denmark);Per Sieverts Nielsen (Delhi Technological University);,"2469693970,2137892735,2487742074","Smart city data come from heterogeneous sources including various types of the Internet of Things such as traffic, weather, pollution, noise, and portable devices. They are characterized with diverse quality issues and with different types of sensitive information. This makes data processing and publishing challenging. In this paper, we propose a framework to streamline smart city data management, including data collection, cleansing, anonymization, and publishing. The paper classifies smart city data in sensitive, quasi-sensitive, and open/public levels and then suggests different strategies to process and publish the data within these categories. The paper evaluates the framework using a real-world smart city data set, and the results verify its effectiveness and efficiency. The framework can be a generic solution to manage smart city data.",2017,Knowledge and Information Systems pp 1-24,internet of things;data quality;information privacy;internet privacy;computer security;data mining;database;computer science;
Effective sparsity control in deep belief networks using normal regularization term,Mohammad Ali Keyvanrad (Amirkabir University of Technology);Mohammad Mehdi Homayounpour (Amirkabir University of Technology);,"1349003931,2245826317","Nowadays the use of deep network architectures has become widespread in machine learning. Deep belief networks (DBNs) have deep network architectures to create a powerful generative model using training data. Deep belief networks can be used in classification and feature learning. A DBN can be learned unsupervised, and then the learned features are suitable for a simple classifier (like a linear classifier) with a few labeled data. In addition, according to researches, by using sparsity in DBNs we can learn useful low-level feature representations for unlabeled data. In sparse representation, we have the property that learned features can be interpreted, i.e., correspond to meaningful aspects of the input, and capture factors of variation in the data. Different methods are proposed to build sparse DBNs. In this paper, we proposed a new method that has different behavior according to deviation of the activation of the hidden units from a (low) fixed value. In addition, our proposed regularization term has a variance parameter that can control the force degree of sparseness. According to the results, our new method achieves the best recognition accuracy on the test sets in different datasets with different applications (image, speech and text) and we can achieve incredible results when using a different number of training samples, especially when we have a few samples for training.",2017,Knowledge and Information Systems volume 27 pp 1-18,restricted boltzmann machine;deep belief network;deep learning;data mining;pattern recognition;artificial intelligence;machine learning;statistics;computer science;
Data-dependent dissimilarity measure: an effective alternative to geometric distance measures,"Sunil Aryal (Federation University Australia);Kai Ming Ting (Federation University Australia);Takashi Washio (Osaka University);Gholamreza Haffari (Monash University, Clayton campus);","2172477944,2693809322,1794038515,1432492132","Nearest neighbor search is a core process in many data mining algorithms. Finding reliable closest matches of a test instance is still a challenging task as the effectiveness of many general-purpose distance measures such as \(\ell _p\)-norm decreases as the number of dimensions increases. Their performances vary significantly in different data distributions. This is mainly because they compute the distance between two instances solely based on their geometric positions in the feature space, and data distribution has no influence on the distance measure. This paper presents a simple data-dependent general-purpose dissimilarity measure called ‘\(m_p\)-dissimilarity’. Rather than relying on geometric distance, it measures the dissimilarity between two instances as a probability mass in a region that encloses the two instances in every dimension. It deems two instances in a sparse region to be more similar than two instances of equal inter-point geometric distance in a dense region. Our empirical results in k-NN classification and content-based multimedia information retrieval tasks show that the proposed \(m_p\)-dissimilarity measure produces better task-specific performance than existing widely used general-purpose distance measures such as \(\ell _p\)-norm and cosine distance across a wide range of moderate- to high-dimensional data sets with continuous only, discrete only, and mixed attributes.",2017,Knowledge and Information Systems pp 1-28,total variation distance of probability measures;combinatorics;pattern recognition;machine learning;mathematics;
A formal series-based unification of the frequent itemset mining approaches,Slimane Oulad-Naoui (University of Laghouat);Hadda Cherroun (University of Laghouat);Djelloul Ziadi (University of Rouen);,"64926029,2574646238,34571083","Over the last two decades, a great deal of work has been devoted to the algorithmic aspects of the frequent itemset (FI) mining problem, leading to a phenomenal number of algorithms and associated implementations, each of which claims supremacy. Meanwhile, it is generally well agreed that developing a unifying theory is one of the most important issues in data mining research. Hence, our primary motivation for this work is to introduce a high-level formalism for this basic problem, which induces a unified vision of the algorithmic approaches presented so far. The key distinctive feature of the introduced model is that it combines, in one fashion, both the qualitative and the quantitative aspects of this basic problem. In this paper, we propose a new model for the FI-mining task based on formal series. In fact, we encode the itemsets as words over a sorted alphabet and express this problem by a formal series over the counting semiring \((\mathbb N,+,\times ,0,1)\), whose range represents the itemsets, and the coefficients are their supports. The aim is threefold: First, to define a clear, unified and extensible theoretical framework through which we can state the main FI-approaches. Second, to prove a convenient connection between the determinization of the acyclic weighted automaton that represents a transaction dataset and the computation of the associated collection of FI. Finally, to devise a first algorithmic transcription, baptized Wafi, of our model by means of weighted automata, which we evaluate against representative leading algorithms. The obtained results show the suitability of our formalism.",2017,Knowledge and Information Systems pp 1-39,unification;theoretical computer science;data mining;database;artificial intelligence;machine learning;programming language;algorithm;computer science;mathematics;
The MASSIF platform: a modular and semantic platform for the development of flexible IoT services,Pieter Bonte (Ghent University);Femke Ongenae (Ghent University);Femke De Backere (Ghent University);Jeroen Schaballie (Ghent University);Dörthe Arndt (Ghent University);Stijn Verstichel (Ghent University);Erik Mannens (Ghent University);Rik Van de Walle (Ghent University);Filip De Turck (Ghent University);,"2148532501,21094659,2505001338,2247215144,2097381408,2599971223,1978788732,2004763151,264213217","In the Internet of Things (IoT), data-producing entities sense their environment and transmit these observations to a data processing platform for further analysis. Applications can have a notion of context awareness by combining this sensed data, or by processing the combined data. The processes of combining data can consist both of merging the dynamic sensed data, as well as fusing the sensed data with background and historical data. Semantics can aid in this task, as they have proven their use in data integration, knowledge exchange and reasoning. Semantic services performing reasoning on the integrated sensed data, combined with background knowledge, such as profile data, allow extracting useful information and support intelligent decision making. However, advanced reasoning on the combination of this sensed data and background knowledge is still hard to achieve. Furthermore, the collaboration between semantic services allows to reach complex decisions. The dynamic composition of such collaborative workflows that can adapt to the current context, has not received much attention yet. In this paper, we present MASSIF, a data-driven platform for the semantic annotation of and reasoning on IoT data. It allows the integration of multiple modular reasoning services that can collaborate in a flexible manner to facilitate complex decision-making processes. Data-driven workflows are enabled by letting services specify the data they would like to consume. After thorough processing, these services can decide to share their decisions with other consumers. By defining the data these services would like to consume, they can operate on a subset of data, improving reasoning efficiency. Furthermore, each of these services can integrate the consumed data with background knowledge in its own context model, for rapid intelligent decision making. To show the strengths of the platform, two use cases are detailed and thoroughly evaluated.",2017,Knowledge and Information Systems volume 51 issue 1 pp 89-126,internet of things;ontology;semantic web;service oriented architecture;reason;world wide web;data mining;database;artificial intelligence;computer science;
Supervised item response models for informative prediction,Tsuyoshi Idé (IBM);Amit Dhurandhar (IBM);,"2107715069,2061123877","Supporting human decision-making is a major goal of data mining. The more decision-making is critical, the more interpretability is required in the predictive model. This paper proposes a new framework to build a fully interpretable predictive model for questionnaire data, while maintaining a reasonable prediction accuracy with regard to the final outcome. Such a model has applications in project risk assessment, in healthcare, in social studies, and, presumably, in any real-world application that relies on questionnaire data for informative and accurate prediction. Our framework is inspired by models in item response theory (IRT), which were originally developed in psychometrics with applications to standardized academic tests. We extend these models, which are essentially unsupervised, to the supervised setting. For model estimation, we introduce a new iterative algorithm by combining Gauss–Hermite quadrature with an expectation–maximization algorithm. The learned probabilistic model is linked to the metric learning framework for informative and accurate prediction. The model is validated by three real-world data sets: Two are from information technology project failure prediction and the other is an international social survey about people’s happiness. To the best of our knowledge, this is the first work that leverages the IRT framework to provide informative and accurate prediction on ordinal questionnaire data.",2017,Knowledge and Information Systems volume 51 issue 1 pp 235-257,item response theory;data science;data mining;machine learning;statistics;computer science;
Online evaluation of bid prediction models in a large-scale computational advertising platform: decision making and insights,Shahriar Shariat (Rutgers University);Burkay Orten;Ali Dasdan (Yahoo!);,"2004086288,2634182670,4321789","Online media provides opportunities for marketers through which they can deliver effective brand messages to a wide range of audiences at scale. Advertising technology platforms enable advertisers to reach their target audience by delivering ad impressions to online users in real time. In order to identify the best marketing message for a user and to purchase impressions at the right price, we rely heavily on bid prediction and optimization models. Even though the bid prediction models are well studied in the literature, the equally important subject of model evaluation is usually overlooked or not discussed in detail. Effective and reliable evaluation of an online bidding model is crucial for making faster model improvements as well as for utilizing the marketing budgets more efficiently. In this paper, we present an experimentation framework for bid prediction models where our focus is on the practical aspects of model evaluation. Specifically, we outline the unique challenges we encounter in our platform due to a variety of factors such as heterogeneous goal definitions, varying budget requirements across different campaigns, high seasonality and the auction-based environment for inventory purchasing. Then, we introduce return on investment as a unified model performance (i.e., success) metric and explain its merits over more traditional metrics such as click-through rate or conversion rate. Most importantly, we discuss commonly used evaluation and metric summarization approaches in detail and propose a more accurate method for online evaluation of new experimental models against the baseline. Our meta-analysis-based approach addresses various shortcomings of other methods and yields statistically robust conclusions that allow us to conclude experiments more quickly in a reliable manner. We demonstrate the effectiveness of our evaluation strategy on real campaign data through some experiments.",2017,Knowledge and Information Systems volume 51 issue 1 pp 37-60,real time bidding;meta analysis;data mining;database;artificial intelligence;machine learning;simulation;computer science;
Role updating in information systems using model checking,Jinwei Hu (Qatar University);Khaled M. Khan (Qatar University);Yan Zhang (University of Sydney);Yun Bai (University of Sydney);Ruixuan Li (Huazhong University of Science and Technology);,"2704154559,2115511359,2105096905,2100952137,2665686436","The role-based access control (RBAC) has significantly simplified the management of users and permissions in information systems. In dynamic environments, systems are constantly undergoing changes, and accordingly, the associated configurations need to be updated in order to reflect the systems’ security evolutions. However, such updating process is generally complicated as the resulting system state is expected to meet necessary constraints. This paper presents an approach for assisting administrators to make a desirable update, in light of changes in RBAC systems. We propose a formalization of the update approach, investigate its properties, and develop an updating algorithm based on model checking techniques. Our experimental results demonstrate the effectiveness of the proposed approach.",2017,Knowledge and Information Systems volume 51 issue 1 pp 187-234,model checking;computational complexity theory;knowledge management;data mining;database;computer science;
Context-dependent combination of sensor information in Dempster–Shafer theory for BDI,Sarah Calderwood (Queen's University Belfast);Kevin McAreavey (Queen's University Belfast);Weiru Liu (Queen's University Belfast);Jun Hong (Queen's University Belfast);,"2292647752,12292621,2148409606,2125000923","There has been much interest in the belief–desire–intention (BDI) agent-based model for developing scalable intelligent systems, e.g. using the AgentSpeak framework. However, reasoning from sensor information in these large-scale systems remains a significant challenge. For example, agents may be faced with information from heterogeneous sources which is uncertain and incomplete, while the sources themselves may be unreliable or conflicting. In order to derive meaningful conclusions, it is important that such information be correctly modelled and combined. In this paper, we choose to model uncertain sensor information in Dempster–Shafer (DS) theory. Unfortunately, as in other uncertainty theories, simple combination strategies in DS theory are often too restrictive (losing valuable information) or too permissive (resulting in ignorance). For this reason, we investigate how a context-dependent strategy originally defined for possibility theory can be adapted to DS theory. In particular, we use the notion of largely partially maximal consistent subsets (LPMCSes) to characterise the context for when to use Dempster’s original rule of combination and for when to resort to an alternative. To guide this process, we identify existing measures of similarity and conflict for finding LPMCSes along with quality of information heuristics to ensure that LPMCSes are formed around high-quality information. We then propose an intelligent sensor model for integrating this information into the AgentSpeak framework which is responsible for applying evidence propagation to construct compatible information, for performing context-dependent combination and for deriving beliefs for revising an agent’s belief base. Finally, we present a power grid scenario inspired by a real-world case study to demonstrate our work.",2017,Knowledge and Information Systems volume 51 issue 1 pp 259-285,dempster shafer theory;data mining;artificial intelligence;machine learning;statistics;computer science;mathematics;
Efficient and scalable labeled subgraph matching using SGMatch,Carlos R. Rivero (Rochester Institute of Technology);Hasan M. Jamil (University of Idaho);,"2053568818,2299872231","Graphs are natural candidates for modeling application domains, such as social networks, pattern recognition, citation networks, or protein–protein interactions. One of the most challenging tasks in managing graphs is subgraph matching over data graphs, which attempts to find one-to-one correspondences, called solutions, among the query and data nodes. To compute solutions, most contemporary techniques use backtracking and recursion. An open research question is whether graphs can be matched based on parts and local solutions can be combined to reach a global matching. In this paper, we present an approach based on graph decomposition called SGMatch to match graphs. We represent graphs in smaller units called graphlets and develop a matching technique to leverage this representation. Pruning strategies use a new notion of edge covering called minimum hub cover and metadata, such as statistics and inverted indices, to reduce the number of matching candidates. Our evaluation of SGMatch versus contemporary algorithms, i.e., VF2, GraphQL, QuickSI, GADDI, or SPath, shows that SGMatch substantially improves the performance of current state-of-the-art techniques for larger query graphs with different structures, i.e., cliques, paths or subgraphs.",2017,Knowledge and Information Systems volume 51 issue 1 pp 61-87,partial k tree;factor critical graph;induced subgraph isomorphism problem;indifference graph;forbidden graph characterization;1 planar graph;universal graph;cograph;subgraph isomorphism problem;modular decomposition;maximal independent set;pathwidth;chordal graph;bipartite graph;theoretical computer science;combinatorics;machine learning;mathematics;
Nonparametric discovery and analysis of learning patterns and autism subgroups from therapeutic data,Pratibha Vellanki (Deakin University);Thi V. Duong (Deakin University);Sunil Kumar Gupta 0001 (Deakin University);Svetha Venkatesh (Deakin University);Dinh Q. Phung (Deakin University);,"2278944905,2159716571,2119406083,2146461601,2314522249","The spectrum nature and heterogeneity within autism spectrum disorders (ASD) pose as a challenge for treatment. Personalisation of syllabus for children with ASD can improve the efficacy of learning by adjusting the number of opportunities and deciding the course of syllabus. We research the data-motivated approach in an attempt to disentangle this heterogeneity for personalisation of syllabus. With the help of technology and a structured syllabus, collecting data while a child with ASD masters the skills is made possible. The performance data collected are, however, growing and contain missing elements based on the pace and the course each child takes while navigating through the syllabus. Bayesian nonparametric methods are known for automatically discovering the number of latent components and their parameters when the model involves higher complexity. We propose a nonparametric Bayesian matrix factorisation model that discovers learning patterns and the way participants associate with them. Our model is built upon the linear Poisson gamma model (LPGM) with an Indian buffet process prior and extended to incorporate data with missing elements. In this paper, for the first time we have presented learning patterns deduced automatically from data mining and machine learning methods using intervention data recorded for over 500 children with ASD. We compare the results with non-negative matrix factorisation and K-means, which being parametric, not only require us to specify the number of learning patterns in advance, but also do not have a principle approach to deal with missing data. The F1 score observed over varying degree of similarity measure (Jaccard Index) suggests that LPGM yields the best outcome. By observing these patterns with additional knowledge regarding the syllabus it may be possible to observe the progress and dynamically modify the syllabus for improved learning.",2017,Knowledge and Information Systems volume 51 issue 1 pp 127-157,data mining;artificial intelligence;machine learning;statistics;computer science;mathematics;
A formal framework to represent spatial knowledge,Giuseppe Della Penna (University of L'Aquila);Daniele Magazzeni (King's College London);Sergio Orefice (University of L'Aquila);,"2138724275,145207510,1989025370","Visual representations are an essential element in human–computer interaction and can be conceived as a collection of graphical objects arranged in a two-dimensional space. It is quite natural to model visual representations through the qualitative relationships holding between their objects, and therefore, qualitative spatial relations are a fundamental way of representing spatial knowledge. To this aim, in this paper we present a framework of qualitative spatial relations providing a general, domain-independent approach to specify visual representations.",2017,Knowledge and Information Systems volume 51 issue 1 pp 311-338,spatial relation;information extraction;knowledge management;computer vision;computer science;
Community detection in social networks using user frequent pattern mining,Seyed Ahmad Moosavi (Islamic Azad University);Mehrdad Jalali (Islamic Azad University);Negin Misaghian (Islamic Azad University);Shahaboddin Shamshirband (Information Technology University);Mohammad Hossein Anisi (Information Technology University);,"2430400710,2101573788,2222611237,1635769602,2014060858","Recently, social networking sites are offering a rich resource of heterogeneous data. The analysis of such data can lead to the discovery of unknown information and relations in these networks. The detection of communities including ‘similar’ nodes is a challenging topic in the analysis of social network data, and it has been widely studied in the social networking community in the context of underlying graph structure. Online social networks, in addition to having graph structures, include effective user information within networks. Using this information leads to enhance quality of community discovery. In this study, a method of community discovery is provided. Besides communication among nodes to improve the quality of the discovered communities, content information is used as well. This is a new approach based on frequent patterns and the actions of users on networks, particularly social networking sites where users carry out their preferred activities. The main contributions of proposed method are twofold: First, based on the interests and activities of users on networks, some small communities of similar users are discovered, and then by using social relations, the discovered communities are extended. The F-measure is used to evaluate the results of two real-world datasets (Blogcatalog and Flickr), demonstrating that the proposed method principals to improve the community detection quality.",2017,Knowledge and Information Systems volume 51 issue 1 pp 159-186,social network;big data;data science;world wide web;data mining;computer science;
A survey of query result diversification,Kaiping Zheng (Harbin Institute of Technology);Hongzhi Wang (Harbin Institute of Technology);Zhixin Qi (Harbin Institute of Technology);Jianzhong Li (Harbin Institute of Technology);Hong Gao (Harbin Institute of Technology);,"2560403789,2589657406,2559913172,2130201582,2607637236","Nowadays, in information systems such as web search engines and databases, diversity is becoming increasingly essential and getting more and more attention for improving users’ satisfaction. In this sense, query result diversification is of vital importance and well worth researching. Some issues such as the definition of diversification and efficient diverse query processing are more challenging to handle in information systems. Many researchers have focused on various dimensions of diversify problem. In this survey, we aim to provide a thorough review of a wide range of result diversification techniques including various definitions of diversifications, corresponding algorithms, diversification technique specified for some applications including database, search engines, recommendation systems, graphs, time series and data streams as well as result diversification systems. We also propose some open research directions, which are challenging and have not been explored up till now, to improve the quality of query results.",2017,Knowledge and Information Systems volume 51 issue 1 pp 1-36,web query classification;query expansion;world wide web;information retrieval;data mining;database;computer science;
Building semantic kernels for cross-document knowledge discovery using Wikipedia,Peng Yan (North Dakota State University);Wei Jin (North Dakota State University);,"2699720994,2683675561","Research into text mining has progressed over the past decade. One of the main challenges now is gauging the difficulty of taking advantage of outside knowledge in the discovery process. In this work, to address the limitations of the traditional bag-of- words model and expand the search scope beyond the document collections at hand, we present a new text mining approach incorporating Wikipedia as the background knowledge. Various semantic kernels are built out of the extensive knowledge derived from Wikipedia and applied to the search scenario of detecting potential semantic relationships between topics. We demonstrate the effectiveness of our approach through comparing with competitive baselines, as well as alternative solutions where only part of Wikipedia resources (e.g., the Wiki-article contents or the associated Wiki-categories) is considered.",2017,Knowledge and Information Systems volume 51 issue 1 pp 287-310,explicit semantic analysis;semantic similarity;data science;information retrieval;data mining;database;computer science;
A new accelerated proximal technique for regression with high-dimensional datasets,Mridula Verma (Indian Institute of Technology (BHU) Varanasi);K. K. Shukla (Indian Institute of Technology (BHU) Varanasi);,"2344840930,2027407570","We consider the problem of minimization of the sum of two convex functions, one of which is a smooth function, while another one may be a nonsmooth function. Many high-dimensional learning problems (classification/regression) can be designed using such frameworks, which can be efficiently solved with the help of first-order proximal-based methods. Due to slow convergence of traditional proximal methods, a recent trend is to introduce acceleration to such methods, which increases the speed of convergence. Such proximal gradient methods belong to a wider class of the forward–backward algorithms, which mathematically can be interpreted as fixed-point iterative schemes. In this paper, we design few new proximal gradient methods corresponding to few state-of-the-art fixed-point iterative schemes and compare their performances on the regression problem. In addition, we propose a new accelerated proximal gradient algorithm, which outperforms earlier traditional methods in terms of convergence speed and regression error. To demonstrate the applicability of our method, we conducted experiments for the problem of regression with several publicly available high-dimensional real datasets taken from different application domains. Empirical results exhibit that the proposed method outperforms the previous methods in terms of convergence, accuracy, and objective function values.",2017,Knowledge and Information Systems pp 1-16,proximal gradient methods for learning;proximal gradient methods;regression;econometrics;machine learning;mathematical optimization;statistics;mathematics;
Recent advances in document summarization,Jin-ge Yao (Peking University);Xiaojun Wan (Peking University);Jianguo Xiao (Peking University);,"2251430847,2146508076,2131977055","The task of automatic document summarization aims at generating short summaries for originally long documents. A good summary should cover the most important information of the original document or a cluster of documents, while being coherent, non-redundant and grammatically readable. Numerous approaches for automatic summarization have been developed to date. In this paper we give a self-contained, broad overview of recent progress made for document summarization within the last 5 years. Specifically, we emphasize on significant contributions made in recent years that represent the state-of-the-art of document summarization, including progress on modern sentence extraction approaches that improve concept coverage, information diversity and content coherence, as well as attempts from summarization frameworks that integrate sentence compression, and more abstractive systems that are able to produce completely new sentences. In addition, we review progress made for document summarization in domains, genres and applications that are different from traditional settings. We also point out some of the latest trends and highlight a few possible future directions.",2017,Knowledge and Information Systems pp 1-40,multi document summarization;automatic summarization;text mining;natural language processing;information retrieval;data mining;computer science;
Collegial Activity Learning Between Heterogeneous Sensors,Kyle D. Feuz (Weber State University);Diane J. Cook (Washington State University);,"1982570015,2106796750","Activity recognition algorithms have matured and become more ubiquitous in recent years. However, these algorithms are typically customized for a particular sensor platform. In this paper, we introduce PECO, a Personalized activity ECOsystem, that transfers learned activity information seamlessly between sensor platforms in real time so that any available sensor can continue to track activities without requiring its own extensive labeled training data. We introduce a multi-view transfer learning algorithm that facilitates this information handoff between sensor platforms and provide theoretical performance bounds for the algorithm. In addition, we empirically evaluate PECO using datasets that utilize heterogeneous sensor platforms to perform activity recognition. These results indicate that not only can activity recognition algorithms transfer important information to new sensor platforms, but any number of platforms can work together as colleagues to boost performance.",2017,Knowledge and Information Systems pp 1-28,transfer of learning;activity recognition;data mining;real time computing;artificial intelligence;machine learning;simulation;computer science;
Dynamic sampling of text streams and its application in text analysis,"Gang Tian (Wuhan University);Jiajia Huang (Wuhan University);Min Peng (Wuhan University);Jiahui Zhu (Wuhan University);Yanchun Zhang (Victoria University, Australia);","2653716575,2096326769,2636634401,2293365922,2100438523","A large number of texts are rapidly generated as streaming data in social media. Since it is difficult to process such text streams with limited memory in real time, researchers are resorting to text stream compression and sampling to obtain a small portion of valuable information from the streams. In this study, we investigate the crucial question of how to use less memory space to store more valuable texts to maintain the global information of the stream. First, we propose a text stream sampling framework based on compressed sensing theory, which can sample a text stream with a lightweight framework to reduce the space consumption while still retaining the most valuable texts. We then develop a query word-based retrieval task as well as a topic detection and evolution analysis task on the sample stream to evaluate the performance of the framework in retaining valuable information. The framework is evaluated from several aspects using two representative datasets of social media, including compression ratio, runtime, information reserved rate, and efficiency of the text analysis tasks. Experimental results demonstrate that the proposed framework outperforms baseline methods and is able to complete the text analysis tasks with promising results.",2017,Knowledge and Information Systems volume 23 pp 1-25,compressed sensing;sampling;text mining;world wide web;information retrieval;data mining;database;machine learning;statistics;computer science;
L_p-Support vector machines for uplift modeling,Łukasz Zaniewicz (Polish Academy of Sciences);Szymon Jaroszewicz (Polish Academy of Sciences);,"2343575078,115085028","Uplift modeling is a branch of machine learning which aims to predict not the class itself, but the difference between the class variable behavior in two groups: treatment and control. Objects in the treatment group have been subjected to some action, while objects in the control group have not. By including the control group, it is possible to build a model which predicts the causal effect of the action for a given individual. In this paper, we present a variant of support vector machines designed specifically for uplift modeling. The SVM optimization task has been reformulated to explicitly model the difference in class behavior between two datasets. The model predicts whether a given object will have a positive, neutral or negative response to a given action, and by tuning a parameter of the model the analyst is able to influence the relative proportion of neutral predictions and thus the conservativeness of the model. Further, we extend \(L_p\)-SVMs to the case of uplift modeling and demonstrate that they allow for a more stable selection of the size of negative, neutral and positive groups. Finally, we present quadratic and convex optimization methods for efficiently solving the two proposed optimization tasks.",2017,Knowledge and Information Systems pp 1-28,artificial intelligence;machine learning;simulation;mathematical optimization;
Data placement in massively distributed environments for fast parallel mining of frequent itemsets,Saber Salah (French Institute for Research in Computer Science and Automation);Reza Akbarinia (French Institute for Research in Computer Science and Automation);Florent Masseglia (French Institute for Research in Computer Science and Automation);,"2117227185,295546380,2440503352","Frequent itemset mining presents one of the fundamental building blocks in data mining. However, despite the crucial recent advances that have been made in data mining literature, few of both standard and improved solutions scale. This is particularly the case when (1) the quantity of data tends to be very large and/or (2) the minimum support is very low. In this paper, we address the problem of parallel frequent itemset mining (PFIM) in very large databases and study the impact and effectiveness of using specific data placement strategies in a massively distributed environment. By offering a clever data placement and an optimal organization of the extraction algorithms, we show that the arrangement of both the data and the different processes can make the global job either completely inoperative or very effective. In this setting, we propose two different highly scalable, PFIM algorithms, namely P2S (parallel-2-steps) and PATD (parallel absolute top-down). P2S algorithm allows discovering itemsets from large databases in two simple, yet efficient parallel jobs, while PATD renders the mining process of very large databases more simple and compact. Its mining process is made up of only one parallel job, which dramatically reduces the running time, the communication cost and the energy power consumption overhead in a distributed computational platform. Our different proposed approaches have been extensively evaluated on massive real-world data sets. The experimental results confirm the effectiveness and scalability of our proposals by the important scale-up obtained with very low minimum supports compared to other alternatives.",2017,Knowledge and Information Systems pp 1-31,distributed computing;data mining;database;computer science;
Time-weighted counting for recently frequent pattern mining in data streams,Yongsub Lim (SK Telecom);U. Kang (Seoul National University);,"2609898321,2683961033","How can we discover interesting patterns from time-evolving high-speed data streams? How to analyze the data streams quickly and accurately, with little space overhead? How to guarantee the found patterns to be self-consistent? High-speed data stream has been receiving increasing attention due to its wide applications such as sensors, network traffic, social networks, etc. The most fundamental task on the data stream is frequent pattern mining; especially, focusing on recentness is important in real applications. In this paper, we develop two algorithms for discovering recently frequent patterns in data streams. First, we propose TwMinSwap to find top-k recently frequent items in data streams, which is a deterministic version of our motivating algorithm TwSample providing theoretical guarantees based on item sampling. TwMinSwap improves TwSample in terms of speed, accuracy, and memory usage. Both require only O(k) memory spaces and do not require any prior knowledge on the stream such as its length and the number of distinct items in the stream. Second, we propose TwMinSwap-Is to find top-k recently frequent itemsets in data streams. We especially focus on keeping self-consistency of the discovered itemsets, which is the most important property for reliable results, while using O(k) memory space with the assumption of a constant itemset size. Through extensive experiments, we demonstrate that TwMinSwap outperforms all competitors in terms of accuracy and memory usage, with fast running time. We also show that TwMinSwap-Is is more accurate than the competitor and discovers recently frequent itemsets with reasonably large sizes (at most 5–7) depending on datasets. Thanks to TwMinSwap and TwMinSwap-Is, we report interesting discoveries in real world data streams, including the difference of trends between the winner and the loser of U.S. presidential candidates, and temporal human contact patterns.",2017,Knowledge and Information Systems pp 1-32,sampling;data stream mining;world wide web;data mining;database;statistics;computer science;
Distributed and scalable sequential pattern mining through stream processing,Chun-Chieh Chen (National Taiwan University);Hong-Han Shuai (National Chiao Tung University);Ming-Syan Chen (Center for Information Technology);,"2488244446,2184003779,2122365371","Scalability is a primary issue in existing sequential pattern mining algorithms for dealing with a large amount of data. Previous work, namely sequential pattern mining on the cloud (SPAMC), has already addressed the scalability problem. It supports the MapReduce cloud computing architecture for mining frequent sequential patterns on large datasets. However, this existing algorithm does not address the iterative mining problem, which is the problem that reloading data incur additional costs. Furthermore, it did not study the load balancing problem. To remedy these problems, we devised a powerful sequential pattern mining algorithm, the sequential pattern mining in the cloud-uniform distributed lexical sequence tree algorithm (SPAMC-UDLT), exploiting MapReduce and streaming processes. SPAMC-UDLT dramatically improves overall performance without launching multiple MapReduce rounds and provides perfect load balancing across machines in the cloud. The results show that SPAMC-UDLT can significantly reduce execution time, achieves extremely high scalability, and provides much better load balancing than existing algorithms in the cloud.",2017,Knowledge and Information Systems pp 1-26,sequential pattern mining;cloud computing;big data;parallel computing;data mining;database;computer science;
Effective sparse imputation of patient conditions in electronic medical records for emergency risk predictions,Budhaditya Saha (Deakin University);Sunil Gupta (Deakin University);Dinh Phung (Deakin University);Svetha Venkatesh (Deakin University);,"2125102313,2119406083,2314522249,2146461601","Electronic medical records (EMRs) are being increasingly used for “risk” prediction. By “risks,” we denote outcomes such as emergency presentation, readmission, and the length of hospitalizations. However, EMR data analysis is complicated by missing entries. There are two reasons—the “primary reason for admission” is included in EMR, but the comorbidities (other chronic diseases) are left uncoded, and many zero values in the data are accurate, reflecting that a patient has not accessed medical facilities. A key challenge is to deal with the peculiarities of this data—unlike many other datasets, EMR is sparse, reflecting the fact that patients have some but not all diseases. We propose a novel model to fill-in these missing values and use the new representation for prediction of key hospital events. To “fill-in” missing values, we represent the feature-patient matrix as a product of two low-rank factors, preserving the sparsity property in the product. Intuitively, the product regularization allows sparse imputation of patient conditions reflecting common comorbidities across patients. We develop a scalable optimization algorithm based on Block coordinate descent method to find an optimal solution. We evaluate the proposed framework on two real-world EMR cohorts: Cancer (7000 admissions) and Acute Myocardial Infarction (2652 admissions). Our result shows that the AUC for 3-month emergency presentation prediction is improved significantly from (0.729 to 0.741) for Cancer data and (0.699 to 0.723) for AMI data. Similarly, AUC for 3-month emergency admission prediction from (0.730 to 0.752) for Cancer data and (0.682 to 0.724) for AMI data. We also extend the proposed method to a supervised model for predicting multiple related risk outcomes (e.g., emergency presentations and admissions in hospital over 3, 6, and 12 months period) in an integrated framework. The supervised model consistently outperforms state-of-the-art baseline methods.",2017,Knowledge and Information Systems pp 1-28,multivariate statistics;data science;data mining;statistics;
Spatial and semantical label inference for social media: A cross-network data fusion approach,Yuchi Ma (Sichuan University);Ning Yang (Sichuan University);Lei Zhang (Sichuan University);Philip S. Yu (University of Illinois at Chicago);,"2706249359,2283784084,2660015745,2125104194","Exploring the spatial and semantical knowledge from messages in social media offers us an opportunity to get a deeper understanding about the mobility and activity of users, which can be leveraged to improve the service quality of online applications like recommender systems. In this paper, we investigate the problem of the spatial and semantical label inference, where the challenges come from three aspects: diverse heterogeneous information, uncertainty of individual mobility, and large-scale sparse data. We address the challenges by exploring two types of data fusion, the fusion of heterogeneous social networks and the fusion of heterogeneous features. We build a 4-dimensional tensor, called spatial–temporal semantical tensor (STST), to model the individual mobility and activity by fusing two heterogeneous social networks, a social media network and a location-based social network (LBSN). To address the challenge arising from diverse heterogeneous information and the uncertainty of individual mobility, we construct three types of heterogeneous features and fuse them with STST by exploring their interdependency relationships. Particularly, a spatial tendency feature is constructed to constrain the inference of individual mobility and reduce the uncertainty. To deal with large-scale sparse data, we propose a parallel contextual tensor factorization (PCTF) to concurrently factorize STST. Finally, we integrate these components into an inference framework, called spatial and semantical label inference SSLI. The results of extensive experiments conducted on real datasets and synthetic datasets verify the effectiveness and efficiency of SSLI.",2017,Knowledge and Information Systems pp 1-25,social media;sensor fusion;data science;data mining;database;artificial intelligence;machine learning;computer science;
DANCer: dynamic attributed networks with community structure generation,C. Largeron (University of Lyon);P. N. Mougel (University of Lyon);O. Benyahia (Jean Monnet University);O. R. Zaïane (University of Alberta);,"361319035,1988545203,2292635790,2308328903","Most networks, such as those generated from social media, tend to evolve gradually with frequent changes in the activity and the interactions of their participants. Furthermore, the communities inside the network can grow, shrink, merge, or split, and the entities can move from one community to another. The aim of community detection methods is precisely to detect the evolution of these communities. However, evaluating these algorithms requires tests on real or artificial networks with verifiable ground truth. Dynamic networks generators have been recently proposed for this task, but most of them consider only the structure of the network, disregarding the characteristics of the nodes. In this paper, we propose a new generator for dynamic attributed networks with community structure that follow the properties of real-world networks. The evolution of the network is performed using two kinds of operations: Micro-operations are applied on the edges and vertices, while macro-operations on the communities. Moreover, the properties of real-world networks such as preferential attachment or homophily are preserved during the evolution of the network, as confirmed by our experiments.",2017,Knowledge and Information Systems pp 1-43,evolving networks;dynamic network analysis;community structure;complex network;social network;data mining;database;artificial intelligence;machine learning;statistics;computer science;
Toward intrusion detection using belief decision trees for big data,Imen Boukhris (Institut Supérieur de Gestion);Zied Elouedi (Institut Supérieur de Gestion);Mariem Ajabi (Institut Supérieur de Gestion);,"2701033289,1969625331,2518965900","Big data refers to datasets that we cannot manage with standard tools and within which lie valuable information previously hidden. New data mining techniques are needed to deal with the increasing size of such data, their complex structure as well as their veracity which is on covering questions of data imperfection and uncertainty. Even though big data veracity is often overlooked, it is very challenging and important for an accurate and reliable mining and knowledge discovery. This paper proposes MapReduce-based belief decision trees for big data as classifiers of uncertain large-scale datasets. The proposed averaging and conjunctive classification approaches are experimented for intrusion detection on KDD’99 massive intrusion dataset. Several granularity attacks’ levels have been considered depending on whether dealing with whole kind of attacks, or grouping them in categories or focusing on distinguishing normal and abnormal connections.",2017,Knowledge and Information Systems pp 1-28,intrusion detection system;big data;data science;data mining;database;machine learning;computer science;
"Graphlet decomposition: framework, algorithms, and applications",Nesreen K. Ahmed (Intel);Jennifer Neville (Purdue University);Ryan A. Rossi (PARC);Nick G. Duffield (Texas A&M University);Theodore L. Willke (Intel);,"2119838086,2124572662,2060818872,2017920553,1993295659","From social science to biology, numerous applications often rely on graphlets for intuitive and meaningful characterization of networks. While graphlets have witnessed a tremendous success and impact in a variety of domains, there has yet to be a fast and efficient framework for computing the frequencies of these subgraph patterns. However, existing methods are not scalable to large networks with billions of nodes and edges. In this paper, we propose a fast, efficient, and parallel framework as well as a family of algorithms for counting k-node graphlets. The proposed framework leverages a number of theoretical combinatorial arguments that allow us to obtain significant improvement on the scalability of graphlet counting. For each edge, we count a few graphlets and obtain the exact counts of others in constant time using the combinatorial arguments. On a large collection of $$300+$$300+ networks from a variety of domains, our graphlet counting strategies are on average $$460{\times }$$460× faster than existing methods. This brings new opportunities to investigate the use of graphlets on much larger networks and newer applications as we show in the experiments. To the best of our knowledge, this paper provides the largest graphlet computations to date.",2017,Knowledge and Information Systems volume 50 issue 3 pp 689-722,graph kernel;motif;biological network;biological classification;theoretical computer science;data mining;database;artificial intelligence;machine learning;computer science;mathematics;
Event-based summarization using a centrality-as-relevance model,Luís Marujo (INESC-ID);Ricardo Ribeiro 0001 (INESC-ID);Anatole Gershman (Carnegie Mellon University);David Martins de Matos (INESC-ID);João Paulo Neto (INESC-ID);Jaime G. Carbonell (Carnegie Mellon University);,"1969149255,2633306263,2101109309,2140026351,2124061859,2100444261","Event detection is a fundamental information extraction task, which has been explored largely in the context of question answering, topic detection and tracking, knowledge base population, news recommendation, and automatic summarization. In this article, we explore an event detection framework to improve a key phrase-guided centrality-based summarization model. Event detection is based on the fuzzy fingerprint method, which is able to detect all types of events in the ACE 2005 Multilingual Corpus. Our base summarization approach is a two-stage method that starts by extracting a collection of key phrases that will be used to help the centrality-as-relevance retrieval model. We explored three different ways to integrate event information, achieving state-of-the-art results in text and speech corpora: (1) filtering of nonevents, (2) event fingerprints as features, and (3) combination of filtering of nonevents and event fingerprints as features.",2017,Knowledge and Information Systems volume 50 issue 3 pp 945-968,centrality;multi document summarization;automatic summarization;information retrieval;data mining;pattern recognition;computer science;
SILVERBACK+: scalable association mining via fast list intersection for columnar social data,Yusheng Xie (Northwestern University);Zhengzhang Chen (NEC);Diana Palsetia (Northwestern University);Goce Trajcevski (Northwestern University);Ankit Agrawal (Northwestern University);Alok N. Choudhary (Northwestern University);,"2123854923,2132666618,210558056,98612900,2160807299,2147783234","We present Silverback+, a scalable probabilistic framework for accurate association rule and frequent item-set mining of large-scale social behavioral data. Silverback+ tackles the problem of efficient storage utilization and management via: (1) probabilistic columnar infrastructure and (2) using Bloom filters and sampling techniques. In addition, probabilistic pruning techniques based on Apriori method are developed, for accelerating the mining of frequent item-sets. The proposed target-driven techniques yield a significant reduction of the size of the frequent item-set candidates, as well as the required number of repetitive membership checks through a novel list intersection algorithm. Extensive experimental evaluations demonstrate the benefits of this context-aware consideration and incorporation of the infrastructure limitations when utilizing the corresponding research techniques. When compared to the traditional Hadoop-based approach for improving scalability by straightforwardly adding more hosts, Silverback+ exhibits a much better runtime performance, with negligible loss of accuracy.",2017,Knowledge and Information Systems volume 50 issue 3 pp 969-997,bloom filter;social media;association rule learning;theoretical computer science;world wide web;data mining;database;machine learning;computer science;
An effective and interpretable method for document classification,Ngo Van Linh (Hanoi University of Science and Technology);Nguyen Kim Anh (Hanoi University of Science and Technology);Khoat Than (Hanoi University of Science and Technology);Chien Nguyen Dang (Hanoi University of Science and Technology);,"2102101224,2152255017,2116469936,2518383035","As the number of documents has been rapidly increasing in recent time, automatic text categorization is becoming a more important and fundamental task in information retrieval and text mining. Accuracy and interpretability are two important aspects of a text classifier. While the accuracy of a classifier measures the ability to correctly classify unseen data, interpretability is the ability of the classifier to be understood by humans and provide reasons why each data instance is assigned to a label. This paper proposes an interpretable classification method by exploiting the Dirichlet process mixture model of von Mises---Fisher distributions for directional data. By using the labeled information of the training data explicitly and determining automatically the number of topics for each class, the learned topics are coherent, relevant and discriminative. They help interpret as well as distinguish classes. Our experimental results showed the advantages of our approach in terms of separability, interpretability and effectiveness in classification task of datasets with high dimension and complex distribution. Our method is highly competitive with state-of-the-art approaches.",2017,Knowledge and Information Systems volume 50 issue 3 pp 763-793,biological classification;data mining;pattern recognition;artificial intelligence;machine learning;statistics;mathematics;
DBMUTE: density-based majority under-sampling technique,Chumphol Bunkhumpornpat (Chiang Mai University);Krung Sinapiromsaran (Chulalongkorn University);,"2424468823,736112386","Class imbalance is a challenging problem that demonstrates the unsatisfactory classification performance of a minority class. A trivial classifier is biased toward minority instances because of their tiny fraction. In this paper, our density function is defined as the distance along the shortest path between each majority instance and a minority-cluster pseudo-centroid in an underlying cluster graph. A short path implies highly overlapping dense minority instances. In contrast, a long path indicates a sparsity of instances. A new under-sampling algorithm is proposed to eliminate majority instances with low distances because these instances are insignificant and obscure the classification boundary in the overlapping region. The results show predictive improvements on a minority class from various classifiers on different UCI datasets.",2017,Knowledge and Information Systems volume 50 issue 3 pp 827-850,data mining;pattern recognition;machine learning;computer science;mathematics;
Context-aware query expansion method using Language Models and Latent Semantic Analyses,Btihal El Ghali (Mohammed V University);Abderrahim El Qadi (Telecom Italia Mobile);,"2035558104,2069609640","One of the key difficulties for users in information retrieval is to formulate appropriate queries to submit to the search engine. In this paper, we propose an approach to enrich the user's queries by additional context. We used the Language Model to build the query context, which is composed of the most similar queries to the query to expand and their top-ranked documents. Then, we applied a query expansion approach based on the query context and the Latent Semantic Analyses method. Using a web test collection, we tested our approach on short and long queries. We varied the number of recommended queries and the number of expansion terms to specify the appropriate parameters for the proposed approach. Experimental results show that the proposed approach improves the effectiveness of the information retrieval system by 19.23 % for short queries and 52.94 % for long queries according to the retrieval results using the original users' queries.",2017,Knowledge and Information Systems volume 50 issue 3 pp 751-762,ranking;sargable;queries per second;rdf query language;concept search;web search query;web query classification;spatial query;query expansion;query optimization;query language;language model;information retrieval;data mining;database;computer science;
Sharing beliefs among agents with different degrees of credibility,Luciano H. Tamargo (UniSource Energy Corporation);Sebastian Gottifredi (UniSource Energy Corporation);Alejandro Javier García (UniSource Energy Corporation);Guillermo Ricardo Simari (UniSource Energy Corporation);,"2582711934,2305395560,2114286633,2245549248","This paper introduces an approach for sharing beliefs in collaborative multi-agent application domains where some agents can be more credible than others. In this context, we propose a formalization where every agent has its own partial order among its peers representing the credibility the agent assigns to its informants; each agent will also have a belief base where each sentence is attached with an agent identifier which represents the credibility of that sentence. We define four different forwarding criteria for computing the credibility information for a belief to be forwarded, and for determining how the receiver should handle the incoming information; the proposal considers both the sender’s and the receiver’s points of view with respect to the credibility of the source of the information.",2017,Knowledge and Information Systems volume 50 issue 3 pp 999-1031,trustworthy computing;multi agent system;data mining;artificial intelligence;computer science;
Toward value difference metric with attribute weighting,Chaoqun Li (China University of Geosciences);Liangxiao Jiang (China University of Geosciences);Hongwei Li (China University of Geosciences);Jia Wu (Information Technology University);Peng Zhang (Information Technology University);,"2129023193,2150766703,2711264754,2151584597,2699269147","In distance metric learning, recent work has shown that value difference metric (VDM) with a strong attribute independence assumption outperforms other existing distance metrics. However, an open question is whether VDM with a less restrictive assumption can perform even better. Many approaches have been proposed to improve VDM by weakening the assumption. In this paper, we make a comprehensive survey on the existing improved approaches and then propose a new approach to improve VDM by attribute weighting. We name the proposed new distance function as attribute-weighted value difference metric (AWVDM). Moreover, we propose a modified attribute-weighted value difference metric (MAWVDM) by incorporating the learned attribute weights into the conditional probability estimates of AWVDM. AWVDM and MAWVDM significantly outperform VDM and inherit the computational simplicity of VDM simultaneously. Experimental results on a large number of UCI data sets validate the performance of AWVDM and MAWVDM.",2017,Knowledge and Information Systems volume 50 issue 3 pp 795-825,mutual information;discrete mathematics;data mining;machine learning;statistics;mathematics;
Active inference for dynamic Bayesian networks with an application to tissue engineering,Caner Komurlu (Illinois Institute of Technology);Jinjian Shao (Illinois Institute of Technology);Banu Akar (Illinois Institute of Technology);Elif S. Bayrak (Illinois Institute of Technology);Eric M. Brey (Illinois Institute of Technology);Ali Cinar (Illinois Institute of Technology);Mustafa Bilgic (Illinois Institute of Technology);,"2200462323,2209217484,2116107782,2092599465,2252623449,670545612,2010936866","In temporal domains, agents need to actively gather information to make more informed decisions about both the present and the future. When such a domain is modeled as a temporal graphical model, what the agent observes can be incorporated into the model by setting the respective random variables as evidence. Motivated by a tissue engineering application where the experimenter needs to decide how early a laboratory experiment can be stopped so that its possible future outcomes can be predicted within an acceptable uncertainty, we first present a dynamic Bayesian network (DBN) model of vascularization in engineered tissues and compare it with both real-world experimental data and agent-based simulations. We then formulate the question of ""how early an experiment can be stopped to guarantee an acceptable uncertainty about the final expected outcome"" as an active inference problem for DBNs and empirically and analytically evaluate several search algorithms that aim to find the ideal time to stop a tissue engineering laboratory experiment.",2017,Knowledge and Information Systems volume 50 issue 3 pp 917-943,dynamic bayesian network;tissue engineering;data mining;artificial intelligence;machine learning;simulation;statistics;computer science;
Top-k coupled keyword recommendation for relational keyword queries,"Xiangfu Meng (Liaoning Technical University);Longbing Cao (University of Technology, Sydney);Xiaoyan Zhang (Liaoning Technical University);Jingyu Shao (University of Technology, Sydney);","2703902334,2115085568,2606854178,2231241261","Providing top-k typical relevant keyword queries would benefit the users who cannot formulate appropriate queries to express their imprecise query intentions. By extracting the semantic relationships both between keywords and keyword queries, this paper proposes a new keyword query suggestion approach which can provide typical and semantically related queries to the given query. Firstly, a keyword coupling relationship measure, which considers both intra- and inter-couplings between each pair of keywords, is proposed. Then, the semantic similarity of different keyword queries can be measured by using a semantic matrix, in which the coupling relationships between keywords in queries are reserved. Based on the query semantic similarities, we next propose an approximation algorithm to find the most typical queries from query history by using the probability density estimation method. Lastly, a threshold-based top-k query selection method is proposed to expeditiously evaluate the top-k typical relevant queries. We demonstrate that our keyword coupling relationship and query semantic similarity measures can capture the coupling relationships between keywords and semantic similarities between keyword queries accurately. The efficiency of query typicality analysis and top-k query selection algorithm is also demonstrated.",2017,Knowledge and Information Systems volume 50 issue 3 pp 883-916,keyword density;web search query;web query classification;spatial query;query optimization;query language;information retrieval;data mining;database;computer science;
DASC: data aware algorithm for scalable clustering,"Vasudha Bhatnagar (Dept. of Computer Science, University of Delhi);Sharanjit Kaur (University of Delhi);Rakhi Saxena (University of Delhi);Dhriti Khanna (Indraprastha Institute of Information Technology);","2590701236,2136566649,2521221453,2165892334","Emergence of MapReduce (MR) framework for scaling data mining and machine learning algorithms provides for Volume, while handling of Variety and Velocity needs to be skilfully crafted in algorithms. So far, scalable clustering algorithms have focused solely on Volume, taking advantage of the MR framework. In this paper we present a MapReduce algorithm--data aware scalable clustering (DASC), which is capable of handling the 3 Vs of big data by virtue of being (i) single scan and distributed to handle Volume, (ii) incremental to cope with Velocity and (iii) versatile in handling numeric and categorical data to accommodate Variety. DASC algorithm incrementally processes infinitely growing data set stored on distributed file system and delivers quality clustering scheme while ensuring recency of patterns. The up-to-date synopsis is preserved by the algorithm for the data seen so far. Each new data increment is processed and merged with the synopsis. Since the synopsis itself may grow very large in size, the algorithm stores it as a file. This makes DASC algorithm truly scalable. Exclusive clusters are obtained on demand by applying connected component analysis (CCA) algorithm over the synopsis. CCA presents subtle roadblock to effective parallelism during clustering. This problem is overcome by accomplishing the task in two stages. In the first stage, hyperclusters are identified based on prevailing data characteristics. The second stage utilizes this knowledge to determine the degree of parallelism, thereby making DASC data aware. Hyperclusters are distributed over the available compute nodes for discovering embedded clusters in parallel. Staged approach for clustering yields dual advantage of improved parallelism and desired complexity in $$\mathcal {MRC}^0$$MRC0 class. DASC algorithm is empirically compared with incremental Kmeans and Scalable Kmeans++ algorithms. Experimentation on real-world and synthetic data with approximately 1.2 billion data points demonstrates effectiveness of DASC algorithm. Empirical observations of DASC execution are in consonance with the theoretical analysis with respect to stability in resources utilization and execution time.",2017,Knowledge and Information Systems volume 50 issue 3 pp 851-881,theoretical computer science;data mining;database;artificial intelligence;machine learning;computer science;
Towards efficient top-k reliability search on uncertain graphs,Rong Zhu (Harbin Institute of Technology);Zhaonian Zou (Harbin Institute of Technology);Jianzhong Li (Harbin Institute of Technology);,"2435875582,2121850449,2130201582","Uncertain graph has been widely used to represent graph data with inherent uncertainty in structures. Reliability search is a fundamental problem in uncertain graph analytics. This paper investigates on a new problem with broad real-world applications, the top-k reliability search problem on uncertain graphs, that is, finding the k vertices v with the highest reliabilities of connections from a source vertex s to v. Note that the existing algorithm for the threshold-based reliability search problem is inefficient for the top-k reliability search problem. We propose a new algorithm to efficiently solve the top-k reliability search problem. The algorithm adopts two important techniques, namely the BFS sharing technique and the offline sampling technique. The BFS sharing technique exploits overlaps among different sampled possible worlds of the input uncertain graph and performs a single BFS on all possible worlds simultaneously. The offline sampling technique samples possible worlds offline and stores them using a compact structure. The algorithm also takes advantages of bit vectors and bitwise operations to improve efficiency. In addition, we generalize the top-k reliability search problem from single-source case to the multi-source case and show that the multi-source case of the problem can be equivalently converted to the single-source case of the problem. Moreover, we define two types of the reverse top-k reliability search problems with different semantics on uncertain graphs. We propose appropriate solutions for both of them. Extensive experiments carried out on both real and synthetic datasets verify that the optimized algorithm outperforms the baselines by 1–2 orders of magnitude in execution time while achieving comparable accuracy. Meanwhile, the optimized algorithm exhibits linear scalability with respect to the size of the input uncertain graph.",2017,Knowledge and Information Systems volume 50 issue 3 pp 723-750,theoretical computer science;discrete mathematics;data mining;database;machine learning;mathematical optimization;mathematics;
Measuring in-network node similarity based on neighborhoods: a unified parametric approach,Yu Yang (Simon Fraser University);Jian Pei (Simon Fraser University);Abdullah Al-Barakati (King Abdulaziz University);,"2631208825,2126330539,1582813313","In many applications, we need to measure similarity between nodes in a large network based on features of their neighborhoods. Although in-network node similarity based on proximity has been well investigated, surprisingly, measuring in-network node similarity based on neighborhoods remains a largely untouched problem in literature. One challenge is that in different applications we may need different measurements that manifest different meanings of similarity. Furthermore, we often want to make trade-offs between specificity of neighborhood matching and efficiency. In this paper, we investigate the problem in a principled and systematic manner. We develop a unified parametric model and a series of four instance measures. Those instance similarity measures not only address a spectrum of various meanings of similarity, but also present a series of trade-offs between computational cost and strictness of matching between neighborhoods of nodes being compared. By extensive experiments and case studies, we demonstrate the effectiveness of the proposed model and its instances.",2017,Knowledge and Information Systems pp 1-28,random walk;discrete mathematics;combinatorics;machine learning;statistics;mathematics;
Contextual information fusion for intrusion detection: a survey and taxonomy,"Ahmed Aleroud (Yarmouk University);George Karabatis (University of Maryland, Baltimore County);","2481213832,2659574104","Research in cyber-security has demonstrated that dealing with cyber-attacks is by no means an easy task. One particular limitation of existing research originates from the uncertainty of information that is gathered to discover attacks. This uncertainty is partly due to the lack of attack prediction models that utilize contextual information to analyze activities that target computer networks. The focus of this paper is a comprehensive review of data analytics paradigms for intrusion detection along with an overview of techniques that apply contextual information for intrusion detection. A new research taxonomy is introduced consisting of several dimensions of data mining techniques, which create attack prediction models. The survey reveals the need to use multiple categories of contextual information in a layered manner with consistent, coherent, and feasible evidence toward the correct prediction of cyber-attacks.",2017,Knowledge and Information Systems pp 1-57,intrusion detection system;semantics;world wide web;computer security;data mining;database;machine learning;computer science;
"FCloSM, FGenSM: two efficient algorithms for mining frequent closed and generator sequences using the local pruning strategy",Bac Le (Verenigde Nederlandse Uitgeverijen);Hai Duong;Tin Truong;Philippe Fournier-Viger (Harbin Institute of Technology);,"2110272132,2657655919,2730869928,165198604","Mining frequent sequences in sequential databases are highly valuable for many real-life applications. However, in several cases, especially when databases are huge and when low minimum support thresholds are used, the cardinality of the result set can be enormous. Consequently, algorithms for discovering frequent sequences exhibit poor performance, showing an important increase in execution time, memory consumption and storage space usage. To address this issue, researchers have studied the tasks of mining frequent closed and generator sequences, as they provide several benefits when compared to the set of frequent sequences. One of the most important benefits is that the cardinalities of frequent closed and generator sequences are generally much less than the cardinality of frequent sequences. Hence, humans find it more convenient to analyze the information provided by closed and generator sequences. Moreover, it was shown that frequent closed sequences have the advantage of being lossless, and they thus preserve information about the frequency of all frequent subsequences, while generator sequences can provide higher accuracy for sequence classification tasks since they are the smallest patterns that characterize groups of sequences. Besides, frequent closed sequences can be combined with generators to produce non-redundant sequential rules and recover the complete set of frequent sequences and their frequencies. This paper proposes two novel algorithms named FCloSM and FGenSM to mine frequent closed and generator sequences efficiently. These algorithms are based on new pruning conditions called extended early elimination (3E) and early pruning techniques named EPCLO and EPGEN, designed to identify non-closed and non-generator patterns early. Based on these techniques, two local pruning strategies called LPCLO and LPGEN are proposed to eliminate non-closed and non-generator patterns more efficiently at two successive levels of the prefix search tree without performing subsequence relation checking. These theoretical results, which are the basis of FCloSM and FGenSM, are mathematically proved and are shown to be more general than those presented in previous work. Extensive experiments show that FCloSM and FGenSM are one to two orders of magnitude faster than the state-of-the-art algorithms for discovering frequent closed sequences (CloSpan, BIDE, ClaSP and CM-ClaSP) and for mining frequent generators (FEAT, FSGP and VGEN), and that FCloSM and FGenSM consume much less memory.",2017,Knowledge and Information Systems pp 1-37,sequential pattern mining;data mining;machine learning;algorithm;computer science;mathematics;
The cascading neural network: building the Internet of Smart Things,Sam Leroux (Ghent University);Steven Bohez (Ghent University);Elias De Coninck (Ghent University);Tim Verbelen (Ghent University);Bert Vankeirsbilck (Ghent University);Pieter Simoens (Ghent University);Bart Dhoedt (Ghent University);,"2167557612,297262771,2041760992,1192672773,1523199369,2114043725,1990677562","Most of the research on deep neural networks so far has been focused on obtaining higher accuracy levels by building increasingly large and deep architectures. Training and evaluating these models is only feasible when large amounts of resources such as processing power and memory are available. Typical applications that could benefit from these models are, however, executed on resource-constrained devices. Mobile devices such as smartphones already use deep learning techniques, but they often have to perform all processing on a remote cloud. We propose a new architecture called a cascading network that is capable of distributing a deep neural network between a local device and the cloud while keeping the required communication network traffic to a minimum. The network begins processing on the constrained device, and only relies on the remote part when the local part does not provide an accurate enough result. The cascading network allows for an early-stopping mechanism during the recall phase of the network. We evaluated our approach in an Internet of Things context where a deep neural network adds intelligence to a large amount of heterogeneous connected devices. This technique enables a whole variety of autonomous systems where sensors, actuators and computing nodes can work together. We show that the cascading architecture allows for a substantial improvement in evaluation speed on constrained devices while the loss in accuracy is kept to a minimum.",2017,Knowledge and Information Systems pp 1-24,internet of things;cloud computing;deep learning;embedded system;world wide web;distributed computing;data mining;database;machine learning;computer science;
Efficient nonparametric and asymptotic Bayesian model selection methods for attributed graph clustering,Zhiqiang Xu (King Abdullah University of Science and Technology);James Cheng (The Chinese University of Hong Kong);Xiaokui Xiao (Nanyang Technological University);Ryohei Fujimaki (NEC);Yusuke Muraoka (NEC);,"2139463856,2304873892,2157867657,1124837368,2260273064","Attributed graph clustering, also known as community detection on attributed graphs, attracts much interests recently due to the ubiquity of attributed graphs in real life. Many existing algorithms have been proposed for this problem, which are either distance based or model based. However, model selection in attributed graph clustering has not been well addressed, that is, most existing algorithms assume the cluster number to be known a priori. In this paper, we propose two efficient approaches for attributed graph clustering with automatic model selection. The first approach is a popular Bayesian nonparametric method, while the second approach is an asymptotic method based on a recently proposed model selection criterion, factorized information criterion. Experimental results on both synthetic and real datasets demonstrate that our approaches for attributed graph clustering with automatic model selection significantly outperform the state-of-the-art algorithm.",2017,Knowledge and Information Systems pp 1-30,correlation clustering;null model;model selection;cluster analysis;data mining;pattern recognition;machine learning;statistics;mathematics;
Efficient Monte Carlo clustering in subspaces,Clark F. Olson (University of Washington);David C. Hunn (Microsoft);Henry J. Lyons (Microsoft);,"2153264145,2587927562,2703658776","Clustering of high-dimensional data is an important problem in many application areas, including image classification, genetic analysis, and collaborative filtering. However, it is common for clusters to form in different subsets of the dimensions. We present a randomized algorithm for subspace and projected clustering that is both simple and efficient. The complexity of the algorithm is linear in the number of data points and low-order polynomial in the number of dimensions. We present the results of a thorough evaluation of the algorithm using the OpenSubspace framework. Our algorithm outperforms competing subspace and projected clustering algorithms on both synthetic and real-world data sets.",2017,Knowledge and Information Systems pp 1-22,flame clustering;k medians clustering;subclu;brown clustering;canopy clustering algorithm;determining the number of clusters in a data set;k medoids;dbscan;correlation clustering;data stream clustering;cure data clustering algorithm;single linkage clustering;affinity propagation;fuzzy clustering;monte carlo algorithm;clustering high dimensional data;cluster analysis;biological classification;consensus clustering;biclustering;pattern recognition;machine learning;mathematical optimization;mathematics;
An automatic human chromosome metaspread image selection technique,Tanvi Arora (Dr. B. R. Ambedkar National Institute of Technology Jalandhar);Renu Dhir (Dr. B. R. Ambedkar National Institute of Technology Jalandhar);,"2599968142,2155046697","The human chromosome metaspread images are used to generate the karyogram that is used for the diagnosis of the genetic defects. The genetic defects occur due to variation in either the structure of the chromosomes or the number of chromosomes present in the cell. The human chromosome metaspread image selection process is very critical in the karyogram generation task. It is very tedious and time-consuming process and is generally done manually by an expert cytogeneticist. The manual selection results may be biased, and it is possible that the whole search space is not explored to find the best metaspread image. The mood of the cytogeneticist will also greatly affect the selection results. So there is a strong need to automate the process of human chromosome metaspread image selection process. The proposed approach ranks the metaspread images based upon the quality score that is calculated using the count of the chromosomes of various orientations present in the metaspread image. The ranking has been done based upon ordinal ranking process, wherein a unique rank is assigned to each image based upon a set of rules. The rule base aids in the tiebreaking process in case the same quality score is derived for more than one metaspread image. The decision-making process of the expert cytogeneticist has been emulated by using a set of if–then rules. The proposed technique helps to select the best metaspread image, by exploring the complete set of images that can be used for the karyogram generation.",2017,Knowledge and Information Systems pp 1-18,ranking;chromosome;feature extraction;biological classification;data mining;pattern recognition;machine learning;statistics;computer science;mathematics;
Monitoring stealthy diffusion,Nika Haghtalab (Carnegie Mellon University);Aron Laszka (Vanderbilt University);Ariel D. Procaccia (Carnegie Mellon University);Yevgeniy Vorobeychik (Vanderbilt University);Xenofon Koutsoukos (Vanderbilt University);,"1563078655,92711084,160737525,2305697090,271970670","A broad variety of problems, such as targeted marketing and the spread of viruses and malware, have been modeled as maximizing the reach of diffusion through a network. In cyber-security applications, however, a key consideration largely ignored in this literature is stealth. In particular, an attacker who has a specific target in mind succeeds only if the target is reached before the malicious payload is detected and corresponding countermeasures deployed. The dual side of this problem is deployment of a limited number of monitoring units, such as cyber-forensics specialists, to limit the success of such targeted and stealthy diffusion processes. We investigate the problem of optimal monitoring of targeted stealthy diffusion processes. While natural variants of this problem are NP-hard, we show that if stealthy diffusion starts from randomly selected nodes, the defender’s objective is submodular and can be approximately optimized. In addition, we present approximation algorithms for the setting where the choice of the starting point is adversarial. We further extend our results to settings where the diffusion starts at multiple-seed nodes simultaneously, and where there is an inherent delay in detecting the infection. Our experimental results show that the proposed algorithms are highly effective and scalable.",2017,Knowledge and Information Systems pp 1-29,telecommunications;computer security;simulation;
Missing value estimation for microarray data through cluster analysis,"Soumen Kumar Pati (St. Thomas' College of Engineering and Technology);Asit Kumar Das (Indian Institute of Engineering Science and Technology, Shibpur);","2120480877,2147492310","Microarray datasets with missing values need to impute accurately before analyzing diseases. The proposed method first discretizes the samples and temporarily assigns a value in missing position of a gene by the mean value of all samples in the same class. The frequencies of each gene value in both types of samples for all genes are calculated separately and if the maximum frequency occurs for same expression value in both types, then the whole gene is entered into a subset; otherwise, each portion of the gene of respective sample type (i.e., normal or disease) is entered into two separate subsets. Thus, for each gene expression value, maximum three different clusters of genes are formed. Each gene subset is further partitioned into a stable number of clusters using proposed splitting and merging clustering algorithm that overcomes the weakness of Euclidian distance metric used in high-dimensional space. Finally, similarity between a gene with missing values and centroids of the clusters are measured and the missing values are estimated by corresponding expression values of a centroid having maximum similarity. The method is compared with various statistical, cluster-based and regression-based methods with respect to statistical and biological metrics using microarray datasets to measure its effectiveness.",2017,Knowledge and Information Systems pp 1-42,cluster analysis;bioinformatics;data mining;machine learning;statistics;computer science;mathematics;
Uncovering and predicting the dynamic process of information cascades with survival model,Linyun Yu (Tsinghua University);Peng Cui (Tsinghua University);Fei Wang (Beijing University of Technology);Chaoming Song (University of Miami);Shiqiang Yang (Tsinghua University);,"2223066044,2113115369,2679019772,2668391791,2127183023","Cascades are ubiquitous in various network environments. Predicting these cascades is decidedly nontrivial in various important applications, such as viral marketing, epidemic prevention, and traffic management. Most previous works have focused on predicting the final cascade sizes. As cascades are dynamic processes, it is always interesting and important to predict the cascade size at any given time, or to predict the time when a cascade will reach a certain size (e.g., the threshold for an outbreak). In this paper, we unify all these tasks into a fundamental problem: cascading process prediction. That is, given the early stage of a cascade, can we predict its cumulative cascade size at any later time? For such a challenging problem, an understanding of the micromechanism that drives and generates the macrophenomena (i.e., the cascading process) is essential. Here, we introduce behavioral dynamics as the micromechanism to describe the dynamic process of an infected node's neighbors getting infected by a cascade (i.e., one-hop sub-cascades). Through data-driven analysis, we find out the common principles and patterns lying in the behavioral dynamics and propose the novel NEtworked WEibull Regression model for modeling it. We also propose a novel method for predicting cascading processes by effectively aggregating behavioral dynamics and present a scalable solution to approximate the cascading process with a theoretical guarantee. We evaluate the proposed method extensively on a large-scale social network dataset. The results demonstrate that the proposed method can significantly outperform other state-of-the-art methods in multiple tasks including cascade size prediction, outbreak time prediction, and cascading process prediction.",2017,Knowledge and Information Systems volume 50 issue 2 pp 633-659,information cascade;social network;artificial intelligence;machine learning;simulation;computer science;
"Cloud resource allocation schemes: review, taxonomy, and opportunities",Abdullah Yousafzai (Information Technology University);Abdullah Gani (Information Technology University);Rafidah Md Noor (Information Technology University);Mehdi Sookhak (Information Technology University);Hamid Talebian (University of Malaya);Muhammad Shiraz (Federal Urdu University);Muhammad Khurram Khan (King Saud University);,"2377500134,2120729324,2574907299,1977028700,2282524963,2116099056,2125956071","Cloud computing has emerged as a popular computing model to process data and execute computationally intensive applications in a pay-as-you-go manner. Due to the ever-increasing demand for cloud-based applications, it is becoming difficult to efficiently allocate resources according to user requests while satisfying the service-level agreement between service providers and consumers. Furthermore, cloud resource heterogeneity, the unpredictable nature of workload, and the diversified objectives of cloud actors further complicate resource allocation in the cloud computing environment. Consequently, both the industry and academia have commenced substantial research efforts to efficiently handle the aforementioned multifaceted challenges with cloud resource allocation. The lack of a comprehensive review covering the resource allocation aspects of optimization objectives, design approaches, optimization methods, target resources, and instance types has motivated a review of existing cloud resource allocation schemes. In this paper, current state-of-the-art cloud resource allocation schemes are extensively reviewed to highlight their strengths and weaknesses. Moreover, a thematic taxonomy is presented based on resource allocation optimization objectives to classify the existing literature. The cloud resource allocation schemes are analyzed based on the thematic taxonomy to highlight the commonalities and deviations among them. Finally, several opportunities are suggested for the design of optimal resource allocation schemes.",2017,Knowledge and Information Systems volume 50 issue 2 pp 347-381,in situ resource utilization;cloud computing;resource allocation;resource management;management science;data mining;simulation;computer science;
MiMAG: mining coherent subgraphs in multi-layer graphs with edge labels,Brigitte Boden (RWTH Aachen University);Stephan Günnemann (Technische Universität München);Holger Hoffmann (RWTH Aachen University);Thomas Seidl (RWTH Aachen University);,"2117690819,316694267,2224264343,2140301036","Detecting dense subgraphs such as cliques or quasi-cliques is an important graph mining problem. While this task is established for simple graphs, today's applications demand the analysis of more complex graphs: In this work, we consider a frequently observed type of graph where edges represent different types of relations. These multiple edge types can also be viewed as different ""layers"" of a graph, which is denoted as a ""multi-layer graph"". Additionally, each edge might be annotated by a label characterizing the given relation in more detail. By simultaneously exploiting all this information, the detection of more interesting subgraphs can be supported. We introduce the multi-layer coherent subgraph model, which defines clusters of vertices that are densely connected by edges with similar labels in a subset of the graph layers. We avoid redundancy in the result by selecting only the most interesting, non-redundant subgraphs for the output. Based on this model, we introduce the best-first search algorithm MiMAG. In thorough experiments, we demonstrate the strengths of MiMAG in comparison with related approaches on synthetic as well as real-world data sets.",2017,Knowledge and Information Systems volume 50 issue 2 pp 417-446,factor critical graph;distance hereditary graph;simplex graph;strength of a graph;voltage graph;complement graph;forbidden graph characterization;multiple edges;graph power;graph factorization;butterfly graph;comparability graph;universal graph;null graph;graph labeling;split graph;clique width;line graph;multigraph;planar graph;graph;cluster analysis;theoretical computer science;discrete mathematics;combinatorics;machine learning;computer science;mathematics;
Intelligent bus routing with heterogeneous human mobility patterns,Yanchi Liu (Rutgers–Newark);Chuanren Liu (Drexel University);Nicholas Jing Yuan (Microsoft);Lian Duan (Hofstra University);Yanjie Fu (Rutgers–Newark);Hui Xiong (Rutgers–Newark);Songhua Xu (New Jersey Institute of Technology);Junjie Wu (Beihang University);,"2159798580,2169554947,2096490164,2657865753,2168873515,2153710278,2713965158,2149366604","Optimal planning for public transportation is one of the keys helping to bring a sustainable development and a better quality of life in urban areas. Compared to private transportation, public transportation uses road space more efficiently and produces fewer accidents and emissions. However, in many cities people prefer to take private transportation other than public transportation due to the inconvenience of public transportation services. In this paper, we focus on the identification and optimization of flawed region pairs with problematic bus routing to improve utilization efficiency of public transportation services, according to people's real demand for public transportation. To this end, we first provide an integrated mobility pattern analysis between the location traces of taxicabs and the mobility records in bus transactions. Based on the mobility patterns, we propose a localized transportation mode choice model, with which we can dynamically predict the bus travel demand for different bus routing by taking into account both bus and taxi travel demands. This model is then used for bus routing optimization which aims to convert as many people from private transportation to public transportation as possible given budget constraints on the bus route modification. We also leverage the model to identify region pairs with flawed bus routes, which are effectively optimized using our approach. To validate the effectiveness of the proposed methods, extensive studies are performed on real-world data collected in Beijing which contains 19 million taxi trips and 10 million bus trips.",2017,Knowledge and Information Systems volume 50 issue 2 pp 383-415,public transport;transportation planning;computer security;
Attributed graph mining in the presence of automorphism,Claude Pasquier (Centre national de la recherche scientifique);Frédéric Flouvat (University of New Caledonia);Jérémy Sanhes (University of New Caledonia);Nazha Selmaoui-Folcher (University of New Caledonia);,"2304010088,2093516567,1826130176,1967950925","Attributed directed graphs are directed graphs in which nodes are associated with sets of attributes. Many data from the real world can be naturally represented by this type of structure, but few algorithms are able to directly handle these complex graphs. Mining attributed graphs is a difficult task because it requires combining the exploration of the graph structure with the identification of frequent itemsets. In addition, due to the combinatorics on itemsets, subgraph isomorphisms (which have a significant impact on performances) are much more numerous than in labeled graphs. In this paper, we present a new data mining method that can extract frequent patterns from one or more directed attributed graphs. We show how to reduce the combinatorial explosion induced by subgraph isomorphisms thanks to an appropriate processing of automorphic patterns.",2017,Knowledge and Information Systems volume 50 issue 2 pp 569-584,molecule mining;automorphism;discrete mathematics;combinatorics;data mining;mathematics;
k-Degree anonymity and edge selection: improving data utility in large networks,Jordi Casas-Roma (Open University of Catalonia);Jordi Herrera-Joancomartí (Autonomous University of Barcelona);Vicenç Torra (University of Skövde);,"147439060,65517402,2616878830","The problem of anonymization in large networks and the utility of released data are considered in this paper. Although there are some anonymization methods for networks, most of them cannot be applied in large networks because of their complexity. In this paper, we devise a simple and efficient algorithm for k-degree anonymity in large networks. Our algorithm constructs a k-degree anonymous network by the minimum number of edge modifications. We compare our algorithm with other well-known k-degree anonymous algorithms and demonstrate that information loss in real networks is lowered. Moreover, we consider the edge relevance in order to improve the data utility on anonymized networks. By considering the neighbourhood centrality score of each edge, we preserve the most important edges of the network, reducing the information loss and increasing the data utility. An evaluation of clustering processes is performed on our algorithm, proving that edge neighbourhood centrality increases data utility. Lastly, we apply our algorithm to different large real datasets and demonstrate their efficiency and practical utility.",2017,Knowledge and Information Systems volume 50 issue 2 pp 447-474,privacy;social network;internet privacy;world wide web;data mining;computer science;
Selective AnDE for large data learning: a low-bias memory constrained approach,"Shenglei Chen (Nanjing Audit University);Ana M. Martínez (Faculty of Information Technology, University Džemal Bijedić of Mostar);Geoffrey I. Webb (Faculty of Information Technology, University Džemal Bijedić of Mostar);Limin Wang (Jilin University);","2164062850,2239008456,2126304162,2699764641","Learning from data that are too big to fit into memory poses great challenges to currently available learning approaches. Averaged n-Dependence Estimators (AnDE) allows for a flexible learning from out-of-core data, by varying the value of n (number of super parents). Hence, AnDE is especially appropriate for learning from large quantities of data. Memory requirement in AnDE, however, increases combinatorially with the number of attributes and the parameter n. In large data learning, number of attributes is often large and we also expect high n to achieve low-bias classification. In order to achieve the lower bias of AnDE with higher n but with less memory requirement, we propose a memory constrained selective AnDE algorithm, in which two passes of learning through training examples are involved. The first pass performs attribute selection on super parents according to available memory, whereas the second one learns an AnDE model with parents only on the selected attributes. Extensive experiments show that the new selective AnDE has considerably lower bias and prediction error relative to A\(n'\)DE, where \(n' = n-1\), while maintaining the same space complexity and similar time complexity. The proposed algorithm works well on categorical data. Numerical data sets need to be discretized first.",2017,Knowledge and Information Systems volume 50 issue 2 pp 475-503,naive bayes classifier;feature selection;data mining;database;pattern recognition;artificial intelligence;machine learning;statistics;computer science;
Developing provenance-aware query systems: an occurrence-centric approach,Eladio Domínguez (University of Zaragoza);Beatriz Pérez 0001 (University of La Rioja);Angel Luis Rubio;María Antonia Zapata (University of Zaragoza);Alberto Allué;Antonio López (University of the Basque Country);,"2136170585,2569812361,2602192323,2150431161,2158878730,2098857825","In recent years, research on provenance has increased exponentially, and such studies in the field of business process monitoring have been especially remarkable. Business process monitoring deals with recording information about the actual execution of processes to then extract valuable knowledge that can be utilized for business process quality improvement. In prior research, we developed an occurrence-centric approach built on our notion of occurrence that provides a holistic perspective of system dynamics. Based on this concept, more complex structures are defined herein, namely Occurrence Base (OcBase) and Occurrence Management System (OcSystem), which serve as scaffolding to develop business process monitoring systems. This paper focuses primarily on the critical provenance task of extracting valuable knowledge from such systems by proposing an Occurrence Query Framework that includes the definition of an Occurrence Base Metamodel and an Occurrence Query Language based on this metamodel. Our framework provides a way of working for the construction of business process monitoring systems that are provenance aware. As a proof of concept, a tool implementing the various components of the framework is presented. This tool has been tested against a real system in the context of biobanks.",2017,Knowledge and Information Systems volume 50 issue 2 pp 661-688,artifact centric business process model;protocol;health;knowledge management;data mining;database;artificial intelligence;programming language;computer science;
Visual domain adaptation via transfer feature learning,Jafar Tahmoresnezhad (Shiraz University);Sattar Hashemi (Shiraz University);,"1651559907,2198869282","One of the serious challenges in computer vision and image classification is learning an accurate classifier for a new unlabeled image dataset, considering that there is no available labeled training data. Transfer learning and domain adaptation are two outstanding solutions that tackle this challenge by employing available datasets, even with significant difference in distribution and properties, and transfer the knowledge from a related domain to the target domain. The main difference between these two solutions is their primary assumption about change in marginal and conditional distributions where transfer learning emphasizes on problems with same marginal distribution and different conditional distribution, and domain adaptation deals with opposite conditions. Most prior works have exploited these two learning strategies separately for domain shift problem where training and test sets are drawn from different distributions. In this paper, we exploit joint transfer learning and domain adaptation to cope with domain shift problem in which the distribution difference is significantly large, particularly vision datasets. We therefore put forward a novel transfer learning and domain adaptation approach, referred to as visual domain adaptation (VDA). Specifically, VDA reduces the joint marginal and conditional distributions across domains in an unsupervised manner where no label is available in test set. Moreover, VDA constructs condensed domain invariant clusters in the embedding representation to separate various classes alongside the domain transfer. In this work, we employ pseudo target labels refinement to iteratively converge to final solution. Employing an iterative procedure along with a novel optimization problem creates a robust and effective representation for adaptation across domains. Extensive experiments on 16 real vision datasets with different difficulties verify that VDA can significantly outperform state-of-the-art methods in image classification problem.",2017,Knowledge and Information Systems volume 50 issue 2 pp 585-605,transfer of learning;data mining;pattern recognition;artificial intelligence;machine learning;computer science;mathematics;
DIDES: a fast and effective sampling for clustering algorithm,Frédéric Ros (University of Orléans);Serge Guillaume (SupAgro);,"2162439603,2306864108","As clustering algorithms become more and more sophisticated to cope with current needs, large data sets of increasing complexity, sampling is likely to provide an interesting alternative. The proposal is a distance-based algorithm: The idea is to iteratively include in the sample the furthest item from all the already selected ones. Density is managed within a postprocessing step, and either low- or high-density areas are considered. The algorithm has some nice properties: insensitive to initialization, data size and noise, it is accurate according to the Rand index and avoids many distance calculations thanks to internal optimization. Moreover, it is driven by only one, meaningful, parameter, called granularity, which impacts the sample size. Compared with concurrent approaches, it proved to be as powerful as the best known methods, with the lowest CPU cost.",2017,Knowledge and Information Systems volume 50 issue 2 pp 543-568,rand index;distance;density;data mining;machine learning;mathematical optimization;statistics;mathematics;
Radius-aware approximate blank node matching using signatures,Christina Lantzaki (University of Crete);Panagiotis Papadakos (University of Crete);Anastasia Analyti (Foundation for Research & Technology – Hellas);Yannis Tzitzikas (University of Crete);,"1093412166,2149125156,2282574431,64415330","In the linked open data cloud, the biggest open data graph that currently exists, a remarkable percentage of data are unnamed resources, also called blank nodes. Several fundamental tasks, such as graph isomorphism checking and RDF data versioning, require computing a map between the sets of blank nodes of two graphs. This map aims at minimizing the delta size, i.e. the number of change operations that are required to make the graphs isomorphic. Computing the optimal map is NP-Hard in the general case, and various approximation algorithms have been proposed. In this work, we propose a novel radius-aware signature-based algorithm that is not restricted to the direct neighborhood of the compared blank nodes. Contrary to the older algorithms, the proposed algorithm manages to decrease the deviation from the optimal solution even for graphs that contain connected blank nodes in large and dense structures. The conducted experiments over real and synthetically generated datasets (including datasets from the Billion Triple Challenge 2012 and 2014) show the significantly smaller deltas. For isomorphism checking (simple RDF equivalence), with a wise configuration of radius, the proposed algorithm achieves optimality for $$100\,\%$$100% of the datasets, while in non-isomorphic datasets the deltas are on average 50---75 % smaller than those of the previous algorithms. Finally, the trade-off between radius, deviation from the optimum and time efficiency is analyzed.",2017,Knowledge and Information Systems volume 50 issue 2 pp 505-542,rdf;equivalence;isomorphism;theoretical computer science;combinatorics;data mining;database;artificial intelligence;machine learning;algorithm;mathematics;
Fuzzy knowledge-based token-ordering policies for bullwhip effect management in supply chains,M. H. Zarandi (Amirkabir University of Technology);F. Shabany Moghadam (Amirkabir University of Technology);,"2735310712,2016281208","The ""Bullwhip Effect"" is a well-known example of supply chain inefficiencies and refers to demand amplification as moving up toward upstream echelons in a supply chain. This paper concentrates on representing a robust token-based ordering policy to facilitate information sharing in supply chains in order to manage the bullwhip effect. Takagi---Sugeno---Kang and hybrid multiple-input single-output fuzzy models are proposed to model the mechanism of token ordering in the token-based ordering policy. The main advantage of proposed fuzzy models is that they eliminate the exogenous and constant variables from the procedure of obtaining the optimal amount of tokens which should be ordered in every period. These fuzzy approaches model the mentioned mechanism through a push---pull policy. A four-echelon SC with fuzzy lead time and unlimited production capacity and inventory is considered to survey the outcomes. Numerical experiments confirm the effectiveness of proposed policies in alleviating BWE, inventory costs and variations.",2017,Knowledge and Information Systems volume 50 issue 2 pp 607-631,bullwhip effect;supply chain management;
False data separation for data security in smart grids,Hao Huang (Wuhan University);Qian Yan (Wuhan University);Yao Zhao (University of Calgary);Wei Lu (Renmin University of China);Zhenguang Liu (National University of Singapore);Zongpeng Li (University of Calgary);,"2587592628,2628006928,2502507093,2673990587,2629926969,2686031815","The smart grid is emerging as an efficient paradigm for electric power generation, transmission, and consumption, based on optimized decision making and control that leverage the measurement data of sensors and meters in the grid. False data injection is a new type of power grid attacks aiming to tamper such important data. For the security and robustness of the grid, it is critical to separate the false data injected by such attacks and recover the original measurement data. Nonetheless, the existing approaches often neglect the true changes on original measurement data that are caused by the real perturbations on grid states and hence have a risk of removing these true changes as injected false data during the data recovery. In this paper, we preserve these true changes by modeling the false data problem as a rank-bounded \(L_1\) norm optimization and propose both offline and online algorithms to filter out the injected false data and recover original measurement data. Trace-driven simulations verify the efficacy of our solution.",2017,Knowledge and Information Systems pp 1-20,data recovery;grassmannian;data security;theoretical computer science;computer security;data mining;statistics;computer science;
Online transfer learning by leveraging multiple source domains,Qingyao Wu (South China University of Technology);Xiaoming Zhou (South China University of Technology);Yuguang Yan (South China University of Technology);Hanrui Wu (South China University of Technology);Huaqing Min (South China University of Technology);,"2116674287,2438421878,2157894861,2573835747,2151120605","Transfer learning aims to enhance performance in a target domain by exploiting useful information from auxiliary or source domains when the labeled data in the target domain are insufficient or difficult to acquire. In some real-world applications, the data of source domain are provided in advance, but the data of target domain may arrive in a stream fashion. This kind of problem is known as online transfer learning. In practice, there can be several source domains that are related to the target domain. The performance of online transfer learning is highly associated with selected source domains, and simply combining the source domains may lead to unsatisfactory performance. In this paper, we seek to promote classification performance in a target domain by leveraging labeled data from multiple source domains in online setting. To achieve this, we propose a new online transfer learning algorithm that merges and leverages the classifiers of the source and target domain with an ensemble method. The mistake bound of the proposed algorithm is analyzed, and the comprehensive experiments on three real-world data sets illustrate that our algorithm outperforms the compared baseline algorithms.",2017,Knowledge and Information Systems pp 1-21,transfer of learning;world wide web;data mining;machine learning;computer science;
An efficient algorithm for mining top-k on-shelf high utility itemsets,"Thu-Lan Dam (Hunan University);Kenli Li (Hunan University);Philippe Fournier-Viger (Harbin Institute of Technology);Quang-Huy Duong (Faculty of Information Technology, University Džemal Bijedić of Mostar);","2301987581,2137759557,165198604,2298748804","High on-shelf utility itemset (HOU) mining is an emerging data mining task which consists of discovering sets of items generating a high profit in transaction databases. The task of HOU mining is more difficult than traditional high utility itemset (HUI) mining, because it also considers the shelf time of items, and items having negative unit profits. HOU mining can be used to discover more useful and interesting patterns in real-life applications than traditional HUI mining. Several algorithms have been proposed for this task. However, a major drawback of these algorithms is that it is difficult for users to find a suitable value for the minimum utility threshold parameter. If the threshold is set too high, not enough patterns are found. And if the threshold is set too low, too many patterns will be found and the algorithm may use an excessive amount of time and memory. To address this issue, we propose to address the problem of top-k on-shelf high utility itemset mining, where the user directly specifies k, the desired number of patterns to be output instead of specifying a minimum utility threshold value. An efficient algorithm named KOSHU (fast top-K on-shelf high utility itemset miner) is proposed to mine the top-k HOUs efficiently, while considering on-shelf time periods of items, and items having positive and/or negative unit profits. KOSHU introduces three novel strategies, named efficient estimated co-occurrence maximum period rate pruning, period utility pruning and concurrence existing of a pair 2-itemset pruning to reduce the search space. KOSHU also incorporates several novel optimizations and a faster method for constructing utility-lists. An extensive performance study on real-life and synthetic datasets shows that the proposed algorithm is efficient both in terms of runtime and memory consumption and has excellent scalability.",2017,Knowledge and Information Systems pp 1-35,data science;data mining;database;computer science;
Prediction of places of visit using tweets,Arun Chauhan (Indian Institute of Technology Roorkee);Krishna Kummamuru (IBM);Durga Toshniwal (Indian Institute of Technology Roorkee);,"2481523251,2243413059,215839793","We study the problem of predicting likely places of visit of users using their past tweets. What people write on their microblogs reflects their intent and desire relating to most of their common day interests. Taking this as a strong evidence, we hypothesize that tweets of the person can also be treated as source of strong indicator signals for predicting their places of visits. In this paper, we propose a novel approach for predicting place of visit within a given geospatial range considering the past tweets and the time of visit. These predictions can be used for generating places recommendation or for promotions. In this approach, we analyze use of various features that can be extracted from the historical tweets—for example, personality traits estimated from the past tweets and the actual words mentioned in the tweets. We performed extensive empirical experiments involving, real data derived from twitter timelines of 4600 persons with multi-label classification as predictive model. The performances of proposed approach outperform the four baselines with accuracy reaching 90 % for top five predictions. Based on our experimental study, we come up with general guidelines on building the prediction model in terms of the type of features extracted from historical tweets, window size of historical tweets and on the optimal radius of query around the place of visit at a given time.",2017,Knowledge and Information Systems volume 50 issue 1 pp 145-166,location based service;information system;internet privacy;world wide web;data mining;database;computer science;
Information warfare: a lightweight matrix-based approach for database recovery,Ramzi A. Haraty (Lebanese American University);Mohamed El Sai (Lebanese American University);,"338706015,2526262343","The age of Internet technology has introduced new types of attacks to new assets that did not exist before. Databases that represent information assets are subject to attacks that have malicious intentions, such as stealing sensitive data, deleting records or violating the integrity of the database. Many counter measures have been designed and implemented to protect the databases and the information they host from attacks. While preventive measures could be overcome and detection measures could detect an attack late after damage has occurred, there is a need for a recovery algorithm that will recover the database to its correct previous state before the attack. Numerous damage assessment and recovery algorithms have been proposed by researchersIn this work, we present an efficient lightweight detection and recovery algorithm that is based on the matrix approach and that can be used to recover from malicious attacks. We compare our algorithm with other approaches and show the performance results.",2017,Knowledge and Information Systems volume 50 issue 1 pp 287-313,recovery;internet privacy;world wide web;computer security;data mining;database;
"Representation and analysis of enterprise models with semantic techniques: an application to ArchiMate, e3value and business model canvas",Artur Caetano (University of Lisbon);Gonçalo Antunes (INESC-ID);João Pombinho (University of Lisbon);Marzieh Bakhshandeh (INESC-ID);José Granjo (INESC-ID);José Luis Borbinha (University of Lisbon);Miguel Mira da Silva (University of Lisbon);,"2132744973,2011432940,2461478935,2167916138,2479245988,1254911949,2295848789","Enterprise models assist the governance and transformation of organizations through the specification, communication and analysis of strategy, goals, processes, information, along with the underlying application and technological infrastructure. Such models cross-cut different concerns and are often conceptualized using domain-specific modelling languages. This paper explores the application of graph-based semantic techniques to specify, integrate and analyse multiple, heterogeneous enterprise models. In particular, the proposal described in this paper (1) specifies enterprise models as ontological schemas, (2) uses transformation mapping functions to integrate the ontological schemas and (3) analyses the integrated schemas with graph querying and logical inference. The proposal is evaluated through a scenario that integrates three distinct enterprise modelling languages: the business model canvas, e3value, and the business layer of the ArchiMate language. The results show, on the one hand, that the graph-based approach is able to handle the specification, integration and analysis of enterprise models represented with different modelling languages and, on the other, that the integration challenge resides in defining appropriate mapping functions between the schemas.",2017,Knowledge and Information Systems volume 50 issue 1 pp 315-346,integrated enterprise modeling;business model canvas;enterprise modelling;enterprise integration;enterprise systems engineering;enterprise information system;ontology;knowledge management;data mining;database;computer science;
Emergency management using geographic information systems: application to the first Romanian traveling salesman problem instance,Gloria Cerasela Crisan (University at Buffalo);Camelia-Mihaela Pintea (Technical University of Cluj-Napoca);Vasile Palade (Coventry University);,"1997410625,2068871680,2690558848","The strategic design of logistic networks, such as roads, railways or mobile phone networks, is essential for efficiently managing emergency situations. The geographic coordinate systems could be used to produce new traveling salesman problem (TSP) instances with geographic information systems (GIS) features. The current paper introduces a recurrent framework designed for building a sequence of instances in a systematic way. The framework intends to model real-life random adverse events manifested on large areas, as massive rainfalls or the arrival of a polar front, or targeted relief supply in early stages of the response. As a proof of concept for this framework, we use the first Romanian TSP instance with the main human settlements, in order to derive several sequences of instances. Nowadays state-of-the-art algorithms for TSP are used to solve these instances. A branch-and-cut algorithm delivers the integer exact solutions, using substantial computing resources. An implementation of the Lin–Kernighan heuristic is used to rapidly find very good near-optimal integer solutions to the same instances. The Lin–Kernighan heuristic shows stability on the tested instances. Further work could be done to better exploit GIS features in order to efficiently tackle operations on large areas and to test the solutions delivered by other algorithms on new instances, derived using the introduced framework.",2017,Knowledge and Information Systems volume 50 issue 1 pp 265-285,emergency management;geographic coordinate system;travelling salesman problem;geomatics;operations research;data mining;artificial intelligence;machine learning;simulation;computer science;
A highly scalable parallel algorithm for maximally informative k-itemset mining,Saber Salah (French Institute for Research in Computer Science and Automation);Reza Akbarinia (French Institute for Research in Computer Science and Automation);Florent Masseglia (French Institute for Research in Computer Science and Automation);,"2117227185,295546380,2440503352","The discovery of informative itemsets is a fundamental building block in data analytics and information retrieval. While the problem has been widely studied, only few solutions scale. This is particularly the case when (1) the data set is massive, calling for large-scale distribution, and/or (2) the length k of the informative itemset to be discovered is high. In this paper, we address the problem of parallel mining of maximally informative k-itemsets (miki) based on joint entropy. We propose PHIKS (Parallel Highly Informative $$\underline{K}$$K?-ItemSet), a highly scalable, parallel miki mining algorithm. PHIKS renders the mining process of large-scale databases (up to terabytes of data) succinct and effective. Its mining process is made up of only two efficient parallel jobs. With PHIKS, we provide a set of significant optimizations for calculating the joint entropies of miki having different sizes, which drastically reduces the execution time, the communication cost and the energy consumption, in a distributed computational platform. PHIKS has been extensively evaluated using massive real-world data sets. Our experimental results confirm the effectiveness of our proposal by the significant scale-up obtained with high itemsets length and over very large databases.",2017,Knowledge and Information Systems volume 50 issue 1 pp 1-26,spark;joint entropy;big data;data science;data mining;database;statistics;computer science;
Boosting for graph classification with universum,"Shirui Pan (University of Technology, Sydney);Jia Wu (University of Technology, Sydney);Xingquan Zhu (Florida Atlantic University);Guodong Long (University of Technology, Sydney);Chengqi Zhang (University of Technology, Sydney);","2132909836,2151584597,2618356905,2140909072,2166080598","Recent years have witnessed extensive studies of graph classification due to the rapid increase in applications involving structural data and complex relationships. To support graph classification, all existing methods require that training graphs should be relevant (or belong) to the target class, but cannot integrate graphs irrelevant to the class of interest into the learning process. In this paper, we study a new universum graph classification framework which leverages additional ""non-example"" graphs to help improve the graph classification accuracy. We argue that although universum graphs do not belong to the target class, they may contain meaningful structure patterns to help enrich the feature space for graph representation and classification. To support universum graph classification, we propose a mathematical programming algorithm, ugBoost, which integrates discriminative subgraph selection and margin maximization into a unified framework to fully exploit the universum. Because informative subgraph exploration in a universum setting requires the search of a large space, we derive an upper bound discriminative score for each subgraph and employ a branch-and-bound scheme to prune the search space. By using the explored subgraphs, our graph classification model intends to maximize the margin between positive and negative graphs and minimize the loss on the universum graph examples simultaneously. The subgraph exploration and the learning are integrated and performed iteratively so that each can be beneficial to the other. Experimental results and comparisons on real-world dataset demonstrate the performance of our algorithm.",2017,Knowledge and Information Systems volume 50 issue 1 pp 53-77,clique width;graph;null model;boosting;supervised learning;data mining;pattern recognition;machine learning;computer science;mathematics;
An adaptive version of k-medoids to deal with the uncertainty in clustering heterogeneous data using an intermediary fusion approach,Aalaa Mojahed (University of East Anglia);Beatriz de la Iglesia (University of East Anglia);,"2683766873,1976212365","This paper introduces Hk-medoids, a modified version of the standard k-medoids algorithm. The modification extends the algorithm for the problem of clustering complex heterogeneous objects that are described by a diversity of data types, e.g. text, images, structured data and time series. We first proposed an intermediary fusion approach to calculate fused similarities between objects, SMF, taking into account the similarities between the component elements of the objects using appropriate similarity measures. The fused approach entails uncertainty for incomplete objects or for objects which have diverging distances according to the different component. Our implementation of Hk-medoids proposed here works with the fused distances and deals with the uncertainty in the fusion process. We experimentally evaluate the potential of our proposed algorithm using five datasets with different combinations of data types that define the objects. Our results show the feasibility of the our algorithm, and also they show a performance enhancement when comparing to the application of the original SMF approach in combination with a standard k-medoids that does not take uncertainty into account. In addition, from a theoretical point of view, our proposed algorithm has lower computation complexity than the popular PAM implementation.",2017,Knowledge and Information Systems volume 50 issue 1 pp 27-52,k medoids;sensor fusion;cluster analysis;uncertainty;data mining;database;artificial intelligence;machine learning;statistics;computer science;
Clinical evidence framework for Bayesian networks,Barbaros Yet (Queen Mary University of London);Zane B. Perkins (Queen Mary University of London);Nigel R. M. Tai (Barts and The London School of Medicine and Dentistry);D. William R. Marsh (Queen Mary University of London);,"2305335316,2213494951,2307513401,1963870346","There is poor uptake of prognostic decision support models by clinicians regardless of their accuracy. There is evidence that this results from doubts about the basis of the model as the evidence behind clinical models is often not clear to anyone other than their developers. In this paper, we propose a framework for representing the evidence-base of a Bayesian network (BN) decision support model. The aim of this evidence framework is to be able to present all the clinical evidence alongside the BN itself. The evidence framework is capable of presenting supporting and conflicting evidence, and evidence associated with relevant but excluded factors. It also allows the completeness of the evidence to be queried. We illustrate this framework using a BN that has been previously developed to predict acute traumatic coagulopathy, a potentially fatal disorder of blood clotting, at early stages of trauma care.",2017,Knowledge and Information Systems volume 50 issue 1 pp 117-143,bayesian network;knowledge engineering;data mining;artificial intelligence;machine learning;computer science;
Rule-based inference and decomposition for distributed in-network processing in wireless sensor networks,Ozgur Sanli (Middle East Technical University);Ibrahim Korpeoglu (Bilkent University);Adnan Yazici (Middle East Technical University);,"2584965172,128899508,2119209520","Wireless sensor networks are application specific and necessitate the development of specific network and information processing architectures that can meet the requirements of the applications involved. A common type of application for wireless sensor networks is the event-driven reactive application, which requires reactive actions to be taken in response to events. In such applications, the interest is in the higher-level information described by complex event patterns, not in the raw sensory data of individual nodes. Although the central processing of information produces the most accurate results, it is not an energy-efficient method because it requires a continuous flow of raw sensor readings over the network. As communication operations are the most expensive in terms of energy usage, the distributed processing of information is indispensable for viable deployments of applications in wireless sensor networks. This method not only helps in reducing the total amount of packets transmitted in the network and the total energy consumed by the sensor nodes, but also produces scalable and fault-tolerant networks. For this purpose, we present two schemes that distribute information processing to appropriate nodes in the network. These schemes use reactive rules, which express relations between event patterns and actions, in order to capture reactive behavior. We also share the results of the performance of our algorithms and the simulations based on our approach that show the success of our methods in decreasing network traffic while still realizing the desired functionality.",2017,Knowledge and Information Systems volume 50 issue 1 pp 231-264,key distribution in wireless sensor networks;visual sensor network;mobile wireless sensor network;wireless sensor network;theoretical computer science;distributed computing;data mining;real time computing;machine learning;computer science;
Correlation analysis techniques for uncertain time series,Mahsa Orang (Concordia University);Nematollaah Shiri (Concordia University);,"2227187283,2207586587","Many applications such as location-based services and wireless sensor networks generate and deal with uncertain time series (UTS), where the ""exact"" value at each timestamp is unknown. Traditional correlation analysis and search techniques developed for standard time series are inadequate for UTS data analysis required in such applications. Motivated by this need, we propose suitable concepts and techniques for UTS correlation analysis. We formalize the notion of normalization and correlation for UTS in two general settings based on the available information at each timestamp: (1) PDF-based UTS (having probability density function) and (2) multiset-based UTS (having multiset of observed values). For each case, we formulate correlation as a random variable and develop techniques to determine the underlying probability density function. For setup (2), we also present probabilistic pruning and sampling techniques to speed up the search process. We conducted numerous experiments to evaluate the performance of the proposed techniques under different configurations using the UCR benchmark datasets. Our results indicate effectiveness of the proposed techniques. For setup (2), in particular, our results show significant improvement in space utilization and computation time. We believe the proposed ideas and solutions lend themselves to powerful tools for UTS analysis and search tasks.",2017,Knowledge and Information Systems volume 50 issue 1 pp 79-116,query optimization;theoretical computer science;data mining;database;artificial intelligence;machine learning;statistics;computer science;mathematics;
Handling failing RDF queries: from diagnosis to relaxation,Géraud Fokou (University of Poitiers);Stéphane Jean (University of Poitiers);Allel Hadjali (University of Poitiers);Mickael Baron (University of Poitiers);,"2106248343,2137727852,2692898027,2609348702","Recent years have witnessed the development of large knowledge bases (KBs). Due to the lack of information about the content and schema semantics of KBs, users are often not able to correctly formulate KB queries that return the intended result. In this paper, we consider the problem of failing RDF queries, i.e., queries that return an empty set of answers. Query relaxation is one cooperative technique proposed to solve this problem. In the context of RDF data, several works proposed query relaxation operators and ranking models for relaxed queries. But none of them tried to find the causes of an RDF query failure given by Minimal Failing Subqueries (MFSs) as well as successful queries that have a maximal number of triple patterns named Ma$$\underline{x}$$x?imal Succeeding Subqueries (XSSs). Inspired by previous work in the context of relational databases and recommender systems, we propose two complementary approaches to fill this gap. The lattice-based approach (LBA) leverages the theoretical properties of MFSs and XSSs to efficiently explore the subquery lattice of the failing query. The matrix-based approach computes a matrix that records alternative answers to the failing query with the triple patterns they satisfy. The skyline of this matrix directly gives the XSSs of the failing query. This matrix can also be used as an index to improve the performance of LBA. The practical interest of these two approaches are shown via a set of experiments conducted on the LUBM benchmark and a comparative study with baseline and related work algorithms.",2017,Knowledge and Information Systems volume 50 issue 1 pp 167-195,range query;rdf query language;sparql;spatial query;semantic web;query language;knowledge base;information retrieval;data mining;database;artificial intelligence;computer science;
Truss decomposition of uncertain graphs,Zhaonian Zou (Harbin Institute of Technology);Rong Zhu (Harbin Institute of Technology);,"2714813144,2715442858","The k-truss of a graph is the largest edge-induced subgraph such that every edge is contained in at least k triangles within the subgraph, where a triangle is a cycle consisting of three vertices. As a new notion of cohesive subgraphs, truss has recently attracted a lot of research attentions in the database and data mining fields. At the same time, uncertainty is an intrinsic property of massive graph data, and truss decomposition (i.e., finding all k-trusses of a graph) has become a key primitive on uncertain graphs. In this paper, we study the truss decomposition problem on uncertain graphs, that is, finding all highly probable k-trusses of an uncertain graph. We first give an formal statement of the truss decomposition problem on uncertain graphs. Then, we prove that the truss decomposition of an uncertain graph attains two elegant properties, namely uniqueness and hierarchy. We show that the truss decomposition of an uncertain graph can be found in $$O(m^{1.5}Q)$$O(m1.5Q) time by proposing an in-memory algorithm called $$\mathtt {TD_{mem}}$$TDmem, where m is the number of edges of the uncertain graph, and Q is at most the maximum number of common neighbors of the endpoints of an edge. When an uncertain graph is too large to fit into main memory, we propose an external-memory algorithm $$\mathtt {TD_{I/O}}$$TDI/O to find the truss decomposition of the uncertain graph. Extensive experiments have been carried out to evaluate the practical performance of the proposed algorithms. The experimental results verify that both $$\mathtt {TD_{mem}}$$TDmem and $$\mathtt {TD_{I/O}}$$TDI/O are efficient when an uncertain graph is small enough to fit into main memory, and that $$\mathtt {TD_{I/O}}$$TDI/O is much faster than $$\mathtt {TD_{mem}}$$TDmem when the graph is too large to fit into main memory.",2017,Knowledge and Information Systems volume 50 issue 1 pp 197-230,factor critical graph;distance hereditary graph;voltage graph;graph factorization;butterfly graph;universal graph;modular decomposition;truss;line graph;discrete mathematics;combinatorics;mathematical optimization;mathematics;
Integrating heterogeneous information via flexible regularization framework for recommendation,Chuan Shi (Beijing University of Posts and Telecommunications);Jian Liu (Beijing University of Posts and Telecommunications);Fuzhen Zhuang (Chinese Academy of Sciences);Philip S. Yu (University of Illinois at Chicago);Bin Wu (Beijing University of Posts and Telecommunications);,"2252461150,2501726924,2050314250,2125104194,2464938123","Recently, there is a surge of social recommendation, which leverages social relations among users to improve recommendation performance. However, in many applications, social relations are very sparse or absent. Meanwhile, the attribute information of users or items may be rich. It is a big challenge to exploit this attribute information for the improvement of recommendation performance. In this paper, we organize objects and relations in recommender system as a heterogeneous information network and introduce meta-path-based similarity measure to evaluate the similarity of users or items. Furthermore, a matrix factorization-based dual regularization framework SimMF is proposed to flexibly integrate different types of information through adopting users' and items' similarities as regularization on latent factors of users and items. Extensive experiments not only validate the effectiveness of SimMF but also reveal some interesting findings. We find that attribute information of users and items can significantly improve recommendation accuracy, and their contribution seems more important than that of social relations. The experiments also reveal that different regularization models have obviously different impacts on users and items.",2016,Knowledge and Information Systems volume 49 issue 3 pp 835-859,matrix decomposition;recommender system;information retrieval;data mining;database;machine learning;computer science;
"Cloud resource provisioning: survey, status and future research directions",Sukhpal Singh (Thapar University);Inderveer Chana (Thapar University);,"2135468612,1273331293","Cloud resource provisioning is a challenging job that may be compromised due to unavailability of the expected resources. Quality of Service (QoS) requirements of workloads derives the provisioning of appropriate resources to cloud workloads. Discovery of best workload---resource pair based on application requirements of cloud users is an optimization problem. Acceptable QoS cannot be provided to the cloud users until provisioning of resources is offered as a crucial ability. QoS parameters-based resource provisioning technique is therefore required for efficient provisioning of resources. This research depicts a broad methodical literature analysis of cloud resource provisioning in general and cloud resource identification in specific. The existing research is categorized generally into various groups in the area of cloud resource provisioning. In this paper, a methodical analysis of resource provisioning in cloud computing is presented, in which resource management, resource provisioning, resource provisioning evolution, different types of resource provisioning mechanisms and their comparisons, benefits and open issues are described. This research work also highlights the previous research, current status and future directions of resource provisioning and management in cloud computing.",2016,Knowledge and Information Systems volume 49 issue 3 pp 1005-1069,thin provisioning;cloud computing;systematic review;autonomic computing;resource management;world wide web;computer network;computer science;
ALDROID: efficient update of Android anti-virus software using designated active learning methods,Nir Nissim (Ben-Gurion University of the Negev);Robert Moskovitch (Columbia University);Oren BarAd (Ben-Gurion University of the Negev);Lior Rokach (Ben-Gurion University of the Negev);Yuval Elovici (Ben-Gurion University of the Negev);,"2306244810,2623583326,2065453009,1979308116,284313219","Many new unknown malwares aimed at compromising smartphones are created constantly. These widely used smartphones are very dependent on anti-virus solutions due to their limited resources. To update the anti-virus signature repository, anti-virus vendors must deal with vast quantities of new applications daily in order to identify new unknown malwares. Machine learning algorithms have been used to address this task, yet they must also be efficiently updated on a daily basis. To improve detection and updatability, we introduce a new framework, ""ALDROID"" and active learning (AL) methods on which ALDROID is based. Our methods are aimed at selecting only new informative applications (benign and especially malicious), thus reducing the labeling efforts of security experts, and enable a frequent and efficient process of enhancing the framework's detection model and Android's anti-virus software. Results indicate that our AL methods outperformed other solutions including the existing AL method and heuristic engine. Our AL methods acquired the largest number and percentage of new malwares, while preserving the detection models' detection capabilities (high TPR and low FPR rates). Specifically, our methods acquired more than double the amount of new malwares acquired by the heuristic engine and 6.5 times more malwares than the existing AL method.",2016,Knowledge and Information Systems volume 49 issue 3 pp 795-833,android;malware;active learning;world wide web;computer security;data mining;database;artificial intelligence;machine learning;computer science;
Detecting emerging and evolving novelties with locally adaptive density ratio estimation,Yun-Qian Miao (University of Waterloo);Ahmed K. Farahat (University of Waterloo);Mohamed S. Kamel (University of Waterloo);,"2156652529,2107050089,2096364092","In today's dynamic environment, there naturally exist two types of novelties: emerging and evolving. Emerging novelties are represented by concepts which are completely different from previously seen instances, while evolving novelties are characterized by relatively new aspects of existing concepts. Most existing algorithms for novelty detection focus on detecting only one type of novelty, giving little or no attention to the other. In real situations, the challenge is that these two types of novelties are not easily distinguishable and sometimes a truly novel concept does not fit perfectly under one of these categories. In this paper, a locally adaptive kernel density ratio method is proposed to capture the two characteristics in one formula. In specific, the density ratio between new and reference data is used to capture evolving novelties, and at the same time, the locally adaptive kernel is employed into the density ratio objective function to capture emerging novelties based on the local neighborhood structure. The effectiveness and robustness of the proposed method are demonstrated in the detection of novel handwritten digits and a set of benchmark novelty detection tasks. Additionally, we further examine its applicability in detecting novelties in social media data, which are characterized by a mixture of emerging and evolving topics over time.",2016,Knowledge and Information Systems volume 49 issue 3 pp 1131-1159,data mining;artificial intelligence;machine learning;mathematics;
Privacy-preserving kernel k-means clustering outsourcing with random transformation,Keng-Pei Lin (National Sun Yat-sen University);,2170960541,"Clustering is a common task for organizing data into clusters. The kernel k-means identifies clusters of nonlinearly separable data by applying the kernel trick to the commonly used k-means clustering to group data in the kernel-induced feature space. Since the kernel k-means is costly in computation due to the quadratic complexity, outsourcing the computations of kernel k-means to external computing service providers can benefit the data owner who has only limited computing resources. However, data privacy is a critical concern in outsourcing since the data may contain sensitive information. Existing works of privacy-preserving outsourcing for general kernel methods based on distance preservation are weak in security. We propose a privacy-preserving outsourcing scheme for the kernel k-means based on the randomly linear transformation and the random perturbation of the kernel matrix. The data sent to the service provider are encrypted, and the service provider solves the kernel k-means from the encrypted data. The proposed scheme is much stronger in security than existing works, and the experimental results show that the proposed privacy-preserving kernel k-means method has similar clustering performance with a normal large-scale kernel k-means algorithm and imposes very little overhead on the data owner.",2016,Knowledge and Information Systems volume 49 issue 3 pp 885-908,tree kernel;outsourcing;kernel method;cluster analysis;theoretical computer science;data mining;database;machine learning;computer science;
Measure prediction capability of data for collaborative filtering,Xijun Liang (China University of Petroleum);Zhonghang Xia (Western Kentucky University);Liping Pang (Dalian University of Technology);Liwei Zhang (Dalian University of Technology);Hongwei Zhang (Dalian University of Technology);,"2113429964,2158884915,2099238136,2661384831,2707603632","Collaborative filtering (CF) approaches have been widely been employed in e-commerce to help users find items they like. Whereas most of existing work focuses on improving algorithmic performance, it is important to know whether the recommendation for users and items can be trustworthy. In this paper, we propose a metric, ""relatedness,"" to measure the potential that a user's preference on an item can be accurately predicted. The relatedness of a user---item pair is determined by a community which consists of users and items most related to the pair. The relatedness is computed by solving a constrained $$\ell _{1}^{}$$l1-regularized least square problem with a generalized homotopy algorithm, and we design the homotopy-based community search algorithm to identify the community by alternately selecting the most related users and items. As an application of the relatedness metric, we develop the data-oriented combination (DOC) method for recommender systems by integrating a group of benchmark CF methods based on the relatedness of user---item pairs. In experimental studies, we examine the effectiveness of the relatedness metric and validate the performance of the DOC method by comparing it with benchmark methods.",2016,Knowledge and Information Systems volume 49 issue 3 pp 975-1004,self determination theory;collaborative filtering;community;data mining;machine learning;mathematical optimization;statistics;computer science;mathematics;
Multi-dimension reviewer credibility quantification across diverse travel communities,Yuanyuan Wang (Shenzhen University);Stephen Chi Fai Chan (Hong Kong Polytechnic University);Hong Va Leong (Hong Kong Polytechnic University);Grace Ngai (Hong Kong Polytechnic University);Norman Au (School of Hotel and Tourism Management);,"2284213101,2167014661,2233290192,2098651954,2637713365","The rapid development of social media technologies enables travellers to share travel experiences and opinions online by posting reviews, which then serve as information source for other travellers. However, the explosive growth of reviews and the proliferation of uninformative, biased or even false information make it very challenging for travellers to find credible information. To help travellers seek credible information, most current work apply mainly qualitative approaches to investigate the credibility of reviews or reviewers. This paper adopts an Impact Index to quantify the credibility of reviewers by simultaneously evaluating the expertise and trustworthiness of reviewers based on the number of reviews posted by them and the number of helpful votes received by those reviews. Furthermore, the Impact Index is enhanced into the Exposure-Impact Index by considering in addition reviewers' breadth of expertise in the form of the number of destinations on which reviewers posted reviews. To examine the effectiveness and applicability of Impact Index and Exposure-Impact Index, this paper evaluates them on several data sets collected from two rather different online travel communities: TripAdvisor, the world's largest travel community, and Qunar, one of the most popular travel communities in China. Experimental results show that both Impact Index and Exposure-Impact Index lead to more consistent results with human judgments than the state-of-the-art method in measuring the credibility of reviewers from diverse communities, manifesting their effectiveness and applicability.",2016,Knowledge and Information Systems volume 49 issue 3 pp 1071-1096,tourism;data mining;
Ensemble constrained Laplacian score for efficient and robust semi-supervised feature selection,Khalid Benabdeslem (University of Lyon);Haytham Elghazel (University of Lyon);Mohammed Hindawi (Zirve University);,"230725257,2291370321,2069767124","In this paper, we propose an efficient and robust approach for semi-supervised feature selection, based on the constrained Laplacian score. The main drawback of this method is the choice of the scant supervision information, represented by pairwise constraints. In fact, constraints are proven to have some noise which may deteriorate learning performance. In this work, we try to override any negative effects of constraint set by the variation of their sources. This is achieved by an ensemble technique using both a resampling of data (bagging) and a random subspace strategy. Experiments on high-dimensional datasets are provided for validating the proposed approach and comparing it with other representative feature selection methods.",2016,Knowledge and Information Systems volume 49 issue 3 pp 1161-1185,feature selection;data mining;pattern recognition;machine learning;computer science;mathematics;
A new approach for training Lagrangian support vector regression,S. Balasundaram (Jawaharlal Nehru University);Yogendra Meena (Jawaharlal Nehru University);,"2688399215,2484475957","In this paper, a novel root finding problem for the Lagrangian support vector regression in 2-norm (LSVR) is formulated in which the number of unknowns becomes the number of training examples. Further, it is proposed to solve it by functional iterative and Newton methods. Under sufficient conditions, we proved their linear rate of convergence. Experiments are performed on a number of synthetic and real-world benchmark datasets, and their results are compared with support vector regression (SVR) and its variants such as least squares SVR and LSVR. Similar generalization performance with improved or comparable learning speed to SVR and its variants demonstrates the usefulness of the proposed formulation solved by the iterative methods.",2016,Knowledge and Information Systems volume 49 issue 3 pp 1097-1129,least squares support vector machine;support vector machine;mathematical analysis;machine learning;mathematical optimization;computer science;mathematics;
On efficiently mining high utility sequential patterns,Jun-Zhe Wang (National Chiao Tung University);Jiun-Long Huang (National Chiao Tung University);Yi-Cheng Chen (Tamkang University);,"2110273631,2148510829,2309950950","High utility sequential pattern mining is an emerging topic in pattern mining, which refers to identify sequences with high utilities (e.g., profits) but probably with low frequencies. To identify high utility sequential patterns, due to lack of downward closure property in this problem, most existing algorithms first generate candidate sequences with high sequence-weighted utilities (SWUs), which is an upper bound of the utilities of a sequence and all its supersequences, and then calculate the actual utilities of these candidates. This causes a large number of candidates since SWU is usually much larger than the real utilities of a sequence and all its supersequences. In view of this, we propose two tight utility upper bounds, prefix extension utility and reduced sequence utility, as well as two companion pruning strategies, and devise HUS-Span algorithm to identify high utility sequential patterns by employing these two pruning strategies. In addition, since setting a proper utility threshold is usually difficult for users, we also propose algorithm TKHUS-Span to identify top-k high utility sequential patterns by using these two pruning strategies. Three searching strategies, guided depth-first search (GDFS), best-first search (BFS) and hybrid search of BFS and GDFS, are also proposed to improve the efficiency of TKHUS-Span. Experimental results on some real and synthetic datasets show that HUS-Span and TKHUS-Span with strategy BFS are able to generate less candidate sequences and thus outperform other prior algorithms in terms of mining efficiency.",2016,Knowledge and Information Systems volume 49 issue 2 pp 597-627,data mining;machine learning;mathematical optimization;mathematics;
Dynamic socialized Gaussian process models for human behavior prediction in a health social network,Yelong Shen (Kent State University);NhatHai Phan (University of Oregon);Xiao Xiao (University of Oregon);Ruoming Jin (Kent State University);Junfeng Sun (National Institutes of Health);Brigitte Piniewski;David Kil;Dejing Dou (University of Oregon);,"2665888088,2147653065,2603860799,2119237514,2634678710,2006322229,2197540355,2040419331","Modeling and predicting human behaviors, such as the level and intensity of physical activity, is a key to preventing the cascade of obesity and helping spread healthy behaviors in a social network. In our conference paper, we have developed a social influence model, named socialized Gaussian process (SGP), for socialized human behavior modeling. Instead of explicitly modeling social influence as individuals' behaviors influenced by their friends' previous behaviors, SGP models the dynamic social correlation as the result of social influence. The SGP model naturally incorporates personal behavior factor and social correlation factor (i.e., the homophily principle: Friends tend to perform similar behaviors) into a unified model. And it models the social influence factor (i.e., an individual's behavior can be affected by his/her friends) implicitly in dynamic social correlation schemes. The detailed experimental evaluation has shown the SGP model achieves better prediction accuracy compared with most of baseline methods. However, a Socialized Random Forest model may perform better at the beginning compared with the SGP model. One of the main reasons is the dynamic social correlation function is purely based on the users' sequential behaviors without considering other physical activity-related features. To address this issue, we further propose a novel ""multi-feature SGP model"" (mfSGP) which improves the SGP model by using multiple physical activity-related features in the dynamic social correlation learning. Extensive experimental results illustrate that the mfSGP model clearly outperforms all other models in terms of prediction accuracy and running time.",2016,Knowledge and Information Systems volume 49 issue 2 pp 455-479,artificial intelligence;machine learning;simulation;
Diffusion archeology for diffusion progression history reconstruction,Emre Sefer (Carnegie Mellon University);Carl Kingsford (Carnegie Mellon University);,"1069722141,2236568161","Diffusion through graphs can be used to model many real-world processes, such as the spread of diseases, social network memes, computer viruses, or water contaminants. Often, a real-world diffusion cannot be directly observed, while it is occurring--perhaps it is not noticed until some time has passed, continuous monitoring is too costly, or privacy concerns limit data access. This leads to the need to reconstruct how the present state of the diffusion came to be from partial diffusion data. Here, we tackle the problem of reconstructing a diffusion history from one or more snapshots of the diffusion state. This ability can be invaluable to learn when certain computer nodes are infected or which people are the initial disease spreaders to control future diffusions. We formulate this problem over discrete-time SEIRS-type diffusion models in terms of maximum likelihood. We design methods that are based on submodularity and a novel Prize Collecting Dominating Set Vertex cover relaxation that can identify likely diffusion steps with some provable performance guarantees. Our methods are the first to be able to reconstruct complete diffusion histories accurately in real and simulated situations. As a special case, they can also identify the initial spreaders better than the existing methods for that problem. Our results for both meme and contaminant diffusion show that the partial diffusion data problem can be overcome with proper modeling and methods, and that hidden temporal characteristics of diffusion can be predicted from limited data.",2016,Knowledge and Information Systems volume 49 issue 2 pp 403-427,social network;diffusion;operations research;artificial intelligence;simulation;computer science;
Query intent inference via search engine log,Di Jiang (Beijing University of Posts and Telecommunications);Lingxiao Yang (Beijing University of Posts and Telecommunications);,"2487323874,2483431567","Mining the latent intents behind search queries is critical for contemporary search engines. Therefore, there has been lots of effort on studying how to infer the intents of search queries via search engine query log. However, the task of query log-based intent inference is not trivial, since it involves cross-disciplinary knowledge of data modeling and data mining. In this paper, we tackle the problem of query intent inference by integrating multiple information sources in a seamless manner. We first propose a comprehensive data model called Search Query Log Structure (SQLS) that represents the relation between search queries via the User dimension, the URL dimension, the Session dimension and the Term dimension. In order to explore the effective ways of using such multidimensional information modeled by SQLS, we survey and compare three frameworks, namely Result-Oriented Framework, Laplacian-Oriented Framework and Topic-Oriented Framework, to infer the intents of search queries. Experimental results show that the three frameworks significantly outperform the state-of-the-art approach and meet the diverse requirements arising from different application scenarios.",2016,Knowledge and Information Systems volume 49 issue 2 pp 661-685,search analytics;beam search;web search query;web query classification;query expansion;search engine;semantic search;information retrieval;data mining;database;computer science;
Approximating behavioral equivalence for scaling solutions of I-DIDs,Yifeng Zeng (Xiamen University);Prashant Doshi (University of Georgia);Yingke Chen (Sichuan University);Yinghui Pan (Jiangxi University of Finance and Economics);Hua Mao (Sichuan University);Muthukumaran Chandrasekaran (University of Georgia);,"2681216690,2137660656,2699151692,2583943217,2165434981,2045476514","Interactive dynamic influence diagram (I-DID) is a recognized graphical framework for sequential multiagent decision making under uncertainty. I-DIDs concisely represent the problem of how an individual agent should act in an uncertain environment shared with others of unknown types. I-DIDs face the challenge of solving a large number of models that are ascribed to other agents. A known method for solving I-DIDs is to group models of other agents that are behaviorally equivalent. Identifying model equivalence requires solving models and comparing their solutions generally represented as policy trees. Because the trees grow exponentially with the number of decision time steps, comparing entire policy trees becomes intractable, thereby limiting the scalability of previous I-DID techniques. In this article, our specific approaches focus on utilizing partial policy trees for comparison and determining the distance between updated beliefs at the leaves of the trees. We propose a principled way to determine how much of the policy trees to consider, which trades off solution quality for efficiency. We further improve on this technique by allowing the partial policy trees to have paths of differing lengths. We evaluate these approaches in multiple problem domains and demonstrate significantly improved scalability over previous approaches.",2016,Knowledge and Information Systems volume 49 issue 2 pp 511-552,influence diagram;multi agent system;discrete mathematics;combinatorics;data mining;artificial intelligence;machine learning;mathematical optimization;computer science;mathematics;
Scalable models for computing hierarchies in information networks,Baoxu Shi (University of Notre Dame);Tim Weninger (University of Notre Dame);,"2222650814,2037649753","Information hierarchies are organizational structures that often used to organize and present large and complex information as well as provide a mechanism for effective human navigation. Fortunately, many statistical and computational models exist that automatically generate hierarchies; however, the existing approaches do not consider linkages in information networks that are increasingly common in real-world scenarios. Current approaches also tend to present topics as an abstract probably distribution over words, etc., rather than as tangible nodes from the original network. Furthermore, the statistical techniques present in many previous works are not yet capable of processing data at Web-scale. In this paper, we present the hierarchical document-topic model (HDTM), which uses a distributed vertex-programming process to calculate a nonparametric Bayesian generative model. Experiments on three medium- size data sets and the entire Wikipedia data set show that HDTM can infer accurate hierarchies even over large information networks.",2016,Knowledge and Information Systems volume 49 issue 2 pp 687-717,topic model;hierarchical clustering;theoretical computer science;data mining;database;artificial intelligence;machine learning;statistics;computer science;
Improving a multi-objective evolutionary algorithm to discover quantitative association rules,M. Martínez-Ballesteros (University of Seville);A. Troncoso (Pablo de Olavide University);F. Martínez-Álvarez (Pablo de Olavide University);J. C. Riquelme (University of Seville);,"736261373,2156326631,1995864619,2106300476","This work aims at correcting flaws existing in multi-objective evolutionary schemes to discover quantitative association rules, specifically those based on the well-known non-dominated sorting genetic algorithm-II (NSGA-II). In particular, a methodology is proposed to find the most suitable configurations based on the set of objectives to optimize and distance measures to rank the non-dominated solutions. First, several quality measures are analyzed to select the best set of them to be optimized. Furthermore, different strategies are applied to replace the crowding distance used by NSGA-II to sort the solutions for each Pareto-front since such distance is not suitable for handling many-objective problems. The proposed enhancements have been integrated into the multi-objective algorithm called MOQAR. Several experiments have been carried out to assess the algorithm's performance by using different configuration settings, and the best ones have been compared to other existing algorithms. The results obtained show a remarkable performance of MOQAR in terms of quality measures.",2016,Knowledge and Information Systems volume 49 issue 2 pp 481-509,association rule learning;evolutionary computation;data mining;machine learning;mathematical optimization;computer science;mathematics;
A feature location approach supported by time-aware weighting of terms associated with developer expertise profiles,Sima Zamani (Information Technology University);Sai Peck Lee (Information Technology University);Ramin Shokripour (Information Technology University);John Anvik (University of Lethbridge);,"2112492276,2518246836,138035632,2190880782","Feature location is a frequent software maintenance activity that aims to identify initial source code location pertinent to a software feature. Most of feature location approaches are based, at least in part, on text analysis methods which originate from the natural language context. However, the natural language context and the text data in software repositories have different properties that reveal the need for adaption of the methods to apply in the context of software repositories. One of the differences is the existence of a set of metadata, such as developer information and time stamp, which is associated with the data in the repositories. However, this difference has not been fully considered in previous feature location research studies. This study proposes a feature location approach that analyzes developer expertise profiles, which contain source code entities modified by the associated software developers, to identify the most similar location pertinent to a desired feature. This approach uses a time-aware term-weighting technique to determine the similarity. An experimental evaluation on four open-source projects shows an improvement in the accuracy, performance, and effectiveness up to 55, 39, and 29 %, respectively, compared to the high-performing information retrieval methods used in feature location. Moreover, the proposed time-aware technique increases the accuracy, performance, and effectiveness of the typical term-weighting technique, tf-idf, as much as 15, 11, and 13 %, respectively. Finally, the proposed approach outperforms our previous approach, noun-based feature location, as much as 17 %. These experimental results demonstrate that time-aware analysis of developers' expertise significantly improves the feature location process.",2016,Knowledge and Information Systems volume 49 issue 2 pp 629-659,feature model;feature;text mining;world wide web;information retrieval;data mining;database;computer science;
Neighborhood system S-approximation spaces and applications,Ali Shakiba (Yazd University);Mohammad Reza Hooshmandasl (Yazd University);,"1975184406,2272479821","In this paper, we will study neighborhood system S-approximation spaces, i.e., combination of S-approximation spaces with identical elements except that they have different knowledge mappings, e.g., the knowledge mappings differ due to different experimental conditions and/or sampling methodology. In such situations, there is a risk of contradictory knowledge sets which can lead to different decisions by the same query. These situations are studied in this paper in detail. Moreover, neighborhood system S-approximation spaces are investigated from a three-way decisions viewpoint with respect to different deciders. In addition, completeness results are shown for optimistic and pessimistic neighborhood system S-approximation spaces, i.e., these constructions can be represented by an ordinary S-approximation space. Also, the concept of knowledge significance is proposed and studied in detail, and we have shown that computing a minimal set of knowledge mappings for a neighborhood system S-approximation space is $${\mathbf {NP}}$$NP-hard. Finally, the paper is concluded by two illustrative medical examples.",2016,Knowledge and Information Systems volume 49 issue 2 pp 749-794,t1 space;discrete mathematics;combinatorics;mathematics;
Fast classification of univariate and multivariate time series through shapelet discovery,Josif Grabocka (University of Hildesheim);Martin Wistuba (University of Hildesheim);Lars Schmidt-Thieme (University of Hildesheim);,"2064655889,2114698921,78243962","Time-series classification is an important problem for the data mining community due to the wide range of application domains involving time-series data. A recent paradigm, called shapelets, represents patterns that are highly predictive for the target variable. Shapelets are discovered by measuring the prediction accuracy of a set of potential (shapelet) candidates. The candidates typically consist of all the segments of a dataset; therefore, the discovery of shapelets is computationally expensive. This paper proposes a novel method that avoids measuring the prediction accuracy of similar candidates in Euclidean distance space, through an online clustering/pruning technique. In addition, our algorithm incorporates a supervised shapelet selection that filters out only those candidates that improve classification accuracy. Empirical evidence on 45 univariate datasets from the UCR collection demonstrates that our method is 3---4 orders of magnitudes faster than the fastest existing shapelet discovery method, while providing better prediction accuracy. In addition, we extended our method to multivariate time-series data. Runtime results over four real-life multivariate datasets indicate that our method can classify MB-scale data in a matter of seconds and GB-scale data in a matter of minutes. The achievements do not compromise quality; on the contrary, our method is even superior to the multivariate baseline in terms of classification accuracy.",2016,Knowledge and Information Systems volume 49 issue 2 pp 429-454,data mining;machine learning;statistics;mathematics;
Compressed $$\text {k}\mathsf {^d}\text {-tree}$$kd-tree for temporal graphs,Diego Caro (University of Concepción);M. Andrea Rodríguez (University of Concepción);Nieves R. Brisaboa (University of A Coruña);Antonio Fariña (University of A Coruña);,"2128151103,2161985520,2224363331,2299966568","Temporal graphs represent vertices and binary relations that change along time. The work in this paper proposes to represent temporal graphs as cells in a 4D binary matrix: two dimensions to represent extreme vertices of an edge and two dimensions to represent the temporal interval when the edge exists. This strategy generalizes the idea of the adjacency matrix for storing static graphs. The proposed structure called Compressed $$\text {k}\mathsf {^d}\text {-tree}$$kd-tree ($$\text {ck}\mathsf {^d}\text {-tree}$$ckd-tree) is capable of dealing with unclustered data with a good use of space. The $$\text {ck}\mathsf {^d}\text {-tree}$$ckd-tree uses asymptotically the same space than the (worst case) lower bound for storing cells in a 4D binary matrix, without considering any regularity. Techniques that group leaves into buckets and compress nodes with few children show to improve the performance in time and space. An experimental evaluation compares the $$\text {ck}\mathsf {^d}\text {-tree}$$ckd-tree with $$\text {k}\mathsf {^d}\text {-tree}$$kd-tree (the d-dimensional extension of the $$\text {k}\mathsf {^2}\text {-tree}$$k2-tree) and with other up-to-date compressed data structures based on inverted indexes and $$\mathsf {Wavelet}\text { Tree}$$WaveletTrees, showing the potential use of the $$\text {ck}\mathsf {^d}\text {-tree}$$ckd-tree for different types of temporal graphs.",2016,Knowledge and Information Systems volume 49 issue 2 pp 553-595,theoretical computer science;discrete mathematics;combinatorics;mathematics;
CPB: a classification-based approach for burst time prediction in cascades,Senzhang Wang (Beihang University);Zhao Yan (Beihang University);Xia Hu (Texas A&M University);Philip S. Yu (University of Illinois at Chicago);Zhoujun Li (Beihang University);Biao Wang (University of International Relations);,"2108200212,2123938954,2161448330,2125104194,2133880114,2310874514","Studying the bursty nature of cascades in social media is practically important in many real applications such as product sales prediction, disaster relief, and stock market prediction. Although both the cascade size prediction and the burst patterns of the cascades have been extensively studied, how to predict when a burst will come remains an open problem. It is challenging for traditional time-series-based models such as regression models to address this task directly. Firstly, times-series-based prediction models focus on predicting the future values based on previously observed ones. It is hard to apply them to predict the time of a bursts with the ""quick rise-and-fall"" pattern. Secondly, besides the cascade popularity, a lot of other side information like user profile and social relation are available in social media. Although the potential utility of such information can be high, it is also hard for time-series-based models to capture and integrate these rich information with diverse formats seamlessly. This paper proposes a classification-based approach for burst time prediction by exploiting rich knowledge in information diffusion. Particularly, we first propose a time-window-based transformation to predict in which time window the burst will appear. By dividing the time spans of all the cascades into the same number of time windows K, the cascades with diverse time spans can thus be handled uniformly. To exploit the rich and heterogenous information in social media, we next propose a scale-independent feature extraction framework to model the heterogenous knowledge in a scale-independent manner. Systematical evaluations are conducted on the Sina Weibo reposting dataset and MemeTracker dataset. Besides the superior performance of the proposed approach, we also observe that: (1) surprisingly, social/structure knowledge is more indicative of the bursts than the cascade popularity information, especially for the bursts occurring in a farther future. (2) Larger cascades are harder to predict as the spreading process of the cascades with higher popularity is usually more diverse and fluctuant. (3) The proposed approach is robust in the sense that the result is not much sensitive to the popularity of the training cascades.",2016,Knowledge and Information Systems volume 49 issue 1 pp 243-271,data mining;artificial intelligence;machine learning;simulation;statistics;
Managing dimensionality in data privacy anonymization,Hessam Zakerzadeh (University of Calgary);Charu C. Aggarwal (IBM);Ken Barker (University of Calgary);,"2226660582,2146335907,2108872182","The curse of dimensionality has remained a challenge for a wide variety of algorithms in data mining, clustering, classification, and privacy. Recently, it was shown that an increasing dimensionality makes the data resistant to effective privacy. The theoretical results seem to suggest that the dimensionality curse is a fundamental barrier to privacy preservation. However, in practice, we show that some of the common properties of real data can be leveraged in order to greatly ameliorate the negative effects of the curse of dimensionality. In real data sets, many dimensions contain high levels of inter-attribute correlations. Such correlations enable the use of a process known as vertical fragmentation in order to decompose the data into vertical subsets of smaller dimensionality. An information-theoretic criterion of mutual information is used in the vertical decomposition process. This allows the use of an anonymization process, which is based on combining results from multiple independent fragments. We present a general approach, which can be applied to the k-anonymity, $$\ell $$l-diversity, and t-closeness models. In the presence of inter-attribute correlations, such an approach continues to be much more robust in higher dimensionality, without losing accuracy. We present experimental results illustrating the effectiveness of the approach. This approach is resilient enough to prevent identity, attribute, and membership disclosure attack.",2016,Knowledge and Information Systems volume 49 issue 1 pp 341-373,privacy;internet privacy;computer security;data mining;database;computer science;
Set containment join revisited,Panagiotis Bouros (Aarhus University);Nikos Mamoulis (University of Hong Kong);Shen Ge (University of Hong Kong);Manolis Terrovitis (Institute for the Management of Information Systems);,"1991808135,18851973,2311362784,34369697","Given two collections of set objects R and S, the $$R\bowtie _{\subseteq }S$$Rź⊆S set containment join returns all object pairs $$(r,s) \in R\times S$$(r,s)źR×S such that $$r\subseteq s$$r⊆s. Besides being a basic operator in all modern data management systems with a wide range of applications, the join can be used to evaluate complex SQL queries based on relational division and as a module of data mining algorithms. The state-of-the-art algorithm for set containment joins ($$\mathtt {PRETTI}$$PRETTI) builds an inverted index on the right-hand collection S and a prefix tree on the left-hand collection R that groups set objects with common prefixes and thus, avoids redundant processing. In this paper, we present a framework which improves $$\mathtt {PRETTI}$$PRETTI in two directions. First, we limit the prefix tree construction by proposing an adaptive methodology based on a cost model; this way, we can greatly reduce the space and time cost of the join. Second, we partition the objects of each collection based on their first contained item, assuming that the set objects are internally sorted. We show that we can process the partitions and evaluate the join while building the prefix tree and the inverted index progressively. This allows us to significantly reduce not only the join cost, but also the maximum memory requirements during the join. An experimental evaluation using both real and synthetic datasets shows that our framework outperforms $$\mathtt {PRETTI}$$PRETTI by a wide margin.",2016,Knowledge and Information Systems volume 49 issue 1 pp 375-402,recursive join;sort merge join;hash join;trie;inverted index;data mining;database;algorithm;computer science;mathematics;
Robust time-series retrieval using probabilistic adaptive segmental alignment,Shahriar Shariat (Rutgers University);Vladimir Pavlovic (Rutgers University);,"2004086288,1969768610","Traditional pairwise sequence alignment is based on matching individual samples from two sequences, under time monotonicity constraints. However, in many application settings, matching subsequences (segments) instead of individual samples may bring in additional robustness to noise or local non-causal perturbations. This paper presents an approach to segmental sequence alignment that jointly segments and aligns two sequences, generalizing the traditional per-sample alignment. To accomplish this task, we introduce a distance metric between segments based on average pairwise distances and then present a modified pair-HMM (PHMM) that incorporates the proposed distance metric to solve the joint segmentation and alignment task. We also propose a relaxation to our model that improves the computational efficiency of the generic segmental PHMM. Our results demonstrate that this new measure of sequence similarity can lead to improved classification performance, while being resilient to noise, on a variety of sequence retrieval problems, from EEG to motion sequence classification.",2016,Knowledge and Information Systems volume 49 issue 1 pp 91-119,segmentation;metric;time series;biological classification;pattern recognition;machine learning;mathematical optimization;statistics;mathematics;
Exploring demographic information in social media for product recommendation,Wayne Xin Zhao (Renmin University of China);Sui Li (Peking University);Yulan He (Aston University);Liwei Wang (Peking University);Ji-Rong Wen (Renmin University of China);Xiaoming Li (Peking University);,"2307999729,2282731649,2124193838,2662753378,2593770520,2633873560","In many e-commerce Web sites, product recommendation is essential to improve user experience and boost sales. Most existing product recommender systems rely on historical transaction records or Web-site-browsing history of consumers in order to accurately predict online users' preferences for product recommendation. As such, they are constrained by limited information available on specific e-commerce Web sites. With the prolific use of social media platforms, it now becomes possible to extract product demographics from online product reviews and social networks built from microblogs. Moreover, users' public profiles available on social media often reveal their demographic attributes such as age, gender, and education. In this paper, we propose to leverage the demographic information of both products and users extracted from social media for product recommendation. In specific, we frame recommendation as a learning to rank problem which takes as input the features derived from both product and user demographics. An ensemble method based on the gradient-boosting regression trees is extended to make it suitable for our recommendation task. We have conducted extensive experiments to obtain both quantitative and qualitative evaluation results. Moreover, we have also conducted a user study to gauge the performance of our proposed recommender system in a real-world deployment. All the results show that our system is more effective in generating recommendation results better matching users' preferences than the competitive baselines.",2016,Knowledge and Information Systems volume 49 issue 1 pp 61-89,social media;e commerce;world wide web;data mining;database;computer science;
Finding desirable objects under group categorical preferences,Nikos Bikakis (National Technical University of Athens);Karim Benouaret (Centre national de la recherche scientifique);Dimitris Sacharidis (Vienna University of Technology);,"1141791694,2695756414,1944603549","Considering a group of users, each specifying individual preferences over categorical attributes, the problem of determining a set of objects that are objectively preferable by all users is challenging on two levels. First, we need to determine the preferable objects based on the categorical preferences for each user, and second, we need to reconcile possible conflicts among users' preferences. A naive solution would first assign degrees of match between each user and each object, by taking into account all categorical attributes, and then for each object combine these matching degrees across users to compute the total score of an object. Such an approach, however, performs two series of aggregation, among categorical attributes and then across users, which completely obscure and blur individual preferences. Our solution, instead of combining individual matching degrees, is to directly operate on categorical attributes and define an objective Pareto-based aggregation for group preferences. Building on our interpretation, we tackle two distinct, but relevant problems: finding the Pareto-optimal objects and objectively ranking objects with respect to the group preferences. To increase the efficiency when dealing with categorical attributes, we introduce an elegant transformation of categorical attribute values into numerical values, which exhibits certain nice properties and allows us to use well-known index structures to accelerate the solutions to the two problems. In fact, experiments on real and synthetic data show that our index-based techniques are an order of magnitude faster than baseline approaches, scaling up to millions of objects and thousands of users.",2016,Knowledge and Information Systems volume 49 issue 1 pp 273-313,recommender system;world wide web;data mining;database;machine learning;computer science;mathematics;
An efficient pruning strategy for approximate string matching over suffix tree,Huan Hu (Harbin Institute of Technology);Hongzhi Wang (Harbin Institute of Technology);Jianzhong Li (Harbin Institute of Technology);Hong Gao (Harbin Institute of Technology);,"2425713852,2589657406,2130201582,2607637236","Approximate string matching over suffix tree with depth-first search (ASM_ST_DFS), a classical algorithm in the field of approximate string matching, was originally proposed by Ricardo A. Baeza-Yates and Gaston H. Gonnet in 1990. The algorithm is one of the most excellent algorithms for approximate string matching if combined with other indexing techniques. However, its time complexity is sensitive to the length of pattern string because it searches $$m+k$$m+k characters on each path from the root before backtracking. In this paper, we propose an efficient pruning strategy to solve this problem. We prove its correctness and efficiency in theory. Particularly, we proved that if the pruning strategy is adopted, it averagely searches O(k) characters on each path before backtracking instead of O(m). Considering each internal node of suffix tree has multiple branches, the pruning strategy should work very well. We also experimentally show that when k is much smaller than m, the efficiency improves hundreds of times, and when k is not much smaller than m, it is still several times faster. This is the first paper that tries to solve the backtracking problem of ASM_ST_DFS in both theory and practice.",2016,Knowledge and Information Systems volume 49 issue 1 pp 121-141,commentz walter algorithm;generalized suffix tree;compressed suffix array;approximate string matching;depth first search;backtracking;string searching algorithm;dynamic programming;theoretical computer science;machine learning;algorithm;computer science;mathematics;
Relationships between knowledge bases and related results,Zhaowen Li (Guangxi University for Nationalities);Yuanyuan Liu (Guangxi University for Nationalities);Qingguo Li (Hunan University);Bin Qin (Guangxi University of Finance and Economics);,"2703838656,2561053605,2102202704,2715636758","Relationships between information systems are very important topics in the field of artificial intelligence. The concept of homomorphisms is an effective mathematical tool to study relationships between information systems. A knowledge base is a special relation information system. This paper investigates invariant characteristics of knowledge bases under the homomorphism. The fact that knowledge bases themselves are invariant and inverse invariant under the homomorphism is firstly proved. Next, some invariant characteristics and inverse invariant characteristics under the homomorphism, such as the dependency of knowledge bases, knowledge reductions, coordinate families and necessary relations, are obtained based on this fact. Finally, lattice characteristics of the dependency of knowledge bases are given. These results will be significant for establishing a framework of granular computing in knowledge bases.",2016,Knowledge and Information Systems volume 49 issue 1 pp 171-195,homomorphism;dependency;lattice;knowledge base;pure mathematics;discrete mathematics;computer science;mathematics;
A high utility itemset mining algorithm based on subsume index,Wei Song (North China University of Technology);Zihan Zhang (North China University of Technology);Jinhong Li (North China University of Technology);,"2712890623,2564333014,2634377196","High utility itemset mining addresses the limitations of frequent itemset mining by introducing measures of interestingness that reflect the significance of an itemset beyond its frequency of occurrence. Among such algorithms, level-wise candidate generation-and-test approaches suffer from the drawbacks of having an immense candidate pool and requiring several database scans. Meanwhile, methods based on pattern growth tend to consume large amounts of memory to store conditional trees. We propose an efficient algorithm, called Index High Utility Itemsets Mine (IHUI-Mine), for application to high utility itemsets. The subsume index, which has been employed to mine frequent itemsets, is extended in IHUI-Mine to the discovery of high utility itemsets. In addition to the enumeration and search strategies inherited from the subsume index, we introduce a new property to specifically accelerate the computation of transaction-weighted utilization for high utility itemsets. Furthermore, given that bitmaps are used for database representation, the real utility of candidates can be verified from the recorded transactions rather than by resorting to the entire database. The computational complexity of IHUI-Mine is analyzed, and tests conducted on publicly available synthetic and real datasets further demonstrate that the proposed algorithm outperforms existing state-of-the-art algorithms.",2016,Knowledge and Information Systems volume 49 issue 1 pp 315-340,data science;data mining;database;computer science;
A scalable approach for detecting plagiarized mobile applications,Ciprian Oprişa (Technical University of Cluj-Napoca);Dragoş Gavriluź (Alexandru Ioan Cuza University);George CabăźU (Technical University of Cluj-Napoca);,"20749283,2588024779,2587824748","Plagiarism cases are quite common in mobile applications ecosystems like the Android market. An application can be decompiled, modified and repackaged with a different author name. The modifications can affect the user's privacy or even contain malicious logic. If the original application is supported by advertisements, they are usually replaced so the ad revenue will go to the repackager. Such events can cause the legitimate author damage both in reputation and financially so they need to be detected. A plagiarism detection system is proposed that can detect plagiarized applications based on the features extracted from code. Two similarity functions are given along with techniques for finding similar applications in a large collection. The main issue with this search is that it cannot be performed sequentially, by comparing a given item with every other item in the collection. The built solution will improve the search time by comparing the searched item only with those with a high probability of being similar. The greatest advantage of our approach is scalability. The system's database can be built, updated and queried in reasonable time, even when large quantities of data are involved. Our experiments were conducted on a large collection of over one million samples and managed to identify a concerning number of plagiarism cases.",2016,Knowledge and Information Systems volume 49 issue 1 pp 143-169,scalability;cluster analysis;internet privacy;world wide web;data mining;database;artificial intelligence;machine learning;statistics;computer science;
Roller: a novel approach to Web information extraction,Patricia Jiménez (ETSI);Rafael Corchuelo (ETSI);,"2116938888,197690615","The research regarding Web information extraction focuses on learning rules to extract some selected information from Web documents. Many proposals are ad hoc and cannot benefit from the advances in machine learning; furthermore, they are likely to fade away as the Web evolves, and their intrinsic assumptions are not satisfied. Some authors have explored transforming Web documents into relational data and then using techniques that got inspiration from inductive logic programming. In theory, such proposals should be easier to adapt as the Web evolves because they build on catalogues of features that can be adapted without changing the proposals themselves. Unfortunately, they are difficult to scale as the number of documents or features increases. In the general field of machine learning, there are propositio-relational proposals that attempt to provide effective and efficient means to learn from relational data using propositional techniques, but they have seldom been explored regarding Web information extraction. In this article, we present a new proposal called Roller: it relies on a search procedure that uses a dynamic flattening technique to explore the context of the nodes that provide the information to be extracted; it is configured with an open catalogue of features, so that it can adapt to the evolution of the Web; it also requires a base learner and a rule scorer, which helps it benefit from the continuous advances in machine learning. Our experiments confirm that it outperforms other state-of-the-art proposals in terms of effectiveness and that it is very competitive in terms of efficiency; we have also confirmed that our conclusions are solid from a statistical point of view.",2016,Knowledge and Information Systems volume 49 issue 1 pp 197-241,web modeling;web intelligence;data science;world wide web;data mining;database;artificial intelligence;machine learning;computer science;
Large-scale supervised similarity learning in networks,Shiyu Chang (Beckman Institute for Advanced Science and Technology);Guo Jun Qi (University of Central Florida);Yingzhen Yang (Beckman Institute for Advanced Science and Technology);Charu C. Aggarwal (IBM);Jiayu Zhou (Michigan State University);Meng Wang (Hefei University of Technology);Thomas S. Huang (Beckman Institute for Advanced Science and Technology);,"2098291119,2237849324,2703393238,2146335907,2097879502,2585926715,2149631809","The problem of similarity learning is relevant to many data mining applications, such as recommender systems, classification, and retrieval. This problem is particularly challenging in the context of networks, which contain different aspects such as the topological structure, content, and user supervision. These different aspects need to be combined effectively, in order to create a holistic similarity function. In particular, while most similarity learning methods in networks such as SimRank utilize the topological structure, the user supervision and content are rarely considered. In this paper, a factorized similarity learning (FSL) is proposed to integrate the link, node content, and user supervision into a uniform framework. This is learned by using matrix factorization, and the final similarities are approximated by the span of low-rank matrices. The proposed framework is further extended to a noise-tolerant version by adopting a hinge loss alternatively. To facilitate efficient computation on large-scale data, a parallel extension is developed. Experiments are conducted on the DBLP and CoRA data sets. The results show that FSL is robust and efficient and outperforms the state of the art. The code for the learning algorithm used in our experiments is available at http://www.ifp.illinois.edu/~chang87/.",2016,Knowledge and Information Systems volume 48 issue 3 pp 707-740,semi supervised learning;data mining;database;pattern recognition;artificial intelligence;machine learning;computer science;
Continuous similarity search for evolving queries,Xiaoning Xu (Fortinet);Chuancong Gao (Simon Fraser University);Jian Pei (Simon Fraser University);Ke Wang (Simon Fraser University);Abdullah Al-Barakati (King Abdulaziz University);,"2567438119,2564339887,2126330539,2626264286,1582813313","In this paper, we study a novel problem of continuous similarity search for evolving queries. Given a set of objects, each being a set or multiset of items, and a data stream, we want to continuously maintain the top-k most similar objects using the last n items in the stream as an evolving query. We show that the problem has several important applications. At the same time, the problem is challenging. We develop a filtering-based method and a hashing-based method. Our experimental results on both real data sets and synthetic data sets show that our methods are effective and efficient.",2016,Knowledge and Information Systems volume 48 issue 3 pp 649-678,nearest neighbor search;information retrieval;data mining;database;machine learning;computer science;
Cost-aware triage ranking algorithms for bug reporting systems,Jin-woo Park (Pohang University of Science and Technology);Mu-Woong Lee (Pohang University of Science and Technology);Jinhan Kim (Pohang University of Science and Technology);Seung-won Hwang (Yonsei University);Sunghun Kim (Hong Kong University of Science and Technology);,"2223782829,2098448934,2102749958,2168667670,2164738181","Bug triaging of deciding whom to fix the bug has been studied actively. However, existing work does not consider varying cost of the same bug over developers with diverse backgrounds and experiences. In clear contrast, we argue the ""cost"" of one bug can be low for one developer, while high for another. Based on this view, we study an automatic triaging system considering both accuracy and cost. Our preliminary solution, CosTriage, models user-specific experiences and estimated cost on each bug category, obtained from topic modeling, and assigns the bug to the developer who not only can, but also is expected to fix fast. For user-specific cost modeling, we are inspired by recommender system work, of estimating user-specific rating of items, e.g., movies. With this view, existing triaging work of categorizing bugs and assigning developers with experiences in the category falls into content-based recommendation (CBR). However, CBR is well known to cause overspecialization because it recommends only the types of bugs that each developer has solved before. This problem is critical because the experienced developers can become overloaded with bugs they hate to fix, though there exist other categories he can fix faster. CosTriage adopts content-boosted collaborative filtering (CBCF), considering not only similar bugs (content-based) but similar developers (collaborative) for estimating user-specific cost. In this paper, we extend to include special scenarios. First, bug may not have textual report (e.g., crash report) or textual report may lack a topic word (e.g., 1957 of 48,424 in Mozilla reports) Mozilla reports. Second, in some scenarios, developer profiles may change over time. For these scenarios, we extend CosTriage to support non-textual description and dynamic profiles, which we denote as CosTriage+. Our experimental evaluation shows that our solution reduces the cost efficiently by 30 % without seriously compromising accuracy in comparison with the baseline only considering accuracy.",2016,Knowledge and Information Systems volume 48 issue 3 pp 679-705,ranking;user modeling;world wide web;data mining;database;statistics;computer science;
Mining contentious documents,Amine Trabelsi (University of Alberta);Osmar R. Zaïane (University of Alberta);,"2160139293,2308328903","This work proposes an unsupervised method intended to enhance the quality of opinion mining in contentious text. It presents a Joint Topic Viewpoint (JTV) probabilistic model to analyze the underlying divergent arguing expressions that may be present in a collection of contentious documents. It extends the original Latent Dirichlet Allocation, which makes it domain and thesaurus independent, e.g., does not rely on WordNet coverage. The conceived JTV has the potential of automatically carrying the tasks of extracting associated terms denoting an arguing expression, according to the hidden topics it discusses and the embedded viewpoint it voices. Furthermore, JTV's structure enables the unsupervised grouping of obtained arguing expressions according to their viewpoints, using a constrained clustering approach. Experiments are conducted on three types of contentious documents: polls, online debates and editorials. The qualitative and quantitative analyses of the experimental results show the effectiveness of our model to handle six different contentious issues when compared to a state-of-the-art method. Moreover, the ability to automatically generate distinctive and informative patterns of arguing expressions is demonstrated. Furthermore, the coherence of these arguing expressions is proved to be of a high quality when evaluated on the basis of recently introduced automatic coherence measure.",2016,Knowledge and Information Systems volume 48 issue 3 pp 537-560,sentiment analysis;data science;data mining;database;artificial intelligence;machine learning;computer science;
A survey of the dendritic cell algorithm,Zeineb Chelly (Institut Supérieur de Gestion);Zied Elouedi (Institut Supérieur de Gestion);,"853691073,1969625331","The dendritic cell algorithm (DCA) is a classification algorithm based on the functioning of natural immune dendritic cells. Recently, the DCA has caught the attention of researchers due to its worthy characteristics as it exhibits several interesting and potentially beneficial features for binary classification problems. Although the studies related with the DCA are increasingly becoming popular giving rise to several DCA hybrid algorithms, according to our best knowledge, there is no study summarizing the basic features of these algorithms nor their application areas all in one paper. Therefore, this study aims at summarizing the powerful characteristics of the DCA as well as making a general review of it. In addition, the DCA hybrid algorithms are reviewed and open research areas are discussed for further research.",2016,Knowledge and Information Systems volume 48 issue 3 pp 505-535,biological classification;evolutionary computation;bioinformatics;artificial intelligence;machine learning;computer science;
Information diffusion in a multi-social-network scenario: framework and ASP-based analysis,Giuseppe Marra;Domenico Ursino;Francesco Ricca (University of Calabria);Giorgio Terracina (University of Calabria);,"2722876585,2040027413,2120918653,2071483913","Information diffusion is a classical problem in social network analysis, which has been largely investigated with reference to single social networks. However, the current scenario is multi-social-network. Here, many social networks coexist and are strictly connected to each other, thanks to those users who join more social networks, acting as bridges among them. But, what happens to information diffusion when passing from a single-social-network context to a multi-social-network scenario? In this paper, we answer this question. In particular, thanks to the definition of a framework for handling these issues and to a large set of experiments, we show that, in this context, new actors and new features play the key roles. We also identify two possible improvements of our framework, namely the management of some ""activation nucleuses"" (i.e., some starting-node configurations that are likely to improve information diffusion) and the management of topics concerning the information to spread. In these activities, answer set programming provided us with a powerful and flexible tool for an easy setup and implementation of our investigation.",2016,Knowledge and Information Systems volume 48 issue 3 pp 619-648,answer set programming;social network analysis;logic programming;theoretical computer science;knowledge management;data mining;database;artificial intelligence;machine learning;programming language;computer science;
Top-K Miner: top-K identical frequent itemsets discovery without user support threshold,Saif-Ur-Rehman (Kohat University of Science and Technology);Jawad Ashraf (Kohat University of Science and Technology);Asad Habib (Kohat University of Science and Technology);Abdus Salam (Abasyn University);,"2569353346,2207094124,2130042769,2111928468","Frequent itemsets (FIs) mining is a prime research area in association rule mining. The customary techniques find FIs or its variants on the basis of either support threshold value or by setting two generic parameters, i.e., N (topmost itemsets) and $$K_\mathrm{{max}}$$Kmax (size of the itemsets). However, users are unable to mine the absolute desired number of patterns because they tune these approaches with their approximate parameters settings. We proposed a novel technique, top-K Miner that does not require setting of support threshold, N and $$K_\mathrm{{max}}$$Kmax values. Top-K Miner requires the user to specify only a single parameter, i.e., K to find the desired number of frequent patterns called identical frequent itemsets (IFIs). Top-K Miner uses a novel candidate production algorithm called join-FI algorithm. This algorithm uses frequent 2-itemsets to yield one or more candidate itemsets of arbitrary size. The join-FI algorithm follows bottom-up recursive technique to construct candidate-itemsets-search tree. Finally, the generated candidate itemsets are manipulated by the Maintain-Top-K_List algorithm to produce Top-K_List of the IFIs. The proposed top-K Miner algorithm significantly outperforms the generic benchmark techniques even when they are running with the ideal parameters settings.",2016,Knowledge and Information Systems volume 48 issue 3 pp 741-762,association rule learning;data science;data mining;database;machine learning;computer science;mathematics;
Models of time series with time granulation,Rami Al-Hmouz (King Abdulaziz University);Witold Pedrycz (King Abdulaziz University);,"224408075,2484732729","Albeit simple and easy to interpret, a piecewise representation of time series comes with discontinuities that inevitably lead to substantial representation (approximation) error. In this study, we present models of time series with time granulation that reduce representation errors and subsequently give rise to the better approximation abilities and classification rates of classifiers of time series. The jumps (discontinuities) occurring because of the local piecewise representation of time series over disjoint time windows are eliminated due to the use of fuzzy sets describing overlapping time segments (temporal windows). We engage particle swarm optimization (PSO) as an optimization vehicle to minimize the representation error based on the adjustments of the length of the segments and the degree of overlap among membership functions of the temporal windows. We also consider PSO to minimize the classification error of classifiers of time series. In a series of experiments, we consider two commonly used piecewise techniques of approximation of time series such as piecewise aggregate approximation (PAA) and piecewise linear representation (PLR). The results demonstrate that PLR models produce lower approximation errors in comparison with those obtained for the PAA representation and quantify an impact of fuzzy temporal segmentation on the overall quality of the model. Results of comprehensive comparative studies are provided as well.",2016,Knowledge and Information Systems volume 48 issue 3 pp 561-580,time series;discrete mathematics;machine learning;mathematical optimization;statistics;mathematics;
Privacy-concerned multiagent planning,Jan ToźIăźKa (Czech Technical University in Prague);Jan JakubźV (Czech Technical University in Prague);Antonín Komenda (Czech Technical University in Prague);Michal PĕchouăźEk (Czech Technical University in Prague);,"2567138184,2550461605,2017738022,2563600993","Coordinated sequential decision making of a team of cooperative agents can be described by principles of multiagent planning. Provided that the mechanics of the environment the agents act in is described as a deterministic transitions system, an appropriate planning model is MA-Strips. Multiagent planning modeled as MA-Strips prescribes exactly what information has to be kept private and which information can be communicated in order to coordinate toward shared or individual goals. We propose a multiagent planning approach which combines compilation for a classical state-of-the-art planner together with a compact representation of local plans in the form of finite-state machines. Proving soundness and completeness of the approach, the planner efficiency is further boosted up using distributed delete-relaxation heuristics and using an approximative local plan analysis. We experimentally evaluate applicability of our approach in full privacy setting where only public information can be communicated. We analyze properties of standard multiagent benchmarks from the perspective of classification of private and public information. We show that our approach can be used with different privacy settings and that it outperforms state-of-the-art planners designed directly for particular privacy classification.",2016,Knowledge and Information Systems volume 48 issue 3 pp 581-618,finite state machine;knowledge management;data mining;database;artificial intelligence;machine learning;computer science;
Inferring lockstep behavior from connectivity pattern in large graphs,Meng Jiang (Tsinghua University);Peng Cui (Tsinghua University);Alex Beutel (Carnegie Mellon University);Christos Faloutsos (Carnegie Mellon University);Shiqiang Yang (Tsinghua University);,"2115305989,2113115369,2045447989,2198983026,2127183023","Given multimillion-node graphs such as ""who-follows-whom"", ""patent-cites-patent"", ""user-likes-page"" and ""actor/director-makes-movie"" networks, how can we find unexpected behaviors? When companies operate on the graphs with monetary incentives to sell Twitter ""Followers"" and Facebook page ""Likes"", the graphs show strange connectivity patterns. In this paper, we study a complete graph from a large Twitter-style social network, spanning up to 3.33 billion edges. We report strange deviations from typical patterns like smooth degree distributions. We find that such deviations are often due to ""lockstep behavior"" that large groups of followers connect to the same groups of followees. Our first contribution is that we study strange patterns on the adjacency matrix and in the spectral subspaces with respect to several flavors of lockstep. We discover that (a) the lockstep behaviors on the graph shape dense ""block"" in its adjacency matrix and creates ""rays"" in spectral subspaces, and (b) partially overlapping of the behaviors shape ""staircase"" in its adjacency matrix and creates ""pearls"" in spectral subspaces. The second contribution is that we provide a fast algorithm, using the discovery as a guide for practitioners, to detect users who offer the lockstep behaviors in undirected/directed/bipartite graphs. We carry out extensive experiments on both synthetic and real datasets, as well as public datasets from IMDb and US Patent. The results demonstrate the scalability and effectiveness of our proposed algorithm.",2016,Knowledge and Information Systems volume 48 issue 2 pp 399-428,discrete mathematics;combinatorics;data mining;artificial intelligence;machine learning;mathematics;
Word network topic model: a simple but general solution for short and imbalanced texts,Yuan Zuo (Beihang University);Jichang Zhao (Beihang University);Ke Xu (Beihang University);,"2719789531,2153657934,2600070012","The short text has been the prevalent format for information of Internet, especially with the development of online social media. Although sophisticated signals delivered by the short text make it a promising source for topic modeling, its extreme sparsity and imbalance bring unprecedented challenges to conventional topic models like LDA and its variants. Aiming at presenting a simple but general solution for topic modeling in short texts, we present a word co-occurrence network-based model named WNTM to tackle the sparsity and imbalance simultaneously. Different from previous approaches, WNTM models the distribution over topics for each word instead of learning topics for each document, which successfully enhances the semantic density of data space without importing too much time or space complexity. Meanwhile, the rich contextual information preserved in the word---word space also guarantees its sensitivity in identifying rare topics with convincing quality. Furthermore, employing the same Gibbs sampling as LDA makes WNTM easily to be extended to various application scenarios. Extensive validations on both short and normal texts testify the outperformance of WNTM as compared to baseline methods. And we also demonstrate its potential in precisely discovering newly emerging topics or unexpected events in Weibo at pretty early stages.",2016,Knowledge and Information Systems volume 48 issue 2 pp 379-398,topic model;data science;data mining;database;artificial intelligence;machine learning;computer science;
CloFAST: closed sequential pattern mining using sparse and vertical id-lists,Fabio Fumarola (University of Bari);Pasqua Fabiana Lanotte (University of Bari);Michelangelo Ceci (University of Bari);Donato Malerba (University of Bari);,"1137343302,1977118810,2114873949,2360612151","Sequential pattern mining is a computationally challenging task since algorithms have to generate and/or test a combinatorially explosive number of intermediate subsequences. In order to reduce complexity, some researchers focus on the task of mining closed sequential patterns. This not only results in increased efficiency, but also provides a way to compact results, while preserving the same expressive power of patterns extracted by means of traditional (non-closed) sequential pattern mining algorithms. In this paper, we present CloFAST, a novel algorithm for mining closed frequent sequences of itemsets. It combines a new data representation of the dataset, based on sparse id-lists and vertical id-lists, whose theoretical properties are studied in order to fast count the support of sequential patterns, with a novel one-step technique both to check sequence closure and to prune the search space. Contrary to almost all the existing algorithms, which iteratively alternate itemset extension and sequence extension, CloFAST proceeds in two steps. Initially, all closed frequent itemsets are mined in order to obtain an initial set of sequences of size 1. Then, new sequences are generated by directly working on the sequences, without mining additional frequent itemsets. A thorough performance study with both real-world and artificially generated datasets empirically proves that CloFAST outperforms the state-of-the-art algorithms, both in time and memory consumption, especially when mining long closed sequences.",2016,Knowledge and Information Systems volume 48 issue 2 pp 429-463,sequential pattern mining;data mining;pattern recognition;machine learning;computer science;
Performance evaluation of word-aligned compression methods for bitmap indices,Gheorghi Guzun (University of Iowa);Guadalupe Canahuate (University of Iowa);,"278146597,2291962010","Bitmap indices are a widely used scheme for large read-only repositories in data warehouses and scientific databases. This binary representation allows the use of bit-wise operations for fast query processing and is typically compressed using run-length encoding techniques. Most bitmap compression techniques are aligned using a fixed encoding length (32 or 64 bits) to avoid explicit decompression during query time. They have been proposed to extend or enhance word-aligned hybrid (WAH) compression. This paper presents a comparative study of four bitmap compression techniques: WAH, PLWAH, CONCISE, and EWAH. Experiments are targeted to identify the conditions under which each method should be applied and quantify the overhead incurred during query processing. Performance in terms of compression ratio and query time is evaluated over synthetic-generated bitmap indices, and results are validated over bitmap indices generated from real data sets. Different query optimizations are explored, query time estimation formulas are defined, and the conditions under which one method should be preferred over another are formalized.",2016,Knowledge and Information Systems volume 48 issue 2 pp 277-304,bitmap index;data warehouse;theoretical computer science;data mining;database;computer science;
Comparison of different weighting schemes for the kNN classifier on time-series data,Zoltan Geler (University of Novi Sad);Vladimir Kurbalija (University of Novi Sad);Miloš Radovanović (University of Novi Sad);Mirjana Ivanović (University of Novi Sad);,"526279058,159591547,2165712131,2027550000","Many well-known machine learning algorithms have been applied to the task of time-series classification, including decision trees, neural networks, support vector machines and others. However, it was shown that the simple 1-nearest neighbor (1NN) classifier, coupled with an elastic distance measure like Dynamic Time Warping (DTW), often produces better results than more complex classifiers on time-series data, including k-nearest neighbor (kNN) for values of $$k>1$$k>1. In this article, we revisit the kNN classifier on time-series data by considering ten classic distance-based vote weighting schemes in the context of Euclidean distance, as well as four commonly used elastic distance measures: DTW, Longest Common Subsequence, Edit Distance with Real Penalty and Edit Distance on Real sequence. Through experiments on the complete collection of UCR time-series datasets, we confirm the view that the 1NN classifier is very hard to beat. Overall, for all considered distance measures, we found that variants of the Dudani weighting scheme produced the best results.",2016,Knowledge and Information Systems volume 48 issue 2 pp 331-378,nearest neighbor search;k nearest neighbors algorithm;time series;biological classification;data mining;pattern recognition;machine learning;statistics;computer science;mathematics;
An entropy-based clustering ensemble method to support resource allocation in business process management,Weidong Zhao (Fudan University);Haitao Liu (Fudan University);Weihui Dai (Fudan University);Jian Ma (City University of Hong Kong);,"2672471270,2710616989,2648139515,2302734485","Resource allocation, as a crucial task of business process management, has been widely acknowledged by its importance for process performance improvement. Although some methods have been proposed to support resource allocation, there is little effort to allocate resources from the task preference perspective. This paper proposes a novel mechanism in which resource allocation is considered as a multi-criteria decision problem and solved by a new entropy-based clustering ensemble approach. By mining resource characteristics and task preference patterns from past process executions, the ""right"" resources could be recommended to improve resource utility. Further, to support dynamic resource allocation in the context of multiple process instances running concurrently, a heuristic method is devised to deal with resource conflicts caused by the interplay between various instances. The effectiveness of this study is evaluated with a real-life scenario, and the simulation results indicate that resource utility can be improved and resource workload can be balanced with the support of resource recommendation.",2016,Knowledge and Information Systems volume 48 issue 2 pp 305-330,human resource management system;process mining;business process management;resource allocation;management science;knowledge management;data mining;database;computer science;
Local search and pseudoinversion: an hybrid approach to neural network training,Luca Rubini (University of Turin);Rossella Cancelliere (University of Turin);Patrick Gallinari (Pierre-and-Marie-Curie University);Andrea Grosso (University of Turin);,"2266374909,2095045415,2235456028,2115739611","We consider recent successful techniques proposed for neural network training that set randomly the weights from input to hidden layer, while weights from hidden to output layer are analytically determined by Moore---Penrose generalized inverse. This study aimed to analyse the impact on performances when the completely random sampling of the space of input weights is replaced by a local search procedure over a discretized set of weights. The performances of the proposed training methods are assessed through computational experience on several UCI datasets.",2016,Knowledge and Information Systems volume 48 issue 2 pp 493-503,local search;information system;artificial neural network;theoretical computer science;data mining;artificial intelligence;machine learning;computer science;
IRAFCA: an O(n) information retrieval algorithm based on formal concept analysis,Fethi Fkih (University of Monastir);Mohamed Nazih Omri (University of Monastir);,"18190479,1942823723","With the exponential increase in the quantity of information circulating on the Internet, an evolution of information-retrieval systems becomes paramount. Indeed, current approaches for information systems design remain unable to meet the needs of users, either in performance (precision and recall) or response time. In this paper, we propose a new information-retrieval algorithm based on formal concept analysis. The proposed algorithm deals with disjunctive and conjunctive queries. In fact, information retrieval is a direct application of the formal concept analysis (FCA). This makes the adaptation of this theory to this field an easy and intuitive task. In this context, we exploited the theoretical basis provided by the FCA to design an efficient and flexible approach for information retrieval.",2016,Knowledge and Information Systems volume 48 issue 2 pp 465-491,human computer information retrieval;concept search;conjunctive query;formal concept analysis;theoretical computer science;information retrieval;data mining;database;machine learning;computer science;
Phoneme sequence recognition via DTW-based classification,Hossein Hamooni (University of New Mexico);Abdullah Mueen (University of New Mexico);Amy Neel (University of New Mexico);,"308734036,2083987245,2234469311","Phonemes are the smallest units of sound produced by a human being. Automatic classification of phonemes is a well-researched topic in linguistics due to its potential for robust speech recognition. With the recent advancement of phonetic segmentation algorithms, it is now possible to generate datasets of millions of phonemes automatically. Phoneme classification on such datasets is a challenging data mining task because of the large number of classes (over a hundred) and complexities of the existing methods. In this paper, we introduce the phoneme classification problem as a data mining task. We propose a dual-domain (time and frequency) hierarchical classification algorithm. Our method uses a dynamic time warping (DTW)-based classifier in the top layers and time---frequency features in the lower layer. We cross-validate our method on phonemes from three online dictionaries and achieved up to 35 % improvement in classification compared with existing techniques. We further modify our vowel classifier by adopting DTW distance over time---frequency coefficients and gain an additional 3 % improvement. We provide case studies on classifying accented phonemes and speaker-invariant phoneme classification. Finally, we show a demonstration of how phoneme classification can be used to recognize speech.",2016,Knowledge and Information Systems volume 48 issue 2 pp 253-275,big data;natural language processing;speech recognition;data mining;pattern recognition;computer science;
A GPU-oriented online recommendation algorithm for efficient processing of time-varying continuous data streams,Chandima HewaNadungodage (Purdue University);Yuni Xia (Purdue University);John Jaehwan Lee (Purdue University College of Engineering);,"2491573301,2334154954,2668789577","Research on recommendation systems has gained a considerable amount of attention over the past decade as the number of online users and online contents continue to grow at an exponential rate. With the evolution of the social web, people generate and consume data in real time using online services such as Twitter, Facebook, and web news portals. With the rapidly growing online community, web-based retail systems and social media sites have to process several millions of user requests per day. Generating quality recommendations using this vast amount of data is itself a very challenging task. Nevertheless, opposed to the web-based retailers such as Amazon and Netflix, the above-mentioned social networking sites have to face an additional challenge when generating recommendations as their contents are very rapidly changing. Therefore, providing fresh information in the least amount of time is a major objective of such recommender systems. Although collaborative filtering is a widely used technique in recommendation systems, generating the recommendation model using this approach is a costly task, and often done offline. Hence, it is difficult to use collaborative filtering in the presence of dynamically changing contents, as such systems require frequent updates to the recommendation model to maintain the accuracy and the freshness of the recommendations. Parallel processing power of graphic processing units (GPUs) can be used to process large volumes of data with dynamically changing contents in real time, and accelerate the recommendation process for social media data streams. In this paper, we address the issue of rapidly changing contents, and propose a parallel on-the-fly collaborative filtering algorithm using GPUs to facilitate frequent updates to the recommendations model. We use a hybrid similarity calculation method by combining the item–item collaborative filtering with item category information and temporal information. The experimental results on real-world datasets show that the proposed algorithm outperformed several existing online CF algorithms in terms of accuracy, memory consumption, and runtime. It was also observed that the proposed algorithm scaled well with the data rate and the data volume, and generated recommendations in a timely manner.",2016,Knowledge and Information Systems pp 1-34,social media;collaborative filtering;data stream mining;world wide web;data mining;database;artificial intelligence;machine learning;computer science;
Multi-type clustering in heterogeneous information networks,Wangqun Lin (University of Illinois at Chicago);Philip S. Yu (University of Illinois at Chicago);Yuchen Zhao (University of Illinois at Chicago);Bo Deng;,"2118475452,2125104194,2147878578,2496928123","Heterogeneous information networks have drawn much attention in recent years due to their significant applications, such as text mining, e-commerce, social networks, and bioinformatics. Clustering different types of objects simultaneously based upon not only their relations of the same type, but also the relations between different types of objects can improve the clustering quality mutually. In this paper, we propose a general model, in which both the homogeneous and heterogeneous relations are considered simultaneously, to describe the structure of the heterogeneous information networks and devise a novel parametric free multi-type overlapped clustering approach. In this model, different types of relations between different types of objects are represented by a group of matrices. In this way, we transfer the multi-type clustering problem into the information compression problem. Subsequently, greedy search approaches, which aim at describing the group of relational matrices with least bits, are proposed. Moreover, by discovering the discriminative clusters among different types of objects, we devise effective parameter-free strategies to discover either overlapping or non-overlapping structure among different types of clusters. Extensive experiments on real-world and synthetic data sets demonstrate our methods are effective and efficient.",2016,Knowledge and Information Systems volume 48 issue 1 pp 143-178,flame clustering;brown clustering;correlation clustering;constrained clustering;fuzzy clustering;cluster;cluster analysis;consensus clustering;theoretical computer science;data mining;machine learning;computer science;mathematics;
A compression method of double-array structures using linear functions,Shunsuke Kanda (University of Tokushima);Masao Fuketa (University of Tokushima);Kazuhiro Morita (University of Tokushima);Jun-ichi Aoe (University of Tokushima);,"2264336447,2059996369,2171470526,2037412677","A trie is one of the data structures for keyword search algorithms and is utilized in natural language processing, reserved words search for compilers and so on. The double-array and LOUDS are efficient representation methods for the trie. The double-array provides fast traversal at time complexity of O(1), but the space usage of the double-array is larger than that of LOUDS. LOUDS is a succinct data structure with bit-string, and its space usage is extremely compact. However, its traversal speed is not so fast. This paper presents a new compression method of the double-array with keeping the retrieval speed. Our new method compresses the double-array by dividing the double-array into blocks and by using linear functions. Experimental results for varied keywords show that our new method reduced space usage of the double-array up to about 44 %, and the retrieval speed of the new method was 9---14 times faster than that of LOUDS. Moreover, the results show that the construction speed of the new method was faster than that of the conventional method for a large keyword set.",2016,Knowledge and Information Systems volume 48 issue 1 pp 55-80,trie;theoretical computer science;database;algorithm;computer science;
Process monitoring using maximum sequence divergence,Yihuang Kang (National Sun Yat-sen University);Vladimir Zadorozhny (University of Pittsburgh);,"2573209722,179379646","Process monitoring involves tracking a system's behaviors, evaluating the current state of the system, and discovering interesting events that require immediate actions. In this paper, we consider monitoring temporal system state sequences to help detect the changes of dynamic systems, check the divergence of the system development, and evaluate the significance of the deviation. We begin with discussions of data reduction, symbolic data representation, and anomaly detection in temporal discrete sequences. Time-series representation methods are also discussed and used in this paper to discretize raw data into sequences of system states. Markov chains and stationary-state distributions are continuously generated from temporal sequences to represent snapshots of the system dynamics in different time frames. We use generalized Jensen---Shannon divergence as the measure to monitor changes of the stationary symbol probability distributions and evaluate the significance of system deviations. We prove that the proposed approach is able to detect deviations of the systems we monitor and assess the deviation significance in probabilistic manner.",2016,Knowledge and Information Systems volume 48 issue 1 pp 81-109,process mining;anomaly detection;information theory;data science;data mining;machine learning;statistics;computer science;mathematics;
Transfer learning for class imbalance problems with inadequate data,Samir Al-Stouhi (Honda);Chandan K. Reddy (Wayne State University);,"239452622,2100435683","A fundamental problem in data mining is to effectively build robust classifiers in the presence of skewed data distributions. Class imbalance classifiers are trained specifically for skewed distribution datasets. Existing methods assume an ample supply of training examples as a fundamental prerequisite for constructing an effective classifier. However, when sufficient data are not readily available, the development of a representative classification algorithm becomes even more difficult due to the unequal distribution between classes. We provide a unified framework that will potentially take advantage of auxiliary data using a transfer learning mechanism and simultaneously build a robust classifier to tackle this imbalance issue in the presence of few training samples in a particular target domain of interest. Transfer learning methods use auxiliary data to augment learning when training examples are not sufficient and in this paper we will develop a method that is optimized to simultaneously augment the training data and induce balance into skewed datasets. We propose a novel boosting-based instance transfer classifier with a label-dependent update mechanism that simultaneously compensates for class imbalance and incorporates samples from an auxiliary domain to improve classification. We provide theoretical and empirical validation of our method and apply to healthcare and text classification applications.",2016,Knowledge and Information Systems volume 48 issue 1 pp 201-228,weighted majority algorithm;transfer of learning;adaboost;text mining;data mining;pattern recognition;artificial intelligence;machine learning;statistics;computer science;
Vertex cover-based binary tree algorithm to detect all maximum common induced subgraphs in large communication networks,Parisutham Nirmala (PSG College of Technology);Ramasubramony Sulochana Lekshmi (PSG College of Technology);Rethnasamy Nadarajan (PSG College of Technology);,"2025029414,2597880650,2555700059","Maximum common induced subgraph (MCIS) of a communication network graph database determine the common substructures which are always active and retain the links between any pair of nodes exactly as in all graphs of the database. Many benchmark graph algorithms to predict MCIS of a graph database deal only with two graphs at a time and seek isomorphism, for which a high computational cost is to be paid. This gradually reduces the performance of the existing algorithms when the database has huge graph data. The proposed binary caterpillar MCIS algorithm to predict all MCIS of the database works for communication network graph database each of whose vertices has a unique label (IP address). In this, a new data structure which is a caterpillar-based binary tree is defined to reduce the search space of the problem using the concept of vertex cover and it takes into account all graphs of the database simultaneously to predict all MCIS of the database. This has substantially reduced unwanted comparisons among the datasets, when compared to the existing algorithms, as well as the difficulty of seeking isomorphism is avoided due to unique vertex labels. The experimental results further ensure the efficiency of the proposed algorithm with respect to existing works.",2016,Knowledge and Information Systems volume 48 issue 1 pp 229-252,factor critical graph;distance hereditary graph;induced subgraph isomorphism problem;voltage graph;complement graph;forbidden graph characterization;graph bandwidth;graph power;graph factorization;butterfly graph;universal graph;feedback vertex set;graph homomorphism;graph property;graph database;subgraph isomorphism problem;line graph;vertex;graph isomorphism;degree;information extraction;combinatorics;data mining;machine learning;computer science;mathematics;
Multi-graph-view subgraph mining for graph classification,"Jia Wu (University of Technology, Sydney);Zhibin Hong (University of Technology, Sydney);Shirui Pan (University of Technology, Sydney);Xingquan Zhu (Florida Atlantic University);Zhihua Cai (China University of Geosciences);Chengqi Zhang (University of Technology, Sydney);","2151584597,2110797701,2132909836,2618356905,2136429757,2166080598","In this paper, we formulate a new multi-graph-view learning task, where each object to be classified contains graphs from multiple graph-views. This problem setting is essentially different from traditional single-graph-view graph classification, where graphs are collected from one single-feature view. To solve the problem, we propose a cross graph-view subgraph feature-based learning algorithm that explores an optimal set of subgraphs, across multiple graph-views, as features to represent graphs. Specifically, we derive an evaluation criterion to estimate the discriminative power and redundancy of subgraph features across all views, with a branch-and-bound algorithm being proposed to prune subgraph search space. Because graph-views may complement each other and play different roles in a learning task, we assign each view with a weight value indicating its importance to the learning task and further use an optimization process to find optimal weight values for each graph-view. The iteration between cross graph-view subgraph scoring and graph-view weight updating forms a closed loop to find optimal subgraphs to represent graphs for multi-graph-view learning. Experiments and comparisons on real-world tasks demonstrate the algorithm's superior performance.",2016,Knowledge and Information Systems volume 48 issue 1 pp 29-54,degeneracy;partial k tree;factor critical graph;implicit graph;distance hereditary graph;induced subgraph isomorphism problem;forbidden graph characterization;graph factorization;comparability graph;universal graph;block graph;cograph;claw free graph;split graph;graph homomorphism;subgraph isomorphism problem;line graph;information system;feature selection;graph theory;combinatorics;pattern recognition;machine learning;computer science;mathematics;
Local bilateral clustering for identifying research topics and groups from bibliographical data,Sara Elena Garza Villarreal (Universidad Autónoma de Nuevo León);Satu Elisa Schaeffer (Universidad Autónoma de Nuevo León);,"2114867390,2082477893","The structure of scientific collaboration networks provides insight on the relationships between people and disciplines. In this paper, we study a bipartite graph connecting authors to publications and extract from it clusters of authors and articles, interpreting the author clusters as research groups and the article clusters as research topics. Visualisations are proposed to ease the interpretation of such clusters in terms of discovering leaders, the activity level, and other semantic aspects. We discuss the process of obtaining and preprocessing the information from scientific publications, the formulation and implementation of the clustering algorithm, and the creation of the visualisations. Experiments on a test data set are presented, using an initial prototype implementation of the proposed modules.",2016,Knowledge and Information Systems volume 48 issue 1 pp 179-199,network analysis;cluster analysis;knowledge extraction;data science;information retrieval;data mining;database;artificial intelligence;machine learning;computer science;
Exploring probabilistic follow relationship to prevent collusive peer-to-peer piracy,Wenjia Niu (Chinese Academy of Sciences);Endong Tong (Institute of Acoustics);Qian Li (Chinese Academy of Sciences);Gang Li (Deakin University);Xuemin Wen (Chinese Academy of Sciences);Jianlong Tan (Chinese Academy of Sciences);Li Guo (Chinese Academy of Sciences);,"2135491977,2112680616,2642754292,2098057600,2710422863,2100810457,2122010476","P2P collusive piracy, where paid P2P clients share the content with unpaid clients, has drawn significant concerns in recent years. Study on the follow relationship provides an emerging track of research in capturing the followee (e.g., paid client) for the blocking of piracy spread from all his followers (e.g., unpaid clients). Unfortunately, existing research efforts on the follow relationship in online social network have largely overlooked the time constraint and the content feedback in sequential behavior analysis. Hence, how to consider these two characteristics for effective P2P collusive piracy prevention remains an open problem. In this paper, we proposed a multi-bloom filter circle to facilitate the time-constraint storage and query of P2P sequential behaviors. Then, a probabilistic follow with content feedback model to fast discover and quantify the probabilistic follow relationship is further developed, and then, the corresponding approach to piracy prevention is designed. The extensive experimental analysis demonstrates the capability of the proposed approach.",2016,Knowledge and Information Systems volume 48 issue 1 pp 111-141,bloom filter;behavior;computer security;simulation;programming language;computer science;
Modeling adoptions and the stages of the diffusion of innovations,Yasir Mehmood (Pompeu Fabra University);Nicola Barbieri (Yahoo!);Francesco Bonchi (Institute for Scientific Interchange);,"2344266923,2155070167,2176652147","Understanding the dynamics underlying the diffusion of new ideas or technology in a society is an important task with implications for sciences such sociology and economics, as well as important business applications, especially in marketing. In this article, we take a first step in this direction, by studying the problem of how to model, in a simple and useful abstraction, the complex process of innovation diffusion. Our unique input is a database of adoptions $$\mathbb {D}$$D, which is a relation (User,Item,Time) where a tuple $${\left\langle u, i, t\right\rangle } \in \mathbb {D}$$u,i,tźD indicates that the user u adopted the item i at time t. For our aim, we propose a stochastic model which decomposes a diffusion trace (i.e., the sequence of adoptions of the same item i) in an ordered sequence of stages, where each stage is intuitively built around two dimensions: users and relative speed at which adoptions happen. Each stage is characterized by a specific rate of adoption and it involves different users to different extent, while the sequentiality in the diffusion is guaranteed by constraining the transition probabilities among stages. An empirical evaluation on synthetic and real-world adoption datasets shows the effectiveness of the proposed framework in summarizing the adoption process, enabling several analysis tasks such as the identification of adopter categories, clustering and characterization of diffusion traces, and prediction of which users will adopt an item in the next future.",2016,Knowledge and Information Systems volume 48 issue 1 pp 1-27,data mining;database;artificial intelligence;simulation;computer science;
Mining exceptional relationships with grammar-guided genetic programming,Jose Maria Luna (University of Córdoba);Mykola Pechenizkiy (Eindhoven University of Technology);Sebastian Ventura (University of Córdoba);,"2130867717,2019713363,2098297204","Given a database of records, it might be possible to identify small subsets of data which distribution is exceptionally different from the distribution in the complete set of data records. Finding such interesting relationships, which we call exceptional relationships, in an automated way would allow discovering unusual or exceptional hidden behaviour. In this paper, we formulate the problem of mining exceptional relationships as a special case of exceptional model mining and propose a grammar-guided genetic programming algorithm (MERG3P) that enables the discovery of any exceptional relationships. In particular, MERG3P can work directly not only with categorical, but also with numerical data. In the experimental evaluation, we conduct a case study on mining exceptional relations between well-known and widely used quality measures of association rules, which exceptional behaviour would be of interest to pattern mining experts. For this purpose, we constructed a data set comprising a wide range of values for each considered association rule quality measure, such that possible exceptional relations between measures could be discovered. Thus, besides the actual validation of MERG3P, we found that the Support and Leverage measures in fact are negatively correlated under certain conditions, while in general experts in the field expect these measures to be positively correlated.",2016,Knowledge and Information Systems volume 47 issue 3 pp 571-594,genetic programming;association rule learning;data mining;artificial intelligence;machine learning;computer science;mathematics;
AMORE: design and implementation of a commercial-strength parallel hybrid movie recommendation engine,Ioannis T. Christou (University of Wisconsin-Madison);Emmanouil Amolochitis (Aalborg University);Zheng-Hua Tan (Aalborg University);,"660025571,2009718112,2128584096","AMORE is a hybrid recommendation system that provides movie recommendation functionality to video-on-demand subscribers of a major triple-play service provider in Greece. Without any user relevance feedback for movies available, all recommendations are solely based on the users' viewing history. To overcome such limitations as well as the extra problem of user histories that are usually the merger of the preferences of all persons in each household, we have performed extensive experiments with open-source recommendation software such as Apache Mahout and Lens-Kit, as well as with our own implementations of several user-based, item-based, and content-based recommendation algorithms. Our results indicate that our own custom multi-threaded implementation of collaborative filtering combined with a custom content-based algorithm outperforms current state-of-the-art implementations of similar algorithms both in solution quality and in response time by margins exceeding 100 % in terms of recall quality and 6300 % in terms of running time. The hybrid nature of the ensemble allows the system to perform well and to overcome inherent limitations of collaborative filtering, such as various cold-start problems. AMORE has been deployed in a production environment where it has contributed to an increase in the provider's rental profits, while at the same time offers customer retention support.",2016,Knowledge and Information Systems volume 47 issue 3 pp 671-696,recommender system;multimedia;world wide web;data mining;database;artificial intelligence;machine learning;simulation;computer science;
A differentially private algorithm for location data release,Ping Xiong (Zhongnan University of Economics and Law);Tianqing Zhu (Deakin University);Wenjia Niu (Chinese Academy of Sciences);Gang Li (Deakin University);,"2020434285,2135547451,2135491977,2098057600","The rise of mobile technologies in recent years has led to large volumes of location information, which are valuable resources for knowledge discovery such as travel patterns mining and traffic analysis. However, location dataset has been confronted with serious privacy concerns because adversaries may re-identify a user and his/her sensitivity information from these datasets with only a little background knowledge. Recently, several privacy-preserving techniques have been proposed to address the problem, but most of them lack a strict privacy notion and can hardly resist the number of possible attacks. This paper proposes a private release algorithm to randomize location dataset in a strict privacy notion, differential privacy, with the goal of preserving users' identities and sensitive information. The algorithm aims to mask the exact locations of each user as well as the frequency that the user visits the locations with a given privacy budget. It includes three privacy-preserving operations: private location clustering shrinks the randomized domain and cluster weight perturbation hides the weights of locations, while private location selection hides the exact locations of a user. Theoretical analysis on privacy and utility confirms an improved trade-off between privacy and utility of released location data. Extensive experiments have been carried out on four real-world datasets, GeoLife, Flickr, Div400 and Instagram. The experimental results further suggest that this private release algorithm can successfully retain the utility of the datasets while preserving users' privacy.",2016,Knowledge and Information Systems volume 47 issue 3 pp 647-669,privacy software;differential privacy;location based service;internet privacy;computer security;data mining;database;computer science;
Constrained pattern mining in the new era,Andreia Silva (Instituto Superior Técnico);Cláudia Antunes (Instituto Superior Técnico);,"2165533671,2081435542","Twenty years of research on frequent itemset mining, or pattern mining, has led to the existence of a set of efficient algorithms for identifying different types of patterns, from transactional to sequential. Despite the great advances in this field, big data brought a completely new context to operate, with new challenges arising from the growth in data size, dynamics and complexity. These challenges include the shift not only from static to dynamic data, but also from tabular to complex data sources, such as social networks (expressed as graphs) and data warehouses (expressed as multi-relational models). In this new context, and more than ever, users need effective ways to control the large number of discovered patterns, and to be able to choose what patterns to consider at each time. The most accepted and common approach to minimize these drawbacks has been to capture and represent the semantics of the domain through constraints, and use them not only to reduce the number of results, but also to focus the algorithms in areas where it is more likely to gain information and return more interesting results. The use of constraints in pattern mining has been widely studied, and there are a lot of proposed types of constraints and pushing strategies. In this paper, we present a new global view of the work done on the incorporation of constraints in the pattern mining process. In particular, we propose a new framework for constrained pattern mining, that allows us to organize and analyze existing algorithms and strategies, based on the different types and properties of constraints, and on the data sources they are able to handle.",2016,Knowledge and Information Systems volume 47 issue 3 pp 489-516,domain knowledge;data stream mining;data science;data mining;database;artificial intelligence;computer science;
Tracking the evolution of social emotions with topic models,Chen Zhu (University of Science and Technology of China);Hengshu Zhu (Baidu);Yong Ge (University of North Carolina System);Enhong Chen (University of Science and Technology of China);Qi Liu (University of Science and Technology of China);Tong Xu (University of Science and Technology of China);Hui Xiong (Rutgers Business School – Newark and New Brunswick);,"2229244414,2098414524,2218492437,2136372366,2420624292,2291800725,2153710278","Many of today's online news Web sites have enabled users to specify different types of emotions (e.g., angry or shocked) they have after reading news. Compared with traditional user feedbacks such as comments and ratings, these specific emotion annotations are more accurate for expressing users' personal emotions. In this paper, we propose to exploit these users' emotion annotations for online news in order to track the evolution of emotions, which plays an important role in various online services. A critical challenge is how to model emotions with respect to time spans. To this end, we propose a time-aware topic modeling perspective for solving this problem. Specifically, we first develop two models named emotion-Topic over Time (eToT) and mixed emotion-Topic over Time (meToT), in which the topics of news are represented as a beta distribution over time and a multinomial distribution over emotions. While they can uncover the latent relationship among news, emotion and time directly, they cannot capture the evolution of topics. Therefore, we further develop another model named emotion-based Dynamic Topic Model (eDTM), where we explore the state space model for tracking the evolution of topics. In addition, we demonstrate that all of proposed models could enable several potential applications, such as emotion prediction, emotion-based news recommendations, and emotion anomaly detections. Finally, we validate the proposed models with extensive experiments with a real-world data set.",2016,Knowledge and Information Systems volume 47 issue 3 pp 517-544,topic model;sentiment analysis;multimedia;data mining;artificial intelligence;machine learning;computer science;
A transversal hypergraph approach for the frequent itemset hiding problem,Elias C. Stavropoulos (Hellenic Open University);Vassilios S. Verykios (Hellenic Open University);Vasileios Kagklis (Hellenic Open University);,"2441972159,77221159,2553785743","We propose a methodology for hiding all sensitive frequent itemsets in a transaction database. Our methodology relies on a novel technique that enumerates the minimal transversals of a hypergraph in order to induce the ideal border between frequent and sensitive itemsets. The ideal border is then utilized to formulate an integer linear program (ILP) that answers whether a feasible sanitized database that attains the ideal border, exists. The solution of the program identifies the set of transactions that need to be modified (sanitized) so that the hiding can be achieved with the maximum accuracy. If no solution exists, we modify the ILP by relaxing the constraints needed to be satisfied so that the sanitized database preserves the privacy with guarantee but with minimum effect in data quality. Experimental evaluation of the proposed approach on a number of real datasets has shown that the produced sanitized databases exhibit higher accuracy when compared with the solutions of other well-known approaches.",2016,Knowledge and Information Systems volume 47 issue 3 pp 625-645,discrete mathematics;data mining;database;mathematics;
Partial materialization for online analytical processing over multi-tagged document collections,Grzegorz Drzadzewski (David R. Cheriton School of Computer Science);Frank Wm. Tompa (David R. Cheriton School of Computer Science);,"253169367,2271303423","The New York Times Annotated Corpus, the ACM Digital Library, and PubMed are three prototypical examples of document collections in which each document is tagged with keywords or phrases. Such collections can be viewed as high-dimensional document cubes against which browsers and search systems can be applied in a manner similar to online analytical processing against data cubes. After examining the tagging patterns in these collections, a partial materialization strategy is developed to provide efficient storage and access to centroids for document subsets that are defined through queries over tags. By adopting this strategy, summary measures dependent on centroids (including measures involving medoids, sets of representative documents, or sets of representative terms) can be efficiently computed. The proposed design is evaluated on the three collections and on several synthetically generated collections to validate that it outperforms alternative storage strategies.",2016,Knowledge and Information Systems volume 47 issue 3 pp 697-732,online analytical processing;text mining;information retrieval;data mining;database;computer science;
Splitting anonymization: a novel privacy-preserving approach of social network,Yongjiao Sun (Northeastern University);Ye Yuan (Northeastern University);Guoren Wang (Northeastern University);Yurong Cheng (Northeastern University);,"2122068162,2104055677,2166985210,2128669861","Large amount of personal social information is collected and published due to the rapid development of social network technologies and applications, and thus, it is quite essential to take privacy preservation and prevent sensitive information leakage. Most of current anonymizing techniques focus on the preservation to privacies, but cannot provide accurate answers to utility queries even at a high price. To solve the problem, a novel anonymizing approach, called splitting anonymization, is introduced in this paper to point against the contradiction of privacy and utility. This approach provides a high-level preservation to the privacy of social network data that is unknown to attackers, which avoids the low utility caused by the enforced noises on knowledge that is already known to the attackers. Social network processed by splitting anonymization can refuse any direct attack, and these strategies are also safe enough to indirect attacks which are usually more dangerous than direct attacks. Finally, strict theoretical analysis and large amount of evaluation results based on real data sets verified the design of this paper.",2016,Knowledge and Information Systems volume 47 issue 3 pp 595-623,attack;social network;internet privacy;computer security;data mining;computer science;
Co-clustering of multi-view datasets,Syed Fawad Hussain;Shariq Bashir (Bahria University);,"2682096272,2116780376","In many clustering problems, we have access to multiple sources of data representing different aspects of the problem. Each of these data separately represents an association between entities. Multi-view clustering involves integrating clustering information from these heterogeneous sources of data and has been shown to improve results over a single-view clustering. On the other hand, co-clustering has been widely used as a technique to improve clustering results on a single view by exploiting the duality between objects and their attributes. In this paper, we propose a multi-view clustering setting in the context of a co-clustering framework. Our underlying assumption is that similarity values generated from the individual data can be transferred from one view to the other(s) resulting in a better clustering of the data. We provide empirical evidence to show that this framework results in a better clustering accuracy than those obtained from any of the single views, tested on different datasets.",2016,Knowledge and Information Systems volume 47 issue 3 pp 545-570,flame clustering;k medians clustering;hierarchical clustering of networks;brown clustering;canopy clustering algorithm;dbscan;correlation clustering;constrained clustering;data stream clustering;cure data clustering algorithm;single linkage clustering;affinity propagation;transfer of learning;fuzzy clustering;clustering high dimensional data;hierarchical clustering;cluster analysis;consensus clustering;biclustering;conceptual clustering;data mining;pattern recognition;machine learning;computer science;
On strategies for building effective ensembles of relative clustering validity criteria,Pablo A. Jaskowiak (University of São Paulo);Davoud Moulavi (University of Alberta);Antonio C. S. Furtado (University of Alberta);Ricardo J. G. B. Campello (University of São Paulo);Arthur Zimek (Ludwig Maximilian University of Munich);Jörg Sander (University of Alberta);,"2041850064,1994079834,2099921943,2043417111,242745652,2118842476","Evaluation and validation are essential tasks for achieving meaningful clustering results. Relative validity criteria are measures usually employed in practice to select and validate clustering solutions, as they enable the evaluation of single partitions and the comparison of partition pairs in relative terms based only on the data under analysis. There is a plethora of relative validity measures described in the clustering literature, thus making it difficult to choose an appropriate measure for a given application. One reason for such a variety is that no single measure can capture all different aspects of the clustering problem and, as such, each of them is prone to fail in particular application scenarios. In the present work, we take advantage of the diversity in relative validity measures from the clustering literature. Previous work showed that when randomly selecting different relative validity criteria for an ensemble (from an initial set of 28 different measures), one can expect with great certainty to only improve results over the worst criterion included in the ensemble. In this paper, we propose a method for selecting measures with minimum effectiveness and some degree of complementarity (from the same set of 28 measures) into ensembles, which show superior performance when compared to any single ensemble member (and not just the worst one) over a variety of different datasets. One can also expect greater stability in terms of evaluation over different datasets, even when considering different ensemble strategies. Our results are based on more than a thousand datasets, synthetic and real, from different sources.",2016,Knowledge and Information Systems volume 47 issue 2 pp 329-354,correlation clustering;fuzzy clustering;combination;ensembl;cluster analysis;data mining;machine learning;statistics;computer science;mathematics;
Leveraging path information to generate predictions for parallel business processes,Merve Unuvar (IBM);Geetika T. Lakshmanan (IBM);Yurdaer N. Doganata (IBM);,"1965781404,2150824222,2024289304","In semi-structured processes, the set of activities that need to be performed, their order and whether additional steps are required are determined by human judgment. There is a growing demand for operational support of such processes during runtime particularly in the form of predictions about the likelihood of future tasks. We address the problem of making predictions for a running instance of a semi-structured process that contains parallel execution paths where the execution path taken by a process instance influences its outcome. In particular, we consider five different models for how to represent an execution trace as a path attribute for training a prediction model. We provide a methodology to determine whether parallel paths are independent, and whether it is worthwhile to model execution paths as independent based on a comparison of the information gain obtained by dependent and independent path representations. We tested our methodology by simulating a marketing campaign as a business process model and selected decision trees as the prediction model. In the evaluation, we compare the complexity and prediction accuracy of a prediction model trained with five different models.",2016,Knowledge and Information Systems volume 47 issue 2 pp 433-461,complexity;decision tree;accuracy and precision;predictive modelling;prediction;theoretical computer science;data mining;machine learning;statistics;computer science;
Mining and clustering mobility evolution patterns from social media for urban informatics,Chien-Cheng Chen (National Chiao Tung University);Meng-Fen Chiang (Singapore Management University);Wen-Chih Peng (National Chiao Tung University);,"2642927697,2656114234,2100961564","In this paper, given a set of check-in data, we aim at discovering representative daily movement behavior of users in a city. For example, daily movement behavior on a weekday may show users moving from one to another spatial region associated with time information. Since check-in data contain both spatial and temporal information, we propose a mobility evolution pattern to capture the daily movement behavior of users in a city. Furthermore, given a set of daily mobility evolution patterns, we formulate their similarity distances and then discover representative mobility evolution patterns via the clustering process. Representative mobility evolution patterns are able to infer major movement behavior in a city, which could bring some valuable knowledge for urban planning. Specifically, mobility evolution patterns consist of segments with the spatial region distribution and the corresponding time interval. To measure good segmentation from a set of check-in data, we formulate the problem of mining evolution patterns as a compression problem. In particular, we compute the representation length of the patterns based on the Minimum Description Length principle. Since the number of daily mobility evolution patterns is huge, we further cluster the daily mobility evolution patterns into groups and discover representative patterns. Note that we use the concept of locality-sensitive hashing to accelerate the cluster performance. To evaluate our proposed algorithms, we conducted experiments on the Gowalla and Brightkite datasets, and the experimental results show the effectiveness and efficiency of our proposed algorithms.",2016,Knowledge and Information Systems volume 47 issue 2 pp 381-403,urban planning;data science;data mining;simulation;computer science;
Context-aware location recommendation by using a random walk-based approach,Hakan Bagci (Middle East Technical University);Pinar Karagoz (Middle East Technical University);,"1983079674,2068042486","The location-based social networks (LBSN) enable users to check in their current location and share it with other users. The accumulated check-in data can be employed for the benefit of users by providing personalized recommendations. In this paper, we propose a context-aware location recommendation system for LBSNs using a random walk approach. Our proposed approach considers the current context (i.e., current social relations, personal preferences and current location) of the user to provide personalized recommendations. We build a graph model of LBSNs for performing a random walk approach with restart. Random walk is performed to calculate the recommendation probabilities of the nodes. A list of locations are recommended to users after ordering the nodes according to the estimated probabilities. We compare our algorithm, CLoRW, with popularity-based, friend-based and expert-based baselines, user-based collaborative filtering approach and a similar work in the literature. According to experimental results, our algorithm outperforms these approaches in all of the test cases.",2016,Knowledge and Information Systems volume 47 issue 2 pp 241-260,random walk;world wide web;data mining;database;simulation;statistics;computer science;
Locality-sensitive and Re-use Promoting Personalized PageRank computations,Jung Hyun Kim (Arizona State University);K. Selçuk Candan (Arizona State University);Maria Luisa Sapino (University of Turin);,"2115081567,674992784,351919550","Node distance/proximity measures are used for quantifying how nearby or otherwise related two or more nodes on a graph are. In particular, personalized PageRank (PPR) based measures of node proximity have been shown to be highly effective in many prediction and recommendation applications. Despite its effectiveness, however, the use of personalized PageRank for large graphs is difficult due to its high computation cost. In this paper, we propose a Locality-sensitive, Re-use promoting, approximate Personalized PageRank (LR-PPR) algorithm for efficiently computing the PPR values relying on the localities of the given seed nodes on the graph: (a) The LR-PPR algorithm is locality sensitive in the sense that it reduces the computational cost of the PPR computation process by focusing on the local neighborhoods of the seed nodes. (b) LR-PPR is re-use promoting in that instead of performing a monolithic computation for the given seed node set using the entire graph, LR-PPR divides the work into localities of the seeds and caches the intermediary results obtained during the computation. These cached results are then reused for future queries sharing seed nodes. Experiment results for different data sets and under different scenarios show that LR-PPR algorithm is highly-efficient and accurate.",2016,Knowledge and Information Systems volume 47 issue 2 pp 261-299,scalability;theoretical computer science;world wide web;distributed computing;data mining;database;computer science;
Data clustering using side information dependent Chinese restaurant processes,Cheng Li (Deakin University);Santu Rana (Deakin University);Dinh Q. Phung (Deakin University);Svetha Venkatesh (Deakin University);,"2307881337,2142238370,2314522249,2146461601","Side information, or auxiliary information associated with documents or image content, provides hints for clustering. We propose a new model, side information dependent Chinese restaurant process, which exploits side information in a Bayesian nonparametric model to improve data clustering. We introduce side information into the framework of distance dependent Chinese restaurant process using a robust decay function to handle noisy side information. The threshold parameter of the decay function is updated automatically in the Gibbs sampling process. A fast inference algorithm is proposed. We evaluate our approach on four datasets: Cora, 20 Newsgroups, NUS-WIDE and one medical dataset. Types of side information explored in this paper include citations, authors, tags, keywords and auxiliary clinical information. The comparison with the state-of-the-art approaches based on standard performance measures (NMI, F1) clearly shows the superiority of our approach.",2016,Knowledge and Information Systems volume 47 issue 2 pp 463-488,similarity;cluster analysis;data science;data mining;database;machine learning;statistics;computer science;
Enhancing web search by using query-based clusters and multi-document summaries,Rani Qumsiyeh (Brigham Young University);Yiu-Kai Ng (Brigham Young University);,"2259000910,2135939557","Current web search engines, such as Google, Bing, and Yahoo!, rank the set of documents SD retrieved in response to a user query and display each document D in SD with a title and a snippet, which serves as an abstract of D. Snippets, however, are not as useful as they are designed for, i.e., assisting its users to quickly identify results of interest, if they exist. These snippets are inadequate in providing distinct information and capturing the main contents of the corresponding documents. Moreover, when the intended information need specified in a search query is ambiguous, it is very difficult, if not impossible, for a search engine to identify precisely the set of documents that satisfy the user's intended request without requiring additional inputs. Furthermore, a document title is not always a good indicator of the content of the corresponding document. All of these design problems can be solved by our proposed query-based cluster and summarizer, called $$Q_{Sum}$$QSum. $$Q_{Sum}$$QSum generates a concise/comprehensive summary for each cluster of documents retrieved in response to a user query, which saves the user's time and effort in searching for specific information of interest without having to browse through the documents one by one. Experimental results show that $$Q_{Sum}$$QSum is effective and efficient in generating a high-quality summary for each cluster of documents on a specific topic.",2016,Knowledge and Information Systems volume 47 issue 2 pp 355-380,ranking;web search query;web query classification;query expansion;cluster analysis;automatic summarization;world wide web;information retrieval;data mining;database;machine learning;computer science;
Discovering compressing serial episodes from event sequences,A. Ibrahim (Indian Institute of Science);Shivakumar Sastry (University of Akron);P. S. Sastry (Indian Institute of Science);,"2369016151,2106192464,2520038081","Most pattern mining methods yield a large number of frequent patterns, and isolating a small relevant subset of patterns is a challenging problem of current interest. In this paper, we address this problem in the context of discovering frequent episodes from symbolic time-series data. Motivated by the Minimum Description Length principle, we formulate the problem of selecting relevant subset of patterns as one of searching for a subset of patterns that achieves best data compression. We present algorithms for discovering small sets of relevant non-redundant episodes that achieve good data compression. The algorithms employ a novel encoding scheme and use serial episodes with inter-event constraints as the patterns. We present extensive simulation studies with both synthetic and real data, comparing our method with the existing schemes such as GoKrimp and SQS. We also demonstrate the effectiveness of these algorithms on event sequences from a composable conveyor system; this system represents a new application area where use of frequent patterns for compressing the event sequence is likely to be important for decision support and control.",2016,Knowledge and Information Systems volume 47 issue 2 pp 405-432,data mining;pattern recognition;machine learning;computer science;
TKAP: Efficiently processing top-k query on massive data by adaptive pruning,Xixian Han (Harbin Institute of Technology);Xianmin Liu (Harbin Institute of Technology);Jianzhong Li (Harbin Institute of Technology);Hong Gao (Harbin Institute of Technology);,"2113522248,2139131606,2130201582,2607637236","In many applications, top-k query is an important operation to return a set of interesting points in a potentially huge data space. The existing algorithms, either maintaining too many candidates, or requiring assistant structures built on the specific attribute subset, or returning results with probabilistic guarantee, cannot process top-k query on massive data efficiently. This paper proposes a sorted-list-based TKAP algorithm, which utilizes some data structures of low space overhead, to efficiently compute top-k results on massive data. In round-robin retrieval on sorted lists, TKAP performs adaptive pruning operation and maintains the required candidates until the stop condition is satisfied. The adaptive pruning operation can be adjusted by the information obtained in round-robin retrieval to achieve a better pruning effect. The adaptive pruning rule is developed in this paper, along with its theoretical analysis. The extensive experimental results, conducted on synthetic and real-life data sets, show the significant advantage of TKAP over the existing algorithms.",2016,Knowledge and Information Systems volume 47 issue 2 pp 301-328,principal variation search;pruning;data mining;database;machine learning;computer science;
Faster and more accurate classification of time series by exploiting a novel dynamic time warping averaging algorithm,"François Petitjean (Monash University);Germain Forestier (University of Strasbourg);Geoffrey I. Webb (Monash University);Ann E. Nicholson (Monash University);Yanping Chen 0005 (University of California, Riverside);Eamonn J. Keogh (University of California, Riverside);","2709762332,2008768452,2126304162,2115409756,2472401419,2170070822","A concerted research effort over the past two decades has heralded significant improvements in both the efficiency and effectiveness of time series classification. The consensus that has emerged in the community is that the best solution is a surprisingly simple one. In virtually all domains, the most accurate classifier is the nearest neighbor algorithm with dynamic time warping as the distance measure. The time complexity of dynamic time warping means that successful deployments on resource-constrained devices remain elusive. Moreover, the recent explosion of interest in wearable computing devices, which typically have limited computational resources, has greatly increased the need for very efficient classification algorithms. A classic technique to obtain the benefits of the nearest neighbor algorithm, without inheriting its undesirable time and space complexity, is to use the nearest centroid algorithm. Unfortunately, the unique properties of (most) time series data mean that the centroid typically does not resemble any of the instances, an unintuitive and underappreciated fact. In this paper we demonstrate that we can exploit a recent result by Petitjean et al. to allow meaningful averaging of ""warped"" time series, which then allows us to create super-efficient nearest ""centroid"" classifiers that are at least as accurate as their more computationally challenged nearest neighbor relatives. We demonstrate empirically the utility of our approach by comparing it to all the appropriate strawmen algorithms on the ubiquitous UCR Benchmarks and with a case study in supporting insect classification on resource-constrained sensors.",2016,Knowledge and Information Systems volume 47 issue 1 pp 1-26,dynamic time warping;time series;biological classification;data mining;pattern recognition;machine learning;statistics;computer science;
Efficient discovery of contrast subspaces for object explanation and characterization,Lei Duan (Sichuan University);Guanting Tang (Simon Fraser University);Jian Pei (Simon Fraser University);James Bailey (University of Melbourne);Guozhu Dong (Wright State University);Vinh Nguyen (University of Melbourne);Akiko Campbell;Changjie Tang (Sichuan University);,"2618270272,2136282317,2126330539,2131557737,2164298414,2422307276,2152383629,2099022773","We tackle the novel problem of mining contrast subspaces. Given a set of multidimensional objects in two classes $$C_+$$C+ and $$C_-$$C- and a query object $$o$$o, we want to find the top-$$k$$k subspaces that maximize the ratio of likelihood of $$o$$o in $$C_+$$C+ against that in $$C_-$$C-. Such subspaces are very useful for characterizing an object and explaining how it differs between two classes. We demonstrate that this problem has important applications, and, at the same time, is very challenging, being MAX SNP-hard. We present CSMiner, a mining method that uses kernel density estimation in conjunction with various pruning techniques. We experimentally investigate the performance of CSMiner on a range of data sets, evaluating its efficiency, effectiveness, and stability and demonstrating it is substantially faster than a baseline method.",2016,Knowledge and Information Systems volume 47 issue 1 pp 99-129,kernel density estimation;pattern recognition;machine learning;mathematical optimization;statistics;computer science;mathematics;
A framework for anomaly detection in maritime trajectory behavior,Po-Ruey Lei (United States Naval Academy);,2633883781,"Rapid growth in location data acquisition techniques has led to a proliferation of trajectory data related to moving objects. This large body of data has expanded the scope for trajectory research and made it applicable to a more diverse range of fields. However, data uncertainty, which is naturally inherent in the trajectory data, brings the challenge in trajectory data mining and affects the quality of the results. Specifically, unlike trajectory collected from vehicles moving along road networks, trajectory data generated by vessels moving free in maritime space have increased the difficulty of sea traffic analysis and anomalous behavior detection. Furthermore, due to the huge volume and complexity of maritime trajectory data, it is hard to define the abnormality of movement behavior and detect anomalies. Additionally, traditional analysis and evaluation by human intelligence is overloaded with the dramatic increasing in amount of maritime trajectory data and is an inefficient approach. Thus, an effective automated method for mining trajectory data and detecting anomalies would be a valuable contribution to maritime surveillance. This paper explores the maritime trajectory data for anomalous behavior detection. We propose a framework for maritime trajectory modeling and anomaly detection, called MT-MAD. Our model takes into account the fact that anomalous behavior manifests in unusual location points and sub-trajectories in the spatial domain as well as in the sequence and manner in which these locations and sub-trajectories occur. This study therefore began by identifying outlying features required for anomaly detection, including spatial, sequential, and behavioral features. We then explore the movement behavior from historical trajectories and build a maritime trajectory model for anomaly detection. The proposed model accurately describes movement behavior and captures outlying features in trajectory data. We then developed an anomaly detection algorithm based on this model in which an indicator is used to evaluate suspicious behavior and scores trajectory behavior according to the defined outlying features. Experiment results demonstrate that the proposed MT-MAD framework is capable of effectively detecting anomalies in maritime trajectories.",2016,Knowledge and Information Systems volume 47 issue 1 pp 189-214,anomaly detection;machine learning;simulation;computer science;
Modelling human preferences for ranking and collaborative filtering: a probabilistic ordered partition approach,Truyen Tran 0001 (Deakin University);Dinh Q. Phung (Deakin University);Svetha Venkatesh (Deakin University);,"2157392948,2314522249,2146461601","Learning preference models from human generated data is an important task in modern information processing systems. Its popular setting consists of simple input ratings, assigned with numerical values to indicate their relevancy with respect to a specific query. Since ratings are often specified within a small range, several objects may have the same ratings, thus creating ties among objects for a given query. Dealing with this phenomena presents a general problem of modelling preferences in the presence of ties and being query-specific. To this end, we present in this paper a novel approach by constructing probabilistic models directly on the collection of objects exploiting the combinatorial structure induced by the ties among them. The proposed probabilistic setting allows exploration of a super-exponential combinatorial state-space with unknown numbers of partitions and unknown order among them. Learning and inference in such a large state-space are challenging, and yet we present in this paper efficient algorithms to perform these tasks. Our approach exploits discrete choice theory, imposing generative process such that the finite set of objects is partitioned into subsets in a stagewise procedure, and thus reducing the state-space at each stage significantly. Efficient Markov chain Monte Carlo algorithms are then presented for the proposed models. We demonstrate that the model can potentially be trained in a large-scale setting of hundreds of thousands objects using an ordinary computer. In fact, in some special cases with appropriate model specification, our models can be learned in linear time. We evaluate the models on two application areas: (i) document ranking with the data from the Yahoo! challenge and (ii) collaborative filtering with movie data. We demonstrate that the models are competitive against state-of-the-arts.",2016,Knowledge and Information Systems volume 47 issue 1 pp 157-188,preference learning;collaborative filtering;probabilistic logic;learning to rank;theoretical computer science;data mining;database;artificial intelligence;machine learning;statistics;computer science;mathematics;
PISAGOR: a proactive software agent for monitoring interactions,Özgür Kafalı (North Carolina State University);Pınar Yolum (Boğaziçi University);,"98697097,267358967","E-commerce has become an integral part of everyday life. Many e-commerce systems enable users to browse products, place orders, and track deliveries online. However, these interactions is time-consuming, especially if they do not proceed as expected. Instead of a human user, software can offer automated support to monitor e-commerce transactions to ensure that they are completed by the businesses as planned. Accordingly, this paper studies methods to develop a software agent for monitoring its users' interactions with businesses (PISAGOR). The interactions are represented as commitments, which have been extensively studied in multiagent systems. With a commitment-based specification at hand, the agent knows what the meaning of its interactions are and can therefore reason over its actions. The reasoning is accompanied with the as-good-as relation that compares the agent's current state with its expectations from a transaction. This enables PISAGOR to decide whether the transaction is progressing as expected. Moreover, we propose operational rules for the agent to create expectations based on its commitments and check its progress toward them. We demonstrate via a case study how PISAGOR can detect if a user's interactions are not progressing well and identify a problem for the user to take action.",2016,Knowledge and Information Systems volume 47 issue 1 pp 215-239,multi agent system;knowledge management;computer security;data mining;database;artificial intelligence;simulation;computer science;
A new boosting algorithm for provably accurate unsupervised domain adaptation,Amaury Habrard (Centre national de la recherche scientifique);Jean-Philippe Peyrache (Centre national de la recherche scientifique);Marc Sebban (Centre national de la recherche scientifique);,"1915993033,2336239122,2217963768","Domain adaptation (DA) is a new learning framework dealing with learning problems where the target test data are drawn from a distribution different from the one that has generated the learning source data. In this article, we introduce self-labeling domain adaptation boosting (SLDAB), a new DA algorithm that falls both within the theory of DA and the theory of Boosting, allowing us to derive strong theoretical properties. SLDAB stands in the unsupervised DA setting where labeled data are only available in the source domain. To deal with this more complex situation, the strategy of SLDAB consists in jointly minimizing the empirical error on the source domain while limiting the violations of a natural notion of pseudo-margin over the target domain instances. Another contribution of this paper is the definition of a new divergence measure aiming at penalizing models that induce a large discrepancy between the two domains, reducing the production of degenerate models. We provide several theoretical results that justify this strategy. The practical efficiency of our model is assessed on two widely used datasets.",2016,Knowledge and Information Systems volume 47 issue 1 pp 45-73,transfer of learning;boosting;data mining;pattern recognition;machine learning;mathematical optimization;computer science;mathematics;
Soft-constrained Laplacian score for semi-supervised multi-label feature selection,Abdelouahid Alalga;Khalid Benabdeslem (University of Lyon);Nora Taleb;,"2559807570,230725257,2721979946","Feature selection, semi-supervised learning and multi-label classification are different challenges for machine learning and data mining communities. While other works have addressed each of these problems separately, in this paper we show how they can be addressed together. We propose a unified framework for semi-supervised multi-label feature selection, based on Laplacian score. In particular, we show how to constrain the function of this score, when data are partially labeled and each instance is associated with a set of labels. We transform the labeled part of data into soft constraints and show how to integrate them in a measure of feature relevance, according to the available labels. Experiments on benchmark data sets are provided for validating the proposed approach and comparing it with some other state-of-the-art feature selection methods in a multi-label context.",2016,Knowledge and Information Systems volume 47 issue 1 pp 75-98,dimensionality reduction;feature;feature selection;data mining;pattern recognition;machine learning;computer science;
Cross-lingual sentiment classification with stacked autoencoders,Guangyou Zhou (Central China Normal University);Zhiyuan Zhu;Tingting He (Central China Normal University);Xiaohua Tony Hu (Central China Normal University);,"2669572628,2477492998,2634632412,2432183245","Cross-lingual sentiment classification is a popular research topic in natural language processing. The fundamental challenge of cross-lingual learning stems from a lack of overlap between the feature spaces of the source language data and the target language data. In this article, we propose a new model which uses stacked autoencoders to learn language-independent high-level feature representations for the both languages in an unsupervised fashion. The proposed framework aims to force the aligned input bilingual sentences into a common latent space, and the objective function is defined by minimizing the input and output vector representations as well as the distance of the common representations in the latent space. Sentiment classifiers trained on the source language can be adapted to predict sentiment polarity of the target language with the language-independent high-level feature representations. We conduct extensive experiments on English---Chinese sentiment classification tasks of multiple data sets. Our experimental results demonstrate the efficacy of the proposed cross-lingual approach.",2016,Knowledge and Information Systems volume 47 issue 1 pp 27-44,sentiment analysis;natural language processing;speech recognition;data mining;pattern recognition;machine learning;computer science;
Speeding up ALS learning via approximate methods for context-aware recommendations,Balázs Hidasi (Budapest University of Technology and Economics);Domonkos Tikk (Óbuda University);,"1950878799,165729861","Implicit feedback-based recommendation problems, typically set in real-world applications, recently have been receiving more attention in the research community. From the practical point of view, scalability of such methods is crucial. However, factorization-based algorithms efficient in explicit rating data applied directly to implicit data are computationally inefficient; therefore, different techniques are needed to adapt to implicit feedback. For alternating least squares (ALS) learning, several research contributions have proposed efficient adaptation techniques for implicit feedback. These algorithms scale linearly with the number of nonzero data points, but cubically in the number of features, which is a computational bottleneck that prevents the efficient usage of accurate high factor models. Also, map-reduce type big data techniques are not viable with ALS learning, because there is no known technique that solves the high communication overhead required for random access of the feature matrices. To overcome this drawback, here we present two generic approximate variants for fast ALS learning, using conjugate gradient (CG) and coordinate descent (CD). Both CG and CD can be coupled with all methods using ALS learning. We demonstrate the advantages of fast ALS variants on iTALS, a generic context-aware algorithm, which applies ALS learning for tensor factorization on implicit data. In the experiments, we compare the approximate techniques with the base ALS learning in terms of training time, scalability, recommendation accuracy, and convergence. We show that the proposed solutions offer a trade-off between recommendation accuracy and speed of training time; this makes it possible to apply ALS-based methods efficiently even for billions of data points.",2016,Knowledge and Information Systems volume 47 issue 1 pp 131-155,scalability;recommender system;theoretical computer science;data mining;database;artificial intelligence;machine learning;mathematical optimization;statistics;computer science;mathematics;
Discovery of comet communities in temporal and labeled graphs Com $$^2$$2,Miguel Araujo (Carnegie Mellon University);Stephan Günnemann (Carnegie Mellon University);Spiros Papadimitriou (Rutgers University);Christos Faloutsos (Carnegie Mellon University);Prithwish Basu (BBN Technologies);Ananthram Swami (United States Army Research Laboratory);Evangelos E. Papalexakis (Carnegie Mellon University);Danai Koutra (Carnegie Mellon University);,"2124308955,316694267,2650391232,2198983026,2111459030,2059211748,1418764031,1524801041","While the analysis of unlabeled networks has been studied extensively in the past, finding patterns in different kinds of labeled graphs is still an open challenge. Given a large edge-labeled network, e.g., a time-evolving network, how can we find interesting patterns? We propose Com $$^2$$2 , a novel, fast and incremental tensor analysis approach which can discover communities appearing over subsets of the labels. The method is (a) scalable, being linear on the input size, (b) general, (c) needs no user-defined parameters and (d) effective, returning results that agree with intuition. We apply our method to real datasets, including a phone call network, a computer-traffic network and a flight information network. The phone call network consists of 4 million mobile users, with 51 million edges (phone calls), over 14 days, while the flights dataset consists of 7733 airports and 5995 airline companies flying 67,663 different routes. We show that Com $$^2$$2 spots intuitive patterns regarding edge labels that carry temporal or other discrete information. Our findings include large ""star""-like patterns, near-bipartite cores, as well as tiny groups (five users), calling each other hundreds of times within a few days. We also show that we are able to automatically identify competing airline companies.",2016,Knowledge and Information Systems volume 46 issue 3 pp 657-677,temporal database;world wide web;data mining;database;artificial intelligence;machine learning;simulation;computer science;
Shifting multi-hypergraphs via collaborative probabilistic voting,Yang Wang (University of New South Wales);Xuemin Lin (University of New South Wales);Lin Wu (University of New South Wales);Qing Zhang (University of New South Wales);Wenjie Zhang (University of New South Wales);,"2629454295,2125481875,2147397982,2515257247,2232753573","Graphs are widely utilized to characterize the complex relationship among big data. Graph mode seeking is of great importance to many applications in data mining and machine learning era, and it attracts a number of approaches. Typically, existing methods, e.g., graph shift, focus on shifting vertices based on pairwise edges (i.e., an edge connecting two vertices) to find the cohesively dense subgraph. However, they overlooked the semantics of these subgraphs, resulting into undesirable results to the users in specific applications, e.g., saliency detection. In this paper, we propose a novel paradigm aimed at shifting high-order edges (i.e., hyperedges) to deliver graph modes, via a novel probabilistic voting strategy. As a result, the generated graph modes based on dense subhypergraphs may more accurately capture the semantics of objects besides the self-cohesiveness requirement. It is widely known that data objects are always described by multiple features or multi-views, e.g., an image has a color feature and shape feature, where the information provided by all views are complementary to each other. Based on such fact, we propose another novel technique of shifting multiple hypergraphs, each of which corresponds to one view, by conducting a novel collaborative probabilistic voting strategy, named SMHCPV, so as to further improve the performance over hypergraph shift method. Extensive experiments are conducted on both synthetic and real-world datasets to validate the superiority of our proposed technique for both hypergraph shift and SMHCPV.",2016,Knowledge and Information Systems volume 46 issue 3 pp 515-536,theoretical computer science;data mining;database;artificial intelligence;machine learning;computer science;
An ensemble of cluster-based classifiers for semi-supervised classification of non-stationary data streams,Mohammad Javad Hosseini (Sharif University of Technology);Ameneh Gholipour (Sharif University of Technology);Hamid Beigy (Sharif University of Technology);,"2156702240,1975456290,2076361465","Recent advances in storage and processing have provided the possibility of automatic gathering of information, which in turn leads to fast and continuous flows of data. The data which are produced and stored in this way are called data streams. Data streams are produced in large size, and much dynamism and have some unique properties which make them applicable to model many real data mining applications. The main challenge of streaming data is the occurrence of concept drift. In addition, regarding the costs of labeling of instances, it is often assumed that only a small fraction of instances are labeled. In this paper, we propose an ensemble algorithm to classify instances of non-stationary data streams in a semi-supervised environment. Furthermore, this method is intended to recognize recurring concept drifts of data streams. In the proposed algorithm, a pool of classifiers is maintained by the algorithm with each classifier being representative of one single concept. At first, a batch of instances is classified by the algorithm. Thereafter, some of these instances are labeled and this partially labeled batch is used to update the classifiers in the pool. This process repeats for consecutive batches of the streams. The main advantage of the algorithm is that it uses unlabeled instances as well as labeled ones in the learning task. Experimental results show the effectiveness of the proposed algorithm over the state-of-the-art methods, in different aspects.",2016,Knowledge and Information Systems volume 46 issue 3 pp 567-597,concept drift;ensemble learning;data mining;pattern recognition;machine learning;computer science;
Classifying imbalanced data in distance-based feature space,Shin Ando (Tokyo University of Science);,2607818638,"Class imbalance is a significant issue in practical classification problems. Important countermeasures, such as re-sampling, instance-weighting, and cost-sensitive learning have been developed, but there are limitations as well as advantages to respective approaches. The synthetic re-sampling methods have wide applicability, but require a vector representation to generate additional instances. The instance-based methods can be applied to distance space data, but are not tractable with regard to a global objective. The cost-sensitive learning can minimize the expected cost given the costs of error, but generally does not extend to nonlinear measures, such as F-measure and area under the curve. In order to address the above shortcomings, this paper proposes a nearest neighbor classification model which employs a class-wise weighting scheme to counteract the class imbalance and a convex optimization technique to learn its weight parameters. As a result, the proposed model maintains the simple instance-based rule for prediction, yet retains a mathematical support for learning to maximize a nonlinear performance measure over the training set. An empirical study is conducted to evaluate the performance of the proposed algorithm on the imbalanced distance space data and make comparison with existing methods.",2016,Knowledge and Information Systems volume 46 issue 3 pp 707-730,data mining;pattern recognition;artificial intelligence;machine learning;statistics;mathematics;
Analysis of incomplete and inconsistent clinical survey data,Suzan Arslanturk (Oakland University);Mohammad-Reza Siadat (Oakland University);Theophilus Ogunyemi (Oakland University);Kim Killinger (Oakland University);Ananias Diokno (Institute for Social Research);,"963006894,2251579331,2067615220,1480984080,128577641","It is common for clinical data in survey trials to be incomplete and inconsistent for several reasons. Inconsistent data occur when more than one set of exclusive alternative questions are answered. One objective of this study was to identify and eliminate inconsistent data as an important data mining preprocessing step. We define three types of incomplete data: missing data due to skip pattern (SPMD), undetermined missing data (UMD), and genuine missing data (GMD). Identifying the type of missing data is another important objective as all missing data types cannot be treated the same. This goal cannot be achieved manually on large data of complex surveys since each subject should be processed individually. The analyses are accomplished in a mathematical framework by exploiting graph theoretic structure inherent in the questionnaire. An undirected graph is built using mutually inconsistent responses as well as its complement. The responses not in the largest maximal clique of complement graph are considered inconsistent. This guarantees removing as few responses as possible so that remaining ones are mutually consistent. Further, all potential paths in questionnaire's graph are considered, based on the responses of subjects, to identify each type of incomplete data. Experiments are conducted on MESA data. Results show 15.4 % GMD, 9.8 % SPMD, 12.9 % UMD, and 0.021 % inconsistent data. Further utility of the approach is using a) the SPMD for data stratification, and b) inconsistent data for noise estimation. Proposed method is a preprocessing prerequisite for any data mining of clinical survey data.",2016,Knowledge and Information Systems volume 46 issue 3 pp 731-750,missing data;graph theory;data mining;database;statistics;computer science;
Extend tree edit distance for effective object identification,"Yue Wang (Harbin Institute of Technology);Hongzhi Wang (Harbin Institute of Technology);Liyan Zhang (University of California, Irvine);Yang Wang (Harbin Institute of Technology);Jianzhong Li (Harbin Institute of Technology);Hong Gao (Harbin Institute of Technology);","2676258686,2589657406,2649890403,2693183795,2130201582,2607637236","Similarity join on XML documents which are usually modeled as rooted ordered labeled trees is widely applied, due to the ambiguity of references to the real-world objects. The conventional method dealing with this issue is based on tree edit distance, which is shortage of flexibility and efficiency. In this paper, we propose two novel edit operations together with extended tree edit distance, which can achieve good performance in similarity matching with hierarchical data structures [the run-time is $$O(n^{3})$$O(n3) in the worst case]. And then, we propose $$k$$k-generation set distance as a good approximation of the tree edit distance to further improve the join efficiency with quadric time complexity. Experiments on real and synthetic databases demonstrate the benefit of our method in efficiency and scalability.",2016,Knowledge and Information Systems volume 46 issue 3 pp 629-656,string to string correction problem;wagner fischer algorithm;jaro winkler distance;damerau levenshtein distance;edit distance;cluster;theoretical computer science;pattern recognition;programming language;algorithm;computer science;mathematics;
Frequent pattern mining in attributed trees: algorithms and applications,Claude Pasquier (University of Nice Sophia Antipolis);Jérémy Sanhes (University of New Caledonia);Frédéric Flouvat (University of New Caledonia);Nazha Selmaoui-Folcher (University of New Caledonia);,"2304010088,1826130176,2093516567,1967950925","Frequent pattern mining is an important data mining task with a broad range of applications. Initially focused on the discovery of frequent itemsets, studies were extended to mine structural forms like sequences, trees or graphs. In this paper, we introduce a new domain of patterns, attributed trees (atrees), and a method to extract these patterns in a forest of atrees. Attributed trees are trees in which vertices are associated with itemsets. Mining this type of patterns (called asubtrees), which combines tree mining and itemset mining, requires the exploration of a huge search space. To make our approach scalable, we investigate the mining of condensed representations. For attributed trees, the classical concept of closure involves both itemset closure and structural closure. We present three algorithms for mining all patterns, closed patterns w.r.t. itemsets (content) and/or structure in attributed trees. We show that, for low support values, mining content-closed attributed trees is a good compromise between non-redundancy of solutions and execution time.",2016,Knowledge and Information Systems volume 46 issue 3 pp 491-514,data mining;pattern recognition;machine learning;mathematics;
Hypergraph querying using structural indexing and layer-related-closure verification,Xinran Yu (University of Texas at San Antonio);Turgay Korkmaz (University of Texas at San Antonio);,"2105866037,2119891203","Graph indexing and querying mechanisms have been receiving significant attention due to their importance in analyzing the growing graph datasets in many domains. Although much work has been done in the context of simple graphs, they are not directly applicable to hypergraphs that represent more complex relationships in various applications. The key problem here is to search a given subhypergraph query in a larger hypergraph dataset. This search problem is known to be NP-hard as it is related to graph isomorphism. To solve this search problem in an efficient manner, we first create an index set by extracting the common subhypergraph structures from the given hypergraph dataset. Upon receiving a query, we use the same indexing techniques and create a query index set from the given subhypergraph. Utilizing both indices, we identify the possible locations of the query in the hypergraph dataset. We then start the subhypergraph search to verify whether the query really appears at each location by using an accelerated verification mechanism called layer-related-closure method. Through experiments on a real hypergraph dataset and random datasets, we demonstrate the efficiency and effectiveness of hypergraph indexing and our verification method.",2016,Knowledge and Information Systems volume 46 issue 3 pp 537-565,search engine indexing;theoretical computer science;data mining;database;pattern recognition;computer science;
The logic transformations for reducing the complexity of the discernibility function-based attribute reduction problem,Mehmet Hacibeyoglu (Selçuk University);Mohammad Shukri Salman;Murat Selek (Selçuk University);Sirzat Kahramanli (Selçuk University);,"170069353,2690464100,1895001301,2012387348","The basic solution for locating an optimal reduct is to generate all possible reducts and select the one that best meets the given criterion. Since this problem is NP-hard, most attribute reduction algorithms use heuristics to find a single reduct with the risk to overlook for the best ones. There is a discernibility function (DF)-based approach that generates all reducts but may fail due to memory overflows even for datasets with dimensionality much below the medium. In this study, we show that the main shortcoming of this approach is its excessively high space complexity. To overcome this, we first represent a DF of $$n$$n attributes by a bit-matrix (BM). Second, we partition the BM into no more than $$n-1$$n-1 sub-BMs (SBMs). Third, we convert each SBM into a subset of reducts by preventing the generation of redundant products, and finally, we unite the subsets into a complete set of reducts. Among the SBMs of a BM, the most complex one is the first SBM with a space complexity not greater than the square root of that of the original BM. The proposed algorithm converts such a SBM with $$n$$n attributes into the subset of reducts with the worst case space complexity of $$\left( _{n/2}^n \right) /2$$n/2n/2.",2016,Knowledge and Information Systems volume 46 issue 3 pp 599-628,set cover problem;computational complexity theory;discrete mathematics;machine learning;algorithm;computer science;mathematics;
Learning user credibility for product ranking,Rong Zhang (East China Normal University);Ming Gao (East China Normal University);Xiaofeng He (East China Normal University);Aoying Zhou (East China Normal University);,"2423758776,2233450724,2005481251,2111470091","As the explosion of user-generated data (UGC) in electronic commerce, this kind of data is scanned for trust or credibility calculation, which plays an important role in business selection. The commonly used UGC is user reviews and ratings. A new consumer without any experience with some product will read these UGCs to get an overview. However, the open and dynamic e-commerce platforms may rise the generation of unfair or deceitful reviews and ratings. Then, detecting trustful reviewers or generating authentic ratings for customers is urgent and useful. In this paper, we present a twin-bipartite graph model to catch the review and ranking relationship among users, products and shops. We design a feedback mechanism to get the consistent ranking among different level of objects, which are users and items. In the algorithm, we adjust customer credibility values by the feedback considering the rating consistency; we adjust ratings by combining customer credibility together with originally assigned ratings. We increase the credibility for a customer if the customer gives a high (low) score to a good (bad) product and decrease the value if the customer gives a low (high) score to a good (bad) product. We detect the inconsistency between semantic ratings (the review comments) and numerical ratings (scores). To deal with it, we train a classifier on the training data that are constructed automatically. The trained classifier is used to predict the semantic scores from review comments. Finally, we calculate the scores of products by considering both the customer credibility and the predicted scores. We conduct experiments using a large amount of real-world data. The experimental results show that our proposed approach provides better products ranking than the baseline systems.",2016,Knowledge and Information Systems volume 46 issue 3 pp 679-705,world wide web;data mining;database;
A survey on indexing techniques for big data: taxonomy and performance evaluation,Abdullah Gani (Information Technology University);Aisha Siddiqa (Information Technology University);Shahaboddin Shamshirband (Information Technology University);Fariza Hanum (Information Technology University);,"2120729324,2156156061,1635769602,2511999723","The explosive growth in volume, velocity, and diversity of data produced by mobile devices and cloud applications has contributed to the abundance of data or `big data.' Available solutions for efficient data storage and management cannot fulfill the needs of such heterogeneous data where the amount of data is continuously increasing. For efficient retrieval and management, existing indexing solutions become inefficient with the rapidly growing index size and seek time and an optimized index scheme is required for big data. Regarding real-world applications, the indexing issue with big data in cloud computing is widespread in healthcare, enterprises, scientific experiments, and social networks. To date, diverse soft computing, machine learning, and other techniques in terms of artificial intelligence have been utilized to satisfy the indexing requirements, yet in the literature, there is no reported state-of-the-art survey investigating the performance and consequences of techniques for solving indexing in big data issues as they enter cloud computing. The objective of this paper is to investigate and examine the existing indexing techniques for big data. Taxonomy of indexing techniques is developed to provide insight to enable researchers understand and select a technique as a basis to design an indexing mechanism with reduced time and space consumption for BD-MCC. In this study, 48 indexing techniques have been studied and compared based on 60 articles related to the topic. The indexing techniques' performance is analyzed based on their characteristics and big data indexing requirements. The main contribution of this study is taxonomy of categorized indexing techniques based on their method. The categories are non-artificial intelligence, artificial intelligence, and collaborative artificial intelligence indexing methods. In addition, the significance of different procedures and performance is analyzed, besides limitations of each technique. In conclusion, several key future research topics with potential to accelerate the progress and deployment of artificial intelligence-based cooperative indexing in BD-MCC are elaborated on.",2016,Knowledge and Information Systems volume 46 issue 2 pp 241-284,cloud computing;search engine indexing;big data;data science;information retrieval;data mining;database;artificial intelligence;machine learning;computer science;
Multiple task transfer learning with small sample sizes,Budhaditya Saha (Deakin University);Sunil Kumar Gupta 0001 (Deakin University);Dinh Q. Phung (Deakin University);Svetha Venkatesh (Deakin University);,"2125102313,2119406083,2314522249,2146461601","Prognosis, such as predicting mortality, is common in medicine. When confronted with small numbers of samples, as in rare medical conditions, the task is challenging. We propose a framework for classification with data with small numbers of samples. Conceptually, our solution is a hybrid of multi-task and transfer learning, employing data samples from source tasks as in transfer learning, but considering all tasks together as in multi-task learning. Each task is modelled jointly with other related tasks by directly augmenting the data from other tasks. The degree of augmentation depends on the task relatedness and is estimated directly from the data. We apply the model on three diverse real-world data sets (healthcare data, handwritten digit data and face data) and show that our method outperforms several state-of-the-art multi-task learning baselines. We extend the model for online multi-task learning where the model parameters are incrementally updated given new data or new tasks. The novelty of our method lies in offering a hybrid multi-task/transfer learning model to exploit sharing across tasks at the data-level and joint parameter learning.",2016,Knowledge and Information Systems volume 46 issue 2 pp 315-342,online machine learning;inductive transfer;multi task learning;generalization error;adaptation;technology;active learning;semi supervised learning;unsupervised learning;data science;data mining;database;artificial intelligence;machine learning;computer science;
HICC: an entropy splitting-based framework for hierarchical co-clustering,"Wei Cheng (University of North Carolina at Chapel Hill);Xiang Zhang (Case Western Reserve University);Feng Pan (Microsoft);Wei Wang (University of California, Los Angeles);","2620045292,2553248206,2620055343,2315689540","Two-dimensional contingency tables or co-occurrence matrices arise frequently in various important applications such as text analysis and web-log mining. As a fundamental research topic, co-clustering aims to generate a meaningful partition of the contingency table to reveal hidden relationships between rows and columns. Traditional co-clustering algorithms usually produce a predefined number of flat partition of both rows and columns, which do not reveal relationship among clusters. To address this limitation, hierarchical co-clustering algorithms have attracted a lot of research interests recently. Although successful in various applications, the existing hierarchical co-clustering algorithms are usually based on certain heuristics and do not have solid theoretical background. In this paper, we present a new co-clustering algorithm, HICC, with solid theoretical background. It simultaneously constructs a hierarchical structure of both row and column clusters, which retains sufficient mutual information between rows and columns of the contingency table. An efficient and effective greedy algorithm is developed, which grows a co-cluster hierarchy by successively performing row-wise or column-wise splits that lead to the maximal mutual information gain. Extensive experiments on both synthetic and real datasets demonstrate that our algorithm can reveal essential relationships of row (and column) clusters and has better clustering precision than existing algorithms. Moreover, the experiments on real dataset show that HICC can effectively reveal hidden relationships between rows and columns in the contingency table.",2016,Knowledge and Information Systems volume 46 issue 2 pp 343-367,contingency table;entropy;biclustering;text mining;data mining;database;artificial intelligence;machine learning;statistics;computer science;mathematics;
Efficient entity resolution based on subgraph cohesion,Hongzhi Wang (Harbin Institute of Technology);Jianzhong Li (Harbin Institute of Technology);Hong Gao (Harbin Institute of Technology);,"2589657406,2130201582,2607637236","Entity resolution has wide applications and receives considerable attentions in literature. For entity resolution, similarity functions are often used to judge whether two data objects refer to the same real-world entity. However, the similar relations determined by many commonly used similarity functions lack transitivity. This fact results in the conflict that $$A$$A and $$B$$B refer to the same entity and $$B$$B and $$C$$C refer to the same entity, but $$A$$A and $$C$$C do not refer to the same entity. To address this problem and make the group-wise entity resolution results consistent with pairwise entity resolution, this paper models the entity resolution problem as the partition of the vertices in a weighted graph into cohesive subgraphs, which is proven to be co-NP-complete. To solve this problem, an approximate algorithm with approximation ratio bound is proposed. For performing entity resolution on a large data set efficiently, a heuristic algorithm is developed to address this problem. In order to implement the heuristic algorithm efficiently, a similarity measure compatible with many measures in common usage is presented. With such similarity measure, indices and efficient implementations for the heuristic algorithm are proposed. Extensive experiments have been performed to verify the efficiency and effectiveness of the methods in this paper.",2016,Knowledge and Information Systems volume 46 issue 2 pp 285-314,name resolution;data quality;data mining;database;algorithm;computer science;mathematics;
POI recommendation through cross-region collaborative filtering,Chenyi Zhang (Zhejiang University);Ke Wang (Simon Fraser University);,"2617976177,2626264286","Recommending points of interest (POIs) to a user according to the user's current location and past check-in activities is the focus in this paper. Previously proposed probabilistic and topic model-based methods predict the POIs based on the distribution of the POIs visited in the past, assuming that the next POI for the user follows the same distribution. Such methods tend to recommend the POIs in the cities or regions that the user has visited before because only such cities or regions have observed ratings for the user. Thus, these works are not suitable for a user who travels to a new city or region where she has not checked-in any POI previously. To address this issue, we distinguish the user preferences on the content of POIs from the user preferences on the POIs themselves. The former is long term and is independent of where POIs are located, and the latter is short term and is constrained by the proximity of the location of the POI and the user's current location. This distinction motivates a location-independent modeling of user's content preferences of POIs, and a location-aware modeling of user's location preferences of POIs. The final recommendation of POIs is derived by combining the predicted rating on content and the predicted rating on location of POI. We evaluate this method using the Yelp and Foursquare data sets. This approach has superiority over the state-of-the-art and works well in the ""new city"" situation in which the user has not rated any of the POIs in the current region.",2016,Knowledge and Information Systems volume 46 issue 2 pp 369-387,collaborative filtering;multimedia;world wide web;machine learning;computer science;
Missing value imputation using a fuzzy clustering-based EM approach,Md. Geaur Rahman (Charles Sturt University);Zahidul Islam (Charles Sturt University);,"2115150292,2124035941","Data preprocessing and cleansing play a vital role in data mining by ensuring good quality of data. Data-cleansing tasks include imputation of missing values, identification of outliers, and identification and correction of noisy data. In this paper, we present a novel technique called A Fuzzy Expectation Maximization and Fuzzy Clustering-based Missing Value Imputation Framework for Data Pre-processing (FEMI). It imputes numerical and categorical missing values by making an educated guess based on records that are similar to the record having a missing value. While identifying a group of similar records and making a guess based on the group, it applies a fuzzy clustering approach and our novel fuzzy expectation maximization algorithm. We evaluate FEMI on eight publicly available natural data sets by comparing its performance with the performance of five high-quality existing techniques, namely EMI, GkNN, FKMI, SVR and IBLLS. We use thirty-two types (patterns) of missing values for each data set. Two evaluation criteria namely root mean squared error and mean absolute error are used. Our experimental results indicate (according to a confidence interval and $$t$$t test analysis) that FEMI performs significantly better than EMI, GkNN, FKMI, SVR, and IBLLS.",2016,Knowledge and Information Systems volume 46 issue 2 pp 389-422,data cleansing;fuzzy clustering;data pre processing;imputation;data quality;missing data;data mining;pattern recognition;statistics;computer science;
Learning from automatically labeled data: case study on click fraud prediction,Daniel Berrar (Tokyo Institute of Technology);,116046974,"In the era of big data, both class labels and covariates may result from proprietary algorithms or ground models. The predictions of these ground models, however, are not the same as the unknown ground truth. Thus, the automatically generated class labels are inherently uncertain, making subsequent supervised learning from such data a challenging task. Fine-tuning a new classifier could mean that, at the extreme, this new classifier will try to replicate the decision heuristics of the ground model. However, few new insights can be expected from a model that tries to merely emulate another one. Here, we study this problem in the context of click fraud prediction from highly skewed data that were automatically labeled by a proprietary detection algorithm. We propose a new approach to generate click profiles for publishers of online advertisements. In a blinded test, our ensemble of random forests achieved an average precision of only 36.2 %, meaning that our predictions do not agree very well with those of the ground model. We tried to elucidate this discrepancy and made several interesting observations. Our results suggest that supervised learning from automatically labeled data should be complemented by an interpretation of conflicting predictions between the new classifier and the ground model. If the ground truth is not known, then elucidating such disagreements might be more relevant than improving the performance of the new classifier.",2016,Knowledge and Information Systems volume 46 issue 2 pp 477-490,random forest;biological classification;big data;ensemble learning;data science;data mining;database;artificial intelligence;machine learning;statistics;computer science;
Minimizing response time in time series classification,Shin Ando (Tokyo University of Science);Einoshin Suzuki (Kyushu University);,"2607818638,2228069075","Providing a timely output is one of the important criteria in applications of time series classification. Recent studies have been motivated to explore models of early prediction, prediction based on truncated temporal observations. The truncation of input improves the response time, but generally reduces the reliability of the prediction. The trade-off between the earliness and the accuracy is an inherent challenge of learning an early prediction model. In this paper, we present an optimization-based approach for learning an ensemble model for timely prediction with an intuitive objective function. The proposed model is comprised of time series classifiers with different response time, and a sequential aggregation procedure to determine the single timing of its output. We formalize the training of the ensemble classifier as a quadratic programming problem and present an iterative algorithm which minimizes an empirical risk function and the response time required to achieve the minimal risk simultaneously. We conduct an empirical study using a collection of behavior and time series datasets to evaluate the proposed algorithm. In the comparisons of the traditional and time-sensitive performance measures, the ensemble framework showed significant advantages over the existing methods on early prediction.",2016,Knowledge and Information Systems volume 46 issue 2 pp 449-476,data mining;pattern recognition;machine learning;statistics;computer science;
A fast and efficient Hamming LSH-based scheme for accurate linkage,Dimitrios Karapiperis (Hellenic Open University);Vassilios S. Verykios (Hellenic Open University);,"1879788522,77221159","In this paper, we propose an efficient scheme for privacy-preserving record linkage by using the Hamming locality-sensitive hashing technique as the blocking mechanism and the Bloom filter-based encoding method for anonymizing the data sets at hand. We achieve highly accurate results and simultaneously reduce significantly the computational cost by minimizing the number of distance computations performed. Our scheme provides theoretical guarantees for identifying the similar anonymized record pairs by conducting redundant blocking and by performing a distance computation only if the corresponding anonymized record pair is formulated a specified number of times. A series of experiments illustrate the efficacy of our scheme in identifying the similar record pairs, while simultaneously keeping the running time exceptionally low.",2016,Knowledge and Information Systems volume 49 issue 3 pp 861-884,locality sensitive hashing;bloom filter;blocking;theoretical computer science;distributed computing;data mining;machine learning;statistics;computer science;mathematics;
A new transfer learning framework with application to model-agnostic multi-task learning,Sunil Kumar Gupta 0001 (Deakin University);Santu Rana (Deakin University);Budhaditya Saha (Deakin University);Dinh Q. Phung (Deakin University);Svetha Venkatesh (Deakin University);,"2119406083,2142238370,2125102313,2314522249,2146461601","Learning from small number of examples is a challenging problem in machine learning. An effective way to improve the performance is through exploiting knowledge from other related tasks. Multi-task learning (MTL) is one such useful paradigm that aims to improve the performance through jointly modeling multiple related tasks. Although there exist numerous classification or regression models in machine learning literature, most of the MTL models are built around ridge or logistic regression. There exist some limited works, which propose multi-task extension of techniques such as support vector machine, Gaussian processes. However, all these MTL models are tied to specific classification or regression algorithms and there is no single MTL algorithm that can be used at a meta level for any given learning algorithm. Addressing this problem, we propose a generic, model-agnostic joint modeling framework that can take any classification or regression algorithm of a practitioner’s choice (standard or custom-built) and build its MTL variant. The key observation that drives our framework is that due to small number of examples, the estimates of task parameters are usually poor, and we show that this leads to an under-estimation of task relatedness between any two tasks with high probability. We derive an algorithm that brings the tasks closer to their true relatedness by improving the estimates of task parameters. This is achieved by appropriate sharing of data across tasks. We provide the detail theoretical underpinning of the algorithm. Through our experiments with both synthetic and real datasets, we demonstrate that the multi-task variants of several classifiers/regressors (logistic regression, support vector machine, K-nearest neighbor, Random Forest, ridge regression, support vector regression) convincingly outperform their single-task counterparts. We also show that the proposed model performs comparable or better than many state-of-the-art MTL and transfer learning baselines.",2016,Knowledge and Information Systems volume 49 issue 3 pp 933-973,multi task learning;regression;biological classification;data mining;database;pattern recognition;artificial intelligence;machine learning;statistics;computer science;
Combining supervised term-weighting metrics for SVM text classification with extended term representation,Mounia Haddoud (University of Rouen);Aïcha Mokhtari;Thierry Lecroq (University of Rouen);Saïd Abdeddaïm (University of Rouen);,"2273813922,2659762838,116349491,2049301195","The accuracy of a text classification method based on a SVM learner depends on the weighting metric used in order to assign a weight to a term. Weighting metrics can be classified as supervised or unsupervised according to whether they use prior information on the number of documents belonging to each category. A supervised metric should be highly informative about the relation of a document term to a category, and discriminative in separating the positive documents from the negative documents for this category. In this paper, we propose 80 metrics never used for the term-weighting problem and compare them to 16 functions of the literature. A large number of these metrics were initially proposed for other data mining problems: feature selection, classification rules and term collocations. While many previous works have shown the merits of using a particular metric, our experience suggests that the results obtained by such metrics can be highly dependent on the label distribution on the corpus and on the performance measures used (microaveraged or macroaveraged \(F_1\)-Score). The solution that we propose consists in combining the metrics in order to improve the classification. More precisely, we show that using a SVM classifier which combines the outputs of SVM classifiers that utilize different metrics performs well in all situations. The second main contribution of this paper is an extended term representation for the vector space model that improves significantly the prediction of the text classifier.",2016,Knowledge and Information Systems volume 49 issue 3 pp 909-931,support vector machine;data mining;pattern recognition;machine learning;computer science;mathematics;
Constrained-meta-path-based ranking in heterogeneous information network,Chuan Shi (Beijing University of Posts and Telecommunications);Yitong Li (Beijing University of Posts and Telecommunications);Philip S. Yu (University of Illinois at Chicago);Bin Wu (Beijing University of Posts and Telecommunications);,"2252461150,2104429188,2125104194,2464938123","Recently, there is a surge of interests on heterogeneous information network analysis, where the network includes different types of objects or links. As a newly emerging network model, heterogeneous information networks have many unique features, e.g., complex structure and rich semantics. Moreover, meta path, the sequence of relations connecting two object types, is widely used to integrate different types of objects and mine the semantics information in this kind of networks. The object ranking is an important and basic function in network analysis, which has been extensively studied in homogeneous networks including the same type of objects and links. However, it is not well exploited in heterogeneous networks until now, since the characteristics of heterogeneous networks introduce new challenges for object ranking. In this paper, we study the ranking problem in heterogeneous networks and propose the HRank method to evaluate the importance of multiple types of objects and meta paths. Since the traditional meta path coarsely embodies path semantics, we propose a constrained meta path to subtly capture the refined semantics through confining constraints on objects. Based on a path-constrained random walk process, HRank can simultaneously determine the importance of objects and constrained meta paths through applying the tensor analysis. Extensive experiments on three real datasets show that HRank can effectively evaluate the importance of objects and paths together. Moreover, the constrained meta path shows its potential on mining subtle semantics by obtaining more accurate ranking results.",2016,Knowledge and Information Systems volume 49 issue 2 pp 1-29,ranking;tensor;random walk;theoretical computer science;data mining;database;machine learning;statistics;mathematics;
Multi-view ensemble learning: an optimal feature set partitioning for high-dimensional data classification,Vipin Kumar (Jawaharlal Nehru University);Sonajharia Minz (Jawaharlal Nehru University);,"2287094540,2031755020","Multi-view ensemble learning has the potential to address issues related to the high dimensionality of data. It attempts to utilize all the relevant only discarding the irrelevant features. The view of a dataset is the sub-table of the training data with respect to a subset of the feature set. The problem of discarding the irrelevant features and obtaining subsets of the relevant features is useful for dimension reduction and dealing with the problem of having fewer training examples than even the reduced set of relevant features. A feature set partitioning resulting in the blocks of relevant features may not yield multiple-view-based classifiers with good classification performance. In this work the optimal feature set partition approach has been proposed. Further, the ensemble learning from views aims to maximize the performance of the classifier. The experiments study the performance of random feature set partitioning, attribute bagging, view generation using attribute clustering, view construction using genetic algorithm and OFSP proposed method. The blocks of relevant feature subsets are used to construct the multi-view classifier ensemble using K-nearest neighbor, Naive Bayesian and support vector machine algorithm applied to sixteen high-dimensional data sets from UCI machine learning repository. The performance parameters considered for comparison are classification accuracy, disagreement among the classifiers, execution time and percentage reduction of attributes.",2016,Knowledge and Information Systems volume 49 issue 1 pp 1-59,linear classifier;dimensionality reduction;feature;feature extraction;biological classification;ensemble learning;data mining;pattern recognition;machine learning;computer science;
Privacy-preserving topic model for tagging recommender systems,Tianqing Zhu (Deakin University);Gang Li (Deakin University);Wanlei Zhou (Deakin University);Ping Xiong (Zhongnan University of Economics and Law);Cao Yuan (Wuhan Polytechnic University);,"2135547451,2098057600,2171981955,2020434285,2672479939","Tagging recommender systems provide users the freedom to explore tags and obtain recommendations. The releasing and sharing of these tagging datasets will accelerate both commercial and research work on recommender systems. However, releasing the original tagging datasets is usually confronted with serious privacy concerns, because adversaries may re-identify a user and her/his sensitive information from tagging datasets with only a little background information. Recently, several privacy techniques have been proposed to address the problem, but most of these lack a strict privacy notion, and rarely prevent individuals being re-identified from the dataset. This paper proposes a privacy- preserving tag release algorithm, PriTop. This algorithm is designed to satisfy differential privacy, a strict privacy notion with the goal of protecting users in a tagging dataset. The proposed PriTop algorithm includes three privacy-preserving operations: Private topic model generation structures the uncontrolled tags; private weight perturbation adds Laplace noise into the weights to hide the numbers of tags; while private tag selection finally finds the most suitable replacement tags for the original tags, so the exact tags can be hidden. We present extensive experimental results on four real-world datasets, Delicious, MovieLens, Last.fm and BibSonomy. While the recommendation algorithm is successful in all the cases, our results further suggest the proposed PriTop algorithm can successfully retain the utility of the datasets while preserving privacy.",2016,Knowledge and Information Systems volume 46 issue 1 pp 33-58,privacy software;tag system;differential privacy;topic model;recommender system;internet privacy;world wide web;data mining;database;machine learning;computer science;
Ternary Matrix Factorization: problem definitions and algorithms,Samuel Maurus (Technische Universität München);Claudia Plant (Technische Universität München);,"1989291604,2122910652","Can we learn from the unknown? Logical data sets of the ternary kind are often found in information systems. They contain unknown as well as true/false values. An unknown value may represent a missing entry (lost or indeterminable) or have meaning, like a Don't Know response in a questionnaire. In this paper, we introduce algorithms for reducing the dimensionality of logical data (categorical data in general) in the context of a new data mining challenge: Ternary Matrix Factorization (TMF). For a ternary data matrix, TMF exploits ternary logic to produce a basis matrix (which holds the major patterns in the data) and a usage matrix (which maps patterns to original observations). Both matrices are interpretable, and their ternary matrix product approximates the original matrix. TMF has applications in (1) finding targeted structure in ternary data, (2) imputing values through pattern discovery in highly incomplete categorical data sets, and (3) solving instances of its encapsulated Binary Matrix Factorization problem. Our elegant algorithm FasTer (FASt TERnary Matrix Factorization) has linear run-time complexity with respect to the dimensions of the data set and is parameter-robust. A variant of FasTer that exploits useful results from combinatorics provides accuracy bounds for a core part of the algorithm in certain situations. Experiments on synthetic and real-world data sets show that our algorithms are able to outperform state-of-the-art techniques in all three TMF applications with respect to run-time and effectiveness. Finally, convincing speedup and efficiency results on a parallel version of FasTer demonstrate its suitability for weak- and strong-scaling scenarios.",2016,Knowledge and Information Systems volume 46 issue 1 pp 1-31,logical matrix;imputation;parallel algorithm;dimensionality reduction;matrix decomposition;theoretical computer science;data mining;database;artificial intelligence;machine learning;statistics;computer science;mathematics;
Discovery of time series $$k$$k-motifs based on multidimensional index,"Nguyen Thanh Son (Faculty of Information Technology, University Džemal Bijedić of Mostar);Duong Tuan Anh (Ho Chi Minh City University of Technology);","2557705533,2133075065","Time series motifs are frequently occurring but previously unknown subsequences of a longer time series. Discovering time series motifs is a crucial task in time series data mining. In time series motif discovery algorithm, finding nearest neighbors of a subsequence is the basic operation. To make this basic operation efficient, we can make use of some advanced multidimensional index structure for time series data. In this paper, we propose two novel algorithms for discovering motifs in time series data: The first algorithm is based on $$\hbox {R}^{*}$$R?-tree and early abandoning technique and the second algorithm makes use of a dimensionality reduction method and state-of-the-art Skyline index. We demonstrate that the effectiveness of our proposed algorithms by experimenting on real datasets from different areas. The experimental results reveal that our two proposed algorithms outperform the most popular method, random projection, in time efficiency while bring out the same accuracy.",2016,Knowledge and Information Systems volume 46 issue 1 pp 59-86,r tree;time series;data science;data mining;machine learning;statistics;computer science;
An efficient pattern mining approach for event detection in multivariate temporal data,Iyad Batal (GE Global Research);Gregory F. Cooper (University of Pittsburgh);Dmitriy Fradkin (Siemens);H James Harrison (University of Virginia);Fabian Moerchen (Amazon.com);Milos Hauskrecht (University of Pittsburgh);,"1841724547,2137326150,2028188512,2510947214,1991190249,85581826","This work proposes a pattern mining approach to learn event detection models from complex multivariate temporal data, such as electronic health records. We present recent temporal pattern mining, a novel approach for efficiently finding predictive patterns for event detection problems. This approach first converts the time series data into time-interval sequences of temporal abstractions. It then constructs more complex time-interval patterns backward in time using temporal operators. We also present the minimal predictive recent temporal patterns framework for selecting a small set of predictive and non-spurious patterns. We apply our methods for predicting adverse medical events in real-world clinical data. The results demonstrate the benefits of our methods in learning accurate event detection models, which is a key step for developing intelligent patient monitoring and decision support systems.",2016,Knowledge and Information Systems volume 46 issue 1 pp 115-150,data science;data mining;pattern recognition;computer science;
A novel algorithm for mining closed temporal patterns from interval-based data,Yi-Cheng Chen (Tamkang University);Julia Tzu-Ya Weng (Yuan Ze University);Lin Hui (Tamkang University);,"2309950950,2675093132,2649115387","Closed sequential patterns have attracted researchers' attention due to their capability of using compact results to preserve the same expressive power as conventional sequential patterns. However, studies to date have mainly focused on mining conventional patterns from time interval-based data, where each datum persists for a period of time. Few research efforts have elaborated on discovering closed interval-based sequential patterns (also referred to as closed temporal patterns). Mining closed temporal patterns are an arduous problem since the pairwise relationships between two interval-based events are intrinsically complex. In this paper, we develop an efficient algorithm, CCMiner, which stands for Closed Coincidence Miner to discover frequent closed patterns from interval-based data. The algorithm also employs some optimization techniques to effectively reduce the search space. The experimental results on both synthetic and real datasets indicate that CCMiner not only significantly outperforms the prior interval-based mining algorithms in execution time but also possesses graceful scalability. Furthermore, we also apply CCMiner to a real dataset to show the practicability of time interval-based closed pattern mining.",2016,Knowledge and Information Systems volume 46 issue 1 pp 151-183,data mining;machine learning;algorithm;computer science;
Clustering XML documents by patterns,Maciej Piernik (Poznań University of Technology);Dariusz Brzezinski (Poznań University of Technology);Tadeusz Morzy (Poznań University of Technology);,"1759763813,2328495631,103756503","Now that the use of XML is prevalent, methods for mining semi-structured documents have become even more important. In particular, one of the areas that could greatly benefit from in-depth analysis of XML's semi-structured nature is cluster analysis. Most of the XML clustering approaches developed so far employ pairwise similarity measures. In this paper, we study clustering algorithms, which use patterns to cluster documents without the need for pairwise comparisons. We investigate the shortcomings of existing approaches and establish a new pattern-based clustering framework called XPattern, which tries to address these shortcomings. The proposed framework consists of four steps: choosing a pattern definition, pattern mining, pattern clustering, and document assignment. The framework's distinguishing feature is the combination of pattern clustering and document-cluster assignment, which allows to group documents according to their characteristic features rather than their direct similarity. We experimentally evaluate the proposed approach by implementing an algorithm called PathXP, which mines maximal frequent paths and groups them into profiles. PathXP was found to match, in terms of accuracy, other XML clustering approaches, while requiring less parametrization and providing easily interpretable cluster representatives. Additionally, the results of an in-depth experimental study lead to general suggestions concerning pattern-based XML clustering.",2016,Knowledge and Information Systems volume 46 issue 1 pp 185-212,xml validation;correlation clustering;constrained clustering;data stream clustering;cure data clustering algorithm;fuzzy clustering;clustering high dimensional data;xml;cluster analysis;consensus clustering;document clustering;information retrieval;data mining;database;computer science;
A data- and ontology-driven text mining-based construction of reliability model to analyze and predict component failures,Dnyanesh Rajpathak (General Motors);Soumen De (General Motors);,"1243863422,2117099685","A real-life reliability system is proposed by fusing the field warranty failure data with the failure modes extracted from unstructured repair verbatim data by using the ontology-based natural language processing technique to facilitate accurate estimation of component reliability. Traditionally, the reliability estimation process uses the warranty data, but it provides limited support to handle the ""failure confounding"" problem, whereby different failure modes associated with a component failure are confounded into a single failure mode. The resulting reliability estimation lacks the required level of precision. Because our model takes into account textual failure modes associated with component failures, it enhances the overall reliability estimation. The performance of our system is evaluated with the baseline system for predicting absolute errors by using the real-life data from the automotive domain, e.g., headlamp failure, collected at different miles exposures. In the best case, the absolute errors predicted by our model showed an improvement of 97 % with respect to the baseline model (without considering the failure modes), while in worst case, it was 71 %.",2016,Knowledge and Information Systems volume 46 issue 1 pp 87-113,failure mode and effects analysis;reliability;text mining;reliability engineering;data mining;database;computer science;
Centrality measures-based algorithm to visualize a maximal common induced subgraph in large communication networks,Parisutham Nirmala (PSG College of Technology);Lekshmi Ramasubramony Sulochana (PSG College of Technology);Nadarajan Rethnasamy (PSG College of Technology);,"2025029414,2559165998,891158537","Communication networks are ubiquitous, increasingly complex, and dynamic. Predicting and visualizing common patterns in such a huge graph data of communication network is an essential task to understand active patterns evolved in the network. In this work, the problem is to find an active pattern in a communication network which is modeled as detection of a maximal common induced subgraph (CIS). The state of the communication network is captured as a time series of graphs which has periodic snapshots of logical communications within the network. A new centrality measure is proposed to assess the variation in successive graphs and to identify the behavior of each node in the time series graph. It extents help in the process of selecting a suitable candidate vertex for maximality in each step of the proposed algorithm. This paper is a pioneer attempt in using centrality measures to detect a maximal CIS of the huge graph database, which gives promising effect in the resultant graph in terms of large number of vertices. The algorithm has polynomial time complexity, and the efficiency of the algorithm is demonstrated by a series of experiments with synthetic graph datasets of different orders. The performance of real-time datasets further ensured the competence of the proposed algorithm.",2016,Knowledge and Information Systems volume 46 issue 1 pp 213-239,alpha centrality;degeneracy;distance hereditary graph;simplex graph;strength of a graph;voltage graph;complement graph;graph bandwidth;graph power;moral graph;null graph;centrality;network science;graph;telecommunications network;information extraction;combinatorics;data mining;database;machine learning;computer science;mathematics;
Evaluating link prediction methods,Yang Yang (University of Notre Dame);Ryan N. Lichtenwalter (University of Notre Dame);Nitesh V. Chawla (University of Notre Dame);,"2673426936,278554428,1979796846","Link prediction is a popular research area with important applications in a variety of disciplines, including biology, social science, security, and medicine. The fundamental requirement of link prediction is the accurate and effective prediction of new links in networks. While there are many different methods proposed for link prediction, we argue that the practical performance potential of these methods is often unknown because of challenges in the evaluation of link prediction, which impact the reliability and reproducibility of results. We describe these challenges, provide theoretical proofs and empirical examples demonstrating how current methods lead to questionable conclusions, show how the fallacy of these conclusions is illuminated by methods we propose, and develop recommendations for consistent, standard, and applicable evaluation metrics. We also recommend the use of precision-recall threshold curves and associated areas in lieu of receiver operating characteristic curves due to complications that arise from extreme imbalance in the link prediction classification problem.",2015,Knowledge and Information Systems volume 45 issue 3 pp 751-782,sampling;econometrics;data mining;statistics;mathematics;
Estimating robustness in large social graphs,Fragkiskos D. Malliaros (École Polytechnique);Vasileios Megalooikonomou (University of Patras);Christos Faloutsos (Carnegie Mellon University);,"6790893,2173849879,2198983026","Given a large social graph, what can we say about its robustness? Broadly speaking, the property of robustness is crucial in real graphs, since it is related to the structural behavior of graphs to retain their connectivity properties after losing a portion of their edges/nodes. Can we estimate a robustness index for a graph quickly? Additionally, if the graph evolves over time, how this property changes? In this work, we are trying to answer the above questions studying the expansion properties of large social graphs. First, we present a measure that characterizes the robustness properties of a graph and also serves as global measure of the community structure (or lack thereof). We show how to compute this measure efficiently by exploiting the special spectral properties of real-world networks. We apply our method on several diverse real networks with millions of nodes, and we observe interesting properties for both static and time-evolving social graphs. As an application example, we show how to spot outliers and anomalies in graphs over time. Finally, we examine how graph generating models that mimic several properties of real-world graphs and behave in terms of robustness dynamics.",2015,Knowledge and Information Systems volume 45 issue 3 pp 645-678,critical graph;graph property;null model;social network analysis;economic graph;discrete mathematics;combinatorics;data mining;machine learning;statistics;mathematics;
Mining sequential patterns for classification,Dmitriy Fradkin (Siemens);Fabian Mörchen (Amazon.com);,"2028188512,1886239512","While a number of efficient sequential pattern mining algorithms were developed over the years, they can still take a long time and produce a huge number of patterns, many of which are redundant. These properties are especially frustrating when the goal of pattern mining is to find patterns for use as features in classification problems. In this paper, we describe BIDE-Discriminative, a modification of BIDE that uses class information for direct mining of predictive sequential patterns. We then perform an extensive evaluation on nine real-life datasets of the different ways in which the basic BIDE-Discriminative can be used in real multi-class classification problems, including 1-versus-rest and model-based search tree approaches. The results of our experiments show that 1-versus-rest provides an efficient solution with good classification performance.",2015,Knowledge and Information Systems volume 45 issue 3 pp 731-749,sequential pattern mining;kullback leibler divergence;data mining;pattern recognition;machine learning;statistics;computer science;
A survey on data stream clustering and classification,"Hai-Long Nguyen (Agency for Science, Technology and Research);Yew-Kwong Woon (Airbus Group);Wee Keong Ng (Nanyang Technological University);","2648854424,2579311635,2120738522","Nowadays, with the advance of technology, many applications generate huge amounts of data streams at very high speed. Examples include network traffic, web click streams, video surveillance, and sensor networks. Data stream mining has become a hot research topic. Its goal is to extract hidden knowledge/patterns from continuous data streams. Unlike traditional data mining where the dataset is static and can be repeatedly read many times, data stream mining algorithms face many challenges and have to satisfy constraints such as bounded memory, single-pass, real-time response, and concept-drift detection. This paper presents a comprehensive survey of the state-of-the-art data stream mining algorithms with a focus on clustering and classification because of their ubiquitous usage. It identifies mining constraints, proposes a general model for data stream mining, and depicts the relationship between traditional data mining and data stream mining. Furthermore, it analyzes the advantages as well as limitations of data stream algorithms and suggests potential areas for future research.",2015,Knowledge and Information Systems volume 45 issue 3 pp 535-569,data stream clustering;cluster analysis;biological classification;concept mining;data stream mining;data science;data mining;database;machine learning;computer science;
On the anonymizability of graphs,Charu C. Aggarwal (IBM);Yao Li (University of Illinois at Chicago);Philip S. Yu (University of Illinois at Chicago);,"2146335907,2228755178,2125104194","Many applications such as social networks, recommendation systems, email communication patterns, and other collaborative applications are built on top of graph infrastructures. The data stored on such networks may contain personal information about individuals and may therefore be sensitive from a privacy point of view. Therefore, a natural solution is to remove identifying information from the nodes and perturb the graph structure, so that re-identification becomes difficult. Typical graphs encountered in real applications are sparse. In this paper, we will show that sparse graphs have certain theoretical properties which make them susceptible to re-identification attacks. We design a systematic way to exploit these theoretical properties in order to construct re-identification signatures, which are also known as characteristic vectors. These signatures have the property that they are extremely robust to perturbations, especially for sparse graphs. We use these signatures in order to create an effective attack algorithm. We supplement our theoretical results with experimental tests using a number of algorithms on real data sets. These results confirm that even low levels of anonymization require perturbation levels which are significant enough to result in a massive loss of utility. Our experimental results also show that the true anonymization level of graphs is much lower than is implied by measures such as $$k$$k-anonymity. Thus, the results of this paper establish that the problem of graph anonymization has fundamental theoretical barriers which prevent a fully effective solution.",2015,Knowledge and Information Systems volume 45 issue 3 pp 571-588,graph;privacy;theoretical computer science;distributed computing;data mining;database;machine learning;computer science;mathematics;
Triangle minimization in large networks,Rong-Hua Li (Shenzhen University);Jeffrey Xu Yu (The Chinese University of Hong Kong);,"2097752033,2119358208","The number of triangles is a fundamental metric for analyzing the structure and function of a network. In this paper, for the first time, we investigate the triangle minimization problem in a network under edge (node) attack, where the attacker aims to minimize the number of triangles in the network by removing $$k$$k edges (nodes). We show that the triangle minimization problem under edge (node) attack is a submodular function maximization problem, which can be solved efficiently. Specifically, we propose a degree-based edge (node) removal algorithm and a near-optimal greedy edge (node) removal algorithm for approximately solving the triangle minimization problem under edge (node) attack. In addition, we introduce two pruning strategies and an approximate marginal gain evaluation technique to further speed up the greedy edge (node) removal algorithm. We conduct extensive experiments over 12 real-world datasets to evaluate the proposed algorithms, and the results demonstrate the effectiveness, efficiency and scalability of our algorithms.",2015,Knowledge and Information Systems volume 45 issue 3 pp 617-643,submodular set function;discrete mathematics;combinatorics;mathematical optimization;mathematics;
The Cloaked-Centroid protocol: location privacy protection for a group of users of location-based services,Maede Ashouri-Talouki (University of Isfahan);Ahmad Baraani-Dastjerdi (University of Isfahan);Ali Aydın Selçuk (TOBB University of Economics and Technology);,"122561345,246144934,2171730961","Several techniques have been recently proposed to protect user location privacy while accessing location-based services (LBSs). However, applying these techniques to protect location privacy for a group of users would lead to user privacy leakage and query inefficiency. In this paper, we propose a two-phase protocol, we name Cloaked-Centroid, which is designed specifically to protect location privacy for a group of users. We identify location privacy issues for a group of users who may ask an LBS for a meeting place that is closest to the group centroid. Our protocol relies on spatial cloaking, an anonymous veto network and a conference key establishment protocol. In the first phase, member locations are cloaked into a single region based on their privacy profiles, and then, a single query is submitted to an LBS. In the second phase, a special secure multiparty computation extracts the meeting point result from the received answer set. Our protocol is resource aware, taking into account the LBS overhead and the communication cost, i.e., the number of nearest neighbor queries sent to a service provider and the number of returned points of interests. Regarding privacy, Cloaked-Centroid protects the location privacy of each group member from those in the group and from anyone outside the group, including the LBS. Moreover, our protocol provides result-set anonymity, which prevents LBS providers and other possible attackers from learning the meeting place location. Extensive experiments show that the proposed protocol is efficient in terms of computation and communication costs. A security analysis shows the resistance of the protocol against collusion, disruption and background knowledge attacks in a malicious model.",2015,Knowledge and Information Systems volume 45 issue 3 pp 589-615,privacy software;location based service;secure multi party computation;information privacy;internet privacy;world wide web;computer security;computer science;
The Mask of ZoRRo: preventing information leakage from documents,Prasad M. Deshpande (IBM);Salil Joshi (IBM);Prateek Dewan (Indraprastha Institute of Information Technology);Karin Murthy (IBM);Mukesh K. Mohania (IBM);Sheshnarayan Agrawal (IBM);,"2304487933,2106463510,2139576573,2105441224,254200932,2161708958","In today's enterprise world, information about business entities such as a customer's or patient's name, address, and social security number is often present in both relational databases as well as content repositories. Information about such business entities is generally well protected in databases by well-defined and fine-grained access control. However, current document retrieval systems do not provide user-specific, fine-grained redaction of documents to prevent leakage of information about business entities from documents. Leaving companies with only two choices: either providing complete access to a document, risking potential information leakage, or prohibiting access to the document altogether, accepting potentially negative impact on business processes. In this paper, we present ZoRRo, an add-on for document retrieval systems to dynamically redact sensitive information of business entities referenced in a document based on access control defined for the entities. ZoRRo exploits database systems' fine-grained, label-based access-control mechanism to identify and redact sensitive information from unstructured text, based on the access privileges of the user viewing it. To make on-the-fly redaction feasible, ZoRRo exploits the concept of $$k$$k-safety in combination with Lucene-based indexing and scoring. We demonstrate the efficiency and effectiveness of ZoRRo through a detailed experimental study.",2015,Knowledge and Information Systems volume 45 issue 3 pp 705-730,internet privacy;world wide web;data mining;database;computer science;
Optimal WCDMA network planning by multiobjective evolutionary algorithm with problem-specific genetic operation,Fangqing Gu (Guangdong University of Technology);Hai-lin Liu (Guangdong University of Technology);Yiu-Ming Cheung (Hong Kong Baptist University);Shengli Xie (Guangdong University of Technology);,"2646628487,2655480759,2161219483,2641420612","The wideband code division multiple access (WCDMA) network planning problem requires to determine the location and the configuration parameters of the base stations (BSs) so as to maximize the capacity and minimize the installation cost. This problem can be formulated as a complex set covering problem. Compared to the classical set covering problems, the coverage area of each BS is unknown in advance. This makes that the selection of each BS location and configuration parameters is determined by the location and configuration parameters of the neighbor BSs. Accordingly, we will conduct a competition and cooperation model based on the re-covered area of the BSs to measure the relationship of the BSs. Then, an efficient genetic operation based on this model is proposed to generate new-quality solutions. Further, four BS configuration parameters, i.e., the antenna height, antenna tilt, sector orientation and pilot signal power, are taken into account as well. Since there are too many combination levels of the configuration parameters, an encoding method based on orthogonal design is presented to reduce the search space. Subsequently, we merge the proposed encoding method and genetic operation into the multiobjective evolutionary algorithm-based decomposition (MOEA/D-M2M) to solve the WCDMA network planning problem. Simulation results show the efficacy of the proposed encoding and genetic operation in comparison with the existing counterpart.",2015,Knowledge and Information Systems volume 45 issue 3 pp 679-703,set cover problem;principle of orthogonal design;database;machine learning;mathematical optimization;computer science;mathematics;
Multiplicative distance: a method to alleviate distance instability for high-dimensional data,Jafar Mansouri (Ferdowsi University of Mashhad);Morteza Khademi (Ferdowsi University of Mashhad);,"2078487401,2001062575","Recently, it has been shown that under a broad set of conditions, the commonly used distance functions will become unstable in high-dimensional data space; i.e., the distance to the farthest data point approaches the distance to the nearest data point of a given query point with increasing dimensionality. It has been shown that if dimensions are independently distributed, and normalized to have zero mean and unit variance, instability happens. In this paper, it is shown that the normalization condition is not necessary, but all appropriate moments must be finite. Furthermore, a new distance function, namely multiplicative distance, is introduced. It is theoretically proved that this function is stable for data with independent dimensions (with identical or nonidentical distribution). In contrast to usual distance functions which are based on the summation of distances over all dimensions (distance components), the multiplicative distance is based on the multiplication of distance components. Experimental results show the stability of the multiplicative distance for data with independent and correlated dimensions in the high-dimensional space and the superiority of the multiplicative distance over the norm distances for the high-dimensional data.",2015,Knowledge and Information Systems volume 45 issue 3 pp 783-805,total variation distance of probability measures;gilbert johnson keerthi distance algorithm;trace distance;weighted voronoi diagram;minkowski distance;distance correlation;distance from a point to a line;absolute value;earth mover s distance;mahalanobis distance;distance matrix;clustering high dimensional data;distance;radial basis function;discrete mathematics;combinatorics;topology;machine learning;statistics;computer science;mathematics;
Anytime density-based clustering of complex data,Son T. Mai (Ludwig Maximilian University of Munich);Xiao He (Ludwig Maximilian University of Munich);Jing Feng (Ludwig Maximilian University of Munich);Claudia Plant (Technische Universität München);Christian Böhm (Ludwig Maximilian University of Munich);,"2167132615,2280202269,2302555519,2122910652,2486446532","Many clustering algorithms suffer from scalability problems on massive datasets and do not support any user interaction during runtime. To tackle these problems, anytime clustering algorithms are proposed. They produce a fast approximate result which is continuously refined during the further run. Also, they can be stopped or suspended anytime to provide an intermediate answer. In this paper, we propose a novel anytime clustering algorithm modeled on the density-based clustering paradigm. Our algorithm called A-DBSCAN is applicable to many complex data such as trajectory and medical data. The general idea of our algorithm is to use a sequence of lower bounding functions (LBs) of the true distance function to produce multiple approximate results of the true density-based clusters. A-DBSCAN operates in multiple levels w.r.t. the LBs and is mainly based on two algorithmic schemes: (1) an efficient distance upgrade scheme which restricts distance calculations to core objects at each level of the LBs and (2) a local reclustering scheme which restricts update operations to the relevant objects only. To further improve the performance, we propose a significant extension version of A-DBSCAN called A-DBSCAN-XS which is built upon the anytime scheme of A-DBSCAN and the $$\mu $$μ-range query scheme of a data structure called extended Xseedlist. A-DBSCAN-XS requires less distance calculations at each level than A-DBSCAN and thus is more efficient. Extensive experiments demonstrate that A-DBSCAN and A-DBSCAN-XS acquire very good clustering results at very early stages of execution and thus save a large amount of computational time. Even if they run to the end, A-DBSCAN and A-DBSCAN-XS are still orders of magnitude faster than the original algorithm DBSCAN and its variants. We also introduce a novel application for our algorithms for the segmentation of the white matter fiber tracts in human brain which is an important tool for studying the brain structure and various diseases such as Alzheimer.",2015,Knowledge and Information Systems volume 45 issue 2 pp 319-355,flame clustering;canopy clustering algorithm;dbscan;correlation clustering;constrained clustering;data stream clustering;cure data clustering algorithm;diffusion mri;fuzzy clustering;clustering high dimensional data;cluster analysis;consensus clustering;theoretical computer science;data mining;database;artificial intelligence;machine learning;statistics;mathematics;
A unified statistical framework for crowd labeling,Jafar Muhammadi (Sharif University of Technology);Hamid R. Rabiee (Sharif University of Technology);Abbas Hosseini (Sharif University of Technology);,"2173761957,2032985511,2295876618","Recently, there has been a burst in the number of research projects on human computation via crowdsourcing. Multiple-choice (or labeling) questions could be referred to as a common type of problem which is solved by this approach. As an application, crowd labeling is applied to find true labels for large machine learning datasets. Since crowds are not necessarily experts, the labels they provide are rather noisy and erroneous. This challenge is usually resolved by collecting multiple labels for each sample and then aggregating them to estimate the true label. Although the mechanism leads to high-quality labels, it is not actually cost-effective. As a result, efforts are currently made to maximize the accuracy in estimating true labels, while fixing the number of acquired labels. This paper surveys methods to aggregate redundant crowd labels in order to estimate unknown true labels. It presents a unified statistical latent model where the differences among popular methods in the field correspond to different choices for the parameters of the model. Afterward, algorithms to make inference on these models will be surveyed. Moreover, adaptive methods which iteratively collect labels based on the previously collected labels and estimated models will be discussed. In addition, this paper compares the distinguished methods and provides guidelines for future work required to address the current open issues.",2015,Knowledge and Information Systems volume 45 issue 2 pp 271-294,crowdsourcing;labeling theory;data science;data mining;artificial intelligence;machine learning;statistics;computer science;
Distributed reasoning with coupled ontologies: the $$E\text {-}{\mathcal {SHIQ}}$$E-SHIQ representation framework,Georgios M. Santipantakis (University of the Aegean);George A. Vouros (University of Piraeus);,"2633022602,2644751122","Combining ontologies in expressive fragments of Description Logics in inherently distributed peer-to-peer settings with autonomous peers is still a challenge in the general case. Although several modular ontology representation frameworks have been proposed for combining Description Logics knowledge bases, each of them has its own strengths and limitations. In this paper, we consider networks of peers, where each peer holds its own ontology within the $${\mathcal {SHIQ}}$$SHIQ fragment of Description Logics, and subjective beliefs on how its knowledge can be coupled with the knowledge of others. To allow peers to reason jointly with their coupled knowledge, while preserving their autonomy on evolving their knowledge, data, and subjective beliefs, we propose the $$E\text {-}{\mathcal {SHIQ}}$$E-SHIQ representation framework. The article motivates the need for $$E\text {-}{\mathcal {SHIQ}}$$E-SHIQ and compares it to existing representation frameworks for modular Description Logics. It discusses the implementation of the $$E\text {-}{\mathcal {SHIQ}}$$E-SHIQ distributed reasoner and presents experimental results on the efficiency of this reasoner.",2015,Knowledge and Information Systems volume 45 issue 2 pp 491-534,description logic;theoretical computer science;knowledge management;data mining;database;artificial intelligence;computer science;mathematics;
A safe region based approach to moving KNN queries in obstructed space,Chuanwen Li (Northeastern University);Yu Gu (Northeastern University);Jianzhong Qi (University of Melbourne);Rui Zhang (University of Melbourne);Ge Yu (Northeastern University);,"2322748572,2306501471,2152307702,2690388134,2168806630","The moving $$k$$k nearest neighbor (MkNN) query has been studied extensively. Most of the studies assume no obstacle in the space. However, obstacles like rivers, buildings and private properties commonly exist in the space, and one may need to go around the obstacles to reach his/her nearest neighbors. In this paper, we study the moving kNN query in obstructed space with no predefined query object trajectory. We take a safe region based approach to solve this problem. In particular, we propose a method to compute a safe region w.r.t. a data object. In this safe region, the query object can move freely, while the data object is kept in the query object's kNN set. By combining the safe regions of the data objects near the query object, we formulate an overall safe region where the query object's kNN set keeps stable. We propose an algorithm based on the safe regions to process the moving kNN query in obstructed space. Extensive experiments show that the proposed algorithm significantly reduces the communication and the computation costs for query processing. Our algorithm outperforms a baseline algorithm by up to two orders of magnitude under various settings.",2015,Knowledge and Information Systems volume 45 issue 2 pp 417-451,query optimization;data mining;database;machine learning;mathematics;
Integration of multiple network views in Wikipedia,Guangyu Wu (University College Dublin);Pádraig Cunningham (University College Dublin);,"2155536304,2120527611",One of the challenges in network data analysis is the determination of the most informative perspective on the network to use in analysis. This is particularly an issue when the network is dynamic and is defined by events that occur over time. We present an example of such a scenario in the analysis of edit networks in Wikipedia--the networks of editors interacting on Wikipedia pages. We propose the prediction of article quality as a task that allows us to quantify the informativeness of alternative network views. We present three fundamentally different views on the data that attempt to capture structural and temporal aspects of the edit networks. We demonstrate that each view captures information that is unique to that view and propose a strategy for integrating the different sources of information.,2015,Knowledge and Information Systems volume 45 issue 2 pp 473-490,organizational network analysis;dynamic network analysis;brand;social network analysis;data quality;biological classification;data science;information retrieval;data mining;database;artificial intelligence;machine learning;statistics;computer science;
Exploiting semantics for filtering and searching knowledge in a software development context,Sonia Bergamaschi (University of Modena and Reggio Emilia);Riccardo Martoglia (Fédération Internationale de Motocyclisme);Serena Sorrentino (University of Modena and Reggio Emilia);,"2000741701,47012279,1967424351","Software development is still considered a bottleneck for Small and Medium Enterprises (SMEs) in the advance of the Information Society. Usually, SMEs store and collect a large number of software textual documentation; these documents might be profitably used to facilitate them in using (and re-using) Software Engineering methods for systematically designing their applications, thus reducing software development cost. Specific and semantics textual filtering/search mechanisms, supporting the identification of adequate processes and practices for the enterprise needs, are fundamental in this context. To this aim, we present an automatic document retrieval method based on semantic similarity and Word Sense Disambiguation techniques. The proposal leverages on the strengths of both classic information retrieval and knowledge-based techniques, exploiting syntactical and semantic information provided by general and specific domain knowledge sources. For any SME, it is as easily and generally applicable as are the search techniques offered by common enterprise Content Management Systems. Our method was developed within the FACIT-SME European FP-7 project, whose aim is to facilitate the diffusion of Software Engineering methods and best practices among SMEs. As shown by a detailed experimental evaluation, the achieved effectiveness goes well beyond typical retrieval solutions.",2015,Knowledge and Information Systems volume 45 issue 2 pp 295-318,semantic similarity;semantic memory;software development;information retrieval;data mining;database;computer science;
Effective record linkage for mining campaign contribution data,Christophe G. Giraud-Carrier (Brigham Young University);J. Goodliffe (Brigham Young University);B. M. Jones (Brigham Young University);S. Cueva (Brigham Young University);,"2181238288,2679306502,2561263078,2096443858","Up to now, most campaign contribution data have been reported at the level of the donation. While these are interesting, one often needs to have information at the level of the donor. Obtaining information at that level is difficult as there is neither a unique repository of donations nor any standard across existing repositories. In order to more meaningfully mine campaign contribution data, political scientists need an accurate way of grouping, or linking, together donations made by the same donor. In this paper, we describe a record linkage technique that is applicable to various sources and across large geographical areas. We show how it may be effectively applied in the context of nationwide donation data and report on new, previously unattainable results about campaign contributors in the 2007---2008 US election cycle.",2015,Knowledge and Information Systems volume 45 issue 2 pp 389-416,record linkage;domain knowledge;world wide web;data mining;artificial intelligence;computer science;
Heuristics for using CP-nets in utility-based negotiation without knowing utilities,Reyhan Aydoğan (Delft University of Technology);Tim Baarslag (University of Southampton);Koen V. Hindriks (Delft University of Technology);Catholijn M. Jonker (Delft University of Technology);Pınar Yolum (Boğaziçi University);,"1928926416,122669167,2051241129,2016708809,267358967","CP-nets have proven to be an effective representation for capturing preferences. However, their use in automated negotiation is not straightforward because, typically, preferences in CP-nets are partially ordered and negotiating agents are required to compare any two outcomes based on a request and an offer in order to negotiate effectively. If agents know how to generate total orders from their CP-nets, they can make this comparison. This paper proposes heuristics that enable the use of CP-nets in utility-based negotiations by generating total orderings. To validate this approach, the paper compares the performance of CP-nets with our heuristics with the performance of UCP-nets that are equipped with complete preference orderings. Our results show that we can achieve comparable performance in terms of the outcome utility. More importantly, one of our proposed heuristics can achieve this performance with significantly smaller number of interactions compared to UCP-nets.",2015,Knowledge and Information Systems volume 45 issue 2 pp 357-388,heuristics;management science;
SET-PSO-based approach for mining positive and negative association rules,Jitendra Agrawal (Rajiv Gandhi Proudyogiki Vishwavidyalaya);Shikha Agrawal (Usman Institute of Technology);Ankita Singhai (Rajiv Gandhi Proudyogiki Vishwavidyalaya);Sanjeev Sharma (Rajiv Gandhi Proudyogiki Vishwavidyalaya);,"2129043502,2584511671,2427554927,2602112857","Data mining is the process of determining new, unanticipated, valuable patterns from existing databases by considering historical and recent developments in statistics, artificial intelligence, and machine learning. It can help companies focus on the most important information in their data warehouses. Association rule mining is one of the most highly researched and popular data mining techniques for finding associations between items in a set. It is frequently used in marketing, advertising, and inventory control. Typically, association rules only consider items in transactions (positive association rules). They do not consider items that do not occur together, which can be used to create rules that are also useful for market basket analysis. Also, existing algorithms often generate too many candidate itemsets when mining the data and scan the database multiple times. To resolve these issues in association rule mining algorithms, we propose SARIC (set particle swarm optimization for association rules using the itemset range and correlation coefficient). Our method uses set particle swarm optimization to generate association rules from a database and considers both positive and negative occurrences of attributes. SARIC applies the itemset range and correlation coefficient so that we do not need to specify the minimum support and confidence, because it automatically determines them quickly and objectively. We verified the efficiency of SARIC using two differently sized databases. Our simulation results demonstrate that SARIC generates more promising results than Apriori, Eclat, HMINE, and a genetic algorithm.",2015,Knowledge and Information Systems volume 45 issue 2 pp 453-471,apriori algorithm;association rule learning;particle swarm optimization;data mining;pattern recognition;machine learning;computer science;
Greedy column subset selection for large-scale data sets,"Ahmed K. Farahat (University of Waterloo);Ahmed Elgohary (University of Maryland, College Park);Ali Ghodsi (University of Waterloo);Mohamed S. Kamel (University of Waterloo);","2107050089,2164540290,734473377,2096364092","In today's information systems, the availability of massive amounts of data necessitates the development of fast and accurate algorithms to summarize these data and represent them in a succinct format. One crucial problem in big data analytics is the selection of representative instances from large and massively distributed data, which is formally known as the Column Subset Selection problem. The solution to this problem enables data analysts to understand the insights of the data and explore its hidden structure. The selected instances can also be used for data preprocessing tasks such as learning a low-dimensional embedding of the data points or computing a low-rank approximation of the corresponding matrix. This paper presents a fast and accurate greedy algorithm for large-scale column subset selection. The algorithm minimizes an objective function, which measures the reconstruction error of the data matrix based on the subset of selected columns. The paper first presents a centralized greedy algorithm for column subset selection, which depends on a novel recursive formula for calculating the reconstruction error of the data matrix. The paper then presents a MapReduce algorithm, which selects a few representative columns from a matrix whose columns are massively distributed across several commodity machines. The algorithm first learns a concise representation of all columns using random projection, and it then solves a generalized column subset selection problem at each machine in which a subset of columns are selected from the sub-matrix on that machine such that the reconstruction error of the concise representation is minimized. The paper demonstrates the effectiveness and efficiency of the proposed algorithm through an empirical evaluation on benchmark data sets.",2015,Knowledge and Information Systems volume 45 issue 1 pp 1-34,greedy algorithm;big data;theoretical computer science;data mining;database;machine learning;statistics;computer science;
Class imbalance revisited: a new experimental setup to assess the performance of treatment methods,Ronaldo C. Prati (Universidade Federal do ABC);Gustavo Enrique de Almeida Prado Alves Batista (Spanish National Research Council);Diego Furtado Silva (Spanish National Research Council);,"2028602492,2165222361,2138204294","In the last decade, class imbalance has attracted a huge amount of attention from researchers and practitioners. Class imbalance is ubiquitous in Machine Learning, Data Mining and Pattern Recognition applications; therefore, these research communities have responded to such interest with literally dozens of methods and techniques. Surprisingly, there are still many fundamental open-ended questions such as ""Are all learning paradigms equally affected by class imbalance?"", ""What is the expected performance loss for different imbalance degrees?"" and ""How much of the performance losses can be recovered by the treatment methods?"". In this paper, we propose a simple experimental design to assess the performance of class imbalance treatment methods. This experimental setup uses real data set with artificially modified class distributions to evaluate classifiers in a wide range of class imbalance. We apply such experimental design in a large-scale experimental evaluation with 22 data set and seven learning algorithms from different paradigms. We also propose a statistical procedure aimed to evaluate the relative degradation and recoveries, based on confidence intervals. This procedure allows a simple yet insightful visualization of the results, as well as provide the basis for drawing statistical conclusions. Our results indicate that the expected performance loss, as a percentage of the performance obtained with the balanced distribution, is quite modest (below 5 %) for the most balanced distributions up to 10 % of minority examples. However, the loss tends to increase quickly for higher degrees of class imbalance, reaching 20 % for 1 % of minority class examples. Support Vector Machine is the classifier paradigm that is less affected by class imbalance, being almost insensitive to all but the most imbalanced distributions. Finally, we show that the treatment methods only partially recover the performance losses. On average, typically, about 30 % or less of the performance that was lost due to class imbalance was recovered by these methods.",2015,Knowledge and Information Systems volume 45 issue 1 pp 247-270,sampling;artificial intelligence;machine learning;statistics;computer science;
Sliding windows over uncertain data streams,Michele Dallachiesa (University of Trento);Gabriela Jacques-Silva (IBM);Buğra Gedik (Bilkent University);Kun-Lung Wu (IBM);Themis Palpanas (University of Trento);,"309381702,1136292423,1814817956,2095612124,2010554420","Uncertain data streams can have tuples with both value and existential uncertainty. A tuple has value uncertainty when it can assume multiple possible values. A tuple is existentially uncertain when the sum of the probabilities of its possible values is $$<$$<1. A situation where existential uncertainty can arise is when applying relational operators to streams with value uncertainty. Several prior works have focused on querying and mining data streams with both value and existential uncertainty. However, none of them have studied, in depth, the implications of existential uncertainty on sliding window processing, even though it naturally arises when processing uncertain data. In this work, we study the challenges arising from existential uncertainty, more specifically the management of count-based sliding windows, which are a basic building block of stream processing applications. We extend the semantics of sliding window to define the novel concept of uncertain sliding windows and provide both exact and approximate algorithms for managing windows under existential uncertainty. We also show how current state-of-the-art techniques for answering similarity join queries can be easily adapted to be used with uncertain sliding windows. We evaluate our proposed techniques under a variety of configurations using real data. The results show that the algorithms used to maintain uncertain sliding windows can efficiently operate while providing a high-quality approximation in query answering. In addition, we show that sort-based similarity join algorithms can perform better than index-based techniques (on 17 real datasets) when the number of possible values per tuple is low, as in many real-world applications.",2015,Knowledge and Information Systems volume 45 issue 1 pp 159-190,theoretical computer science;data mining;database;computer science;
Classification of multivariate time series via temporal abstraction and time intervals mining,Robert Moskovitch (Ben-Gurion University of the Negev);Yuval Shahar (Ben-Gurion University of the Negev);,"319285823,700636827","Classification of multivariate time series data, often including both time points and intervals at variable frequencies, is a challenging task. We introduce the KarmaLegoSification (KLS) framework for classification of multivariate time series analysis, which implements three phases: (1) application of a temporal abstraction process that transforms a series of raw time-stamped data points into a series of symbolic time intervals; (2) mining these symbolic time intervals to discover frequent time-interval-related patterns (TIRPs), using Allen's temporal relations; and (3) using the TIRPs as features to induce a classifier. To efficiently detect multiple TIRPs (features) in a single entity to be classified, we introduce a new algorithm, SingleKarmaLego, which can be shown to be superior for that purpose over a Sequential TIRPs Detection algorithm. We evaluated the KLS framework on datasets in the domains of diabetes, intensive care, and infectious hepatitis, assessing the effects of the various settings of the KLS framework. Discretization using Symbolic Aggregate approXimation (SAX) led to better performance than using the equal-width discretization (EWD); knowledge-based cut-off definitions when available were superior to both. Using three abstract temporal relations was superior to using the seven core temporal relations. Using an epsilon value larger than zero tended to result in a slightly better accuracy when using the SAX discretization method, but resulted in a reduced accuracy when using EWD, and overall, does not seem beneficial. No feature selection method we tried proved useful. Regarding feature (TIRP) representation, mean duration performed better than horizontal support, which in turn performed better than the default Binary (existence) representation method.",2015,Knowledge and Information Systems volume 45 issue 1 pp 35-74,biological classification;data mining;pattern recognition;machine learning;statistics;computer science;
Forecasting spatiotemporal impact of traffic incidents for next-generation navigation systems,Bei Pan (University of Southern California);Ugur Demiryurek (University of Southern California);Chetan Gupta (Hitachi);Cyrus Shahabi (University of Southern California);,"2154764302,319304351,2345154248,240820708","The advances in sensor technologies enable real-time collection of high-fidelity spatiotemporal data on transportation networks of major cities. In this paper, using two real-world transportation datasets: (1) incident and (2) traffic data, we address the problem of predicting and quantifying the impact of traffic incidents. Traffic incidents include any nonrecurring events on road networks, such as accidents, weather hazard or road construction. By analyzing archived incident data, we classify incidents based on their features (e.g., time, location, type of incident). Subsequently, we model the impact of each incident class on its surrounding traffic by analyzing the archived traffic data at the time and location of the incidents. Consequently, in real-time, if we observe a similar incident (from real-time incident feeds), we predict and quantify its impact on the surrounding traffic using our models. This information, in turn, can help drivers to effectively avoid impacted areas in real-time. To be useful for such real-time navigation application, and unlike current approaches, we study the dynamic behavior of incidents and model the impact as a quantitative time varying spatial span. In addition, we study a novel approach that improves our classification method by analyzing traffic density around the incident area and the initial behavior of the incident. We evaluated our approach with very large traffic and incident datasets collected from Los Angeles County and the results show by utilizing our impact prediction approach in the navigation system, precision of the travel time calculation can be improved by up to 67 %.",2015,Knowledge and Information Systems volume 45 issue 1 pp 75-104,floating car data;intelligent transportation system;computer security;simulation;computer science;
Enumeration of time series motifs of all lengths,Abdullah Mueen (University of New Mexico);Nikan Chavoshi (University of New Mexico);,"2083987245,2203691504","Time series motifs are repeated patterns in long and noisy time series. Motifs are typically used to understand the dynamics of the source because repeated patterns with high similarity evidentially rule out the presence of noise. Recently, time series motifs have also been used for clustering, summarization, rule discovery and compression as features. For all such purposes, many high-quality motifs of various lengths are desirable and thus originate the problem of enumerating motifs for a wide range of lengths. Existing algorithms find motifs for a given length. A trivial way to enumerate motifs is to run one of the algorithms for the whole range of lengths. However, such parameter sweep is computationally infeasible for large real datasets. In this paper, we describe an exact algorithm, called $${\textit{MOEN}}$$MOEN, to enumerate motifs. The algorithm is an order of magnitude faster than the naive algorithm. The algorithm frees us from re-discovering the same motif at different lengths and tuning multiple data-dependent parameters. The speedup comes from using a novel bound on the similarity function across lengths and the algorithm uses only linear space unlike other motif discovery algorithms. We also describe an approximate extension of MOEN algorithm that is faster and suitable for larger datasets. We describe five case studies in entomology, sensor fusion, power consumption monitoring and activity recognition where $${\textit{MOEN}}$$MOEN enumerates several high-quality motifs.",2015,Knowledge and Information Systems volume 45 issue 1 pp 105-132,correlation;time series;upper and lower bounds;bioinformatics;data mining;machine learning;statistics;mathematics;
Application of smoothing techniques for linear programming twin support vector machines,M. Tanveer (LNM Institute of Information Technology);,2680665395,"In this paper, a new unconstrained minimization problem formulation is proposed for linear programming twin support vector machine (TWSVM) classifiers. The proposed formulation leads to two smaller-sized unconstrained minimization problems having their objective functions piecewise differentiable. However, since their objective functions contain the non-smooth ""plus"" function, two new smoothing approaches are assumed to solve the proposed formulation, and then apply Newton-Armijo algorithm. The idea of our formulation is to reformulate TWSVM as a strongly convex problem by incorporated regularization techniques and then derive smooth 1-norm linear programming formulation for TWSVM to improve robustness. One significant advantage of our proposed algorithm over TWSVM is that the structural risk minimization principle is implemented in the primal problems which embodies the marrow of statistical learning theory. In addition, the solution of two modified unconstrained minimization problems reduces to solving just two systems of linear equations as opposed to solving two quadratic programming problems in TWSVM and TBSVM, which leads to extremely simple and fast algorithm. Our approach has the advantage that a pair of matrix equation of order equals to the number of input examples is solved at each iteration of the algorithm. The algorithm converges from any starting point that can be easily implemented in MATLAB without using any optimization packages. The performance of our proposed method is verified experimentally on several benchmark and synthetic datasets. Experimental results show the effectiveness of our methods in both training time and classification accuracy.",2015,Knowledge and Information Systems volume 45 issue 1 pp 191-214,linear programming;combinatorics;machine learning;mathematical optimization;mathematics;
Efficient location-based search of trajectories with location importance,Da Yan (The Chinese University of Hong Kong);James Cheng (The Chinese University of Hong Kong);Zhou Zhao (Hong Kong University of Science and Technology);Wilfred Siu Hung Ng (Hong Kong University of Science and Technology);,"2146437384,2304873892,2118299058,2170178419","Given a database of trajectories and a set of query locations, location-based trajectory search finds trajectories in the database that are close to all the query locations. Location-based trajectory search has many applications such as providing reference routes for travelers who are planning a trip to multiple places of interest. However, previous studies only consider the spatial aspect of trajectories, which is inadequate for real applications. For example, one may obtain the reference route of a tourist who just passed by a place of interest without paying a visit. We propose the $$k$$kImportant Connected Trajectories (k-ICT) query by associating trajectories with location importance. For any query location, the result trajectories should contain an important point close to it. We describe an effective method to infer the importance of trajectory points from the temporal information. We also propose efficient R-tree-based and grid-based algorithms to answer $$k$$k-ICT queries, and verify the efficiency of our algorithms through extensive experiments on both real and synthetic datasets.",2015,Knowledge and Information Systems volume 45 issue 1 pp 215-245,voronoi diagram;trajectory;computer vision;data mining;simulation;
Synergy of two mutations based immune multi-objective automatic fuzzy clustering algorithm,Ruochen Liu (Xidian University);Lang Zhang (Xidian University);Bingjie Li (Xidian University);Yajuan Ma (Xidian University);Licheng Jiao (Xidian University);,"2137004161,2702798018,2298126187,2281720907,2166558591","In this paper, a synergy of two mutation based immune multi-objective automatic fuzzy clustering algorithm (STMIMAFC) is proposed for the task of automatically evolving the number of clusters as well as a proper partitioning of data set. In the proposed algorithm, firstly, two new mutation operators, which are designed for the different structures of chromosomes respectively, are cooperated with each other to generate the new individuals. Secondly, we propose an exponential function based compactness validity index. The proposed method has been extensively compared with a synergy of genetic algorithm and multi-objective differential evolution, multi-objective modified differential evolution based fuzzy clustering, multi-objective clustering with automatic $$k$$k-determination over a test suit of several real life data sets and synthetic data sets. Experimental results indicate the superiority of the STMIMAFC over other three compared clustering algorithms on clustering accuracy and running time.",2015,Knowledge and Information Systems volume 45 issue 1 pp 133-157,flame clustering;hierarchical clustering of networks;brown clustering;canopy clustering algorithm;determining the number of clusters in a data set;correlation clustering;cure data clustering algorithm;affinity propagation;fuzzy clustering;clustering high dimensional data;multi objective optimization;image segmentation;cluster analysis;consensus clustering;bioinformatics;data mining;machine learning;computer science;mathematics;
Constructing topical hierarchies in heterogeneous information networks,Chi Wang (University of Illinois at Urbana–Champaign);Jialu Liu (University of Illinois at Urbana–Champaign);Nihit Desai (University of Illinois at Urbana–Champaign);Marina Danilevsky (University of Illinois at Urbana–Champaign);Jiawei Han (University of Illinois at Urbana–Champaign);,"2461963590,2095732305,2146108112,2046577858,2121939561","Many digital documentary data collections (e.g., scientific publications, enterprise reports, news articles, and social media) can be modeled as a heterogeneous information network, linking text with multiple types of entities. Constructing high-quality hierarchies that can represent topics at multiple granularities benefits tasks such as search, information browsing, and pattern mining. In this work, we present an algorithm for recursively constructing multi-typed topical hierarchies. Contrary to traditional text-based topic modeling, our approach handles both textual phrases and multiple types of entities by a newly designed clustering and ranking algorithm for heterogeneous network data, as well as mining and ranking topical patterns of different types. Our experiments on datasets from two different domains demonstrate that our algorithm yields high-quality, multi-typed topical hierarchies.",2015,Knowledge and Information Systems volume 44 issue 3 pp 529-558,topic model;text mining;data science;information retrieval;data mining;database;machine learning;computer science;
A probabilistic model to resolve diversity---accuracy challenge of recommendation systems,Amin Javari (Sharif University of Technology);Mahdi Jalili (Sharif University of Technology);,"2019201496,2104502493","Recommendation systems have wide-spread applications in both academia and industry. Traditionally, performance of recommendation systems has been measured by their precision. By introducing novelty and diversity as key qualities in recommender systems, recently increasing attention has been focused on this topic. Precision and novelty of recommendation are not in the same direction, and practical systems should make a trade-off between these two quantities. Thus, it is an important feature of a recommender system to make it possible to adjust diversity and accuracy of the recommendations by tuning the model. In this paper, we introduce a probabilistic structure to resolve the diversity---accuracy dilemma in recommender systems. We propose a hybrid model with adjustable level of diversity and precision such that one can perform this by tuning a single parameter. The proposed recommendation model consists of two models: one for maximization of the accuracy and the other one for specification of the recommendation list to tastes of users. Our experiments on two real datasets show the functionality of the model in resolving accuracy---diversity dilemma and outperformance of the model over other classic models. The proposed method could be extensively applied to real commercial systems due to its low computational complexity and significant performance.",2015,Knowledge and Information Systems volume 44 issue 3 pp 609-627,collaborative filtering;markov chain;maximum likelihood;data mining;database;artificial intelligence;machine learning;simulation;statistics;computer science;
Using knowledge-based relatedness for information retrieval,Arantxa Otegi (University of the Basque Country);Xabier Arregi (University of the Basque Country);Olatz Ansa (University of the Basque Country);Eneko Agirre (University of the Basque Country);,"2152070847,2086057012,2134353982,1956093140","Traditional information retrieval (IR) systems use keywords to index and retrieve documents. The limitations of keywords were recognized since the early days, specially when different but closely related words are used in the query and the relevant document. Query expansion techniques like pseudo-relevance feedback (PRF) and document clustering techniques rely on the target document set in order to bridge the gap between those words. This paper explores the use of knowledge-based semantic relatedness techniques to overcome the vocabulary mismatch between the query and documents, both on IR and Passage Retrieval for question answering. We performed query expansion and document expansion using WordNet, with positive effects over a language modeling baseline on three datasets, and over PRF on two of those datasets. Our analysis shows that our models and PRF are complementary; in that, PRF is better for easy queries, and our models are stronger for difficult queries and that our models generalize better to other collections, being more robust to parameter adjustments. In addition, we show that our method has a positive impact in an end-to-end question answering system for Basque and that it can be readily applied to other knowledge bases, as our good results using Wikipedia show, paving the way for the use of other knowledge structures such as medical ontologies and linked data repositories.",2015,Knowledge and Information Systems volume 44 issue 3 pp 689-718,query expansion;semantic similarity;query language;knowledge based systems;natural language processing;information retrieval;data mining;database;computer science;
A topic-biased user reputation model in rating systems,Baichuan Li (Baidu);Rong-Hua Li (Shenzhen University);Irwin King (The Chinese University of Hong Kong);Michael R. Lyu (The Chinese University of Hong Kong);Jeffrey Xu Yu (The Chinese University of Hong Kong);,"2101585682,2097752033,2121363826,2227744130,2119358208","In rating systems like Epinions and Amazon's product review systems, users rate items on different topics to yield item scores. Traditionally, item scores are estimated by averaging all the ratings with equal weights. To improve the accuracy of estimated item scores, user reputation [a.k.a., user reputation (UR)] is incorporated. The existing algorithms on UR, however, have underplayed the role of topics in rating systems. In this paper, we first reveal that UR is topic-biased from our empirical investigation. However, existing algorithms cannot capture this characteristic in rating systems. To address this issue, we propose a topic-biased model (TBM) to estimate UR in terms of different topics as well as item scores. With TBM, we develop six topic-biased algorithms, which are subsequently evaluated with experiments using both real-world and synthetic data sets. Results of the experiments demonstrate that the topic-biased algorithms effectively estimate UR across different topics and produce more robust item scores than previous reputation-based algorithms, leading to potentially more robust rating systems.",2015,Knowledge and Information Systems volume 44 issue 3 pp 581-607,data science;world wide web;data mining;database;artificial intelligence;statistics;
Anonymizing graphs: measuring quality for clustering,Jordi Casas-Roma (Open University of Catalonia);Jordi Herrera-Joancomartí (Autonomous University of Barcelona);Vicenç Torra (Spanish National Research Council);,"147439060,65517402,2208460423","Anonymization of graph-based data is a problem, which has been widely studied last years, and several anonymization methods have been developed. Information loss measures have been carried out to evaluate the noise introduced in the anonymized data. Generic information loss measures ignore the intended anonymized data use. When data has to be released to third-parties, and there is no control on what kind of analyses users could do, these measures are the standard ones. In this paper we study different generic information loss measures for graphs comparing such measures to the cluster-specific ones. We want to evaluate whether the generic information loss measures are indicative of the usefulness of the data for subsequent data mining processes.",2015,Knowledge and Information Systems volume 44 issue 3 pp 507-528,privacy;world wide web;data mining;database;computer science;
Extended feature combination model for recommendations in location-based mobile services,"Masoud Sattari (Middle East Technical University);Ismail Hakki Toroslu (Middle East Technical University);Pinar Karagoz (Middle East Technical University);Panagiotis Symeonidis (Aristotle University of Thessaloniki);Yannis Manolopoulos (University of Maryland, College Park);","2115689620,2065608331,2068042486,2401499786,276012958","With the increasing availability of location-based services, location-based social networks and smart phones, standard rating schema of recommender systems that involve user and item dimensions is extended to three-dimensional (3-D) schema involving context information. Although there are models proposed for dealing with data in this form, the problem of combining it with additional features and constructing a general model suitable for different forms of recommendation system techniques has not been fully explored. This work proposes a technique to reduce 3-D rating data into 2-D for two reasons: employing already developed efficient methods for 2-D on a 3-D data and expanding it with additional features, which are usually 2-D also, if it is necessary. Our experiments show that this reduction is effective. The proposed 2-D model supports content-based, collaborative filtering and hybrid recommendation approaches effectively, whereas we have achieved the best accuracy results for pure collaborative filtering recommendation model. Since our method was built on efficient singular value decomposition-based dimension reduction idea, it also works very efficiently, and in our experiments, we have obtained better run-time results than standard methods developed for 3-D data using higher-order singular value decomposition.",2015,Knowledge and Information Systems volume 44 issue 3 pp 629-661,tensor;theoretical computer science;data mining;database;machine learning;mathematics;
The 10 million follower fallacy: audience size does not prove domain-influence on Twitter,Mario Cataldi (École Centrale Paris);Marie-Aude Aufaure (École Centrale Paris);,"2101211219,743058936","With the advent of social networks and micro-blogging systems, the way of communicating with other people and spreading information has changed substantially. Persons with different backgrounds, age and education exchange information and opinions, spanning various domains and topics, and have now the possibility to directly interact with popular users and authoritative information sources usually unreachable before the advent of these environments. As a result, the mechanism of information propagation changed deeply, the study of which is indispensable for the sake of understanding the evolution of information networks. To cope up with this intention, in this paper, we propose a novel model which enables to delve into the spread of information over a social network along with the change in the user relationships with respect to the domain of discussion. For this, considering Twitter as a case study, we aim at analyzing the multiple paths the information follows over the network with the goal of understanding the dynamics of the information contagion with respect to the change of the topic of discussion. We then provide a method for estimating the influence among users by evaluating the nature of the relationship among them with respect to the topic of discussion they share. Using a vast sample of the Twitter network, we then present various experiments that illustrate our proposal and show the efficacy of the proposed approach in modeling this information spread.",2015,Knowledge and Information Systems volume 44 issue 3 pp 559-580,social network;social computing;data mining;artificial intelligence;machine learning;computer science;
Adaptive ensemble with trust networks and collaborative recommendations,Haitao Zou (University of Macau);Zhiguo Gong (University of Macau);Nan Zhang (George Washington University);Qing Li (City University of Hong Kong);Yanghui Rao (City University of Hong Kong);,"2165332580,2159588081,2166589344,2490701366,2651583438","Several existing recommender algorithms combine collaborative filtering and social/trust networks together in order to overcome the problems caused by data scarcity and to produce more effective recommendations for users. In general, those methods fuse a user's own taste and his trusted friends/users' tastes using an ensemble model where a parameter is used to balance these two components. However, this parameter is often set as a constant and with no regard to users' individual characteristics. Aiming at introducing personalization to solve the above problem, we propose a local topology-based ensemble model to adaptively combine a user's own taste and his trusted friends/users' tastes. We take users' clustering coefficients in the social/trust networks as the indicator to measure the consistence of their selecting trusted friends/users and leverage this local topology-based parameter in the ensemble model. To predict the likelihood of users' purchasing actions on items, we also combine item ratings and sentiment values which are reflected in the review contents as the input to the adaptive ensemble model. We conduct comprehensive experiments which demonstrate the superiority of our adaptive algorithms over the existing ones.",2015,Knowledge and Information Systems volume 44 issue 3 pp 663-688,collaborative filtering;ensembl;sentiment analysis;world wide web;data mining;database;artificial intelligence;machine learning;computer science;
"Robustness, stability, recoverability, and reliability in constraint satisfaction problems",Federico Barber (Polytechnic University of Valencia);Miguel A. Salido (Polytechnic University of Valencia);,"2121374980,2156559484","Many real-world problems in Artificial Intelligence (AI) as well as in other areas of computer science and engineering can be efficiently modeled and solved using constraint programming techniques. In many real-world scenarios the problem is partially known, imprecise and dynamic such that some effects of actions are undesired and/or several unforeseen incidences or changes can occur. Whereas expressivity, efficiency, and optimality have been the typical goals in the area, there are several issues regarding robustness that have a clear relevance in dynamic Constraint Satisfaction Problems (CSPs). However, there is still no clear and common definition of robustness-related concepts in CSPs. In this paper, we propose two clearly differentiated definitions for robustness and stability in CSP solutions. We also introduce the concepts of recoverability and reliability, which arise in temporal CSPs. All these definitions are based on related well-known concepts, which are addressed in engineering and other related areas.",2015,Knowledge and Information Systems volume 44 issue 3 pp 719-734,constraint satisfaction problem;robustness;stability;mathematical optimization;statistics;algorithm;computer science;mathematics;
Mining sequential patterns from probabilistic databases,Muhammad Muzammal (Bahria University);Rajeev Raman (University of Leicester);,"267849793,2131320315","This paper considers the problem of sequential pattern mining (SPM) in probabilistic databases. Specifically, we consider SPM in situations where there is uncertainty in associating an event with a source, model this kind of uncertainty in the probabilistic database framework and consider the problem of enumerating all sequences whose expected support is sufficiently large. We give an algorithm based on dynamic programming to compute the expected support of a sequential pattern. Next, we propose three algorithms for mining sequential patterns from probabilistic databases. The first two algorithms are based on the candidate generation framework--one each based on a breadth-first (similar to GSP) and a depth-first (similar to SPAM) exploration of the search space. The third one is based on the pattern-growth framework (similar to PrefixSpan). We propose optimizations that mitigate the effects of the expensive dynamic programming computation step. We give an empirical evaluation of the probabilistic SPM algorithms and the optimizations and demonstrate the scalability of the algorithms in terms of CPU time and the memory usage. We also demonstrate the effectiveness of the probabilistic SPM framework in extracting meaningful sequences in the presence of noise.",2015,Knowledge and Information Systems volume 44 issue 2 pp 325-358,probabilistic database;sequential pattern mining;probabilistic analysis of algorithms;technology;data science;data mining;database;artificial intelligence;machine learning;computer science;
Reconstructing individual mobility from smart card transactions: a collaborative space alignment approach,Fuzheng Zhang (University of Science and Technology of China);Nicholas Jing Yuan (Microsoft);Yingzi Wang (University of Science and Technology of China);Xing Xie (Microsoft);,"2110384818,2096490164,2274817761,2125800575","Smart card transactions capture rich information of human mobility and urban dynamics and therefore are of particular interest to urban planners and location-based service providers. However, since most transaction systems are only designated for billing purpose, typically, fine-grained location information, such as the exact boarding and alighting stops of a bus trip, is only partially or not available at all, which blocks deep exploitation of this rich and valuable data at individual level. This paper presents a collaborative space alignment framework to reconstruct individual mobility history from a metropolitan-scale smart card transaction dataset. Specifically, we show that by delicately aligning the monetary space and geospatial space with the temporal space, we are able to extrapolate a series of critical domain-specific constraints. Later, these constraints are naturally incorporated into a semi-supervised conditional random field (CRF) to infer the exact boarding and alighting stops of all transit routes, where the features of the CRF model consist of not only pre-defined indicator features extracted from individual trips but also latent features crafted from different users' trips using collaborative filtering. Here, we consider two types of collaborative features: (1) the similarity in terms of users' choices of bus lines and (2) latent temporal patterns of users' commuting behaviors. Extensive experimental results show that our approach achieves a high accuracy, e.g., given only 10 % trips with known alighting/boarding stops, and we successfully inferred more than 79 % alighting and boarding stops from all unlabeled trips. In particular, we validated that the extracted collaborative features significantly contribute to the accuracy of our model. In addition, we have demonstrated that by applying our approach to enrich the data, the performance of a conventional method for identifying users' home and work places can be dramatically improved (with 83 % improvement on home detection and 38 % improvement on work place detection). The proposed method offers the possibility to mine individual mobility from common public transit transactions, and showcases how uncertain data can be leveraged with domain knowledge and constraints, to support cross-application data-mining tasks.",2015,Knowledge and Information Systems volume 44 issue 2 pp 299-323,smart card;collaborative filtering;mobile computing;world wide web;computer security;data mining;database;machine learning;simulation;computer science;
Compressed vertical partitioning for efficient RDF management,Sandra Álvarez-García (University of A Coruña);Nieves R. Brisaboa (University of A Coruña);Javier D. Fernández (University of Valladolid);Miguel A. Martínez-Prieto (University of Valladolid);Gonzalo Navarro (University of Chile);,"2042101261,2224363331,2127846371,1561992963,2133226413","The Web of Data has been gaining momentum in recent years. This leads to increasingly publish more and more semi-structured datasets following, in many cases, the RDF (Resource Description Framework) data model based on atomic triple units of subject, predicate, and object. Although it is a very simple model, specific compression methods become necessary because datasets are increasingly larger and various scalability issues arise around their organization and storage. This requirement is even more restrictive in RDF stores because efficient SPARQL solution on the compressed RDF datasets is also required. This article introduces a novel RDF indexing technique that supports efficient SPARQL solution in compressed space. Our technique, called $$\hbox {k}^2$$k2-triples, uses the predicate to vertically partition the dataset into disjoint subsets of pairs (subject, object), one per predicate. These subsets are represented as binary matrices of subjects $$\times $$× objects in which 1-bits mean that the corresponding triple exists in the dataset. This model results in very sparse matrices, which are efficiently compressed using $$\hbox {k}^2$$k2-trees. We enhance this model with two compact indexes listing the predicates related to each different subject and object in the dataset, in order to address the specific weaknesses of vertically partitioned representations. The resulting technique not only achieves by far the most compressed representations, but also achieves the best overall performance for RDF retrieval in our experimental setup. Our approach uses up to 10 times less space than a state-of-the-art baseline and outperforms its time performance by several orders of magnitude on the most basic query patterns. In addition, we optimize traditional join algorithms on $$\hbox {k}^2$$k2-triples and define a novel one leveraging its specific features. Our experimental results show that our technique also overcomes traditional vertical partitioning for join solution, reporting the best numbers for joins in which the non-joined nodes are provided, and being competitive in most of the cases.",2015,Knowledge and Information Systems volume 44 issue 2 pp 439-474,rdf;theoretical computer science;data mining;database;artificial intelligence;computer science;
Biclustering neighborhood-based collaborative filtering method for top-n recommender systems,Faris Alqadah (PayPal);Chandan K. Reddy (Wayne State University);Junling Hu (Samsung);Hatim F. Alqadah;,"2307586855,2100435683,2693867536,2700078008","We propose a novel collaborative filtering method for top-$$n$$n recommendation task using bicustering neighborhood approach. Our method takes advantage of local biclustering structure for a more precise and localized collaborative filtering. Using several important properties from the field of Formal Concept Analysis, we build user-specific biclusters that are ""more personalized"" to the users of interest. We create an innovative rank scoring of candidate items that combines local similarity of biclusters with global similarity. Our method is parameter-free, thus removing the need for tuning parameters. It is easily scalable and can efficiently make recommendations. We demonstrate the performance of our algorithm using several standard benchmark datasets and two paypal (in-house) datasets. Our experiments show that our method generates better recommendations compared to several state-of-the-art algorithms, especially in the presence of sparse data. Furthermore, we also demonstrated the robustness of our approach to increasing data sparsity and the number of users.",2015,Knowledge and Information Systems volume 44 issue 2 pp 475-491,formal concept analysis;collaborative filtering;biclustering;recommender system;information retrieval;data mining;database;machine learning;computer science;
A robust one-class transfer learning method with uncertain data,Yanshan Xiao (Guangdong University of Technology);Bo Liu (Guangdong University of Technology);Philip S. Yu (University of Illinois at Chicago);Zhifeng Hao (Guangdong University of Technology);,"2138906516,2504109675,2125104194,2643446293","One-class classification aims at constructing a distinctive classifier based on one class of examples. Most of the existing one-class classification methods are proposed based on the assumptions that: (1) there are a large number of training examples available for learning the classifier; (2) the training examples can be explicitly collected and hence do not contain any uncertain information. However, in real-world applications, these assumptions are not always satisfied. In this paper, we propose a novel approach called uncertain one-class transfer learning with support vector machine (UOCT-SVM), which is capable of constructing an accurate classifier on the target task by transferring knowledge from multiple source tasks whose data may contain uncertain information. In UOCT-SVM, the optimization function is formulated to deal with uncertain data and transfer learning based on one-class SVM. Then, an iterative framework is proposed to solve the optimization function. Extensive experiments have showed that UOCT-SVM can mitigate the effect of uncertain data on the decision boundary and transfer knowledge from source tasks to help build an accurate classifier on the target task, compared with state-of-the-art one-class classification methods.",2015,Knowledge and Information Systems volume 44 issue 2 pp 407-438,multi task learning;one class classification;quadratic classifier;transfer of learning;data mining;pattern recognition;artificial intelligence;machine learning;computer science;
AGraP: an algorithm for mining frequent patterns in a single graph using inexact matching,"Marisol Flores-Garrido;Jesús-Ariel Carrasco-Ochoa;José Fco. Martínez-Trinidad (National Institute of Astrophysics, Optics and Electronics);","1997098389,2412044900,2044182321","Frequent graph mining algorithms commonly use graph isomorphism to identify occurrences of a given pattern, but in the last years, a few works have focused on the case where a pattern could differ from its occurrences, which can be important to analyze noisy data. These later algorithms allow differences in labels and structural differences in edges, but to the best of our knowledge, none of them considers structural differences in vertices. How can we identify occurrences that differ by one (or several) nodes from the pattern they represent? Our work approaches the problem of frequent graph pattern mining with two main characteristics. First, we use inexact matching, allowing structural differences in both edges and vertices. Second, we focus on the problem of mining patterns in a single graph, a problem that has been less explored than the case in which patterns are mined from a graph collection. In this paper, we introduce two similarity functions to compare graphs using inexact matching and an algorithm, AGraP, able to identify patterns that can have structural differences with respect to their occurrences. Our experimental results show that AGraP is able to find patterns that cannot be found by other state-of-the-art algorithms. Additionally, we show that the patterns mined by AGraP are useful in classification tasks.",2015,Knowledge and Information Systems volume 44 issue 2 pp 385-406,combinatorics;pattern recognition;machine learning;mathematics;
A masking index for quantifying hidden glitches,Laure Berti-Équille (International Relief and Development Inc.);Ji Meng Loh (New Jersey Institute of Technology);Tamraparni Dasu (AT&T Labs);,"192351227,2124479687,425249397","Data glitches are errors in a dataset. They are complex entities that often span multiple attributes and records. When they co-occur in data, the presence of one type of glitch can hinder the detection of another type of glitch. This phenomenon is called masking. In this paper, we define two important types of masking and propose a novel, statistically rigorous indicator called masking index for quantifying the hidden glitches. We outline four cases of masking: outliers masked by missing values, outliers masked by duplicates, duplicates masked by missing values, and duplicates masked by outliers. The masking index is critical for data quality profiling and data exploration. It enables a user to measure the extent of masking and hence the confidence in the data. In this sense, it is a valuable data quality index for choosing an anomaly detection method that is best suited for the glitches that are present in any given dataset. We demonstrate the utility and effectiveness of the masking index by intensive experiments on synthetic and real-world datasets.",2015,Knowledge and Information Systems volume 44 issue 2 pp 253-277,masking;anomaly detection;computer security;data mining;machine learning;statistics;computer science;
The Author-Topic-Community model for author interest profiling and community discovery,Chunshan Li (Harbin Institute of Technology);William K. Cheung (Hong Kong Baptist University);Yunming Ye (Harbin Institute of Technology);Xiaofeng Zhang (Harbin Institute of Technology);Dianhui Chu (Harbin Institute of Technology);Xin Li (Beijing Institute of Technology);,"2119804830,2099585548,2167640884,2678979916,2667878678,2632223743","In this paper, we propose a generative model named the author-topic-community (ATC) model for representing a corpus of linked documents. The ATC model allows each author to be associated with a topic distribution and a community distribution as its model parameters. A learning algorithm based on variational inference is derived for the model parameter estimation where the two distributions are essentially reinforcing each other during the estimation. We compare the performance of the ATC model with two related generative models using first synthetic data sets and then real data sets, which include a research community data set, a blog data set, a news-sharing data set, and a microblogging data set. The empirical results obtained confirm that the proposed ATC model outperforms the existing models for tasks such as author interest profiling and author community discovery. We also demonstrate how the inferred ATC model can be used to characterize the roles of users/authors in online communities.",2015,Knowledge and Information Systems volume 44 issue 2 pp 359-383,graphical model;data science;information retrieval;data mining;database;artificial intelligence;machine learning;statistics;computer science;
Bounds on the moments for an ensemble of random decision trees,Amit Dhurandhar (IBM);,2061123877,"An ensemble of random decision trees is a popular classification technique, especially known for its ability to scale to large domains. In this paper, we provide an efficient strategy to compute bounds on the moments of the generalization error computed over all datasets of a particular size drawn from an underlying distribution, for this classification technique. Being able to estimate these moments can help us gain insights into the performance of this model. As we will see in the experimental section, these bounds tend to be significantly tighter than the state-of-the-art Breiman's bounds based on strength and correlation and hence more useful in practice.",2015,Knowledge and Information Systems volume 44 issue 2 pp 279-298,random forest;moment;combinatorics;mathematical optimization;statistics;mathematics;
Automated and weighted self-organizing time maps,Peter Sarlin (Goethe University Frankfurt);,2079213318,"This paper proposes schemes for automated and weighted self-organizing time maps (SOTMs). The SOTM provides means for a visual approach to evolutionary clustering, which aims at producing a sequence of clustering solutions. This task we denote as visual dynamic clustering. The implication of an automated SOTM is not only a data-driven parametrization of the SOTM, but also the feature of adjusting the training to the characteristics of the data at each time step. The aim of the weighted SOTM is to improve learning from more trustworthy or important data with an instance-varying weight. The schemes for automated and weighted SOTMs are illustrated on two real-world datasets: (i) country-level risk indicators to measure the evolution of global imbalances and (ii) credit applicant data to measure the evolution of firm-level credit risks.",2015,Knowledge and Information Systems volume 44 issue 2 pp 493-505,automation;data science;data mining;machine learning;statistics;computer science;
Adapting naive Bayes tree for text classification,Shasha Wang (China University of Geosciences);Liangxiao Jiang (China University of Geosciences);Chaoqun Li (China University of Geosciences);,"2115148437,2711203041,2129023193","Naive Bayes (NB) is one of the top 10 algorithms thanks to its simplicity, efficiency, and interpretability. To weaken its attribute independence assumption, naive Bayes tree (NBTree) has been proposed. NBTree is a hybrid algorithm, which deploys a naive Bayes classifier on each leaf node of the built decision tree and has demonstrated remarkable classification performance. When comes to text classification tasks, multinomial naive Bayes (MNB) has been a dominant modeling approach after the multi-variate Bernoulli model. Inspired by the success of NBTree, we propose a new algorithm called multinomial naive Bayes tree (MNBTree) by deploying a multinomial naive Bayes text classifier on each leaf node of the built decision tree. Different from NBTree, MNBTree builds a binary tree, in which the split attributes' values are just divided into zero and nonzero. At the same time, MNBTree uses the information gain measure instead of the classification accuracy measure to build the tree for reducing the time consumption. To further scale up the classification performance of MNBTree, we propose its multiclass learning version called multiclass multinomial naive Bayes tree (MMNBTree) by applying the multiclass technique to MNBTree. The experimental results on a large number of widely used text classification benchmark datasets validate the effectiveness of our proposed algorithms: MNBTree and MMNBTree.",2015,Knowledge and Information Systems volume 44 issue 1 pp 77-89,bayes error rate;bayes classifier;naive bayes classifier;multiclass classification;data mining;pattern recognition;machine learning;mathematics;
Partial spatio-temporal co-occurrence pattern mining,Mete Celik (Erciyes University);,2306330000,"Spatio-temporal co-occurrence patterns represent subsets of object-types that are often located together in space and time. The aim of the discovery of partial spatio-temporal co-occurrence patterns (PACOPs) is to find co-occurrences of the object-types that are partially present in the database. Discovering PACOPs is an important problem with many applications such as discovering interactions between animals in ecology, identifying tactics in battlefields and games, and identifying crime patterns in criminal databases. However, mining PACOPs is computationally very expensive because the interest measures are computationally complex, databases are larger due to the archival history, and the set of candidate patterns is exponential in the number of object-types. Previous studies on discovering spatio-temporal co-occurrence patterns do not take into account the presence period (i.e., lifetime) of the objects in the database. This paper defines the problem of mining PACOPs, proposes a new monotonic composite interest measure, and proposes novel PACOP mining algorithms. The experimental results show that the proposed algorithms are computationally more efficient than the naive alternatives.",2015,Knowledge and Information Systems volume 44 issue 1 pp 27-49,data science;data mining;machine learning;computer science;
Speeding up multiple instance learning classification rules on GPUs,Alberto Cano (University of Córdoba);Amelia Zafra (University of Córdoba);Sebastián Ventura (University of Córdoba);,"2123697688,2137006243,2098297204","Multiple instance learning is a challenging task in supervised learning and data mining. However, algorithm performance becomes slow when learning from large-scale and high-dimensional data sets. Graphics processing units (GPUs) are being used for reducing computing time of algorithms. This paper presents an implementation of the G3P-MI algorithm on GPUs for solving multiple instance problems using classification rules. The GPU model proposed is distributable to multiple GPUs, seeking for its scalability across large-scale and high-dimensional data sets. The proposal is compared to the multi-threaded CPU algorithm with streaming SIMD extensions parallelism over a series of data sets. Experimental results report that the computation time can be significantly reduced and its scalability improved. Specifically, an speedup of up to 149$$\times $$× can be achieved over the multi-threaded CPU algorithm when using four GPUs, and the rules interpreter achieves great efficiency and runs over 108 billion genetic programming operations per second.",2015,Knowledge and Information Systems volume 44 issue 1 pp 127-145,biological classification;theoretical computer science;parallel computing;data mining;database;machine learning;computer science;
Secure support vector machines outsourcing with random linear transformation,Keng-Pei Lin (National Sun Yat-sen University);Yi-Wei Chang (National Sun Yat-sen University);Ming-Syan Chen (National Taiwan University);,"2170960541,2589650942,2122365371","The support vector machine (SVM) is a popular classification algorithm. Since training the SVM is very time-consuming, outsourcing the computations of solving the SVM to external service providers benefits the data owner who is not familiar with the techniques of the SVM or has only limited computing resources. In outsourcing, the data privacy is a critical concern for some legal or commercial reasons since there may be sensitive information contained in the data. Existing privacy-preserving data mining works are either not appropriate to outsourcing the SVM or weak in security. In this paper, we propose a scheme for privacy-preserving outsourcing of the SVM, which perturbs the data by random linear transformation. The service provider solves the SVM from the perturbed data without knowing the actual content of the data, and the generated SVM classifier is also perturbed, which can only be recovered by the data owner. Both the privacy of data and generated classifiers are protected. The proposed scheme is stronger in security than existing techniques, and incurs very little additional communication and computation cost. The experimental results show that the proposed scheme imposes very little overhead on the data owner, and the classification accuracy is similar to a normal SVM classifier.",2015,Knowledge and Information Systems volume 44 issue 1 pp 147-176,ranking svm;outsourcing;support vector machine;biological classification;computer security;data mining;database;machine learning;computer science;
LC-mine: a framework for frequent subgraph mining with local consistency techniques,Brahim Douar (Tunis El Manar University);Michel Liquiere (American Diabetes Association);Chiraz Latiri (Tunis El Manar University);Yahya Slimani (Carthage University);,"2519430277,256712935,2579121749,2404367413","Developing algorithms that discover all frequently occurring subgraphs in a large graph database is computationally extensive, as graph and subgraph isomorphisms play a key role throughout the computations. Since subgraph isomorphism testing is a hard problem, fragment miners are exponential in runtime. To alleviate the complexity issue, we propose to introduce a bias in the projection operator and instead of using the costly subgraph isomorphism projection, one can use a polynomial projection having a semantically valid structural interpretation. In this paper, our purpose is to present LC-mine, a generic and efficient framework to mine frequent subgraphs by the means of local consistency techniques used in the constraint programming field. Two instances of the framework based on the arc consistency technique are developed and presented in this paper. The first instance follows a breadth-first order, while the second is a pattern-growth approach that follows a depth-first search space exploration strategy. Then, we prove experimentally that we can achieve an important performance gain without or with nonsignificant loss of discovered patterns in terms of quality.",2015,Knowledge and Information Systems volume 44 issue 1 pp 1-25,maximum common subgraph isomorphism problem;factor critical graph;degeneracy;distance hereditary graph;induced subgraph isomorphism problem;forbidden graph characterization;universal graph;color coding;subgraph isomorphism problem;projection;statistical relational learning;discrete mathematics;combinatorics;data mining;database;machine learning;computer science;mathematics;
A comparative study between possibilistic and probabilistic approaches for monolingual word sense disambiguation,Bilel Elayeb (Manouba University);Ibrahim Bounhas (Manouba University);Oussama Ben Khiroun (Manouba University);Fabrice Evrard (ENSEEIHT);Narjès Bellamine Ben Saoud (Manouba University);,"36408374,197975149,1219534554,1984662771,2043040427","This paper proposes and assesses a new possibilistic approach for automatic monolingual word sense disambiguation (WSD). In fact, in spite of their advantages, the traditional dictionaries suffer from the lack of accurate information useful for WSD. Moreover, there exists a lack of high-coverage semantically labeled corpora on which methods of learning could be trained. For these multiple reasons, it became important to use a semantic dictionary of contexts (SDC) ensuring the machine learning in a semantic platform of WSD. Our approach combines traditional dictionaries and labeled corpora to build a SDC and identify the sense of a word by using a possibilistic matching model. Besides, we present and evaluate a second new probabilistic approach for automatic monolingual WSD. This approach uses and extends an existing probabilistic semantic distance to compute similarities between words by exploiting a semantic graph of a traditional dictionary and the SDC. To assess and compare these two approaches, we performed experiments on the standard ROMANSEVAL test collection and we compared our results to some existing French monolingual WSD systems. Experiments showed an encouraging improvement in terms of disambiguation rates of French words. These results reveal the contribution of possibility theory as a mean to treat imprecision in information systems.",2015,Knowledge and Information Systems volume 44 issue 1 pp 91-126,semeval;possibility theory;probability theory;natural language processing;information retrieval;pattern recognition;artificial intelligence;statistics;computer science;
Using proximity and tag weights for focused retrieval in structured documents,Michel Beigbeder (Ecole nationale supérieure des mines de Saint-Étienne);Mathias Géry (University of Lyon);Christine Largeron (University of Lyon);,"686169346,2205491476,361319035","Focused information retrieval is concerned with the retrieval of small units of information. In this context, the structure of the documents as well as the proximity among query terms have been found useful for improving retrieval effectiveness. In this article, we propose an approach combining the proximity of the terms and the tags which mark these terms. Our approach is based on a Fetch and Browse method where the fetch step is performed with BM25 and the browse step with a structure enhanced proximity model. In this way, the ranking of a document depends not only upon the existence of the query terms within the document but also upon the tags which mark these terms. Thus, the document tends to be highly relevant when query terms are close together and are emphasized by tags. The evaluation of this model on a large XML structured collection provided by the INEX 2010 XML IR evaluation campaign shows that the use of term proximity and structure improves the retrieval effectiveness of BM25 in the context of focused information retrieval.",2015,Knowledge and Information Systems volume 44 issue 1 pp 51-76,ranking;human computer information retrieval;visual word;query expansion;vector space model;xml;distance;world wide web;information retrieval;data mining;computer science;
Co-clustering of fuzzy lagged data,Eran Shaham (Bar-Ilan University);David Sarne (Bar-Ilan University);Boaz Ben-Moshe (Ariel University);,"2158094253,2143023648,2224423478","The paper focuses on mining patterns that are characterized by a fuzzy lagged relationship between the data objects forming them. Such a regulatory mechanism is quite common in real-life settings. It appears in a variety of fields: finance, gene expression, neuroscience, crowds and collective movements are but a limited list of examples. Mining such patterns not only helps in understanding the relationship between objects in the domain, but assists in forecasting their future behavior. For most interesting variants of this problem, finding an optimal fuzzy lagged co-cluster is an NP-complete problem. We present a polynomial time Monte Carlo approximation algorithm for mining fuzzy lagged co-clusters. We prove that for any data matrix, the algorithm mines a fuzzy lagged co-cluster with fixed probability, which encompasses the optimal fuzzy lagged co-cluster by a maximum 2 ratio columns overhead and completely no rows overhead. Moreover, the algorithm handles noise, anti-correlations, missing values and overlapping patterns. The algorithm was extensively evaluated using both artificial and real-life datasets. The results not only corroborate the ability of the algorithm to efficiently mine relevant and accurate fuzzy lagged co-clusters, but also illustrate the importance of including fuzziness in the lagged-pattern model.",2015,Knowledge and Information Systems volume 44 issue 1 pp 217-252,biclustering;data science;data mining;machine learning;statistics;computer science;
Improved concept drift handling in surgery prediction and other applications,Ayne A. Beyene (Blekinge Institute of Technology);Tewelle Welemariam (Blekinge Institute of Technology);Marie Persson (Blekinge Institute of Technology);Niklas Lavesson (Blekinge Institute of Technology);,"2175859334,238589021,2108979845,1922797874","The article presents a new algorithm for handling concept drift: the Trigger-based Ensemble (TBE) is designed to handle concept drift in surgery prediction but it is shown to perform well for other classification problems as well. At the primary care, queries about the need for surgical treatment are referred to a surgeon specialist. At the secondary care, referrals are reviewed by a team of specialists. The possible outcomes of this review are that the referral: (i) is canceled, (ii) needs to be complemented, or (iii) is predicted to lead to surgery. In the third case, the referred patient is scheduled for an appointment with a surgeon specialist. This article focuses on the binary prediction of case three (surgery prediction). The guidelines for the referral and the review of the referral are changed due to, e.g., scientific developments and clinical practices. Existing decision support is based on the expert systems approach, which usually requires manual updates when changes in clinical practice occur. In order to automatically revise decision rules, the occurrence of concept drift (CD) must be detected and handled. The existing CD handling techniques are often specialized; it is challenging to develop a more generic technique that performs well regardless of CD type. Experiments are conducted to measure the impact of CD on prediction performance and to reduce CD impact. The experiments evaluate and compare TBE to three existing CD handling methods (AWE, Active Classifier, and Learn++) on one real-world dataset and one artificial dataset. TBA significantly outperforms the other algorithms on both datasets but is less accurate on noisy synthetic variations of the real-world dataset.",2015,Knowledge and Information Systems volume 44 issue 1 pp 177-196,concept drift;data mining;database;artificial intelligence;machine learning;simulation;computer science;
Wavelet energy-based visualization and classification of high-dimensional signal for bearing fault detection,Uk Jung (Dongguk University);Bong-Hwan Koh (Dongguk University);,"2142335923,2107955755","This study investigates a methodology for interpretable visualizing and classifying high-dimensional data such as vibration signals in machine fault detection application. Although principal component analysis is one of the most widely used dimension reduction methods, it does not clearly explain the source of signal variations (i.e., statistical characteristics such as mean and variance), but just locate signals on low-dimensional space which maximizing data dispersion. This deficiency restricts its interpretability to specific problems of process control and thus limits their broader usefulness. To overcome this deficiency, this study exploits the multiscale energy analysis of discrete wavelet transformation, so-called wavelet scalogram, in unsupervised manner. Wavelet scalogram allows us to first obtain a very low-dimensional feature subset of our data, which is strongly correlated with the characteristics of the data without considering the classification method used, although each of these features is uncorrelated with each other. In supervised learning scheme, it can be eventually combined with silhouette statistics for the purpose of more effective visualization of the main sources of different classes and classifying signals into different classes. Finally, nonparametric multi-class classifiers such as classification and regression tree and k-nearest neighbors quantitatively evaluate the performance of our approach for machine fault classification problem in terms of the 10-fold misclassification error rate.",2015,Knowledge and Information Systems volume 44 issue 1 pp 197-215,vibration;visualization;biological classification;data mining;pattern recognition;machine learning;statistics;computer science;mathematics;
Improving contextual advertising matching by using Wikipedia thesaurus knowledge,Guandong Xu (IT University);Zongda Wu (Wenzhou University);Guiling Li (China University of Geosciences);Enhong Chen (University of Science and Technology of China);,"2615081380,2162511255,2558605592,2136372366","As a prevalent type of Web advertising, contextual advertising refers to the placement of the most relevant commercial ads within the content of a Web page, to provide a better user experience and as a result increase the user's ad-click rate. However, due to the intrinsic problems of homonymy and polysemy, the low intersection of keywords, and a lack of sufficient semantics, traditional keyword matching techniques are not able to effectively handle contextual matching and retrieve relevant ads for the user, resulting in an unsatisfactory performance in ad selection. In this paper, we introduce a new contextual advertising approach to overcome these problems, which uses Wikipedia thesaurus knowledge to enrich the semantic expression of a target page (or an ad). First, we map each page into a keyword vector, upon which two additional feature vectors, the Wikipedia concept and category vector derived from the Wikipedia thesaurus structure, are then constructed. Second, to determine the relevant ads for a given page, we propose a linear similarity fusion mechanism, which combines the above three feature vectors in a unified manner. Last, we validate our approach using a set of real ads, real pages along with the external Wikipedia thesaurus. The experimental results show that our approach outperforms the conventional contextual advertising matching approaches and can substantially improve the performance of ad selection.",2015,Knowledge and Information Systems volume 43 issue 3 pp 599-631,brand;contextual advertising;world wide web;information retrieval;data mining;database;computer science;
Online data-driven anomaly detection in autonomous robots,Eliahu Khalastchi (Ben-Gurion University of the Negev);Meir Kalech (Ben-Gurion University of the Negev);Gal A. Kaminka (Bar-Ilan University);Raz Lin (Bar-Ilan University);,"2044898680,11088104,595631857,2150470271","The use of autonomous robots is appealing for tasks, which are dangerous to humans. Autonomous robots might fail to perform their tasks since they are susceptible to varied sorts of faults such as point and contextual faults. Not all faults can be known in advance, and hence, anomaly detection is required. In this paper, we present an online data-driven anomaly detection approach (ODDAD) for autonomous robots. ODDAD is suitable for the dynamic nature of autonomous robots since it declares a fault based only on data collected online. In addition, it is unsupervised, model free and domain independent. ODDAD proceeds in three steps: data filtering, attributes grouping based on dependency between attributes and outliers detection for each group. Above a calculated threshold, an anomaly is declared. We empirically evaluate ODDAD in different domains: commercial unmanned aerial vehicles (UAVs), a vacuum-cleaning robot, a high-fidelity flight simulator and an electrical power system of a spacecraft. We show the significance and impact of each component of ODDAD . By comparing ODDAD to other state-of-the-art competing anomaly detection algorithms, we show its advantages.",2015,Knowledge and Information Systems volume 43 issue 3 pp 657-688,autonomous agent;fault detection and isolation;anomaly detection;robotics;embedded system;computer vision;artificial intelligence;simulation;computer science;
Stabilized sparse ordinal regression for medical risk stratification,Truyen Tran 0001 (Deakin University);Dinh Q. Phung (Deakin University);Wei Luo (Deakin University);Svetha Venkatesh (Deakin University);,"2157392948,2314522249,2601343041,2146461601","The recent wide adoption of electronic medical records (EMRs) presents great opportunities and challenges for data mining. The EMR data are largely temporal, often noisy, irregular and high dimensional. This paper constructs a novel ordinal regression framework for predicting medical risk stratification from EMR. First, a conceptual view of EMR as a temporal image is constructed to extract a diverse set of features. Second, ordinal modeling is applied for predicting cumulative or progressive risk. The challenges are building a transparent predictive model that works with a large number of weakly predictive features, and at the same time, is stable against resampling variations. Our solution employs sparsity methods that are stabilized through domain-specific feature interaction networks. We introduces two indices that measure the model stability against data resampling. Feature networks are used to generate two multivariate Gaussian priors with sparse precision matrices (the Laplacian and Random Walk). We apply the framework on a large short-term suicide risk prediction problem and demonstrate that our methods outperform clinicians to a large margin, discover suicide risk factors that conform with mental health knowledge, and produce models with enhanced stability.",2015,Knowledge and Information Systems volume 43 issue 3 pp 555-582,ordinal regression;ordinal data;stability;econometrics;data mining;machine learning;statistics;computer science;
PGLCM: efficient parallel mining of closed frequent gradual itemsets,Trong Dinh Thac Do (Centre national de la recherche scientifique);Alexandre Termier (Centre national de la recherche scientifique);Anne Laurent (Centre national de la recherche scientifique);Benjamin Negrevergne (Katholieke Universiteit Leuven);Behrooz Omidvar-Tehrani (Centre national de la recherche scientifique);Sihem Amer-Yahia (Centre national de la recherche scientifique);,"2166862765,99113242,2168257648,2309213344,299395660,19633248","Numerical data (e.g., DNA micro-array data, sensor data) pose a challenging problem to existing frequent pattern mining methods which hardly handle them. In this framework, gradual patterns have been recently proposed to extract covariations of attributes, such as: ""When X increases, Y decreases"". There exist some algorithms for mining frequent gradual patterns, but they cannot scale to real-world databases. We present in this paper GLCM, the first algorithm for mining closed frequent gradual patterns, which proposes strong complexity guarantees: the mining time is linear with the number of closed frequent gradual itemsets. Our experimental study shows that GLCM is two orders of magnitude faster than the state of the art, with a constant low memory usage. We also present PGLCM, a parallelization of GLCM capable of exploiting multicore processors, with good scale-up properties on complex datasets. These algorithms are the first algorithms capable of mining large real world datasets to discover gradual patterns.",2015,Knowledge and Information Systems volume 43 issue 3 pp 497-527,data science;data mining;database;computer science;
A modified adaptive differential evolution algorithm for color image segmentation,Ahmad Khan (National University of Computer and Emerging Sciences);M. Arfan Jaffar (Islamic University);Ling Shao (University of Sheffield);,"2566111234,2420227975,2636545837","Image segmentation is an important low-level vision task. It is a perceptual grouping of pixels based on some similarity criteria. In this paper, a new differential evolution (DE) algorithm, modified adaptive differential evolution, is proposed for color image segmentation. The DE/current-to-pbest mutation strategy with optional external archive and opposition-based learning are used to diversify the search space and expedite the convergence process. Control parameters are automatically updated to appropriate values in order to avoid user intervention of parameters setting. To find an optimal number of clusters (the number of regions or segments), the average ratio of fuzzy overlap and fuzzy separation is used as a cluster validity index. The results demonstrate that the proposed technique outperforms state-of-the-art methods.",2015,Knowledge and Information Systems volume 43 issue 3 pp 583-597,scale space segmentation;segmentation based object categorization;crossover;segmentation;image segmentation;mutation;computer vision;data mining;artificial intelligence;machine learning;mathematics;
Scaling cut criterion-based discriminant analysis for supervised dimension reduction,Xiangrong Zhang (Xidian University);Yudi He (Xidian University);Licheng Jiao (Xidian University);Ruochen Liu (Xidian University);Jie Feng (Xidian University);Sisi Zhou (Xidian University);,"2130877461,2635457222,2166558591,2137004161,2617083724,2483302463","Dimension reduction has always been a major problem in many applications of machine learning and pattern recognition. In this paper, the scaling cut criterion-based supervised dimension reduction methods for data analysis are proposed. The scaling cut criterion can eliminate the limit of the hypothesis that data distribution of each class is homoscedastic Gaussian. To obtain a more reasonable mapping matrix and reduce the computational complexity, local scaling cut criterion-based dimension reduction is raised, which utilized the localization strategy of the input data. The localized $$k$$k-nearest neighbor graph is introduced , which relaxes the within-class variance and enlarges the between-class margin. Moreover, by kernelizing the scaling cut criterion and local scaling cut criterion, both methods are extended to efficiently model the nonlinear variability of the data. Furthermore, the optimal dimension scaling cut criterion is proposed, which can automatically select the optimal dimension for the dimension reduction methods. The approaches have been tested on several datasets, and the results have shown a better and efficient performance compared with other linear and nonlinear dimension reduction techniques.",2015,Knowledge and Information Systems volume 43 issue 3 pp 633-655,sufficient dimension reduction;maximum cut;dimensionality reduction;discrete mathematics;combinatorics;machine learning;mathematical optimization;computer science;mathematics;
TDEP: efficiently processing top-k dominating query on massive data,Xixian Han (Harbin Institute of Technology);Jianzhong Li (Harbin Institute of Technology);Hong Gao (Harbin Institute of Technology);,"2113522248,2130201582,2672422064","In many applications including multicriteria decision making, top-k dominating query is a practically useful tool to return k tuples with the highest domination scores in a potentially huge data space. The existing algorithms, either requiring indexes built on the specific attribute subset, or incurring high I/O cost or memory cost, cannot process top-k dominating query on massive data efficiently. In this paper, a novel algorithm TDEP is proposed to utilize sorted lists built for each attribute with low cost to return top-k dominating result on massive data efficiently. Through analysis, it is found that TDEP can be divided into two phases: growing phase and shrinking phase. In each phase, TDEP retrieves the sorted lists in round-robin fashion and maintains the candidates until the stop condition is satisfied. The theoretical analysis is provided for the execution behavior in two phases. An efficient method is developed to compute the domination scores of tuples with the obtained candidates only. Besides, TDEP adopts early pruning to reduce the number of candidate tuples maintained significantly. The extensive experimental results, conducted on synthetic and real-life data sets, show the significant performance advantage of TDEP over the existing algorithms.",2015,Knowledge and Information Systems volume 43 issue 3 pp 689-718,data mining;database;algorithm;mathematics;
SFP-Rank: significant frequent pattern analysis for effective ranking,Yuanfeng Song (Hong Kong University of Science and Technology);Wilfred Ng (Hong Kong University of Science and Technology);Kenneth Wai-Ting Leung (Hong Kong University of Science and Technology);Qiong Fang (Hong Kong University of Science and Technology);,"2128100056,2170178419,1975946032,2229985464","Ranking documents in terms of their relevance to a given query is fundamental to many real-life applications such as information retrieval and recommendation systems. Extensive study in these application domains has given rise to the development of many efficient ranking models. While most existing research focuses on developing learning to rank (LTR) models, the quality of the training features, which plays an important role in ranking performance, has not been fully studied. Thus, we propose a new approach that discovers effective features for the LTR problem. In this paper, we present a theoretical analysis on which frequent patterns are potentially effective for improving the performance of LTR and then propose an efficient method that selects frequent patterns for LTR. First, we define a new criterion, namely feature significance (or simply significance). Specifically, we use each feature's value to rank the training instances and define the ranking effectiveness in terms of a performance measure as the significance of the feature. We show that the significance of an infrequent pattern is limited by using formal connection between pattern support and its significance. Then, we propose a methodology that sets the support value when performing frequent pattern mining. Finally, since frequent patterns are not equally effective for LTR, we further provide a coverage-based significant pattern generation algorithm to discover effective patterns and propose a new ranking approach called Significant Frequent Pattern-based Ranking (SFP-Rank), in which the ranking model is built upon the original features as well as the significant frequent patterns. Our experiments confirm that, by incorporating significant frequent patterns to train the ranking model, the performance of the ranking model can be substantially improved.",2015,Knowledge and Information Systems volume 43 issue 3 pp 529-553,ranking;ranking svm;learning to rank;feature selection;data mining;pattern recognition;machine learning;computer science;mathematics;
A new ant colony optimization-based algorithm for range query answering problem in relational schema-based P2P database systems,Saeedeh Aghamahmoodi (Shahid Beheshti University);Seyed Mohammad Taghi Rouhani Rankoohi (Shahid Beheshti University);Fatemeh Aghamahmoodi;,"14412221,349883525,2304077298","Nowadays, peer-to-peer database systems (P2PDBSs) aiming at data sharing in the Web have become very popular. Due to the absence of global knowledge about data placement in unstructured P2P networks, query processing and answering is a challenging problem in such systems. This process is provided by query routing and is an optimization problem whose goal is to find the maximum results with spending a predetermined cost. With the aim of improving the efficiency of range query answering algorithm in relational schema-based P2PDBSs, the present study, for the first time, adapts the ant colony metaphor with range query answering problem in relational schema-based P2P systems and proposes a new algorithm using the ant colony optimization approach in which the researchers use histogram data structure and apply both positive and negative feedbacks, dynamic learning rate, and local heuristic mechanisms and show that the proposed algorithm gives better results than the comparative greedy-based method. The experimental tests indicate that in the best case, the average number of traveled links for finding one answer (i.e., cost-to-answers ratio) is decreased almost by half in contrast to that of greedy-based algorithm. Furthermore, the achieved results indicate that the proposed algorithm is completely flexible with the users' requests, i.e., more query answers or less query response time, and the algorithm parameters can be properly set to meet the users' requests.",2015,Knowledge and Information Systems volume 43 issue 3 pp 719-749,sargable;boolean conjunctive query;online aggregation;web search query;web query classification;view;query by example;query expansion;query optimization;greedy algorithm;data mining;database;machine learning;computer science;
Fast and scalable support vector clustering for large-scale data analysis,Yuan Ping (Xuchang University);Yun Feng Chang (China Three Gorges University);Yajian Zhou (Beijing University of Posts and Telecommunications);Ying Jie Tian (Chinese Academy of Sciences);Yi Xian Yang (Beijing University of Posts and Telecommunications);Zhili Zhang (Xuchang University);,"2318295239,2502036265,2641257484,2120260257,2240748040,2631615455","As an important boundary-based clustering algorithm, support vector clustering (SVC) benefits multiple applications for its capability of handling arbitrary cluster shapes. However, its popularity is degraded by both its highly intensive pricey computation and poor label performance which are due to redundant kernel function matrix required by estimating a support function and ineffectively checking segmers between all pairs of data points, respectively. To address these two problems, a fast and scalable SVC (FSSVC) method is proposed in this paper to achieve significant improvement on efficiency while guarantees a comparable accuracy with the state-of-the-art methods. The heart of our approach includes (1) constructing the hypersphere and support function by cluster boundaries which prunes unnecessary computation and storage of kernel functions and (2) presenting an adaptive labeling strategy which decomposes clusters into convex hulls and then employs a convex-decomposition-based cluster labeling algorithm or cone cluster labeling algorithm on the basis of whether the radius of the hypersphere is greater than 1. Both theoretical analysis and experimental results (e.g., the first rank of a nonparametric statistical test) show the superiority of our method over the others, especially for large-scale data analysis under limited memory requirements.",2015,Knowledge and Information Systems volume 43 issue 2 pp 281-310,k medians clustering;cluster analysis;data mining;machine learning;mathematical optimization;mathematics;
CloudRec: a framework for personalized service Recommendation in the Cloud,Qi Yu (Rochester Institute of Technology);,2674050924,"The elastic computing power and the pay-as-you-go model of the cloud offer an attractive platform to deploy software as a service applications. The large number of applications expected to heavily take advantage of the cloud will result in an explosive growth of various cloud services. As many cloud services may compete to offer similar functionalities, it is desirable to consider user preferences on the nonfunctional service properties (aka, quality of service, or QoS) when delivering cloud services to the end users. Unfortunately, current approaches primarily rely on the descriptions from the cloud service providers or expert-provided rankings, which are completely orthogonal to the open and distributed nature of the cloud. We present a novel framework (referred to as CloudRec) that exploits a user-centric strategy to achieve personalized QoS assessment of cloud services. CloudRec integrates a novel community-based QoS assessment model with an iterative algorithm to accurately discover a set of homogenous user and service communities from scarce and large-scale QoS data. The communities can serve as a bridge to relate users and services and hence provide an effective means to estimate the QoS of unknown cloud services. The effectiveness of the proposed framework is demonstrated through a rigorous theoretical analysis and an extensive empirical study on real QoS data.",2015,Knowledge and Information Systems volume 43 issue 2 pp 417-443,mobile qos;cloud testing;cloud computing security;cloud computing;quality of service;matrix decomposition;internet privacy;world wide web;data mining;database;simulation;computer science;
Learning to suggest questions in social media,Tom Chao Zhou (Baidu);Michael Rung-Tsong Lyu (The Chinese University of Hong Kong);Irwin King (The Chinese University of Hong Kong);Jie Lou (City University of Hong Kong);,"2130218924,2227744130,2121363826,2646910450","Social media systems with Q&A functionalities have accumulated large archives of questions and answers. Two representative types are online forums and community-based Q&A services. To enable users to explore the large number of questions and answers in social media systems effectively, it is essential to suggest interesting items to an active user. In this article, we address the problem of question suggestion, which targets at suggesting questions that are semantically related to a queried question. Existing bag-of-words approaches suffer from the shortcoming that they could not bridge the lexical chasm between semantically related questions. Therefore, we present a new framework, and propose the topic-enhanced translation-based language model (TopicTRLM), which fuses both the lexical and latent semantic knowledge. This fusing enables TopicTRLM to find semantically related questions to a given question even when there is little word overlap. Moreover, to incorporate the answer information into the model to make the model more complete, we also propose the topic-enhanced translation-based language model with answer ensemble. Extensive experiments have been conducted with real-world datasets. Experimental results indicate our approach is very effective and outperforms other popular methods in several metrics.",2015,Knowledge and Information Systems volume 43 issue 2 pp 389-416,topic model;social media;language model;natural language processing;world wide web;data mining;database;artificial intelligence;machine learning;computer science;
Efficient clustering-based source code plagiarism detection using PIY,Tony Ohmann (University of Massachusetts Amherst);Imad Rahal (Science College);,"2128963765,2708307525","Vast amounts of information available online make plagiarism increasingly easy to commit, and this is particularly true of source code. The traditional approach of detecting copied work in a course setting is manual inspection. This is not only tedious but also typically misses code plagiarized from outside sources or even from an earlier offering of the course. Systems to automatically detect source code plagiarism exist but tend to focus on small submission sets. One such system that has become the standard in automated source code plagiarism detection is measure of software similarity (MOSS) Schleimer et al. in proceedings of the 2003 ACM SIGMOD international conference on management of data, ACM, San Diego, 2003. In this work, we present an approach called program it yourself (PIY) which is empirically shown to outperform MOSS in detection accuracy. By utilizing parallel processing and data clustering, PIY is also capable of maintaining detection accuracy and reasonable runtimes even when using extremely large data repositories.",2015,Knowledge and Information Systems volume 43 issue 2 pp 445-472,non uniform memory access;cluster analysis;world wide web;data mining;database;artificial intelligence;machine learning;computer science;
Answering ordered tree pattern queries over fuzzy XML data,Jian Liu (Northeastern University);Z. M. Ma (Northeastern University);Xue Feng (Shenyang);,"2655723344,2197426428,2696908085","The widespread employment of XML requires the development of efficient methods to support advanced tree pattern queries. In addition, in real-world applications that involve analysis and management of databases, uncertainty is often an inherent property of data. Although a large number of researches have been dedicated to modeling uncertain XML and answering queries over uncertain XML data, how to conduct order-based tree pattern queries on fuzzy XML data remains an open problem. In this work, we aim to fill this gap and tackle the problem of order-based queries on fuzzy XML data. Based on the node encoding, we propose efficient tree pattern matching algorithms serving for ordered queries over fuzzy XML data. In particular, our algorithms adopt a two-phase processing technique: the first phase generates the root-to-leaf path solutions, and the second phase merges these path solutions for producing the desired matches to the ordered tree pattern. Typically, our approach only needs to access data related to the query nodes in the tree pattern once to obtain all the matching answers. Finally, a comprehensive experimental evaluation is carried out to investigate the performance of the proposed approaches.",2015,Knowledge and Information Systems volume 43 issue 2 pp 473-495,xml validation;xml database;xml schema;matching;theoretical computer science;data mining;database;computer science;
STenSr: Spatio-temporal tensor streams for anomaly detection and pattern discovery,"Lei Shi (University of Maryland, Baltimore County);Aryya Gangopadhyay (University of Maryland, Baltimore County);Vandana P. Janeja (University of Maryland, Baltimore County);","2620014270,2132136220,2290556930","The focus of this paper is anomaly detection and pattern discovery in spatio-temporal tensor streams. As an example, sensor networks comprising of multiple individual sensor streams generate spatio-temporal data, which can be captured in tensor streams. Anomaly detection in such data is considered challenging because of the potential complexity and high order of the tensor data from spatio-temporal sources such as sensor networks. In this paper, we propose an innovative approach for anomaly detection and pattern discovery in such tensor streams. We model the tensor stream itself as a single incremental tensor, for example representing the entire sensor network, instead of dealing with each individual tensor in the stream separately. Such a model provides a global view of the tensor stream and enables subsequent in-depth analysis of it. The proposed approach is designed for online analysis of tensor streams with fast runtime. We evaluate our approach for detecting anomalies under different conditions and for identifying complex data patterns. We also compare the proposed approach with the existing tensor stream analysis method (Sun et al. in ACM Trans Knowl Discov Data 2, 2008). Our evaluation uses synthetic data as well as real-world data showing the efficiency and effectiveness of the proposed approach.",2015,Knowledge and Information Systems volume 43 issue 2 pp 333-353,tensor product network;tensor;anomaly detection;data science;data mining;machine learning;computer science;
A model for revocation forecasting in public-key infrastructures,Carlos Gañán (Polytechnic University of Catalonia);Jorge Mata-Díaz (Polytechnic University of Catalonia);Jose L. Muñoz (Polytechnic University of Catalonia);Oscar Esparza (Polytechnic University of Catalonia);Juanjo Alins (Polytechnic University of Catalonia);,"2130442749,319472423,2154359424,2124060489,2469688473","One of the hardest tasks of a certification infrastructure is to manage revocation. This process consists in collecting and making the revocation status of certificates available to users. Research on this topic has focused on the trade-offs that different revocation mechanisms offer. Much less effort has been conducted to understand and model real-world revocation processes. For this reason, in this paper, we present a novel analysis of real-world collected revocation data and we propose a revocation prediction model. The model uses an autoregressive integrated moving average model. Our prediction model enables certification authorities to forecast the number of revoked certificates in short term.",2015,Knowledge and Information Systems volume 43 issue 2 pp 311-331,revocation list;x 509;internet privacy;computer security;data mining;computer science;
"Finding top-$$k\, r$$kr-cliques for keyword search from graphs in polynomial delay",Mehdi Kargar (York University);Aijun An (York University);,"2126665663,2156668854","Keyword search over structured data offers an alternative method to explore and query databases for users that are not familiar with the structure of the data and/or a query language. Structured data are usually modeled as graphs. In this context, an answer is a substructure of the graph that contains all or some of the query keywords. In most of the previous works, a minimal tree that covers all the query keywords are found as the answer. Some recent works offer to find subgraphs rather than minimal trees and show that subgraphs might be more informative for the users. However, current methods suffer from the following problems. Although some of the content nodes (i.e., nodes that contain input keywords) are close to each other in an answer, others might be far from each other. While searching for the best answer, current methods explore the whole graph rather than only the content nodes. This might increase the run time and leads to poor performance. To address these problems, we propose to find top- $$k\, r$$ k r -cliques as the answers to the graph keyword search problem. An $$r$$ r -clique is a set of content nodes that cover all the input keywords, and the distance between each pair of nodes is less than or equal to $$r$$ r . We propose a new weight function that is the sum of distances between each pair of content nodes. We prove that minimizing the new weight function is NP-hard and propose an approximation algorithm that produces $$r$$ r -cliques with 2-approximation ratio in polynomial delay. We further improve the run time of the approximation algorithm with the cost of increasing the approximation ratio. Extensive performance studies using three large real datasets confirm the efficiency and accuracy of finding $$r$$ r -cliques in graphs.",2015,Knowledge and Information Systems volume 43 issue 2 pp 249-280,approximation algorithm;theoretical computer science;combinatorics;data mining;machine learning;computer science;mathematics;
SEPT: an efficient skyline join algorithm on massive data,Xixian Han (Harbin Institute of Technology);Jianzhong Li (Harbin Institute of Technology);Hong Gao (Harbin Institute of Technology);Chengyu Yang (Harbin Institute of Technology);,"2113522248,2130201582,2672422064,2305824611","Skyline join is an important operation in many applications to return all join tuples that are not dominated by any other join tuples. It is found that the existing algorithms cannot process skyline join on massive data efficiently. This paper presents a novel skyline join algorithm SEPT on massive data. SEPT utilizes sorted positional index lists with join information which require low space overhead to reduce I/O cost significantly. The sorted positional index list is constructed for each potential skyline attribute in the joined tables and is arranged in ascending order of the attribute. SEPT consists of two phases. In phase one, SEPT obtains candidate join positional index pairs of skyline join results. During retrieving the sorted positional index lists, SEPT performs pruning on candidate join positional index pairs in order to discard the candidates whose corresponding join tuples are not skyline join results. In phase two, SEPT exploits the obtained candidate join positional index pairs to get skyline join results by a selective and sequential scan on the tables. The experimental results on synthetic and real data sets show that SEPT has a significant advantage over the existing skyline join algorithms.",2015,Knowledge and Information Systems volume 43 issue 2 pp 355-388,block nested loop;recursive join;sort merge join;hash join;pruning;information retrieval;data mining;database;computer science;
Feature-selection-based dynamic transfer ensemble model for customer churn prediction,Jin Xiao (Sichuan University);Yi Xiao (Central China Normal University);Anqiang Huang (Beihang University);Dunhu Liu (Chengdu University of Information Technology);Shouyang Wang (Chinese Academy of Sciences);,"2227981784,2598446708,2145679603,2627460262,2132452411","Customer churn prediction is one of the key steps to maximize the value of customers for an enterprise. It is difficult to get satisfactory prediction effect by traditional models constructed on the assumption that the training and test data are subject to the same distribution, because the customers usually come from different districts and may be subject to different distributions in reality. This study proposes a feature-selection-based dynamic transfer ensemble (FSDTE) model that aims to introduce transfer learning theory for utilizing the customer data in both the target and related source domains. The model mainly conducts a two-layer feature selection. In the first layer, an initial feature subset is selected by GMDH-type neural network only in the target domain. In the second layer, several appropriate patterns from the source domain to target training set are selected, and some features with higher mutual information between them and the class variable are combined with the initial subset to construct a new feature subset. The selection in the second layer is repeated several times to generate a series of new feature subsets, and then, we train a base classifier in each one. Finally, a best base classifier is selected dynamically for each test pattern. The experimental results in two customer churn prediction datasets show that FSDTE can achieve better performance compared with the traditional churn prediction strategies, as well as three existing transfer learning strategies.",2015,Knowledge and Information Systems volume 43 issue 1 pp 29-51,transfer of learning;feature selection;data mining;pattern recognition;machine learning;computer science;
An effective association rule mining scheme using a new generic basis,Jayakrushna Sahoo (Indian Institute of Technology Kharagpur);Ashok Kumar Das (International Institute of Information Technology);Adrijit Goswami (Indian Institute of Technology Kharagpur);,"2223394698,2412059422,2167147347","Association rule mining among itemsets is a fundamental task and is of great importance in many data mining applications including attacks in network data, stock market, financial applications, bioinformatics to find genetic disorders, etc. However, association rule extraction from a reasonable-sized database produces a large number of rules. As a result, many of them are redundant to other rules, and they are practically useless. To overcome this issue, methods for mining non-redundant rules are essentially required. To address such problem, we initially propose a definition for redundancy in sense of minimal knowledge and then a compact representation of non-redundant association rules which we call as compact informative generic basis. We also provide an improved version of the existing DCI_CLOSED algorithm (DCI_PLUS) to find out the frequent closed itemsets (FCI) with their minimal representative generators in combination with BitTable which represents a compact database form in a single scan of the original database. We further introduce an algorithm for constructing the compact informative generic basis from the FCI and their generators in an efficient way. We finally present an inference mechanism in which all association rules can be generated without accessing the database. Experiments are performed on the proposed method. The experimental results show that the proposed method outperforms the other existing related methods.",2015,Knowledge and Information Systems volume 43 issue 1 pp 127-156,association rule learning;data mining;database;algorithm;computer science;
Semi-supervised classification based on subspace sparse representation,Guoxian Yu (Southwest University);Guoji Zhang (South China University of Technology);Zili Zhang (Deakin University);Zhiwen Yu (South China University of Technology);Lin Deng (George Mason University);,"2126069887,2144418109,2667095921,2163362956,2163050207","Graph plays an important role in graph-based semi-supervised classification. However, due to noisy and redundant features in high-dimensional data, it is not a trivial job to construct a well-structured graph on high-dimensional samples. In this paper, we take advantage of sparse representation in random subspaces for graph construction and propose a method called Semi-Supervised Classification based on Subspace Sparse Representation, SSC-SSR in short. SSC-SSR first generates several random subspaces from the original space and then seeks sparse representation coefficients in these subspaces. Next, it trains semi-supervised linear classifiers on graphs that are constructed by these coefficients. Finally, it combines these classifiers into an ensemble classifier by minimizing a linear regression problem. Unlike traditional graph-based semi-supervised classification methods, the graphs of SSC-SSR are data-driven instead of man-made in advance. Empirical study on face images classification tasks demonstrates that SSC-SSR not only has superior recognition performance with respect to competitive methods, but also has wide ranges of effective input parameters.",2015,Knowledge and Information Systems volume 43 issue 1 pp 81-101,sparse approximation;clustering high dimensional data;theoretical computer science;pattern recognition;machine learning;computer science;mathematics;
Transfer learning for Bayesian discovery of multiple Bayesian networks,Diane Oyen (Los Alamos National Laboratory);Terran Lane (Google);,"2114965150,2695203328","Bayesian network structure learning algorithms with limited data are being used in domains such as systems biology and neuroscience to gain insight into the underlying processes that produce observed data. Learning reliable networks from limited data is difficult; therefore, transfer learning can improve the robustness of learned networks by leveraging data from related tasks. Existing transfer learning algorithms for Bayesian network structure learning give a single maximum a posteriori estimate of network models. Yet, many other models may be equally likely, and so a more informative result is provided by Bayesian structure discovery. Bayesian structure discovery algorithms estimate posterior probabilities of structural features, such as edges. We present transfer learning for Bayesian structure discovery which allows us to explore the shared and unique structural features among related tasks. Efficient computation requires that our transfer learning objective factors into local calculations, which we prove is given by a broad class of transfer biases. Theoretically, we show the efficiency of our approach. Empirically, we show that compared to single-task learning, transfer learning is better able to positively identify true edges. We apply the method to whole-brain neuroimaging data.",2015,Knowledge and Information Systems volume 43 issue 1 pp 1-28,variable order bayesian network;online machine learning;stability;wake sleep algorithm;inductive transfer;multi task learning;transfer of learning;bayesian statistics;generalization error;competitive learning;bayesian network;active learning;algorithmic learning theory;semi supervised learning;computational learning theory;instance based learning;intelligent control;unsupervised learning;data mining;pattern recognition;machine learning;statistics;computer science;
Graph-based local concept coordinate factorization,Ping Li (Zhejiang University);Jiajun Bu (Zhejiang University);Lijun Zhang (Michigan State University);Chun Chen (Zhejiang University);,"2628146486,2169876372,2276088550,2151411149","Ubiquitous data are increasingly expanding in large volumes due to human activities, and grouping them into appropriate clusters is an important and yet challenging problem. Existing matrix factorization techniques have shown their significant power in solving this problem, e.g., nonnegative matrix factorization, concept factorization. Recently, one state-of-the-art method called locality-constrained concept factorization is put forward, but its locality constraint does not well reveal the intrinsic data structure since it only requires the concept to be as close to the original data points as possible. To address this issue, we present a graph-based local concept coordinate factorization (GLCF) method, which respects the intrinsic structure of the data through manifold kernel learning in the warped Reproducing Kernel Hilbert Space. Besides, a generalized update algorithm is developed to handle data matrices containing both positive and negative entries. Since GLCF is essentially based on the local coordinate coding and concept factorization, it inherits many advantageous properties, such as the locality and sparsity of the data representation. Moreover, it can better encode the locally geometrical structure via graph Laplacian in the manifold adaptive kernel. Therefore, a more compact and better structured representation can be obtained in the low-dimensional data space. Extensive experiments on several image and gene expression databases suggest the superiority of the proposed method in comparison with some alternatives.",2015,Knowledge and Information Systems volume 43 issue 1 pp 103-126,dixon s factorization method;factor graph;laplacian matrix;cluster analysis;discrete mathematics;combinatorics;machine learning;mathematical optimization;computer science;mathematics;
An ant-based approach to cluster peers in P2P database systems,Mohammad Ebrahimi (Shahid Beheshti University);Seyed Mohammad Taghi Rouhani Rankoohi (Shahid Beheshti University);,"2493025324,349883525","Peer-to-Peer (P2P) systems with distinct advantages such as scalability, robustness and low cost have developed quickly in recent years. The resource search problem, that is to find out a peer having a desired resource, is a fundamental problem to be initially solved in most P2P applications. P2P systems can also be categorized under database systems. One of the most important problems of P2P database systems is the query processing algorithms. Even a rather small improvement over these algorithms can have a significant impact on the performance of a P2P database system. One of the most effective methods to reduce query processing cost is peer clustering. Hence, clustering peers with similar contents makes the queries directed to the appropriate cluster and thus to be efficiently answered. In this paper, we present a new peer clustering algorithm for P2P database systems that uses ant-based clustering algorithms. We have adjusted ant-based clustering algorithm to peer clustering problem. Performance evaluation of the algorithm shows its strength in clustering peers in P2P database systems to be of a higher quality and accuracy compared to the existing systems, to have a relatively short running time and to quickly converge to an optimum. Moreover, our algorithm achieves a high scalability to the increasing problem size.",2015,Knowledge and Information Systems volume 43 issue 1 pp 219-247,brown clustering;canopy clustering algorithm;correlation clustering;constrained clustering;data stream clustering;cure data clustering algorithm;fuzzy clustering;clustering high dimensional data;cluster analysis;world wide web;data mining;database;machine learning;computer science;
Indexing volumetric shapes with matching and packing,David Ryan Koes (University of Pittsburgh);Carlos J. Camacho (University of Pittsburgh);,"2132378014,2111556710","We describe a novel algorithm for bulk-loading an index with high-dimensional data and apply it to the problem of volumetric shape matching. Our matching and packing algorithm is a general approach for packing data according to a similarity metric. First, an approximate $$k$$ k -nearest neighbor graph is constructed using vantage-point initialization, an improvement to previous work that decreases construction time while improving the quality of approximation. Then, graph matching is iteratively performed to pack related items closely together. The end result is a dense index with good performance. We define a new query specification for shape matching that uses minimum and maximum shape constraints to explicitly specify the spatial requirements of the desired shape. This specification provides a natural language for performing volumetric shape matching and is readily supported by the geometry-based similarity search (GSS) tree, an indexing structure that maintains explicit representations of volumetric shape. We describe our implementation of a GSS tree for volumetric shape matching and provide a comprehensive evaluation of parameter sensitivity, performance, and scalability. Compared to previous bulk-loading algorithms, we find that matching and packing can construct a GSS tree index in the same amount of time that is denser, flatter, and better performing, with an observed average performance improvement of 2X.",2015,Knowledge and Information Systems volume 43 issue 1 pp 157-180,3 dimensional matching;shape analysis;combinatorics;machine learning;mathematics;
Analysis and evaluation of the top-$$k$$k most influential location selection query,Jian Chen (South China University of Technology);Jin Huang (University of Melbourne);Zeyi Wen (University of Melbourne);Zhen He (La Trobe University);Kerry Taylor (Commonwealth Scientific and Industrial Research Organisation);Rui Zhang (University of Melbourne);,"2709873104,2192706955,2154047257,2143353724,2106098872,2690388134","In this paper, we propose a new type of queries to retrieve the top-k most influential locations from a candidate set $$C$$ C given sets of customers $$M$$ M and existing facilities $$F$$ F . The influence models the popularity of a facility. Such queries have wide applications in decision support systems. A naive solution sequentially scans (SS) all data sets, which is expensive, and hence, we investigate two branch-and-bound algorithms for the query, namely Estimate Expanding Pruning (EEP) and Bounding Influence Pruning (BIP). Both algorithms follow the best first traverse. On determining the traversal order, while EEP leverages distance metrics between nodes, BIP relies on half plane pruning which avoids the repetitive estimations in EEP. As our experiments shown, BIP is much faster than SS which outperforms EEP, while the worst-case complexity of EEP and BIP is worse than that of SS. To improve the efficiency, we further propose a Nearest Facility Circle Join (NFCJ) algorithm. NFCJ builds an influence R-tree on the influence relationship between customers and existing facilities and joins the candidate R-tree with the influence R-tree to obtain the results. We compare all algorithms and conclude that NFCJ is the best solution, which outperforms SS, EEP, and BIP by orders of magnitude.",2015,Knowledge and Information Systems volume 43 issue 1 pp 181-217,r tree;efficiency;data mining;database;artificial intelligence;machine learning;computer science;
Context-aware MIML instance annotation: exploiting label correlations with classifier chains,Forrest Briggs (Oregon State University);Xiaoli Z. Fern (Oregon State University);Raviv Raich (Oregon State University);,"2165737930,2018589324,2150540218","In multi-instance multi-label (MIML) instance annotation, the goal is to learn an instance classifier while training on a MIML dataset, which consists of bags of instances paired with label sets; instance labels are not provided in the training data. The MIML formulation can be applied in many domains. For example, in an image domain, bags are images, instances are feature vectors representing segments in the images, and the label sets are lists of objects or categories present in each image. Although many MIML algorithms have been developed for predicting the label set of a new bag, only a few have been specifically designed to predict instance labels. We propose MIML-ECC (ensemble of classifier chains), which exploits bag-level context through label correlations to improve instance-level prediction accuracy. The proposed method is scalable in all dimensions of a problem (bags, instances, classes, and feature dimension) and has no parameters that require tuning (which is a problem for prior methods). In experiments on two image datasets, a bioacoustics dataset, and two artificial datasets, MIML-ECC achieves higher or comparable accuracy in comparison with several recent methods and baselines.",2015,Knowledge and Information Systems volume 43 issue 1 pp 53-79,data mining;pattern recognition;machine learning;computer science;
Joint Schatten $$p$$p-norm and $$\ell _p$$ℓp-norm robust matrix completion for missing value recovery,Feiping Nie (University of Texas at Arlington);Hua Wang (Colorado School of Mines);Heng Huang (University of Texas at Arlington);Chris H. Q. Ding (University of Texas at Arlington);,"2245267964,2581948414,2137533801,2119616764","The low-rank matrix completion problem is a fundamental machine learning and data mining problem with many important applications. The standard low-rank matrix completion methods relax the rank minimization problem by the trace norm minimization. However, this relaxation may make the solution seriously deviate from the original solution. Meanwhile, most completion methods minimize the squared prediction errors on the observed entries, which is sensitive to outliers. In this paper, we propose a new robust matrix completion method to address these two problems. The joint Schatten $$p$$ p -norm and $$\ell _p$$ l p -norm are used to better approximate the rank minimization problem and enhance the robustness to outliers. The extensive experiments are performed on both synthetic data and real-world applications in collaborative filtering prediction and social network link recovery. All empirical results show that our new method outperforms the standard matrix completion methods.",2015,Knowledge and Information Systems volume 42 issue 3 pp 525-544,schatten norm;social network;recommender system;discrete mathematics;machine learning;mathematical optimization;computer science;mathematics;
Shared-memory and Shared-nothing Stochastic Gradient Descent Algorithms for Matrix Completion,Faraz Makari (Max Planck Society);Christina Teflioudi (Max Planck Society);Rainer Gemulla (Max Planck Society);Peter J. Haas (IBM);Yannis Sismanis (IBM);,"2071564682,324398196,344318172,2118460571,212794494","We provide parallel algorithms for large-scale matrix completion on problems with millions of rows, millions of columns, and billions of revealed entries. We focus on in-memory algorithms that run either in a shared-memory environment on a powerful compute node or in a shared-nothing environment on a small cluster of commodity nodes; even very large problems can be handled effectively in these settings. Our ASGD, DSGD-MR, DSGD++, and CSGD algorithms are novel variants of the popular stochastic gradient descent (SGD) algorithm, with the latter three algorithms based on a new “stratified SGD” approach. All of the algorithms are cache-friendly and exploit thread-level parallelism, in-memory processing, and asynchronous communication. We investigate the performance of both new and existing algorithms via a theoretical complexity analysis and a set of large-scale experiments. The results show that CSGD is more scalable, and up to 60 % faster, than the best-performing alternative method in the shared-memory setting. DSGD++ is superior in terms of overall runtime, memory consumption, and scalability in the shared-nothing setting. For example, DSGD++ can solve a difficult matrix completion problem on a high-variance matrix with 10M rows, 1M columns, and 10B revealed entries in around 40 min on 16 compute nodes. In general, algorithms based on SGD appear to perform better than algorithms based on alternating minimizations, such as the PALS and DALS alternating least-squares algorithms.",2015,Knowledge and Information Systems volume 42 issue 3 pp 493-523,stochastic gradient descent;recommender system;theoretical computer science;machine learning;mathematical optimization;computer science;
ALPS: an efficient algorithm for top-k spatial preference search in road networks,Hyung-Ju Cho (Ajou University);Se Jin Kwon (Ajou University);Tae-Sun Chung (Ajou University);,"2461579394,2162837408,2128633747","In this paper, we study the processing of top-k spatial preference queries in road networks. A top-k spatial preference query retrieves a ranked list of the k best data objects based on the scores (e.g., qualities) of feature objects in their spatial neighborhoods. Several solutions have been proposed for top-k spatial preference queries in Euclidean space. However, far too little attention has been paid to top-k spatial preference queries in road networks, where the distance between two points is defined by the length of the shortest path connecting them. A simple way to answer top-k spatial preference queries is to examine the scores of feature objects in the proximity of each data object before returning a ranked list of the k best data objects. However, this simple method causes intolerable computation delays, thus rendering online processing inapplicable. Therefore, in this paper, we address this problem by presenting a new algorithm, called ALPS, for top-k spatial preference searches in road networks. Our experimental results demonstrate the superiority and effectiveness of ALPS for a wide range of problem settings.",2015,Knowledge and Information Systems volume 42 issue 3 pp 599-631,spatial query;spatial database;data mining;database;machine learning;computer science;mathematics;
Detecting software design defects using relational association rule mining,Gabriela Czibula;Zsuzsanna Marian;Istvan Gergely Czibula;,"148830721,2004025433,424501783","In this paper, we are approaching, from a machine learning perspective, the problem of automatically detecting defective software entities (classes and methods) in existing software systems, a problem of major importance during software maintenance and evolution. In order to improve the internal quality of a software system, identifying faulty entities such as classes, modules, methods is essential for software developers. As defective software entities are hard to identify, machine learning-based classification models are still developed to approach the problem of detecting software design defects. We are proposing a novel method based on relational association rule mining for detecting faulty entities in existing software systems. Relational association rules are a particular type of association rules and describe numerical orderings between attributes that commonly occur over a dataset. Our method is based on the discovery of relational association rules for identifying design defects in software. Experiments on open source software are conducted in order to detect defective classes in object-oriented software systems, and a comparison of our approach with similar existing approaches is provided. The obtained results show that our method is effective for software design defect detection and confirms the potential of our proposal.",2015,Knowledge and Information Systems volume 42 issue 3 pp 545-577,software analytics;software mining;software construction;software sizing;software visualization;software metric;software framework;software design;software development;association rule learning;data mining;database;machine learning;computer science;
A modified gravitational search algorithm based on sequential quadratic programming and chaotic map for ELD optimization,XiaoHong Han (Chinese Ministry of Education);Long Quan (Chinese Ministry of Education);Xiaoyan Xiong (Chinese Ministry of Education);,"2330964869,2130015787,2405380636","Gravitational search algorithm (GSA) is a stochastic search algorithm based on the law of gravity and mass interactions. For the purpose of enhancing the performance of standard GSA, this paper proposes a robust hybrid gravitational search algorithm (RHGSA). This algorithm makes the best of ergodicity of PieceWise Linear chaotic map to explore the global search while utilizing the sequential quadratic programming to accelerate the local search. To verify the performance of RHGSA, different types of benchmark functions including five unimodal functions and ten functions provided by CEC 2005 special session are tested in the experiments. Comparisons with other new variants of POS and GSA show that RHGSA obtains a promising performance on the majority of the test problems. Furthermore, a practical application problem, the economic load dispatch problem of power systems (ELD), is solved to further evaluate RHGSA. Compared with the previous evolutionary algorithms applied to ELD problem, RHGSA can get better results.",2015,Knowledge and Information Systems volume 42 issue 3 pp 689-708,guided local search;beam search;best first search;binary search algorithm;search algorithm;sequential quadratic programming;swarm intelligence;artificial intelligence;machine learning;mathematical optimization;algorithm;computer science;mathematics;
Interest-driven private friend recommendation,Bharath K. Samanthula (Missouri University of Science and Technology);Wei Jiang (Missouri University of Science and Technology);,"2303546777,2675954469","The emerging growth of online social networks has opened new doors for various kinds of applications such as business intelligence and expanding social connections through friend recommendations. In particular, friend recommendation facilitates users to explore new friendships based on social network structures, user profile information (similar interest) or both. However, as the privacy concerns of users are on the rise, searching for new friends is not a straightforward task under the assumption that users' information is kept private. Along this direction, this paper proposes two private friend recommendation algorithms based on the social network structure and the users' social tags. The first protocol is more efficient from a user's perspective compared to the second protocol, and this efficiency gain comes at the expense of relaxing the underlying privacy assumptions. On the other hand, the second protocol provides the best security guarantee. In addition, we empirically analyze the complexities of the proposed protocols and provide various experimental results.",2015,Knowledge and Information Systems volume 42 issue 3 pp 663-687,privacy;internet privacy;world wide web;data mining;computer science;
Novel harmony search-based algorithms for part-of-speech tagging,Rana Forsati (Shahid Beheshti University);Mehrnoush Shamsfard (Shahid Beheshti University);,"89384258,164497389","As a fast and high-quality tagger algorithm is a crucial task in natural language processing, this paper presents novel language-independent algorithms based on harmony search (HS) optimization method for handling the part-of-speech (PoS) tagging problem. The first proposed algorithm is a framework for applying HS to PoS-tagging which is called HSTAGger. By modifying HS algorithm and proposing more efficient objective functions, two improved versions of the HSTAGger are also introduced. In addition, a novel class of problematic words called erroneous as well as a method of handling them is proposed for the first time to the best of our knowledge. To demonstrate the effectiveness of the proposed algorithms, we have applied them on standard annotated corpus and compare them with other evolutionary-based and classical PoS-tagging approaches. Experimental results indicate that the proposed algorithms outperform the other taggers previously presented in the literature in terms of average precision.",2015,Knowledge and Information Systems volume 42 issue 3 pp 709-736,harmony search;evolutionary algorithm;data mining;artificial intelligence;machine learning;algorithm;computer science;
Privacy-preserving LOF outlier detection,Lu Li (University of Science and Technology of China);Liusheng Huang (University of Science and Technology of China);Wei Yang (University of Science and Technology of China);Xiaohui Yao (University of Science and Technology of China);An Liu (University of Science and Technology of China);,"2619574415,2166308156,2608023336,2698601256,2663608191","LOF is a well-known approach for density-based outlier detection and has received much attention recently. It is important to design a privacy-preserving LOF outlier detection algorithm as the data on which LOF runs is typically spilt among multiple participants and no one is willing to disclose his sensitive information due to legal or moral considerations. This is, however, a hard problem since participants need to find the maximum one of the distances between an object and its k-Nearest Neighbors (k-NN) without learning the information of these objects. In this paper, we propose an efficient protocol for privacy-preserving LOF outlier detection. We first employ a shuffle protocol to permute the distance vectors owned by different participants. Then, we design a secure selection method to obtain the garbled k-NN indexes and shares of k-distance for given objects. For each object, we make use of the k-distance of all objects to construct a vector, based on which the permute protocol is executed again to obtain new shares of k-distance. Finally, the shares corresponding to the garbled k-NN indexes are selected as the expected result. Our protocol ensures that all the intermediates are shared between multiple participants and thus avoid information leaking. In addition, our protocol is efficient as we prove that the computation and communication complexity of our protocol is bounded by $$O(n^2)$$ O ( n 2 ) .",2015,Knowledge and Information Systems volume 42 issue 3 pp 579-597,computer security;data mining;database;machine learning;computer science;
Extended symbolic approximate reasoning based on linguistic modifiers,Saoussen Bel Hadj Kacem (Tunis University);Amel Borgi (National Institute of Applied Science and Technology);Moncef Tagina (Tunis University);,"2310486225,2716239162,2735541099","Approximate reasoning allows inferring with imperfect knowledge. It is based on a generalization of modus ponens (MP) known as generalized modus ponens (GMP). We are interested in approximate reasoning within symbolic multi-valued logic framework. In a previous work, we have proposed a new GMP based on linguistic modifiers in the multi-valued logic framework. The use of linguistic modifiers allows having a gradual reasoning; moreover, it allows checking axiomatics of approximate reasoning. In this paper, we extend our approximate reasoning to hold with complex rules, i.e., rules whose premises are conjunction or disjunction of propositions. For this purpose, we introduce a new operator that aggregates linguistic modifiers and verifies the required properties of logical connectives within the multi-valued logic framework.",2015,Knowledge and Information Systems volume 42 issue 3 pp 633-661,psychology of reasoning;reasoning system;deductive reasoning;artificial intelligence;machine learning;algorithm;mathematics;
"Self-labeled techniques for semi-supervised learning: taxonomy, software and empirical study",Isaac Triguero (University of Granada);Salvador García (University of Jaén);Francisco Herrera (University of Granada);,"2062549064,2145770163,2141382980","Semi-supervised classification methods are suitable tools to tackle training sets with large amounts of unlabeled data and a small quantity of labeled data. This problem has been addressed by several approaches with different assumptions about the characteristics of the input data. Among them, self-labeled techniques follow an iterative procedure, aiming to obtain an enlarged labeled data set, in which they accept that their own predictions tend to be correct. In this paper, we provide a survey of self-labeled methods for semi-supervised classification. From a theoretical point of view, we propose a taxonomy based on the main characteristics presented in them. Empirically, we conduct an exhaustive study that involves a large number of data sets, with different ratios of labeled data, aiming to measure their performance in terms of transductive and inductive classification capabilities. The results are contrasted with nonparametric statistical tests. Note is then taken of which self-labeled models are the best-performing ones. Moreover, a semi-supervised learning module has been developed for the Knowledge Extraction based on Evolutionary Learning software, integrating analyzed methods and data sets.",2015,Knowledge and Information Systems volume 42 issue 2 pp 245-284,biological classification;semi supervised learning;unsupervised learning;data mining;database;pattern recognition;artificial intelligence;machine learning;statistics;computer science;
On team formation with expertise query in collaborative social networks,Cheng-Te Li (National Taiwan University);Man-Kwan Shan (National Chengchi University);Shou-De Lin (National Taiwan University);,"2139086518,2007106882,2114357324","Given a collaborative social network and a task consisting of a set of required skills, the team formation problem aims at finding a team of experts who not only satisfies the requirements of the given task but also is able to communicate with one another in an effective manner. This paper extends the original team formation problem to a generalized version, in which the number of experts selected for each required skill is also specified. The constructed teams need to contain adequate number of experts for each required skill. We develop two approaches to compose teams for the proposed generalized team formation tasks. First, we consider the specific number of experts to devise the generalized Enhanced-Steiner algorithm. Second, we present a grouping-based method condensing the expertise information to a compact representation, group graph, based on the required skills. Group graph can not only reduce the search space but also eliminate redundant communication cost and filter out irrelevant individuals when compiling team members. To further improve the effectiveness of the composed teams, we propose a density-based measure and embed it into the developed methods. Experimental results on the DBLP network show that the teams composed by the proposed methods have better performance in both effectiveness and efficiency.",2015,Knowledge and Information Systems volume 42 issue 2 pp 441-463,social network;knowledge management;data mining;artificial intelligence;machine learning;simulation;computer science;
Trust in networks of ontologies and alignments,Manuel Atencia (Centre national de la recherche scientifique);Mustafa Al-Bakri (Centre national de la recherche scientifique);Marie-Christine Rousset (Centre national de la recherche scientifique);,"2053432906,2120589355,2048382749","In this paper, we introduce a mechanism of trust adapted to semantic peer-to-peer networks in which every peer is free to organize its local resources as instances of classes of its own ontology. Peers use their ontologies to query other peers, and alignments between peers' ontologies make it possible to reformulate queries from one local peer's vocabulary to another. Alignments are typically the result of manual or (semi)automatic ontology matching. However, resulting alignments may be unsound and/or incomplete, and therefore, query reformulation based on alignments may lead to unsatisfactory answers. Trust can assist peers to select the peers in the network that are better suited to answer their queries. In our model, the trust that a peer has toward another peer depends on a specific query, and it represents the probability that the latter peer will provide a satisfactory answer to the query. In order to compute trust, we perform Bayesian inference that exploits ontologies, alignments and user feedback. We have implemented our method and conducted an evaluation. Experimental results show that trust values converge as more queries are sent and answers received. Furthermore, when query answering is guided by trust, the quality of peers' answers, measured with precision and recall, is improved.",2015,Knowledge and Information Systems volume 42 issue 2 pp 353-379,trustworthy computing;ontology;world wide web;data mining;database;computer science;
Fast adaptive kernel density estimator for data streams,Arnold P. Boedihardjo (United States Army Corps of Engineers);Chang-Tien Lu (Virginia Tech);Feng Chen (Carnegie Mellon University);,"240228327,2112878203,2601749234","The probability density function (PDF) is an effective data model for a variety of stream mining tasks. As such, accurate estimates of the PDF are essential to reducing the uncertainties and errors associated with mining results. The nonparametric adaptive kernel density estimator (AKDE) provides accurate, robust, and asymptotically consistent estimates of a PDF. However, due to AKDE's extensive computational requirements, it cannot be directly applied to the data stream environment. This paper describes the development of an AKDE approximation approach that heeds the constraints of the data stream environment and supports efficient processing of multiple queries. To this end, this work proposes (1) the concept of local regions to provide a partition-based variable bandwidth to capture local density structures and enhance estimation quality; (2) a suite of linear-pass methods to construct the local regions and kernel objects online; (3) an efficient multiple queries evaluation algorithm; (4) a set of approximate techniques to increase the throughput of multiple density queries processing; and (5) a fixed-size memory time-based sliding window that updates the kernel objects in linear time. Comprehensive experiments were conducted with real-world and synthetic data sets to validate the effectiveness and efficiency of the approach.",2015,Knowledge and Information Systems volume 42 issue 2 pp 285-317,kernel embedding of distributions;variable kernel density estimation;kernel density estimation;data stream mining;data mining;machine learning;statistics;computer science;
Efficient algorithms for finding optimal binary features in numeric and nominal labeled data,Michael Mampaey (University of Bonn);Siegfried Nijssen (Leiden University);Adrianus Feelders (Utrecht University);Rob Konijn (Leiden University);Arno Knobbe (Leiden University);,"2027051129,2102450877,2240805150,2020123811,1229146049","An important subproblem in supervised tasks such as decision tree induction and subgroup discovery is finding an interesting binary feature (such as a node split or a subgroup refinement) based on a numeric or nominal attribute, with respect to some discrete or continuous target variable. Often one is faced with a trade-off between the expressiveness of such features on the one hand and the ability to efficiently traverse the feature search space on the other hand. In this article, we present efficient algorithms to mine binary features that optimize a given convex quality measure. For numeric attributes, we propose an algorithm that finds an optimal interval, whereas for nominal attributes, we give an algorithm that finds an optimal value set. By restricting the search to features that lie on a convex hull in a coverage space, we can significantly reduce computation time. We present some general theoretical results on the cardinality of convex hulls in coverage spaces of arbitrary dimensions and perform a complexity analysis of our algorithms. In the important case of a binary target, we show that these algorithms have linear runtime in the number of examples. We further provide algorithms for additive quality measures, which have linear runtime regardless of the target type. Additive measures are particularly relevant to feature discovery in subgroup discovery. Our algorithms are shown to perform well through experimentation and furthermore provide additional expressive power leading to higher-quality results.",2015,Knowledge and Information Systems volume 42 issue 2 pp 465-492,convex function;receiver operating characteristic;decision tree;discrete mathematics;data mining;machine learning;mathematical optimization;computer science;mathematics;
Efficient processing of optimal meeting point queries in Euclidean space and road networks,Da Yan (Hong Kong University of Science and Technology);Zhou Zhao (Hong Kong University of Science and Technology);Wilfred Siu Hung Ng (Hong Kong University of Science and Technology);,"2146437384,2118299058,2170178419","Finding an optimal meeting point (OMP) for a group of people (or a set of objects) at different locations is an important problem in spatial query processing. There are many real-life applications related to this problem, such as determining the location of a conference venue, deciding the pick-up location of a tourist bus, and planing tactics of artificial intelligence in real-time strategy games. Formally, given a set $$Q$$ Q of query points in a spatial setting $$P$$ P , an OMP query fetches the point $$o\in P$$ o ? P that minimizes a cost function defined over the distances from $$o$$ o to all points in $$Q$$ Q . Since there are infinitely many locations in a given space setting, it is infeasible to examine all of them to find the OMP, and thus, the problem is challenging. In this paper, we study OMP queries in the following two spatial settings which are common in real-life applications: Euclidean space and road networks. In the setting of Euclidean space, we propose a general framework for answering all OMP query variants and also identify the best algorithms for particular types of OMP queries in the literature. In the setting of road networks, we study how to access only part of the road network and examine part of the candidates. Specifically, we explore two pruning techniques, namely Euclidean distance bound and threshold algorithm, which help improve the efficiency of OMP query processing. Extensive experiments are conducted to demonstrate the efficiency of our proposed approaches on both real and synthetic datasets.",2015,Knowledge and Information Systems volume 42 issue 2 pp 319-351,theoretical computer science;data mining;database;artificial intelligence;machine learning;mathematics;
Dominance relationship analysis with budget constraints,Shen Ge (University of Hong Kong);Leong Hou U (University of Macau);Nikos Mamoulis (University of Hong Kong);David Wai-Lok Cheung (University of Hong Kong);,"2311362784,2164563249,18851973,1979772396","Creating a new product that dominates all its competitors is one of the main objectives in marketing. Nevertheless, this might not be feasible since in practice the development process is confined by some constraints, e.g., limited funding or low target selling price. We model these constraints by a constraint function, which determines the feasible characteristics of a new product. Given such a budget, our task is to decide the best possible features of the new product that maximize its profitability. In general, a product is marketable if it dominates a large set of existing products, while it is not dominated by many. Based on this, we define dominance relationship analysis and use it to measure the profitability of the new product. The decision problem is then modeled as a budget constrained optimization query (BOQ). Computing BOQ is challenging due to the exponential increase in the search space with dimensionality. We propose a divide-and-conquer based framework, which outperforms a baseline approach in terms of not only execution time but also space complexity. Based on the proposed framework, we further study an approximation solution, which provides a good trade-off between computation cost and quality of result.",2015,Knowledge and Information Systems volume 42 issue 2 pp 409-440,mathematical optimization;
MILDE: multiple instance learning by discriminative embedding,Jaume Amores;,2632955850,"While the objective of the standard supervised learning problem is to classify feature vectors, in the multiple instance learning problem, the objective is to classify bags, where each bag contains multiple feature vectors. This represents a generalization of the standard problem, and this generalization becomes necessary in many real applications such as drug activity prediction, content-based image retrieval, and others. While the existing paradigms are based on learning the discriminant information either at the instance level or at the bag level, we propose to incorporate both levels of information. This is done by defining a discriminative embedding of the original space based on the responses of cluster-adapted instance classifiers. Results clearly show the advantage of the proposed method over the state of the art, where we tested the performance through a variety of well-known databases that come from real problems, and we also included an analysis of the performance using synthetically generated data.",2015,Knowledge and Information Systems volume 42 issue 2 pp 381-407,bag of words model;codebook;instance based learning;data mining;pattern recognition;machine learning;statistics;computer science;mathematics;
Defining and evaluating network communities based on ground-truth,Jaewon Yang (Stanford University);Jure Leskovec (Stanford University);,"2131921352,1878631932","Nodes in real-world networks organize into densely linked communities where edges appear with high concentration among the members of the community. Identifying such communities of nodes has proven to be a challenging task due to a plethora of definitions of network communities, intractability of methods for detecting them, and the issues with evaluation which stem from the lack of a reliable gold-standard ground-truth. In this paper, we distinguish between structural and functional definitions of network communities. Structural definitions of communities are based on connectivity patterns, like the density of connections between the community members, while functional definitions are based on (often unobserved) common function or role of the community members in the network. We argue that the goal of network community detection is to extract functional communities based on the connectivity structure of the nodes in the network. We then identify networks with explicitly labeled functional communities to which we refer as ground-truth communities. In particular, we study a set of 230 large real-world social, collaboration, and information networks where nodes explicitly state their community memberships. For example, in social networks, nodes explicitly join various interest-based social groups. We use such social groups to define a reliable and robust notion of ground-truth communities. We then propose a methodology, which allows us to compare and quantitatively evaluate how different structural definitions of communities correspond to ground-truth functional communities. We study 13 commonly used structural definitions of communities and examine their sensitivity, robustness and performance in identifying the ground-truth. We show that the 13 structural definitions are heavily correlated and naturally group into four classes. We find that two of these definitions, Conductance and Triad participation ratio, consistently give the best performance in identifying ground-truth communities. We also investigate a task of detecting communities given a single seed node. We extend the local spectral clustering algorithm into a heuristic parameter-free community detection method that easily scales to networks with more than 100 million nodes. The proposed method achieves 30 % relative improvement over current local clustering methods.",2015,Knowledge and Information Systems volume 42 issue 1 pp 181-213,modularity;community structure;world wide web;data mining;computer science;
A markov prediction model for data-driven semi-structured business processes,Geetika T. Lakshmanan (IBM);Davood Shamsi (Stanford University);Yurdaer N. Doganata (IBM);Merve Unuvar (IBM);Rania Khalaf (IBM);,"2150824222,1995810726,2024289304,1965781404,2125623263","In semi-structured case-oriented business processes, the sequence of process steps is determined by case workers based on available document content associated with a case. Transitions between process execution steps are therefore case specific and depend on independent judgment of case workers. In this paper, we propose an instance-specific probabilistic process model (PPM) whose transition probabilities are customized to the semi-structured business process instance it represents. An instance-specific PPM serves as a powerful representation to predict the likelihood of different outcomes. We also show that certain instance-specific PPMs can be transformed into a Markov chain under some non-restrictive assumptions. For instance-specific PPMs that contain parallel execution of tasks, we provide an algorithm to map them to an extended space Markov chain. This way existing Markov techniques can be leveraged to make predictions about the likelihood of executing future tasks. Predictions provided by our technique could generate early alerts for case workers about the likelihood of important or undesired outcomes in an executing case instance. We have implemented and validated our approach on a simulated automobile insurance claims handling semi-structured business process. Results indicate that an instance-specific PPM provides more accurate predictions than other methods such as conditional probability. We also show that as more document data become available, the prediction accuracy of an instance-specific PPM increases.",2015,Knowledge and Information Systems volume 42 issue 1 pp 97-126,variable order markov model;business process;markov model;decision tree;markov chain;prediction;markov process;data mining;database;artificial intelligence;machine learning;simulation;statistics;computer science;
Fast time intervals mining using the transitivity of temporal relations,Robert Moskovitch (Ben-Gurion University of the Negev);Yuval Shahar (Ben-Gurion University of the Negev);,"319285823,700636827","We introduce an algorithm, called KarmaLego, for the discovery of frequent symbolic time interval-related patterns (TIRPs). The mined symbolic time intervals can be part of the input, or can be generated by a temporal-abstraction process from raw time-stamped data. The algorithm includes a data structure for TIRP-candidate generation and a novel method for efficient candidate-TIRP generation, by exploiting the transitivity property of Allen's temporal relations. Additionally, since the non-ambiguous definition of TIRPs does not specify the duration of the time intervals, we propose to pre-cluster the time intervals based on their duration to decrease the variance of the supporting instances. Our experimental comparison of the KarmaLego algorithm's runtime performance with several existing state of the art time intervals pattern mining methods demonstrated a significant speed-up, especially with large datasets and low levels of minimal vertical support. Furthermore, pre-clustering by time interval duration led to an increase in the homogeneity of the duration of the discovered TIRP's supporting instances' time intervals components, accompanied, however, by a corresponding decrease in the number of discovered TIRPs.",2015,Knowledge and Information Systems volume 42 issue 1 pp 21-48,transitive relation;data science;data mining;algorithm;mathematics;
A hybrid approach for content extraction with text density and visual importance of DOM nodes,Dandan Song (Beijing Institute of Technology);Fei Sun (Beijing Institute of Technology);Lejian Liao (Beijing Institute of Technology);,"2647528830,2476332389,2652518449","Additional contents in web pages, such as navigation panels, advertisements, copyrights and disclaimer notices, are typically not related to the main subject and may hamper the performance of Web data mining. They are traditionally taken as noises and need to be removed properly. To achieve this, two intuitive and crucial kinds of information--the textual information and the visual information of web pages--is considered in this paper. Accordingly, Text Density and Visual Importance are defined for the Document Object Model (DOM) nodes of a web page. Furthermore, a content extraction method with these measured values is proposed. It is a fast, accurate and general method for extracting content from diverse web pages. And with the employment of DOM nodes, the original structure of the web page can be preserved. Evaluated with the CleanEval benchmark and with randomly selected pages from well-known Web sites, where various web domains and styles are tested, the effect of the method is demonstrated. The average F1-scores with our method were 8.7 % higher than the best scores among several alternative methods.",2015,Knowledge and Information Systems volume 42 issue 1 pp 75-96,printer friendly;multimedia;world wide web;information retrieval;data mining;database;artificial intelligence;computer science;
An automatic extraction method of the domains of competence for learning classifiers using data complexity measures,Julián Luengo (University of Burgos);Francisco Herrera (University of Granada);,"2126985344,2141382980","The constant appearance of algorithms and problems in data mining makes impossible to know in advance whether the model will perform well or poorly until it is applied, which can be costly. It would be useful to have a procedure that indicates, prior to the application of the learning algorithm and without needing a comparison with other methods, whether the outcome will be good or bad using the information available in the data. In this work, we present an automatic extraction method to determine the domains of competence of a classifier using a set of data complexity measures proposed for the task of classification. These domains codify the characteristics of the problems that are suitable or not for it, relating the concepts of data geometrical structures that may be difficult and the final accuracy obtained by any classifier. In order to do so, this proposal uses 12 metrics of data complexity acting over a large benchmark of datasets in order to analyze the behavior patterns of the method, obtaining intervals of data complexity measures with good or bad performance. As a representative for classifiers to analyze the proposal, three classical but different algorithms are used: C4.5, SVM and K-NN. From these intervals, two simple rules that describe the good or bad behaviors of the classifiers mentioned each are obtained, allowing the user to characterize the response quality of the methods from a dataset's complexity. These two rules have been validated using fresh problems, showing that they are general and accurate. Thus, it can be established when the classifier will perform well or poorly prior to its application.",2015,Knowledge and Information Systems volume 42 issue 1 pp 147-180,support vector machine;k nearest neighbors algorithm;biological classification;data mining;pattern recognition;artificial intelligence;machine learning;computer science;
Improving the efficiency of traditional DTW accelerators,Romain Tavenard (Idiap Research Institute);Laurent Amsaleg (Centre national de la recherche scientifique);,"138929054,190871858","Dynamic time warping (DTW) is the most popular approach for evaluating the similarity of time series, but its computation is costly. Therefore, simple functions lower bounding DTW distances have been designed, accelerating searches by quickly pruning sequences that could not possibly be best matches. The tighter the bounds, the more they prune and the better the performance. Designing new functions that are even tighter is difficult because their computation is likely to become complex, canceling the benefits of their pruning. It is possible, however, to design simple functions with a higher pruning power by relaxing the no false dismissal assumption, resulting in approximate lower bound functions. This paper describes how very popular approaches accelerating DTW such as $$\text {LB}\_\text {Keogh}{}$$ LB _ Keogh and $$\text {LB}\_\text {PAA}{}$$ LB _ PAA can be made more efficient via approximations. The accuracy of approximations can be tuned, ranging from no false dismissal to potential losses when aggressively set for great response time savings. At very large scale, indexing time series is mandatory. This paper also describes how approximate lower bound functions can be used with iSAX. Furthermore, it shows that a $$k$$ k -means-based quantization step for iSAX gives significant performance gains.",2015,Knowledge and Information Systems volume 42 issue 1 pp 215-243,dynamic time warping;search engine indexing;theoretical computer science;world wide web;data mining;database;artificial intelligence;machine learning;statistics;algorithm;computer science;mathematics;
Effective data summarization for hierarchical clustering in large datasets,"Bidyut Kr. Patra (National Institute of Technology, Rourkela);Sukumar Nandi (Indian Institute of Technology Guwahati);","2432912151,2150739198","Cluster analysis in a large dataset is an interesting challenge in many fields of Science and Engineering. One important clustering approach is hierarchical clustering, which outputs hierarchical (nested) structures of a given dataset. The single-link is a distance-based hierarchical clustering method, which can find non-convex (arbitrary)-shaped clusters in a dataset. However, this method cannot be used for clustering large dataset as this method either keeps entire dataset in main memory or scans dataset multiple times from secondary memory of the machine. Both of them are potentially severe problems for cluster analysis in large datasets. One remedy for both problems is to create a summary of a given dataset efficiently, and the summary is subsequently used to speed up clustering methods in large datasets. In this paper, we propose a summarization scheme termed data sphere (ds) to speed up single-link clustering method in large datasets. The ds utilizes sequential leaders clustering method to collect important statistics of a given dataset. The single-link method is modified to work with ds. Modified clustering method is termed as summarized single-link (SSL). The SSL method is considerably faster than the single-link method applied directly to the dataset, and clustering results produced by SSL method are close to the clustering results produced by single-link method. The SSL method outperforms single-link using data bubble (summarization scheme) both in terms of clustering accuracy and computation time. To speed up proposed summarization scheme, a technique is introduced to reduce a large number of distance computations in leaders method. Experimental studies demonstrate effectiveness of the proposed summarization scheme for large datasets.",2015,Knowledge and Information Systems volume 42 issue 1 pp 1-20,hierarchical clustering of networks;flame clustering;k medians clustering;brown clustering;canopy clustering algorithm;correlation clustering;data stream clustering;cure data clustering algorithm;single linkage clustering;fuzzy clustering;clustering high dimensional data;hierarchical clustering;cluster analysis;consensus clustering;automatic summarization;data science;data mining;machine learning;computer science;
Detecting multiple stochastic network motifs in network data,Kai Liu (Hong Kong Baptist University);William K. Cheung (Hong Kong Baptist University);Jiming Liu (Hong Kong Baptist University);,"2508384906,2099585548,2121827670","Network motifs are referred to as the interaction patterns that occur significantly more often in a complex network than in the corresponding randomized networks. They have been found effective in characterizing many real-world networks. A number of network motif detection algorithms have been proposed in the literature where the interactions in a motif are mostly assumed to be deterministic, i.e., either present or missing. With the conjecture that the real-world networks are resulted from interaction patterns which should be stochastic in nature, the use of stochastic models is proposed in this paper to achieve more robust motif detection. In particular, we propose the use of a finite mixture model to detect multiple stochastic network motifs. A component-wise expectation maximization (CEM) algorithm is derived for the finite mixture of stochastic network motifs so that both the optimal number of motifs and the motif parameters can be automatically estimated. For performance evaluation, we applied the proposed algorithm to both synthetic networks and a number of online social network data sets and demonstrated that it outperformed the deterministic motif detection algorithm FANMOD as well as the conventional EM algorithm in term of its robustness against noise. Also, how to interpret the detected stochastic network motifs to gain insights on the interaction patterns embedded in the network data is discussed. In addition, the algorithm's computational complexity and runtime performance are presented for efficiency evaluation.",2015,Knowledge and Information Systems volume 42 issue 1 pp 49-74,stochastic neural network;network motif;social network;bioinformatics;data mining;machine learning;mathematics;
Fast PageRank approximation by adaptive sampling,Wenting Liu (Nanyang Technological University);Guangxia Li (Nanyang Technological University);James Cheng (The Chinese University of Hong Kong);,"2674113960,2712165724,2304873892","PageRank is typically computed from the power of transition matrix in a Markov Chain model. It is therefore computationally expensive, and efficient approximation methods to accelerate the computation are necessary, especially when it comes to large graphs. In this paper, we propose two sampling algorithms for PageRank efficient approximation: Direct sampling and Adaptive sampling. Both methods sample the transition matrix and use the sample in PageRank computation. Direct sampling method samples the transition matrix once and uses the sample directly in PageRank computation, whereas adaptive sampling method samples the transition matrix multiple times with an adaptive sample rate which is adjusted iteratively as the computing procedure proceeds. This adaptive sample rate is designed for a good trade-off between accuracy and efficiency for PageRank approximation. We provide detailed theoretical analysis on the error bounds of both methods. We also compare them with several state-of-the-art PageRank approximation methods, including power extrapolation and inner---outer power iteration algorithm. Experimental results on several real-world datasets show that our methods can achieve significantly higher efficiency while attaining comparable accuracy than state-of-the-art methods.",2015,Knowledge and Information Systems volume 42 issue 1 pp 127-146,google matrix;power iteration;theoretical computer science;mathematical optimization;statistics;computer science;mathematics;
Parallel matrix factorization for recommender systems,Hsiang-Fu Yu (University of Texas at Austin);Cho-Jui Hsieh (University of Texas at Austin);Si Si (University of Texas at Austin);Inderjit S. Dhillon (University of Texas at Austin);,"2149528706,2148022289,2099379656,2033403132","Matrix factorization, when the matrix has missing values, has become one of the leading techniques for recommender systems. To handle web-scale datasets with millions of users and billions of ratings, scalability becomes an important issue. Alternating least squares (ALS) and stochastic gradient descent (SGD) are two popular approaches to compute matrix factorization, and there has been a recent flurry of activity to parallelize these algorithms. However, due to the cubic time complexity in the target rank, ALS is not scalable to large-scale datasets. On the other hand, SGD conducts efficient updates but usually suffers from slow convergence that is sensitive to the parameters. Coordinate descent, a classical optimization approach, has been used for many other large-scale problems, but its application to matrix factorization for recommender systems has not been thoroughly explored. In this paper, we show that coordinate descent-based methods have a more efficient update rule compared to ALS and have faster and more stable convergence than SGD. We study different update sequences and propose the CCD++ algorithm, which updates rank-one factors one by one. In addition, CCD++ can be easily parallelized on both multi-core and distributed systems. We empirically show that CCD++ is much faster than ALS and SGD in both settings. As an example, with a synthetic dataset containing 14.6 billion ratings, on a distributed memory cluster with 64 processors, to deliver the desired test RMSE, CCD++ is 49 times faster than SGD and 20 times faster than ALS. When the number of processors is increased to 256, CCD++ takes only 16 s and is still 40 times faster than SGD and 20 times faster than ALS.",2014,Knowledge and Information Systems volume 41 issue 3 pp 793-819,low rank approximation;matrix decomposition;recommender system;theoretical computer science;data mining;database;machine learning;computer science;
High utility K-anonymization for social network publishing,Yazhe Wang (Singapore Management University);Long Xie (Singapore Management University);Baihua Zheng (Singapore Management University);Ken C. K. Lee (University of Massachusetts Dartmouth);,"2095619260,2154602265,2200653933,2101841691","Privacy and utility are two main desiderata of good sensitive information publishing schemes. For publishing social networks, many existing algorithms rely on $$k$$ k -anonymity as a criterion to guarantee privacy protection. They reduce the utility loss by first using the degree sequence to model the structural properties of the original social network and then minimizing the changes on the degree sequence caused by the anonymization process. However, the degree sequence-based graph model is simple, and it fails to capture many important graph topological properties. Consequently, the existing anonymization algorithms that rely on this simple graph model to measure utility cannot guarantee generating anonymized social networks of high utility. In this paper, we propose novel utility measurements that are based on more complex community-based graph models. We also design a general $$k$$ k -anonymization framework, which can be used with various utility measurements to achieve $$k$$ k -anonymity with small utility loss on given social networks. Finally, we conduct extensive experimental evaluation on real datasets to evaluate the effectiveness of the new utility measurements proposed. The results demonstrate that our scheme achieves significant improvement on the utility of the anonymized social networks compared with the existing anonymization algorithms. The utility losses of many social network statistics of the anonymized social networks generated by our scheme are under 1 % in most cases.",2014,Knowledge and Information Systems volume 41 issue 3 pp 697-725,utility;privacy;social network;internet privacy;computer security;data mining;computer science;
Multi-document summarization via Archetypal Analysis of the content-graph joint model,Ercan Canhasi (University of Ljubljana);Igor Kononenko (University of Ljubljana);,"47774272,1409703486","In recent years, algebraic methods, more precisely matrix decomposition approaches, have become a key tool for tackling document summarization problem. Typical algebraic methods used in multi-document summarization (MDS) vary from soft and hard clustering approaches to low-rank approximations. In this paper, we present a novel summarization method AASum which employs the archetypal analysis for generic MDS. Archetypal analysis (AA) is a promising unsupervised learning tool able to completely assemble the advantages of clustering and the flexibility of matrix factorization. In document summarization, given a content-graph data matrix representation of a set of documents, positively and/or negatively salient sentences are values on the data set boundary. These extreme values, archetypes, can be computed using AA. While each sentence in a data set is estimated as a mixture of archetypal sentences, the archetypes themselves are restricted to being sparse mixtures, i.e., convex combinations of the original sentences. Since AA in this way readily offers soft clustering, we suggest to consider it as a method for simultaneous sentence clustering and ranking. Another important argument in favor of using AA in MDS is that in contrast to other factorization methods, which extract prototypical, characteristic, even basic sentences, AA selects distinct (archetypal) sentences and thus induces variability and diversity in produced summaries. Experimental results on the DUC generic summarization data sets evidence the improvement of the proposed approach over the other closely related methods.",2014,Knowledge and Information Systems volume 41 issue 3 pp 821-842,matrix decomposition;automatic summarization;data mining;database;pattern recognition;artificial intelligence;machine learning;computer science;
EvoMiner: frequent subtree mining in phylogenetic databases,Akshay Deepak (Iowa State University);David Fernández-Baca (Iowa State University);Srikanta Tirthapura (Iowa State University);Michael J. Sanderson (University of Arizona);Michelle M. McMahon (University of Arizona);,"2150373105,2009961596,1991070240,2306795477,2124933506","The problem of mining collections of trees to identify common patterns, called frequent subtrees (FSTs), arises often when trying to interpret the results of phylogenetic analysis. FST mining generalizes the well-known maximum agreement subtree problem. Here we present EvoMiner, a new algorithm for mining frequent subtrees in collections of phylogenetic trees. EvoMiner is an Apriori-like levelwise method, which uses a novel phylogeny-specific constant-time candidate generation scheme, an efficient fingerprinting-based technique for downward closure, and a lowest-common-ancestor-based support counting step that requires neither costly subtree operations nor database traversal. Our algorithm achieves speedups of up to 100 times or more over Phylominer, the current state-of-the-art algorithm for mining phylogenetic trees. EvoMiner can also work in depth-first enumeration mode to use less memory at the expense of speed. We demonstrate the utility of FST mining as a way to extract meaningful phylogenetic information from collections of trees when compared to maximum agreement subtrees and majority-rule trees--two commonly used approaches in phylogenetic analysis for extracting consensus information from a collection of trees over a common leaf set.",2014,Knowledge and Information Systems volume 41 issue 3 pp 559-590,phylogenetics;bioinformatics;data mining;algorithm;
Efficient mining of discriminative co-clusters from gene expression data,Omar Odibat (Wayne State University);Chandan K. Reddy (Wayne State University);,"112465256,2100435683","Discriminative models are used to analyze the differences between two classes and to identify class-specific patterns. Most of the existing discriminative models depend on using the entire feature space to compute the discriminative patterns for each class. Co-clustering has been proposed to capture the patterns that are correlated in a subset of features, but it cannot handle discriminative patterns in labeled datasets. In certain biological applications such as gene expression analysis, it is critical to consider the discriminative patterns that are correlated only in a subset of the feature space. The objective of this paper is twofold: first, it presents an algorithm to efficiently find arbitrarily positioned co-clusters from complex data. Second, it extends this co-clustering algorithm to discover discriminative co-clusters by incorporating the class information into the co-cluster search process. In addition, we also characterize the discriminative co-clusters and propose three novel measures that can be used to evaluate the performance of any discriminative subspace pattern-mining algorithm. We evaluated the proposed algorithms on several synthetic and real gene expression datasets, and our experimental results showed that the proposed algorithms outperformed several existing algorithms available in the literature.",2014,Knowledge and Information Systems volume 41 issue 3 pp 667-696,negative relationship;discriminative model;biclustering;data science;bioinformatics;data mining;machine learning;computer science;
Explaining prediction models and individual predictions with feature contributions,Erik Štrumbelj (University of Ljubljana);Igor Kononenko (University of Ljubljana);,"1153133090,1409703486","We present a sensitivity analysis-based method for explaining prediction models that can be applied to any type of classification or regression model. Its advantage over existing general methods is that all subsets of input features are perturbed, so interactions and redundancies between features are taken into account. Furthermore, when explaining an additive model, the method is equivalent to commonly used additive model-specific methods. We illustrate the method's usefulness with examples from artificial and real-world data sets and an empirical analysis of running times. Results from a controlled experiment with 122 participants suggest that the method's explanations improved the participants' understanding of the model.",2014,Knowledge and Information Systems volume 41 issue 3 pp 647-665,visualization;knowledge extraction;decision support system;data science;data mining;machine learning;statistics;computer science;
Surfacing code in the dark: an instant clone search approach,Jin-woo Park (Pohang University of Science and Technology);Mu-Woong Lee (Pohang University of Science and Technology);Jong-Won Roh (Pohang University of Science and Technology);Seung-won Hwang (Pohang University of Science and Technology);Sunghun Kim (Hong Kong University of Science and Technology);,"2223782829,2098448934,2138402748,2168667670,2164738181","In this paper, we study how to ""surface"" code for instant reference. A traditional mode of surfacing code has been treating code as text and applying keyword search techniques. However, many prior work observes the limitation of such approach: (1) semantic description of code is limited to comments and (2) syntactic keyword is often not selective enough. In contrast, we discuss enabling techniques and scenarios of instant semantic-based surfacing. For example, developers, during a development session, may reference the existing code sharing similar semantics, using his code so far as a query. In addition to such semantic-based surfacing, we also enhance keyword-based surfacing with semantics, by instantly adding semantic tags for code submitted to the repository. To achieve this goal, we first propose scalable indexing structures on vector abstractions of code. Our experimental results show our techniques outperform a state-of-the-art tool in efficiency without compromising accuracy. We then deploy our technique for instant search and tagging scenarios: For instant code search scenario, we demonstrate an instant clone search tool using our techniques, supporting sub-second search over 54 million LOC. For instant code tagging scenario, we propose an automatic instant code tagging algorithm to mine the meaningful tags from clones.",2014,Knowledge and Information Systems volume 41 issue 3 pp 727-759,redundant code;code generation;software development;source code;world wide web;data mining;database;computer science;
Bridging structured and unstructured data via hybrid semantic search and interactive ontology-enhanced query formulation,Markus Gärtner (Vienna University of Technology);Andreas Rauber (Vienna University of Technology);Helmut Berger (Vienna University of Technology);,"2250990731,2057632188,2118814758","In this paper, we identify the problems of current semantic and hybrid search systems, which seek to bridge structure and unstructured data, and propose solutions. We introduce a novel input mechanism for hybrid semantic search that combines the clean and concise input mechanisms of keyword-based search engines with the expressiveness of the input mechanisms provided by semantic search engines. This interactive input mechanism can be used to formulate ontology-aware search queries without prior knowledge of the ontology. Furthermore, we propose a system architecture for automatically fetching relevant unstructured data, complementing structured data stored in a Knowledge Base, to create a combined index. This combined index can be used to conduct hybrid semantic searches which leverage information from structured and unstructured sources. We present the reference implementation Hybrid Semantic Search System ( $$HS^3$$ H S 3 ), which uses the combined index to put hybrid semantic search into practice and implements the interactive ontology-enhanced keyword-based input mechanism. For demonstration purpose, we apply $$HS^3$$ H S 3 to the tourism domain. We present performance test results and the results of a user evaluation. Finally, we provide instructions on how to apply $$HS^3$$ H S 3 to arbitrary domains.",2014,Knowledge and Information Systems volume 41 issue 3 pp 761-792,semantic web stack;semantic computing;concept search;semantic grid;semantic technology;semantic integration;user interface design;ontology;semantic similarity;semantic web;semantic search;world wide web;information retrieval;data mining;database;artificial intelligence;computer science;
PAKDD'12 best paper: generating balanced classifier-independent training samples from unlabeled data,"Youngja Park (IBM);Zijie Qi (University of California, Davis);Suresh N. Chari (IBM);Ian M. Molloy (IBM);","2634435588,2162338994,2319998151,1991166589","We consider the problem of generating balanced training samples from an unlabeled data set with an unknown class distribution. While random sampling works well when the data are balanced, it is very ineffective for unbalanced data. Other approaches, such as active learning and cost-sensitive learning, are also suboptimal as they are classifier-dependent and require misclassification costs and labeled samples, respectively. We propose a new strategy for generating training samples, which is independent of the underlying class distribution of the data and the classifier that will be trained using the labeled data. Our methods are iterative and can be seen as variants of active learning, where we use semi-supervised clustering at each iteration to perform biased sampling from the clusters. We provide several strategies to estimate the underlying class distributions in the clusters and to increase the balancedness in the training samples. Experiments with both highly skewed and balanced data from the UCI repository and a private data set show that our algorithm produces much more balanced samples than random sampling or uncertainty sampling. Further, our sampling strategy is substantially more efficient than active learning methods. The experiments also validate that, with more balanced training data, classifiers trained with our samples outperform classifiers trained with random sampling or active learning.",2014,Knowledge and Information Systems volume 41 issue 3 pp 871-892,semi supervised learning;data mining;pattern recognition;machine learning;statistics;computer science;
A fragment-based iterative consensus clustering algorithm with a robust similarity,Chih-Heng Chung (National Taiwan University of Science and Technology);Bi-Ru Dai (National Taiwan University of Science and Technology);,"2128262619,2113273755","The consensus clustering technique combines multiple clustering results without accessing the original data. Consensus clustering can be used to improve the robustness of clustering results or to obtain the clustering results from multiple data sources. In this paper, we propose a novel definition of the similarity between points and clusters. With an iterative process, such a definition of similarity can represent how a point should join or leave a cluster clearly, determine the number of clusters automatically, and combine partially overlapping clustering results. We also incorporate the concept of ""clustering fragment"" into our method for increased speed. The experimental results show that our algorithm achieves good performances on both artificial data and real data.",2014,Knowledge and Information Systems volume 41 issue 3 pp 591-609,hierarchical clustering of networks;k medians clustering;flame clustering;brown clustering;canopy clustering algorithm;complete linkage clustering;dbscan;correlation clustering;constrained clustering;data stream clustering;cure data clustering algorithm;single linkage clustering;affinity propagation;fuzzy clustering;clustering high dimensional data;hierarchical clustering;cluster analysis;consensus clustering;biclustering;conceptual clustering;data mining;pattern recognition;machine learning;computer science;mathematics;
"Tackling representation, annotation and classification challenges for temporal knowledge base population","Heng Ji (Rensselaer Polytechnic Institute);Taylor Cassidy (The Graduate Center, CUNY);Qi Li (Rensselaer Polytechnic Institute);Suzanne Tamang (The Graduate Center, CUNY);","2619850707,2557835745,2575709086,2088991616","Temporal Information Extraction (TIE) plays an important role in many natural language processing and database applications. Temporal slot filling (TSF) is a new and ambitious TIE task prepared for the knowledge base population (KBP2011) track of NIST Text Analysis Conference. TSF requires systems to discover temporally bound facts about entities and their attributes in order to populate a structured knowledge base. In this paper, we will provide an overview of the unique challenges of this new task and our novel approaches to address these challenges. We present challenges from three perspectives: (1) Temporal information representation: We will review the relevant linguistic semantic theories of temporal information and their limitations, motivating the need to develop a new (4-tuple) representation framework for the task. (2) Annotation acquisition: The lack of substantial labeled training data for supervised learning is a limiting factor in the design of TSF systems. Our work examines the use of multi-class logistic regression methods to improve the labeling quality of training data obtained by distant supervision. (3) Temporal information classification: Another key challenge lies in capturing relations between salient text elements separated by a long context. We develop two approaches for temporal classification and combine them through cross-document aggregation: a flat approach that uses lexical context and shallow dependency features and a structured approach that captures long syntactic contexts by using a dependency path kernel tailored for this task. Experimental results demonstrated that our annotation enhancement approach dramatically increased the speed of the training procedure (by almost 100 times), and that the flat and structured classification approaches were complementary, together yielding a state-of-the-art TSF system.",2014,Knowledge and Information Systems volume 41 issue 3 pp 611-646,data science;data mining;database;artificial intelligence;machine learning;computer science;
A graph-theoretic approach to optimize keyword queries in relational databases,Jaehui Park (Electronics and Telecommunications Research Institute);Sang-goo Lee (Seoul National University);,"2139797254,2126044724","Keyword search can provide users an easy method to query large and complex databases without any knowledge of structured query languages or underlying database schema. Most of the existing studies have focused on generating candidate structured queries relevant to keywords. Due to the large size of generated queries, the execution costs may be prohibitive. However, existing studies lack the idea of a generalized method to optimize the plan of the large set of generated queries. In this paper, we introduce a graph-theoretic optimization approach. We propose a general graph model, Weighted Operator Graph, to address the costs of keyword query evaluation plans. The proposed model is flexible to integrate all of the cost-based plans in a uniform way. We define a Keyword Query Optimization Problem based on a theoretical cost model as a graph-theoretic problem and show it to be a NP-hard problem. We propose a greedy heuristic Maximum Propagation that reduces the size of the intermediate result as early as possible. The proposed algorithm allows us to achieve efficiency in terms of query evaluation costs. The experimental studies on both synthetic and real data set results show that our work outperforms the existing work.",2014,Knowledge and Information Systems volume 41 issue 3 pp 843-870,sargable;search oriented architecture;boolean conjunctive query;web search query;web query classification;spatial query;view;query by example;query expansion;query optimization;query language;information retrieval;data mining;database;computer science;
Tuple MapReduce and Pangool: an associated implementation,Pedro Ferrera;Ivan De Prado;Eric Palacios;Jose Luis Fernandez-Marquez (University of Geneva);Giovanna Di Marzo Serugendo (University of Geneva);,"2652611601,2109822528,2679086832,69460560,1572301190","This paper presents Tuple MapReduce, a new foundational model extending MapReduce with the notion of tuples. Tuple MapReduce allows to bridge the gap between the low-level constructs provided by MapReduce and higher-level needs required by programmers, such as compound records, sorting, or joins. This paper shows as well Pangool, an open-source framework implementing Tuple MapReduce. Pangool eases the design and implementation of applications based on MapReduce and increases their flexibility, still maintaining Hadoop's performance. Additionally, this paper shows: pseudo-codes for relational joins, rollup, and the PageRank algorithm; a Pangool's code example; benchmark results comparing Pangool with existing approaches; reports from users of Pangool in industry; and the description of a distributed database exploiting Pangool. These results show that Tuple MapReduce can be used as a direct, better-suited replacement of the MapReduce model in current implementations without the need of modifying key system fundamentals.",2014,Knowledge and Information Systems volume 41 issue 2 pp 531-557,scalability;big data;world wide web;data mining;database;computer science;
Dynamic and fast processing of queries on large-scale RDF data,Pingpeng Yuan (Huazhong University of Science and Technology);Changfeng Xie (Huazhong University of Science and Technology);Hai Jin (Huazhong University of Science and Technology);Ling Liu (Science College);Guang Yang (Huazhong University of Science and Technology);Xuanhua Shi (Huazhong University of Science and Technology);,"2709278085,2159978502,2168417342,2125988131,2639091176,2099642806","As RDF data continue to gain popularity, we witness the fast growing trend of RDF datasets in both the number of RDF repositories and the size of RDF datasets. Many known RDF datasets contain billions of RDF triples (subject, predicate and object). One of the grant challenges for managing these huge RDF data is how to execute RDF queries efficiently. In this paper, we address the query processing problems against the billion triple challenges. We first identify some causes for the problems of existing query optimization schemes, such as large intermediate results, initial query cost estimation errors. Then, we present our block-oriented dynamic query plan generation approach powered with pipelining execution. Our approach consists of two phases. In the first phase, a near-optimal execution plan for queries is chosen by identifying the processing blocks of queries. We group the join patterns sharing a join variable into building blocks of the query plan since executing them first provides opportunities to reduce the size of intermediate results generated. In the second phase, we further optimize the initial pipelining for a given query plan. We employ optimization techniques, such as sideways information passing and semi-join, to further reduce the size of intermediate results, improve the query processing cost estimation and speed up the performance of query execution. Experimental results on several RDF datasets of over a billion triples demonstrate that our approach outperforms existing RDF query engines that rely on dynamic programming based static query processing strategies.",2014,Knowledge and Information Systems volume 41 issue 2 pp 311-334,sargable;cwm;rdf query language;rdf schema;sparql;web search query;web query classification;query expansion;query optimization;linked data;operator;query language;information retrieval;data mining;database;computer science;
A new semantic relatedness measurement using WordNet features,Mohamed Ali Hadj Taieb (University of Sousse);Mohamed Ben Aouicha (Paul Sabatier University);Abdelmajid Ben Hamadou (Multimedia University);,"2071157431,53721225,2608261313","Computing semantic similarity/relatedness between concepts and words is an important issue of many research fields. Information theoretic approaches exploit the notion of Information Content (IC) that provides for a concept a better understanding of its semantics. In this paper, we present a complete IC metrics survey with a critical study. Then, we propose a new intrinsic IC computing method using taxonomical features extracted from an ontology for a particular concept. This approach quantifies the subgraph formed by the concept subsumers using the depth and the descendents count as taxonomical parameters. In a second part, we integrate this IC metric in a new parameterized multistrategy approach for measuring word semantic relatedness. This measure exploits the WordNet features such as the noun ""is a"" taxonomy, the nominalization relation allowing the use of verb ""is a"" taxonomy and the shared words (overlaps) in glosses. Our work has been evaluated and compared with related works using a wide set of benchmarks conceived for word semantic similarity/relatedness tasks. Obtained results show that our IC method and the new relatedness measure correlated better with human judgments than related works.",2014,Knowledge and Information Systems volume 41 issue 2 pp 467-497,semantic computing;semantic similarity;natural language processing;information retrieval;computer science;
Parallelizing skyline queries over uncertain data streams with sliding window partitioning and grid index,Xiaoyong Li (National University of Defense Technology);Yijie Wang (National University of Defense Technology);Xiaoling Li (National University of Defense Technology);Yuan Wang (National University of Defense Technology);,"2304592297,2707257740,2286727994,2723460814","Skyline query processing over uncertain data streams has attracted considerable attention in database community recently, due to its importance in helping users make intelligent decisions over complex data in many real applications. Although lots of recent efforts have been conducted to the skyline computation over data streams in a centralized environment typically with one processor, they cannot be well adapted to the skyline queries over complex uncertain streaming data, due to the computational complexity of the query and the limited processing capability. Furthermore, none of the existing studies on parallel skyline computation can effectively address the skyline query problem over uncertain data streams, as they are all developed to address the problem of parallel skyline queries over static certain data sets. In this paper, we formally define the parallel query problem over uncertain data streams with the sliding window streaming model. Particularly, for the first time, we propose an effective framework, named distributed parallel framework to address the problem based on the sliding window partitioning. Furthermore, we propose an efficient approach (parallel streaming skyline) to further optimize the parallel skyline computation with an optimized streaming item mapping strategy and the grid index. Extensive experiments with real deployment over synthetic and real data are conducted to demonstrate the effectiveness and efficiency of the proposed techniques.",2014,Knowledge and Information Systems volume 41 issue 2 pp 277-309,data stream mining;world wide web;data mining;database;machine learning;computer science;
Spatial---temporal compression and recovery in a wireless sensor network in an underground tunnel environment,Bin He (Tongji University);Yonggang Li (Tongji University);Hongwei Huang (Tongji University);Haifeng Tang (Tongji University);,"2655174817,2658575728,2660757215,2665663475","In an exciting new application, wireless sensor networks (WSNs) are increasingly being deployed to monitor the structure health of underground subway tunnels, promising many advantages over traditional monitoring methods. As a result, ensuring efficient data communication, transmission, and storage have become a huge challenge for these systems as they try to cope with ever increasing quantities of data collected by ever growing numbers of sensor nodes. A key approach of managing big data in WSNs is through data compression. Reducing the volume of data traveling between sensor nodes can reduce the high energy cost of data transmission, as well as save space for storage of big data. In this paper, we propose an algorithm for the compression of spatial---temporal data from one data type of sensor node in a WSN deployed in an underground tunnel. The proposed algorithm works efficiently because it considers temporal as well as spatial features of sensor data. A recovery process is required for recovering the data with a close approximation to the original data form nodes. We validate the proposed recovery technique through computational experiments carried out using the data acquired from a real WSN.",2014,Knowledge and Information Systems volume 41 issue 2 pp 449-465,key distribution in wireless sensor networks;visual sensor network;mobile wireless sensor network;wireless sensor network;embedded system;computer security;data mining;computer science;
A hybrid memory built by SSD and DRAM to support in-memory Big Data analytics,Zhiguang Chen (National University of Defense Technology);Yutong Lu (National University of Defense Technology);Nong Xiao (National University of Defense Technology);Fang Liu (National University of Defense Technology);,"2160271175,2705237734,2154015159,2496614950","Big Data requires a shift in traditional computing architecture. The in-memory computing is a new paradigm for Big Data analytics. However, DRAM-based main memory is neither cost-effective nor energy-effective. This work combines flash-based solid state drive (SSD) and DRAM together to build a hybrid memory, which meets both of the two requirements. As the latency of SSD is much higher than that of DRAM, the hybrid architecture should guarantee that most requests are served by DRAM rather than by SSD. Accordingly, we take two measures to enhance the hit ratio of DRAM. First, the hybrid memory employs an adaptive prefetching mechanism to guarantee that data have already been prepared in DRAM before they are demanded. Second, the DRAM employs a novel replacement policy to give higher priority to replace data that are easy to be prefetched because these data can be served by prefetching once they are demanded once again. On the contrary, the data that are hard to be prefetched are protected by DRAM. The prefetching mechanism and replacement policy employed by the hybrid memory rely on access patterns of files. So, we propose a novel pattern recognition method by improving the LZ data compression algorithm to detect access patterns. We evaluate our proposals via prototype and trace-driven simulations. Experimental results demonstrate that the hybrid memory is able to extend the DRAM by more than twice.",2014,Knowledge and Information Systems volume 41 issue 2 pp 335-354,memory rank;in memory processing;registered memory;instruction prefetch;interleaved memory;universal memory;static random access memory;big data;parallel computing;computer hardware;data mining;database;real time computing;computer science;
Security-aware intermediate data placement strategy in scientific cloud workflows,Wei Liu (Wuhan University of Technology);Su Peng (Wuhan University of Technology);Wei Du (Wuhan University of Technology);Wei Wang (Tongji University);Guo Sun Zeng (Tongji University);,"2704837481,2644590900,2646102045,2646626739,2716726186","Massive computation power and storage capacity of cloud computing systems allow scientists to deploy data-intensive applications without the infrastructure investment, where large application datasets can be stored in the cloud. Based on the pay-as-you-go model, data placement strategies have been developed to cost-effectively store large volumes of generated datasets in the scientific cloud workflows. As promising as it is, this paradigm also introduces many new challenges for data security when the users outsource sensitive data for sharing on the cloud servers, which are not within the same trusted domain as the data owners. This challenge is further complicated by the security constraints on the potential sensitive data for the scientific workflows in the cloud. To effectively address this problem, we propose a security-aware intermediate data placement strategy. First, we build a security overhead model to reasonably measure the security overheads incurred by the sensitive data. Second, we develop a data placement strategy to dynamically place the intermediate data for the scientific workflows. Finally, our experimental results show that our strategy can effectively improve the intermediate data security while ensuring the data transfer time during the execution of scientific workflows.",2014,Knowledge and Information Systems volume 41 issue 2 pp 423-447,cloud computing security;aerospace;cloud computing;data security;electronics;world wide web;data mining;database;computer science;
AsyIter: tolerating computational skew of synchronous iterative applications via computing decomposition,Yu Zhang (Huazhong University of Science and Technology);Xiaofei Liao (Huazhong University of Science and Technology);Hai Jin (Huazhong University of Science and Technology);Bing Bing Zhou (University of Sydney);,"2656740672,2117540166,2168417342,2144397146","Iterative computing is pervasive in web applications, data mining and scientific computing. Many parallel algorithms for such applications are synchronous algorithms which need strict synchronization between iterations to ensure their correctness, making the performance sensitive to computational skews in each iteration. Current load balancing approaches may alleviate the effect of computational skew, but cannot completely solve the problem. As a result, for many applications, the skews in each iteration still exist and they are accumulated, seriously affecting the completion time of these applications. In this paper, we propose an effective approach to make synchronous iterative computing applications themselves have the ability to tolerate the negative effects of unresolved computational skews. This approach divides a large computational task in a computing node or worker into a number of sub-tasks which only depend on the states of a few objects from the previous iteration. This allows the sub-tasks in subsequent iterations to proceed in advance whenever the states of related data objects are available. Consequently, the idle time caused by strict synchronization is reduced and the overall performance is thus enhanced. Experimental results show that this approach can improve the overall performance by up to $$2.45\times $$ 2.45 × in comparison with the state-of-the-art approaches.",2014,Knowledge and Information Systems volume 41 issue 2 pp 379-400,theoretical computer science;distributed computing;real time computing;machine learning;computer science;
Data mining-based flatness pattern prediction for cold rolling process with varying operating condition,Ningyun Lu (Nanjing University of Aeronautics and Astronautics);Bin Jiang (Nanjing University of Aeronautics and Astronautics);Jianhua Lu (Southeast University);,"2683834767,2653482835,2689975844","Data-rich environments in modern rolling processes provide a great opportunity for more effective process control and more total quality improvement. Flatness is a key geometrical feature of strip products in a cold rolling process. In order to achieve good flatness, it is necessary to reveal the factors that often influence the flatness quality, to develop a general flatness pattern prediction model that can handle the varying operating condition during the rolling of products with different specifications and to realize an effective flatness feedback control strategy. This paper develops a practical data mining-based flatness pattern prediction method for cold rolling process with varying operating condition. Firstly, the high-dimensional process measurements are projected onto a low-dimensional space (i.e., the latent variable space) using locality preserving projection method; at the same time, the Legendre orthogonal polynomials are used to extract the basic flatness patterns by projecting the high-dimensional flatness measurements into several flatness characteristic coefficients. Secondly, a mixture probabilistic linear regression model is adopted to describe the relationships between the latent variables and the flatness characteristic coefficients. Case study is conducted on a real steel rolling process. Results show that the developed method has not only the satisfactory prediction performance, but good potentials to improve process understanding and strip flatness quality.",2014,Knowledge and Information Systems volume 41 issue 2 pp 355-378,control theory;data mining;computer science;mathematics;
Data-based adaptive online prediction model for plant-wide production indices,Changxin Liu (Northeastern University);Jinliang Ding (Northeastern University);Anthony J. Toprac;Tianyou Chai (Northeastern University);,"2153222237,2692355690,2640578508,2195384296","A data-based adaptive online prediction model is proposed for plant-wide production indices based on support vector regression, a general method which we customized specifically to model very large data sets that are generated dynamically and periodically. The proposed model can update its parameters online according to the statistical properties of the training samples. Further, in order to improve the prediction precision, each sample is weighted with a dynamic penalty factor that considers the effect of each sample on the prediction model accuracy. Moreover, a customized procedure is introduced to handle large training sets. After having been convincingly evaluated on benchmark data, effectiveness and performance of our approach for plant-wide production indices is demonstrated using industrial data from an operating ore dressing plant over a range of scale in training data set size. The higher accuracy and shorter computation times than existing methods suggest that it may prove advantageous in actual application to dynamic production processes.",2014,Knowledge and Information Systems volume 41 issue 2 pp 401-421,aerospace;electronics;support vector machine;data science;data mining;database;artificial intelligence;machine learning;statistics;
Special issue on big data research in China,Nanning Zheng (Xi'an Jiaotong University);Jun Zhang (Sun Yat-sen University);Chenghong Wang (National Science Foundation);,"2656506173,2720078679,2550505085",-,2014,Knowledge and Information Systems volume 41 issue 2 pp 247-249,-
Finding peculiar compositions of two frequent strings with background texts,Daisuke Ikeda (Kyushu University);Einoshin Suzuki (Kyushu University);,"2530697951,2228069075","We consider mining unusual patterns from a set $$T$$ T of target texts. A typical method outputs unusual patterns if their observed frequencies are far from their expectation estimated under an assumed probabilistic model. However, it is difficult for the method to deal with the zero frequency and thus it suffers from data sparseness. We employ another set $$B$$ B of background texts to define a composition $$xy$$ xy to be peculiar if both $$x$$ x and $$y$$ y are more frequent in $$B$$ B than in $$T$$ T and conversely $$xy$$ xy is more frequent in $$T$$ T . $$xy$$ xy is unusual because $$x$$ x and $$y$$ y are infrequent in $$T$$ T while $$xy$$ xy is unexpectedly frequent compared to $$xy$$ xy in $$B$$ B . To find frequent subpatterns and infrequent patterns simultaneously, we develop a fast algorithm using the suffix tree and show that it scales almost linearly under practical settings of parameters. Experiments using DNA sequences show that found peculiar compositions basically appear in rRNA while patterns found by an existing method seem not to relate to specific biological functions. We also show that discovered patterns have similar lengths at which the distribution of frequencies of fixed length substrings begins to skew. This fact explains why our method can find long peculiar compositions.",2014,Knowledge and Information Systems volume 41 issue 2 pp 499-530,text mining;bioinformatics;data mining;algorithm;computer science;mathematics;
Learning to annotate via social interaction analytics,Tong Xu (University of Science and Technology of China);Hengshu Zhu (University of Science and Technology of China);Enhong Chen (University of Science and Technology of China);Baoxing Huai (University of Science and Technology of China);Hui Xiong (Rutgers Business School – Newark and New Brunswick);Jilei Tian (BMW);,"2291800725,2098414524,2136372366,2007497029,2153710278,2098156231","Recent years have witnessed increased interests in exploiting automatic annotating techniques for managing and retrieving media contents. Previous studies on automatic annotating usually rely on the metadata which are often unavailable for use. Instead, multimedia contents usually arouse frequent preference-sensitive interactions in the online social networks of public social media platforms, which can be organized in the form of interaction graph for intensive study. Inspired by this observation, we propose a novel media annotating method based on the analytics of streaming social interactions of media content instead of the metadata. The basic assumption of our approach is that different types of social media content may attract latent social group with different preferences, thus generate different preference-sensitive interactions, which could be reflected as localized dense subgraph with clear preferences. To this end, we first iteratively select nodes from streaming records to build the preference-sensitive subgraphs, then uniformly extract several static and topologic features to describe these subgraphs, and finally integrate these features into a learning-to-rank framework for automatic annotating. Extensive experiments on several real-world date sets clearly show that the proposed approach outperforms the baseline methods with a significant margin.",2014,Knowledge and Information Systems volume 41 issue 2 pp 251-276,social media;aerospace;electronics;learning to rank;multimedia;world wide web;data mining;database;artificial intelligence;machine learning;computer science;
"Email mining: tasks, common techniques, and tools",Guanting Tang (Simon Fraser University);Jian Pei (Simon Fraser University);Wo-Shun Luk (Simon Fraser University);,"2136282317,2126330539,2644286234","Email is one of the most popular forms of communication nowadays, mainly due to its efficiency, low cost, and compatibility of diversified types of information. In order to facilitate better usage of emails and explore business potentials in emailing, various data mining techniques have been applied on email data. In this paper, we present a brief survey of the major research efforts on email mining. To emphasize the differences between email mining and general text mining, we organize our survey on five major email mining tasks, namely spam detection, email categorization, contact analysis, email network property analysis and email visualization. Those tasks are inherently incorporated into various usages of emails. We systematically review the commonly used techniques and also discuss the related software tools available.",2014,Knowledge and Information Systems volume 41 issue 1 pp 1-31,html email;email address harvesting;social network analysis;cluster analysis;biological classification;internet privacy;world wide web;data mining;machine learning;computer science;
Clustering data streams using grid-based synopsis,"Vasudha Bhatnagar (Dept. of Computer Science, University of Delhi);Sharanjit Kaur (University of Delhi);Sharma Chakravarthy (University of Texas at Austin);","2590701236,2136566649,2284350879","Continually advancing technology has made it feasible to capture data online for onward transmission as a steady flow of newly generated data points, termed as data stream. Continuity and unboundedness of data streams make storage of data and multiple scans of data an impractical proposition for the purpose of knowledge discovery. Need to learn structures from data in streaming environment has been a driving force for making clustering a popular technique for knowledge discovery from data streams. Continuous nature of streaming data makes it infeasible to look for point membership among the clusters discovered so far, necessitating employment of a synopsis structure to consolidate incoming data points. This synopsis is exploited for building clustering scheme to meet subsequent user demands. The proposed Exclusive and Complete Clustering (ExCC) algorithm captures non-overlapping clusters in data streams with mixed attributes, such that each point either belongs to some cluster or is an outlier/noise. The algorithm is robust, adaptive to changes in data distribution and detects succinct outliers on-the-fly. It deploys a fixed granularity grid structure as synopsis and performs clustering by coalescing dense regions in grid. Speed-based pruning is applied to synopsis prior to clustering to ensure currency of discovered clusters. Extensive experimentation demonstrates that the algorithm is robust, identifies succinct outliers on-the-fly and is adaptive to change in the data distribution. ExCC algorithm is further evaluated for performance and compared with other contemporary algorithms.",2014,Knowledge and Information Systems volume 41 issue 1 pp 127-152,flame clustering;brown clustering;canopy clustering algorithm;determining the number of clusters in a data set;dbscan;correlation clustering;constrained clustering;data stream clustering;cure data clustering algorithm;affinity propagation;fuzzy clustering;clustering high dimensional data;cluster analysis;consensus clustering;data science;data mining;database;machine learning;computer science;
Improving class probability estimates for imbalanced data,Byron C. Wallace (Brown University);Issa J. Dahabreh (Brown University);,"2723869082,180687455","Obtaining good probability estimates is imperative for many applications. The increased uncertainty and typically asymmetric costs surrounding rare events increase this need. Experts (and classification systems) often rely on probabilities to inform decisions. However, we demonstrate that class probability estimates obtained via supervised learning in imbalanced scenarios systematically underestimate the probabilities for minority class instances, despite ostensibly good overall calibration. To our knowledge, this problem has not previously been explored. We propose a new metric, the stratified Brier score, to capture class-specific calibration, analogous to the per-class metrics widely used to assess the discriminative performance of classifiers in imbalanced scenarios. We propose a simple, effective method to mitigate the bias of probability estimates for imbalanced data that bags estimators independently calibrated over balanced bootstrap samples. This approach drastically improves performance on the minority instances without greatly affecting overall calibration. We extend our previous work in this direction by providing ample additional empirical evidence for the utility of this strategy, using both support vector machines and boosted decision trees as base learners. Finally, we show that additional uncertainty can be exploited via a Bayesian approach by considering posterior distributions over bagged probability estimates.",2014,Knowledge and Information Systems volume 41 issue 1 pp 33-52,alternating decision tree;brier score;support vector machine;calibration;biological classification;data mining;pattern recognition;machine learning;statistics;computer science;mathematics;
Incrementally mining temporal patterns in interval-based databases,Yi-Cheng Chen (Tamkang University);Julia Tzu-Ya Weng (Yuan Ze University);Jun-Zhe Wang (National Chiao Tung University);Chien-Li Chou (National Chiao Tung University);Jiun-Long Huang (National Chiao Tung University);Suh-Yin Lee (National Chiao Tung University);,"2309950950,2675093132,2110273631,2162136626,2148510829,2251835087","In several applications, sequence databases generally update incrementally with time. Obviously, it is impractical and inefficient to re-mine sequential patterns from scratch every time a number of new sequences are added into the database. Some recent studies have focused on mining sequential patterns in an incremental manner; however, most of them only considered patterns extracted from time point-based data. In this paper, we proposed an efficient algorithm, Inc_TPMiner, to incrementally mine sequential patterns from interval-based data. We also employ some optimization techniques to reduce the search space effectively. The experimental results indicate that Inc_TPMiner is efficient in execution time and possesses scalability. Finally, we show the practicability of incremental mining of interval-based sequential patterns on real datasets.",2014,Knowledge and Information Systems volume 46 issue 2 pp 423-448,sequential pattern mining;silicon;algorithm design;data mining;database;machine learning;computer science;
Automatic ranking of retrieval models using retrievability measure,Shariq Bashir (Vienna University of Technology);Andreas Rauber (Vienna University of Technology);,"2116780376,2057632188","Analyzing retrieval model performance using retrievability (maximizing findability of documents) has recently evolved as an important measurement for recall-oriented retrieval applications. Most of the work in this domain is either focused on analyzing retrieval model bias or proposing different retrieval strategies for increasing documents retrievability. However, little is known about the relationship between retrievability and other information retrieval effectiveness measures such as precision, recall, MAP and others. In this study, we analyze the relationship between retrievability and effectiveness measures. Our experiments on TREC chemical retrieval track dataset reveal that these two independent goals of information retrieval, maximizing retrievability of documents and maximizing effectiveness of retrieval models are quite related to each other. This correlation provides an attractive alternative for evaluating, ranking or optimizing retrieval models' effectiveness on a given corpus without requiring any ground truth available (relevance judgments).",2014,Knowledge and Information Systems volume 41 issue 1 pp 189-221,divergence from randomness model;term discrimination;human computer information retrieval;adversarial information retrieval;retrievability;genetic programming;data retrieval;relevance;information retrieval;data mining;pattern recognition;artificial intelligence;machine learning;computer science;
Imprecise prior knowledge incorporating into one-class classification,Lev V. Utkin (Ludwig Maximilian University of Munich);Yulia A. Zhuk;,"2310847206,2048247992","An extension of Campbell and Bennett's novelty detection or one-class classification model incorporating prior knowledge is studied in the paper. The proposed extension relaxes the strong assumption of the empirical probability distribution over elements of a training set and deals with a set of probability distributions produced by prior knowledge about training data. The classification problem is solved by considering extreme points of the probability distribution set or by means of the conjugate duality technique. Special cases of prior knowledge are considered in detail, including the imprecise linear-vacuous mixture model and interval-valued moments of feature values. Numerical experiments show that the proposed models outperform Campbell and Bennett's model for many real and synthetic data.",2014,Knowledge and Information Systems volume 41 issue 1 pp 53-76,one class classification;extreme point;linear programming;data mining;pattern recognition;artificial intelligence;machine learning;statistics;computer science;mathematics;
Improving NCD accuracy by combining document segmentation and document distortion,Ana Granados (Autonomous University of Madrid);Rafael Martínez (Autonomous University of Madrid);David Camacho (Autonomous University of Madrid);Francisco de Borja Rodríguez (Autonomous University of Madrid);,"2154693379,2432164768,2171177052,2138156999","Compression distances have been applied to a broad range of domains because of their parameter-free nature, wide applicability and leading efficacy. However, they have a characteristic that can be a drawback when applied under particular circumstances. Said drawback is that when they are used to compare two very different-sized objects, they do not consider them to be similar even if they are related by a substring relationship. This work focuses on addressing this issue when compression distances are used to calculate similarities between documents. The approach proposed in this paper consists of combining document segmentation and document distortion. On the one hand, it is proposed to use document segmentation to tackle the above mentioned drawback. On the other hand, it is proposed to use document distortion to help compression distances to obtain more reliable similarities. The results show that combining both techniques provides better results than not applying them or applying them separately. The said results are consistent across datasets of diverse nature.",2014,Knowledge and Information Systems volume 41 issue 1 pp 223-245,algorithmic information theory;data compression;data mining;database;pattern recognition;machine learning;statistics;computer science;
Closed motifs for streaming time series classification,Hai-Long Nguyen (Nanyang Technological University);Wee Keong Ng (Nanyang Technological University);Yew-Kwong Woon (Nanyang Technological University);,"2120452001,2120738522,2579311635","A streaming time series is a continuous and unbounded group of chronological observations that are found in many scientific and business applications. Motifs that are frequent subsequences are highly representative for the time series and play an important role in time series mining. Discovering motifs in time series has received much attention during recent years, and several algorithms have been proposed to solve this problem. However, these algorithms can only find motifs with a predefined length, which greatly affects their performance and practicality. Recent algorithms can discover motifs with different lengths, but require multiple scanning of the time series and are thus not applicable to streaming time series. In addition, it is difficult to determine the optimal length of interesting motifs; a suboptimal choice results in missing the key motifs or having too many redundant motifs. To overcome this challenge, we introduce the notion of a $$closed$$ closed motif; a motif is $$closed$$ closed if there is no motif with a longer length having the same number of occurrences. We propose a novel algorithm $$closedMotif$$ closedMotif to discover closed motifs in a single scan for streaming time series. We also use the nearest neighbor classifier with the most distinctive closed motifs to validate their potential in time series classification. Extensive experiments show that our approach can efficiently discover motifs with different lengths. In addition, our closed-motif-based classifier is shown to be more accurate than $$Logical\text{- }Shapelet$$ Logical - Shapelet , a state-of-the-art time series classifier. Finally, we demonstrate the scalability of $$closedMotif$$ closedMotif on several large datasets in diverse domains like video surveillance, sensor networks, and biometrics.",2014,Knowledge and Information Systems volume 41 issue 1 pp 101-125,bioinformatics;data mining;machine learning;computer science;
A dissimilarity function for geospatial polygons,"Deepti Joshi (The Citadel, The Military College of South Carolina);Leen Kiat Soh (University of Nebraska–Lincoln);Ashok Samal (University of Nebraska–Lincoln);Jing Zhang (University of Nebraska–Lincoln);","2303258175,2043041937,2117490995,2423643160","Similarity plays an important role in many data mining tasks and information retrieval processes. Most of the supervised, semi-supervised, and unsupervised learning algorithms depend on using a dissimilarity function that measures the pair-wise similarity between the objects within the dataset. However, traditionally most of the similarity functions fail to adequately treat all the spatial attributes of the geospatial polygons due to the incomplete quantitative representation of structural and topological information contained within the polygonal datasets. In this paper, we propose a new dissimilarity function known as the polygonal dissimilarity function (PDF) that comprehensively integrates both the spatial and the non-spatial attributes of a polygon to specifically consider the density, distribution, and topological relationships that exist within the polygonal datasets. We represent a polygon as a set of intrinsic spatial attributes, extrinsic spatial attributes, and non-spatial attributes. Using this representation of the polygons, PDF is defined as a weighted function of the distance between two polygons in the different attribute spaces. In order to evaluate our dissimilarity function, we compare and contrast it with other distance functions proposed in the literature that work with both spatial and non-spatial attributes. In addition, we specifically investigate the effectiveness of our dissimilarity function in a clustering application using a partitional clustering technique (e.g. $$k$$ k -medoids) using two characteristically different sets of data: (a) Irregular geometric shapes determined by natural processes, i.e., watersheds and (b) semi-regular geometric shapes determined by human experts, i.e., counties.",2014,Knowledge and Information Systems volume 41 issue 1 pp 153-188,polygon;data mining;pattern recognition;machine learning;mathematics;
Mining non-derivable hypercliques,Anna Koufakou (University of Central Florida);,239421741,"A hyperclique (Xiong et al. in Proceedings of the IEEE international conference on data mining, pp 387---394, 2003) is an itemset containing items that are strongly correlated with each other, based on a user-specified threshold. Hypercliques (HCs) have been successfully used in a number of applications, for example, clustering (Xiong et al. in Proceedings of the 4th SIAM international conference on data mining, pp 279---290, 2004) and noise removal (Xiong et al. in IEEE Trans Knowl Data Eng 18(3):304---319, 2006). Even though HC has been shown to respond well to datasets with skewed support distribution and low support threshold, it may still grow very large for dense datasets and lower h-confidence threshold. In this paper, we propose a new pruning method based on combining HCs and non-derivable itemsets (NDIs) (Calders and Goethals in Proceedings of the PKDD international conference on principles of data mining and knowledge discovery, pp 74---85, 2002) in order to substantially reduce the amount of generated HCs. Specifically, we propose a new collection of HCs, called non-derivable hypercliques (NDHCs). The NDHC collection is a lossless representation of HCs, that is, given the itemsets in NDHCs, we can generate the complete HC collection and their support, without additional scanning of the dataset. We present an efficient algorithm to mine all NDHC sets, NDHCMiner, and an algorithm to derive all HC sets and their support from NDHCs, NDHCDeriveAll. We experimentally compare our collection, NDHC with HC, with respect to runtime performance as well as total number of generated sets, using real and artificial data. We also show comparisons with another condensed representation of HCs, maximal hyperclique patterns (MHPs). Our experiments show that the NDHC collection offers substantial advantages over HCs, and even MHPs, especially for dense datasets and lower h-confidence values.",2014,Knowledge and Information Systems volume 41 issue 1 pp 77-99,data science;data mining;database;computer science;
On the difficulty of automatically detecting irony: beyond a simple case of negation,Antonio Reyes (Polytechnic University of Valencia);Paolo Rosso (Polytechnic University of Valencia);,"2100607419,2157200871","It is well known that irony is one of the most subtle devices used to, in a refined way and without a negation marker, deny what is literally said. As such, its automatic detection would represent valuable knowledge regarding tasks as diverse as sentiment analysis, information extraction, or decision making. The research described in this article is focused on identifying key values of components to represent underlying characteristics of this linguistic phenomenon. In the absence of a negation marker, we focus on representing the core of irony by means of three conceptual layers. These layers involve 8 different textual features. By representing four available data sets with these features, we try to find hints about how to deal with this unexplored task from a computational point of view. Our findings are assessed by human annotators in two strata: isolated sentences and entire documents. The results show how complex and subjective the task of automatically detecting irony could be.",2014,Knowledge and Information Systems volume 40 issue 3 pp 595-614,negation;natural language processing;programming language;algorithm;computer science;
Recurrent concepts in data streams classification,João Gama (University of Porto);Petr Kosina (Masaryk University);,"2113857198,2067500809","This work addresses the problem of mining data streams generated in dynamic environments where the distribution underlying the observations may change over time. We present a system that monitors the evolution of the learning process. The system is able to self-diagnose degradations of this process, using change detection mechanisms, and self-repair the decision models. The system uses meta-learning techniques that characterize the domain of applicability of previously learned models. The meta-learner can detect recurrence of contexts, using unlabeled examples, and take pro-active actions by activating previously learned models. The experimental evaluation on three text mining problems demonstrates the main advantages of the proposed system: it provides information about the recurrence of concepts and rapidly adapts decision models when drift occurs.",2014,Knowledge and Information Systems volume 40 issue 3 pp 489-507,concept drift;data stream mining;data mining;artificial intelligence;machine learning;computer science;
Efficient clustering of uncertain data streams,Cheqing Jin (Software Engineering Institute);Jeffrey Xu Yu (The Chinese University of Hong Kong);Aoying Zhou (Software Engineering Institute);Feng Cao (IBM);,"2127683674,2119358208,2111470091,2428634916","Clustering uncertain data streams has recently become one of the most challenging tasks in data management because of the strict space and time requirements of processing tuples arriving at high speed and the difficulty that arises from handling uncertain data. The prior work on clustering data streams focuses on devising complicated synopsis data structures to summarize data streams into a small number of micro-clusters so that important statistics can be computed conveniently, such as Clustering Feature (CF) (Zhang et al. in Proceedings of ACM SIGMOD, pp 103---114, 1996) for deterministic data and Error-based Clustering Feature (ECF) (Aggarwal and Yu in Proceedings of ICDE, 2008) for uncertain data. However, ECF can only handle attribute-level uncertainty, while existential uncertainty, the other kind of uncertainty, has not been addressed yet. In this paper, we propose a novel data structure, Uncertain Feature (UF), to summarize data streams with both kinds of uncertainties: UF is space-efficient, has additive and subtractive properties, and can compute complicated statistics easily. Our first attempt aims at enhancing the previous streaming approaches to handle the sliding-window model by using UF instead of old synopses, inclusive of CluStream (Aggarwal et al. in Proceedings of VLDB, 2003) and UMicro (Aggarwal and Yu in Proceedings of ICDE, 2008). We show that such methods cannot achieve high efficiency. Our second attempt aims at devising a novel algorithm, cluUS , to handle the sliding-window model by using UF structure. Detailed analysis and thorough experimental reports on synthetic and real data sets confirm the advantages of our proposed method.",2014,Knowledge and Information Systems volume 40 issue 3 pp 509-539,data stream clustering;cluster analysis;data science;data mining;database;artificial intelligence;machine learning;statistics;computer science;
An approach of support approximation to discover frequent patterns from concept-drifting data streams based on concept learning,Chao-Wei Li (National Chung Hsing University);Kuen-Fang Jea (National Chung Hsing University);,"2099117483,2105343342","In an online data stream, the composition and distribution of the data may change over time, which is a phenomenon known as concept drift. The occurrence of concept drift can affect considerably the performance of a data stream mining method, especially in relation to mining accuracy. In this paper, we study the problem of mining frequent patterns from transactional data streams in the presence of concept drift, considering the important issue of mining accuracy preservation. In terms of frequent-pattern mining, we give the definitions of concept and concept drift with respect to streaming data; moreover, we present a categorization for concept drift. The concept of streaming data is considered the relationships of frequency between different patterns. Accordingly, we devise approaches to describe the concept concretely and to learn the concept through frequency relationship modeling. Based on concept learning, we propose a method of support approximation for discovering data stream frequent patterns. Our analyses and experimental results have shown that in several studied cases of concept drift, the proposed method not only performs efficiently in terms of time and memory but also preserves mining accuracy well on concept-drifting data streams.",2014,Knowledge and Information Systems volume 40 issue 3 pp 639-671,concept drift;concept mining;data stream mining;concept learning;data science;data mining;machine learning;computer science;
Integration of artificial immune network and K-means for cluster analysis,Ren-Jieh Kuo (National Taiwan University of Science and Technology);S. S. Chen (TSMC);W. C. Cheng (National Taiwan University of Science and Technology);Chieh-Yuan Tsai (Yuan Ze University);,"2637720340,2655835611,2702206696,2168292834","This study is dedicated to propose a cluster analysis algorithm which is integration of artificial immune network (aiNet) and K-means algorithm (aiNetK). Four benchmark data sets, Iris, Wine, Glass, and Breast Cancer, are employed to testify the proposed algorithm. The computational results reveal that aiNetK is superior to particle swam optimization and artificial immune system-related methods.",2014,Knowledge and Information Systems volume 40 issue 3 pp 541-557,artificial immune system;k means clustering;cluster analysis;bioinformatics;artificial intelligence;machine learning;computer science;
Introducing semantic variables in mixed distance measures: Impact on hierarchical clustering,Karina Gibert (Polytechnic University of Catalonia);Aïda Valls;Montserrat Batet (Open University of Catalonia);,"199435517,2101386368,2013827682","Today, it is well known that taking into account the semantic information available for categorical variables sensibly improves the meaningfulness of the final results of any analysis. The paper presents a generalization of mixed Gibert's metrics, which originally handled numerical and categorical variables, to include also semantic variables. Semantic variables are defined as categorical variables related to a reference ontology (ontologies are formal structures to model semantic relationships between the concepts of a certain domain). The superconcept-based distance (SCD) is introduced to compare semantic variables taking into account the information provided by the reference ontology. A benchmark shows the good performance of SCD with respect to other proposals, taken from the literature, to compare semantic features. Mixed Gibert's metrics is generalized incorporating SCD. Finally, two real applications based on touristic data show the impact of the generalized Gibert's metrics in clustering procedures and, in consequence, the impact of taking into account the reference ontology in clustering. The main conclusion is that the reference ontology, when available, can sensibly improve the meaningfulness of the final clusters.",2014,Knowledge and Information Systems volume 40 issue 3 pp 559-593,ontology based data integration;semantic computing;tourism;semantic data model;semantic similarity;domain;similarity;metrics;system;cluster analysis;ontology;data science;data mining;database;machine learning;computer science;
Explication and semantic querying of enterprise information systems,Milan Zdravković (University of Niš);Hervé Panetto (Centre national de la recherche scientifique);Miroslav Trajanović (University of Niš);Alexis Aubry (Centre national de la recherche scientifique);,"2135520437,1914708403,1901950263,2119786686","Many researches show that the ability of independent, heterogeneous enterprises' information systems to interoperate is related to the challenges of making their semantics explicit and formal, so that the messages are not merely exchanged, but interpreted, without ambiguity. In this paper, we present an approach to overcome those challenges by developing a method for explication of the systems' implicit semantics. We define and implement the method for the generation of local ontologies, based on the databases of their systems. In addition, we describe an associated method for the translation between semantic and SQL queries, a process in which implicit semantics of the EIS's databases and explicit semantics of the local ontologies become interrelated. Both methods are demonstrated in the case of creating the local ontology and the semantic querying of OpenERP Enterprise Resource Planning system, for the benefit of the collaborative supply chain planning.",2014,Knowledge and Information Systems volume 40 issue 3 pp 697-724,semantic integration;enterprise information system;ontology;knowledge management;data mining;database;computer science;
Parallel multiple kernel learning: a hybrid alternating direction method of multipliers,Zhen-Yu Chen (Northeastern University);Zhi-Ping Fan (Northeastern University);,"2684431575,2701077090","Multiple kernel learning (MKL) has recently become a hot topic in kernel methods. However, many MKL algorithms suffer from high computational cost. Moreover, standard MKL algorithms face the challenge of the rapid development of distributed computational environment such as cloud computing. In this study, a framework for parallel multiple kernel learning (PMKL) using hybrid alternating direction method of multipliers (H-ADMM) is developed to integrate the MKL algorithms and the multiprocessor system. The global problem with multiple kernel is divided into multiple local problems each of which is optimized in a local processor with a single kernel. An H-ADMM is proposed to make the local processors coordinate with each other to achieve the global optimal solution. The results of computational experiments show that PMKL exhibits high classification accuracy and fast computational speed.",2014,Knowledge and Information Systems volume 40 issue 3 pp 673-696,kernel embedding of distributions;tree kernel;polynomial kernel;radial basis function kernel;kernel method;support vector machine;ensemble learning;theoretical computer science;machine learning;mathematical optimization;computer science;
Efficient processing of streaming updates with archived master data in near-real-time data warehousing,M. Asif Naeem (Auckland University of Technology);Gillian Dobbie (University of Auckland);Gerald Weber (University of Auckland);,"2155091378,2002900660,2120939216","In order to make timely and effective decisions, businesses need the latest information from data warehouse repositories. To keep these repositories up-to-date with respect to end user updates, near-real-time data integration is required. An important phase in near-real-time data integration is data transformation where the stream of updates is joined with disk-based master data. The stream-based algorithm MESHJOIN (Mesh Join) has been proposed to amortize disk access over fast streams. MESHJOIN makes no assumptions about the data distribution. In real-world applications, however, skewed distributions can be found, such as a stream of products sold, where certain products are sold more frequently than the remainder of the products. The question arises is how much does MESHJOIN lose in terms of performance by not adapting to data skew. In this paper we perform a rigorous experimental study analyzing the possible performance improvements while considering typical data distributions. For this purpose we design an algorithm Extended Hybrid Join (X-HYBRIDJOIN) that is complementary to MESHJOIN in that it can adapt to data skew and stores parts of the master data in memory permanently, reducing the disk access overhead significantly. We compare the performance of X-HYBRIDJOIN against the performance of MESHJOIN. We take several precautions to make sure the comparison is adequate and focuses on the utilization of data skew. The experiments show that considering data skew offers substantial room for performance gains that cannot be found in non-adaptive approaches such as MESHJOIN. We also present a cost model for X-HYBRIDJOIN, and based on that cost model, the algorithm is tuned.",2014,Knowledge and Information Systems volume 40 issue 3 pp 615-637,data transformation;data mining;database;real time computing;statistics;computer science;
QoS-aware web service selection with negative selection algorithm,Xinchao Zhao (Beijing University of Posts and Telecommunications);Zichao Wen (Chinese Academy of Sciences);Xingmei Li (North China Electric Power University);,"2705845201,2571065091,2630365654","Web service selection, as an important part of web service composition, has direct influence on the quality of composite service. Many works have been carried out to find the efficient algorithms for quality of service (QoS)-aware service selection problem in recent years. In this paper, a negative selection immune algorithm (NSA) is proposed, and as far as we know, this is the first time that NSA is introduced into web service selection problem. Domain terms and operations of NSA are firstly redefined in this paper aiming at QoS-aware service selection problem. NSA is then constructed to demonstrate how to use negative selection principle to solve this question. Thirdly, an inconsistent analysis between local exploitation and global planning is presented, through which a local alteration of a composite service scheme can transfer to the global exploration correctly. It is a general adjusting method and independent to algorithms. Finally, extensive experimental results illustrate that NSA, especially for NSA with consistency weights adjusting strategy (NSA+), significantly outperforms particle swarm optimization and clonal selection algorithm for QoS-aware service selection problem. The superiority of NSA+ over others is more and more evident with the increase of component tasks and related candidate services.",2014,Knowledge and Information Systems volume 40 issue 2 pp 349-373,negative selection;web service;quality of service;world wide web;data mining;database;artificial intelligence;machine learning;simulation;computer science;
Compressed representations for web and social graphs,Cecilia Hernández (University of Chile);Gonzalo Navarro (University of Chile);,"2167241303,2133226413","Compressed representations have become effective to store and access large Web and social graphs, in order to support various graph querying and mining tasks. The existing representations exploit various typical patterns in those networks and provide basic navigation support. In this paper, we obtain unprecedented results by finding ""dense subgraph"" patterns and combining them with techniques such as node orderings and compact data structures. On those representations, we support out-neighbor and out/in-neighbor queries, as well as mining queries based on the dense subgraphs. First, we propose a compression scheme for Web graphs that reduces edges by representing dense subgraphs with ""virtual nodes""; over this scheme, we apply node orderings and other compression techniques. With this approach, we match the best current compression ratios that support out-neighbor queries (i.e., nodes pointed from a given node), using 1.0---1.8 bits per edge (bpe) on large Web graphs, and retrieving each neighbor of a node in 0.6---1.0 microseconds ( $$\upmu $$ s). When supporting both out- and in-neighbor queries, instead, our technique generally offers the best time when using little space. If the reduced graph, instead, is represented with a compact data structure that supports bidirectional navigation, we obtain the most compact Web graph representations (0.9---1.5 bpe) that support out/in-neighbor navigation; yet, the time per neighbor extracted raises to around 5---20 $$\upmu $$ s. We also propose a compact data structure that represents dense subgraphs without using virtual nodes. It allows us to recover out/in-neighbors and answer other more complex queries on the dense subgraphs identified. This structure is not competitive on Web graphs, but on social networks, it achieves 4---13 bpe and 8---12 $$\upmu $$ s per out/in-neighbor retrieved, which improves upon all existing representations.",2014,Knowledge and Information Systems volume 40 issue 2 pp 279-313,social network;theoretical computer science;combinatorics;data mining;database;machine learning;mathematics;
Constructing Topical Hierarchies in Heterogeneous Networks,Chi Wang (University of Illinois at Urbana–Champaign);Jialu Liu (University of Illinois at Urbana–Champaign);Nihit Desai (University of Illinois at Urbana–Champaign);Marina Danilevsky (University of Illinois at Urbana–Champaign);Jiawei Han (University of Illinois at Urbana–Champaign);,"2461963590,2095732305,2146108112,2046577858,2121939561",-,2014,Knowledge and Information Systems,-
Evaluation and comparison of various indexing schemes in single-channel broadcast communication environment,Jiaofei Zhong (University of Central Missouri);Weili Wu (University of Texas at Dallas);Xiaofeng Gao (Shanghai Jiao Tong University);Yan Shi (University of Wisconsin–Platteville);Xiaodong Yue (University of Central Missouri);,"2147638295,2159309593,2141868871,2303601102,2721986880","Wireless Data Broadcasting is a newly developed data dissemination method for spreading public information to a tremendous number of mobile subscribers. Access Latency and Tuning Time are two main criteria to evaluate the performance of such system. With the help of indexing technology, clients can reduce tuning time significantly by searching indices first and turning to doze mode during waiting period. Different indexing schemes perform differently, so we can hardly compare the efficiency of different indexing schemes. In this paper, we redesigned several most popular indexing schemes for data broadcasting systems, i.e., distributed index, exponential index, hash table, and Huffman tree index. We created a unified communication model and constructed a novel evaluation strategy by using the probability theory to formulate the performance of each scheme theoretically and then conducted simulations to compare their performance by numerical experiments. This is the first work to provide scalable communication environment and accurate evaluation strategies. Our communication model can easily be modified to meet specific requirements. Our comparison model can be used by the service providers to evaluate other indexing schemes to choose the best one for their systems.",2014,Knowledge and Information Systems volume 40 issue 2 pp 375-409,theoretical computer science;world wide web;data mining;database;computer science;
Exploring feature sets for two-phase biomedical named entity recognition using semi-CRFs,Li Yang (Huazhong University of Science and Technology);Yanhong Zhou (Huazhong University of Science and Technology);,"2706088784,2669935493","This paper represents a two-phase approach based on semi-Markov conditional random fields model (semi-CRFs) and explores novel feature sets for identifying the entities in text into 5 types: protein, DNA, RNA, cell_line and cell_type. Semi-CRFs put the label to a segment not a single word which is more natural than the other machine learning methods such as conditional random fields model (CRFs). Our approach divides the biomedical named entity recognition task into two sub-tasks: term boundary detection and semantic labeling. At the first phase, term boundary detection sub-task detects the boundary of the entities and classifies the entities into one type C. At the second phase, semantic labeling sub-task labels the entities detected at the first phase the correct entity type. We explore novel feature sets at both phases to improve the performance. To make a comparison, experiments conducted both on CRFs and on semi-CRFs models at each phase. Our experiments carried out on JNLPBA 2004 datasets achieve an F-score of 74.64 % based on semi-CRFs without deep domain knowledge and post-processing algorithms, which outperforms most of the state-of-the-art systems.",2014,Knowledge and Information Systems volume 40 issue 2 pp 439-453,two phase electric power;conditional random field;data mining;pattern recognition;machine learning;computer science;
GAMer: a synthesis of subspace clustering and dense subgraph mining,Stephan Günnemann (Carnegie Mellon University);Ines Färber (RWTH Aachen University);Brigitte Boden (RWTH Aachen University);Thomas Seidl (RWTH Aachen University);,"316694267,2015027442,2117690819,2140301036","In this work, we propose a new method to find homogeneous object groups in a single vertex-labeled graph. The basic premise is that many prevalent datasets consist of multiple types of information: graph data to represent the relations between objects and attribute data to characterize the single objects. Analyzing both information types simultaneously can increase the expressiveness of the resulting patterns. Our patterns of interest are sets of objects that are densely connected within the associated graph and as well show high similarity regarding their attributes. As for attribute data it is known that full-space clustering often is futile, we have to analyze the similarity of objects regarding subsets of their attributes. In order to take full advantage of all present information, we combine the paradigms of dense subgraph mining and subspace clustering. For our approach, we face several challenges to achieve a sound combination of the two paradigms. We maximize our twofold clusters according to their density, size, and number of relevant dimensions. The optimization of these three objectives usually is conflicting; thus, we realize a trade-off between these characteristics to obtain meaningful patterns. We develop a redundancy model to confine the clustering to a manageable size by selecting only the most interesting clusters for the result set. We prove the complexity of our clustering model and we particularly focus on the exploration of various pruning strategies to design the efficient algorithm GAMer (Graph & Attribute Miner). In thorough experiments on synthetic and real world data we show that GAMer achieves low runtimes and high clustering qualities. We provide all datasets, measures, executables, and parameter settings on our website http://dme.rwth-aachen.de/gamer .",2014,Knowledge and Information Systems volume 40 issue 2 pp 243-278,flame clustering;correlation clustering;fuzzy clustering;cluster analysis;data mining;database;pattern recognition;machine learning;mathematics;
Dual modularity optimization for detecting overlapping communities in bipartite networks,Fatiha Souam;Ali Aïtelhadj;Riadh Baba-Ali (University of Algiers);,"315340077,319891300,2466174795","Many algorithms have been designed to discover community structure in networks. These algorithms are mostly dedicated to detecting disjoint communities. Very few of them are intended to discover overlapping communities, particularly the bipartite networks have hardly been explored for the detection of such communities. In this paper, we describe a new approach which consists in forming overlapping mixed communities in a bipartite network based on dual optimization of modularity. To this end, we propose two algorithms. The first one is an evolutionary algorithm dedicated for global optimization of the Newman's modularity on the line graph. This algorithm has been tested on well-known real benchmark networks and compared with several other existing methods of community detection in networks. The second one is an algorithm that locally optimizes the graph Mancoridis modularity, and we have adapted to a bipartite graph. Specifically, this second algorithm is applied to the decomposition of vertices, resulting from the evolutionary process, and also characterizes the overlapping communities taking into account their semantic aspect. Our approach requires a priori no knowledge on the number of communities searched in the network. We show its interest on two datasets, namely, a group of synthetic networks and real-world network whose structure is also difficult to understand.",2014,Knowledge and Information Systems volume 40 issue 2 pp 455-488,modularity;clique percolation method;bipartite graph;evolutionary algorithm;combinatorics;bioinformatics;data mining;machine learning;computer science;mathematics;
A theoretical model for the automatic generation of tag clouds,Ursula Torres-Parejo (University of Granada);Jesús R. Campaña (University of Granada);M. Amparo Vila (University of Granada);Miguel Delgado (University of Granada);,"2319925363,2131967930,2154173942,2163805578","This paper presents a new approach to information retrieval from non-structured attributes in databases, which involves the processing of text attributes. To make retrieval more effective, frequent text sequences are extracted and mathematically represented as intermediate forms which permit a clearer and more precise definition of operations on texts. These intermediate forms appear to users in the form of tag clouds to facilitate content identification, exploration, and querying. In this sense, tag cloud visualization is a simple, user-friendly visual interface to data. This paper proposes a theoretical model for the representation of frequent text sequences and their operations as well as a general procedure for generating tag clouds from text attributes in databases. The tag clouds thus obtained were compared with conventional tag clouds composed of single terms. Our study showed that automatically generated multi-term tag clouds provide better results than mono-term tag clouds.",2014,Knowledge and Information Systems volume 40 issue 2 pp 315-347,algebra of sets;tag cloud;lattice;semantic search;world wide web;information retrieval;data mining;database;computer science;
Computing and visualizing popular places,Marta Fort (University of Girona);J. Antoni Sellarès (University of Girona);Nacho Valladares (University of Girona);,"2146173109,1976412798,2305750444","Data analysis and knowledge discovery in trajectory databases is an emerging field with a growing number of applications such as managing traffic, planning tourism infrastructures, analyzing professional sport matches or better understanding wildlife. A well-known collection of patterns which can occur for a subset of trajectories of moving objects exists. In this paper, we study the popular places pattern, that is, locations that are visited by many moving objects. We consider two criteria, strong and weak, to establish either the exact number of times that an object has visited a place during its complete trajectory or whether it has visited the place, or not. To solve the problem of reporting popular places, we introduce the popularity map. The popularity of a point is a measure of how many times the moving objects of a set have visited that point. The popularity map is the subdivision, into regions, of a plane where all the points have the same popularity. We propose different algorithms to efficiently compute and visualize popular places, the so-called popular regions and their schematization, by taking advantage of the parallel computing capabilities of the graphics processing units. Finally, we provide and discuss the experimental results obtained with the implementation of our algorithms.",2014,Knowledge and Information Systems volume 40 issue 2 pp 411-437,computational geometry;multimedia;data mining;database;artificial intelligence;simulation;computer science;mathematics;
Ontology alignment design patterns,"François Scharffe (Digital Enterprise Research Institute);Ondřej Zamazal (University of Economics, Prague);Dieter Fensel (University of Innsbruck);","1776636746,1999506056,2303458062","Interoperability between heterogeneous ontological descriptions can be performed through ontology mediation techniques. At the heart of ontology mediation lies the alignment: a specification of correspondences between ontology entities. Ontology matching can bring some automation but are limited to finding simple correspondences. Design patterns have proven themselves useful to capture experience in design problems. In this article, we introduce ontology alignment patterns as reusable templates of recurring correspondences. Based on a detailed analysis of frequent ontology mismatches, we develop a library of common patterns. Ontology alignment patterns can be used to refine correspondences, either by the alignment designer or via pattern detection algorithms. We distinguish three levels of abstraction for ontology alignment representation, going from executable transformation rules, to concrete correspondences between two ontologies, to ontology alignment patterns at the third level. We express patterns using an ontology alignment representation language, making them ready to use in practical mediation tasks. We extract mismatches from vocabularies associated with data sets published as linked open data, and we evaluate the ability of correspondence patterns to provide proper alignments for these mismatches. Finally, we describe an application of ontology alignment patterns for an ontology transformation service.",2014,Knowledge and Information Systems volume 40 issue 1 pp 1-28,ontology components;ontology based data integration;process ontology;upper ontology;owl s;ontology alignment;suggested upper merged ontology;software design pattern;semantic integration;data integration;ontology;information retrieval;data mining;database;computer science;
Reasoning with fuzzy extensions of OWL and OWL 2,Giorgos Stoilos (National Technical University of Athens);Giorgos B. Stamou (National Technical University of Athens);,"170620773,2047892088","Fuzzy Description Logics (f-DLs) have been proposed as logical formalisms capable of representing and reasoning with vague/fuzzy information. They are envisioned to be helpful for many applications that need to cope with such type of information such as multimedia processing, decision making, automatic negotiation and more. Recent results have provided with many tableaux algorithms for supporting reasoning over quite expressive f-DLs. However, no (direct) tableaux algorithm for reasoning with fuzzy extensions of DLs such as $$\mathcal{SHOIQ } $$ and $$\mathcal{SROIQ } $$ exists today. $$\mathcal{SHOIQ } $$ and $$\mathcal{SROIQ } $$ are particularly interesting formalisms as they constitute the logical underpinnings of the Web ontology languages OWL DL and OWL 2 DL. In the current paper, we present an algorithm for reasoning with the fuzzy DLs f- $$\mathcal{SHOIQ } $$ and f- $$\mathcal{SROIQ } $$ . In addition, we also provide a tableaux algorithm for fuzzy nominals, thus providing reasoning support for the fuzzy DL language (we call) f- $$\mathcal{SHO }_\mathtt{{f}}\mathcal{IQ }$$ .",2014,Knowledge and Information Systems volume 40 issue 1 pp 205-242,reason;discrete mathematics;artificial intelligence;algorithm;mathematics;
Group extraction from professional social network using a new semi-supervised hierarchical clustering,Eya Ben Ahmed (Tunis University);Ahlem Nabli (University of Sfax);Faïez Gargouri (University of Sfax);,"2151164718,2041209401,67147395","Recently, social network has been given much attention. This paper addresses the issue of extraction groups from professional social network and enriches the representation of the user profile and its related groups through building a social network warehousing. Several criteria may be applied to detect groups within professional communities, such as the area of expertise, the job openings proposed by the group, the security of the group, and the time of the group creation. In this paper, we aim to find, extract, and fuse the LinkedIn users. Indeed, we deal with the group extraction of LinkedIn users based on their profiles using our innovative semi-supervised clustering method based on quantitative constraints ranking. The encouraging experimental results carried out on our real professional warehouse show the usefulness of our approach.",2014,Knowledge and Information Systems volume 40 issue 1 pp 29-47,group;constraint;social network;cluster analysis;data warehouse;data science;world wide web;data mining;database;machine learning;computer science;
Context-aware multiagent system: Planning home care tasks,Juan A. Fraile (Pontifical University of Salamanca);Yanira De Paz (University of Salamanca);Javier Bajo (Technical University of Madrid);Juan Francisco De Paz (University of Salamanca);Belén Pérez-Lancho (University of Salamanca);,"2123052654,2136177496,2132377945,2096670735,31923850","Context-aware systems are able to capture information from the context in which they are executed, assign a meaning to the gathered information, and change their behavior accordingly. As a result, the systems can offer services to users according to their individual situation within the context. This article analyzes the important aspects of context-aware computing such as capturing information for context attributes and determining the manner of interacting with users in the environment. Used in conjunction with mobile devices, context-aware systems are specifically used to improve the usability of applications and services. This article proposes the home care context-aware computing (HoCCAC) multiagent system that identifies and maintains a permanent fix on the location of patients in their home, and manages the infrastructure of services within their environment securely and reliably by processing and reasoning the data received. Based on the multiagent system, a prototype was developed to monitor patients in their home. The HoCCAC multiagent system uses a critical path method-based planning model that, in the present study, prepares the most optimal task-planning schedule for the patients in their home, is capable of reacting automatically when faced with dangerous or emergency situations, replanning any plans in progress and sending alert messages to the system. The results obtained with this prototype are presented in this article.",2014,Knowledge and Information Systems volume 40 issue 1 pp 171-203,knowledge management;computer security;data mining;database;artificial intelligence;simulation;
Comparison of component frameworks for real-time embedded systems,Tomáš Pop (Charles University in Prague);Petr Hnĕtynka (Charles University in Prague);Petr Hošek (Imperial College London);Michal Malohlava (Purdue University);Tomáš Bureš (Charles University in Prague);,"2129154468,2313741696,2088000110,1899311300,2091830199","The use of components significantly helps in development of real-time embedded systems. There have been a number of component frameworks developed for this purpose, and some of them have already became well established in this area. Even though these frameworks share the general idea of component-based development, they significantly differ in the range of supported features and maturity. This makes it relatively difficult to select the right component framework and thus poses a significant obstacle in adoption of the component-based development approach for developing real-time embedded systems. To provide guidance in choosing a component framework, or at least relevant concepts when building a custom framework, we present a survey, which illustrates distinguishing features and provides comparison of selected modern component-based frameworks for real-time embedded systems. Compared to other existing surveys, this survey focuses specifically on criteria connected with real-time and embedded systems. Further, to be practically relevant, we restrict the survey only to the frameworks that support the full development life cycle (i.e. from design till execution support). In this context, the survey illustrates the complexity of development in each framework by giving specification and code samples.",2014,Knowledge and Information Systems volume 40 issue 1 pp 127-170,common component architecture;component based software engineering;real time computing;simulation;computer science;
Reliable yet flexible software through formal model transformation (rule definition),Abbas Rasoolzadegan (Amirkabir University of Technology);Ahmad Abdollahzadeh Barforoush (Amirkabir University of Technology);,"17427687,2066115691","The results of literature review show that the need for both reliability and flexibility is increasingly becoming important among the various classes of software applications. Developing reliable yet flexible software is a hard problem. Although modeling methods enjoy a lot of advantages, the use of just one of them, in many cases, may not guarantee the development of reliable and flexible software. Formal modeling methods ensure reliability. However, lack of knowledge and high cost practically force developers to use semi-formal methods instead. Semi-formal (visual) modeling methods, which are widely used in practical large-scale software development, are not good enough for reliable software development. This paper proposes a new practical approach to the development of reliable yet flexible software. In the proposed approach, formal (Object-Z) and semi-formal (UML) models are transformed into each other using a set of bidirectional formal rules. Formal modeling and refinement ensure the reliability of software. Visual models facilitate the interactions among stakeholders who are not familiar enough with the complex mathematical concepts of formal methods. Visual models help detect the unexpected behavior and inconsistencies of software. Applying design patterns to visual models improves the flexibility of software. The transformation of formal and visual models into each other through the iterative and evolutionary process, proposed in this paper, helps develop the software applications that need to be highly reliable yet flexible. The feasibility of the proposed approach is evaluated using the multi-lift case study.",2014,Knowledge and Information Systems volume 40 issue 1 pp 79-126,goal driven software development process;software design description;package development process;software reliability testing;avionics software;software walkthrough;software construction;software sizing;software design pattern;verification and validation;software metric;software framework;unified modeling language;software design;software development process;software development;reliability;formal methods;data mining;database;statistics;computer science;
A lightweight privacy preserving SMS-based recommendation system for mobile users,Luca Becchetti (Sapienza University of Rome);Lorenzo Bergamini (Sapienza University of Rome);Ugo Maria Colesanti (Sapienza University of Rome);Luca Filipponi (Sapienza University of Rome);Giuseppe Persiano (University of Salerno);Andrea Vitaletti (Sapienza University of Rome);,"2095497563,2144883509,1770595060,282761747,2291003365,1644138561","In this paper, we propose a fully decentralized approach for recommending new contacts in the social network of mobile phone users. With respect to existing solutions, our approach is characterized by some distinguishing features. In particular, the application we propose does not assume any centralized coordination: It transparently collects and processes user information that is accessible in any mobile phone, such as the log of calls, the list of contacts or the inbox/outbox of short messages and exchanges it with other users. This information is used to recommend new friendships to other users. Furthermore, the information needed to perform recommendation is collected and exchanged between users in a privacy preserving way. Finally, information necessary to implement the application is exchanged transparently and opportunistically, by using the residual space in standard short messages occasionally exchanged between users. As a consequence, we do not ask users to change their habits in using SMS.",2014,Knowledge and Information Systems volume 40 issue 1 pp 49-77,short message service;privacy;internet privacy;world wide web;computer security;data mining;database;computer science;
Bounded matrix factorization for recommender system,Ramakrishnan Kannan (Georgia Institute of Technology);Mariya Ishteva (Vrije Universiteit Brussel);Haesun Park (Georgia Institute of Technology);,"2630633991,2078237305,2123241397","Matrix factorization has been widely utilized as a latent factor model for solving the recommender system problem using collaborative filtering. For a recommender system, all the ratings in the rating matrix are bounded within a pre-determined range. In this paper, we propose a new improved matrix factorization approach for such a rating matrix, called Bounded Matrix Factorization (BMF), which imposes a lower and an upper bound on every estimated missing element of the rating matrix. We present an efficient algorithm to solve BMF based on the block coordinate descent method. We show that our algorithm is scalable for large matrices with missing elements on multicore systems with low memory. We present substantial experimental results illustrating that the proposed method outperforms the state of the art algorithms for recommender system such as stochastic gradient descent, alternating least squares with regularization, SVD++ and Bias-SVD on real-world datasets such as Jester, Movielens, Book crossing, Online dating and Netflix.",2014,Knowledge and Information Systems volume 39 issue 3 pp 491-511,low rank approximation;block;non negative matrix factorization;matrix decomposition;recommender system;theoretical computer science;machine learning;mathematical optimization;computer science;mathematics;
Mood sensing from social media texts and its applications,Thin Nguyen (Deakin University);Dinh Q. Phung (Deakin University);Brett Adams (Curtin University);Svetha Venkatesh (Deakin University);,"2117594642,2314522249,2163867018,2146461601","We present a large-scale mood analysis in social media texts. We organise the paper in three parts: (1) addressing the problem of feature selection and classification of mood in blogosphere, (2) we extract global mood patterns at different level of aggregation from a large-scale data set of approximately 18 millions documents (3) and finally, we extract mood trajectory for an egocentric user and study how it can be used to detect subtle emotion signals in a user-centric manner, supporting discovery of hyper-groups of communities based on sentiment information. For mood classification, two feature sets proposed in psychology are used, showing that these features are efficient, do not require a training phase and yield classification results comparable to state of the art, supervised feature selection schemes; on mood patterns, empirical results for mood organisation in the blogosphere are provided, analogous to the structure of human emotion proposed independently in the psychology literature; and on community structure discovery, sentiment-based approach can yield useful insights into community formation.",2014,Knowledge and Information Systems volume 39 issue 3 pp 667-702,multimedia;data mining;
Controlled permutations for testing adaptive learning models,Indrė Žliobaitė (Helsinki Institute for Information Technology);,2640465377,"We study evaluation of supervised learning models that adapt to changing data distribution over time (concept drift). The standard testing procedure that simulates online arrival of data (test-then-train) may not be sufficient to generalize about the performance, since that single test concludes how well a model adapts to this fixed configuration of changes, while the ultimate goal is to assess the adaptation to changes that happen unexpectedly. We propose a methodology for obtaining datasets for multiple tests by permuting the order of the original data. A random permutation is not suitable, as it makes the data distribution uniform over time and destroys the adaptive learning task. Therefore, we propose three controlled permutation techniques that make it possible to acquire new datasets by introducing restricted variations in the order of examples. The control mechanisms with theoretical guarantees of preserving distributions ensure that the new sets represent close variations of the original learning task. Complementary tests on such sets allow to analyze sensitivity of the performance to variations in how changes happen and this way enrich the assessment of adaptive supervised learning models.",2014,Knowledge and Information Systems volume 39 issue 3 pp 565-578,concept drift;permutation;evaluation;data stream mining;theoretical computer science;data mining;database;artificial intelligence;machine learning;statistics;computer science;mathematics;
Design of viral marketing strategies for product cross-sell through social networks,Ramasuri Narayanam (IBM);Amit Anil Nanavati (IBM);,"1406309357,2170185937","In this paper, we introduce a novel and generalized version of the influence maximization problem in social networks, which we call as Budgeted Influence Maximization with Cross-sell of Products (B-IMCP), and it considers simultaneously the following three practical aspects: (i) Often cross-sell among products is possible, (ii) Product-specific costs (and benefits) for promoting the products have to be considered, and (iii) Since a company often has budget constraints, the initial seeds have to be chosen within a given budget. In particular, we consider that the cross-sell relationships among the products of a single company are given by an arbitrary bipartite graph. We explore two variants of cross-sell, one weak and one strong, and also assume product-specific costs and benefits. This leads to two different versions of the B-IMCP problem. Given a fixed budget, one of the key issues associated with each version of the B-IMCP problem is to choose the initial seeds within this budget not only for the individual products, but also for promoting cross-sell phenomenon among these products. The following are the specific contributions of this paper: (i) We propose a novel influence propagation model to capture both the cross-sell phenomenon and the costs---benefits for the products; (ii) For each version of the B-IMCP problem, we note that the problem turns out to be NP-hard, and then, we present a simple greedy approximation algorithm for the same. We derive the approximation ratio of this greedy algorithm by drawing upon certain key results from the theory of matroids; (iii) We then outline three heuristics based on well-known concepts from the sociology literature; and (iv) Finally, we experimentally compare and contrast the proposed algorithms and heuristics using certain well-known social network data sets such as WikiVote trust network, Epinions, and Telco call detail records data. Based on the experiments, we consistently found that the stronger the cross-sell relationship between the products, the larger the overlap between the seeds of these products and lesser the distances among the corresponding non-overlapping seeds.",2014,Knowledge and Information Systems volume 39 issue 3 pp 609-641,matroid;cross selling;viral marketing;greedy algorithm;cost benefit analysis;social network;management science;data mining;artificial intelligence;machine learning;mathematical optimization;statistics;computer science;mathematics;
Probabilistic opposition-based particle swarm optimization with velocity clamping,Farrukh Shahzad (National University of Computer and Emerging Sciences);Sohail Masood (National University of Computer and Emerging Sciences);Naveed Kazim Khan;,"2119133870,2148662197,2579716511","A probabilistic opposition-based Particle Swarm Optimization algorithm with Velocity Clamping and inertia weights (OvcPSO) is designed for function optimization--to accelerate the convergence speed and to optimize solution's accuracy on standard benchmark functions. In this work, probabilistic opposition-based learning for particles is incorporated with PSO to enhance the convergence rate--it uses velocity clamping and inertia weights to control the position, speed and direction of particles to avoid premature convergence. A comprehensive set of 58 complex benchmark functions including a wide range of dimensions have been used for experimental verification. It is evident from the results that OvcPSO can deal with complex optimization problems effectively and efficiently. A series of experiments have been performed to investigate the influence of population size and dimensions upon the performance of different PSO variants. It also outperforms FDR-PSO, CLPSO, FIPS, CPSO-H and GOPSO on various benchmark functions. Last but not the least, OvcPSO has also been compared with opposition-based differential evolution (ODE); it outperforms ODE on lower swarm population and higher-dimensional functions.",2014,Knowledge and Information Systems volume 39 issue 3 pp 703-737,multi swarm optimization;particle swarm optimization;swarm intelligence;artificial intelligence;machine learning;mathematical optimization;computer science;mathematics;
Remodeling the network for microgroup detection on microblog,Xiaobing Xiong (Beihang University);Gang Zhou;Xiang Niu (Beihang University);Yongzhong Huang;Ke Xu (Beihang University);,"2104286112,2627814038,2635613788,2622398117,2600070012","In this paper, we focus on the problem of community detection on Sina weibo, the most popular microblogging system in China. By characterizing the structure and content of microgroup (community) on Sina weibo in detail, we observe that different from ordinary social networks, the degree assortativity coefficients are negative on most microgroups. In addition, we find that users from the same microgroup tend to share some common attributes (e.g., followers, tags) and interests extracted from their published posts. Inspired by these new findings, we propose a united method to remodel the network for microgroup detection while maintaining the information of link structure and user content. Firstly, the link direction is concerned by assigning greater weight values to more surprising links, while the content similarity is measured by the Jaccard coefficient of common features and interest similarity based on Latent Dirichlet Allocation model. Then, both link direction and content similarity between two users are uniformly converted to the edge weight of a new remodeled network, which is undirected and weighted. Finally, multiple frequently used community detection algorithms that support weighted networks could be employed. Extensive experiments on real-world social networks show that both link structure and user content play almost equally important roles in microgroup detection on Sina weibo. Our method outperforms the traditional methods with average accuracy improvement up to 39 %, and the number of unrecognized users decreased by about 75 %.",2014,Knowledge and Information Systems volume 39 issue 3 pp 643-665,microblogging;internet privacy;world wide web;data mining;database;artificial intelligence;machine learning;computer science;mathematics;
Finding maximal homogeneous clique sets,Pierre-Nicolas Mougel (University of Lyon);Christophe Rigotti (University of Lyon);Marc Plantevit (University of Lyon);Olivier Gandrillon (French Institute for Research in Computer Science and Automation);,"1988545203,2124275409,107359601,131434759","Many datasets can be encoded as graphs with sets of labels associated with the vertices. We consider this kind of graphs and we propose to look for patterns called maximal homogeneous clique sets, where such a pattern is a subgraph that is structured in several large cliques and where all vertices share enough labels. We present an algorithm based on graph enumeration to compute all patterns satisfying user-defined constraints on the number of separated cliques, on the size of these cliques, and on the number of labels shared by all the vertices. Our approach is tested on real datasets based on a social network of scientific collaborations and on a biological network of protein---protein interactions. The experiments show that the patterns are useful to exhibit subgraphs organized in several core modules of interactions. Performances are reported on real data and also on synthetic ones, showing that the approach can be applied on different kinds of large datasets.",2014,Knowledge and Information Systems volume 39 issue 3 pp 579-608,clique sum;intersection number;k tree;split graph;clique graph;chordal graph;clique;interaction network;protein protein interaction;independent set;gene expression;discrete mathematics;combinatorics;bioinformatics;mathematics;
The correctness problem: evaluating the ordering of binary features in rankings,"Kashif Javed (University of Engineering and Technology, Lahore);Mehreen Saeed (National University of Computer and Emerging Sciences);Haroon Atique Babri (University of Engineering and Technology, Lahore);","2055069275,2153626467,2590691506","In machine learning, feature ranking (FR) algorithms are used to rank features by relevance to the class variable. FR algorithms are mostly investigated for the feature selection problem and less studied for the problem of ranking. This paper focuses on the latter. A question asked about the problem of ranking given in the terminology of FR is: as different FR criteria estimate the relationship between a feature and the class variable differently on a given data, can we determine which criterion better captures the ""true"" feature-to-class relationship and thus generates the most ""correct"" order of individual features? This is termed as the ""correctness"" problem. It requires a reference ordering against which the ranks assigned to features by a FR algorithm are directly compared. The reference ranking is generally unknown for real-life data. In this paper, we show through theoretical and empirical analysis that for two-class classification tasks represented with binary data, the ordering of binary features based on their individual predictive powers can be used as a benchmark. Thus, allowing us to test how correct is the ordering of a FR algorithm. Based on these ideas, an evaluation method termed as FR evaluation strategy (FRES) is proposed. Rankings of three different FR criteria (relief, mutual information, and the diff-criterion) are investigated on five artificially generated and four real-life binary data sets. The results indicate that FRES works equally good for synthetic and real-life data and the diff-criterion generates the most correct orderings for binary data.",2014,Knowledge and Information Systems volume 39 issue 3 pp 543-563,data mining;pattern recognition;machine learning;statistics;mathematics;
Intent mining in search query logs for automatic search script generation,Chieh-Jen Wang (National Taiwan University);Hsin-Hsi Chen (National Taiwan University);,"2108622997,2102355148","Capturing users' information needs is essential in decreasing the barriers in information access. This paper mines sequences of actions called search scripts from search query logs which keep large-scale users' search experiences. Search scripts can be applied to guide users to satisfy their information needs, improve the search effectiveness of retrieval systems, recommend advertisements at suitable places, and so on. Information quality, query ambiguity, topic diversity, and document relevancy are four major challenging issues in search script mining. In this paper, we determine the relevance of URLs for a query, adopt the Open Directory Project (ODP) categories to disambiguate queries and URLs, explore various features and clustering algorithms for intent clustering, identify critical actions from each intent cluster to form a search script, generate a nature language description for each action, and summarize a topic for each search script. Experiments show that the complete link hierarchical clustering algorithm with the features of query terms, relevant URLs, and disambiguated ODP categories performs the best. Applying the intent clusters created by the best model to intent boundary identification achieves an $$F$$ score of 0.6666. The intent clusters then are applied to generate search scripts.",2014,Knowledge and Information Systems volume 39 issue 3 pp 513-542,search analytics;concept search;web search query;web query classification;query expansion;search engine;semantic search;data science;world wide web;data mining;database;computer science;
Similarity measures for OLAP sessions,Julien Aligon (François Rabelais University);Matteo Golfarelli (University of Bologna);Patrick Marcel (François Rabelais University);Stefano Rizzi (University of Bologna);Elisa Turricchia (University of Bologna);,"95884971,203998954,2137225863,2152794630,1755492","OLAP queries are not normally formulated in isolation, but in the form of sequences called OLAP sessions. Recognizing that two OLAP sessions are similar would be useful for different applications, such as query recommendation and personalization; however, the problem of measuring OLAP session similarity has not been studied so far. In this paper, we aim at filling this gap. First, we propose a set of similarity criteria derived from a user study conducted with a set of OLAP practitioners and researchers. Then, we propose a function for estimating the similarity between OLAP queries based on three components: the query group-by set, its selection predicate, and the measures required in output. To assess the similarity of OLAP sessions, we investigate the feasibility of extending four popular methods for measuring similarity, namely the Levenshtein distance, the Dice coefficient, the tf–idf weight, and the Smith–Waterman algorithm. Finally, we experimentally compare these four extensions to show that the Smith–Waterman extension is the one that best captures the users’ criteria for session similarity.",2014,Knowledge and Information Systems volume 39 issue 2 pp 463-489,online analytical processing;information retrieval;data mining;database;computer science;
An efficient orientation distance–based discriminative feature extraction method for multi-classification,Bo Liu (Guangdong University of Technology);Yanshan Xiao (Guangdong University of Technology);Philip S. Yu (University of Illinois at Chicago);Zhifeng Hao (Guangdong University of Technology);Longbing Cao (Information Technology University);,"2504109675,2138906516,2125104194,2643446293,2115085568","Feature extraction is an important step before actual learning. Although many feature extraction methods have been proposed for clustering, classification and regression, very limited work has been done on multi-class classification problems. This paper proposes a novel feature extraction method, called orientation distance–based discriminative (ODD) feature extraction, particularly designed for multi-class classification problems. Our proposed method works in two steps. In the first step, we extend the Fisher Discriminant idea to determine an appropriate kernel function and map the input data with all classes into a feature space where the classes of the data are well separated. In the second step, we put forward two variants of ODD features, i.e., one-vs-all-based ODD and one-vs-one-based ODD features. We first construct hyper-plane (SVM) based on one-vs-all scheme or one-vs-one scheme in the feature space; we then extract one-vs-all-based or one-vs-one-based ODD features between a sample and each hyper-plane. These newly extracted ODD features are treated as the representative features and are thereafter used in the subsequent classification phase. Extensive experiments have been conducted to investigate the performance of one-vs-all-based and one-vs-one-based ODD features for multi-class classification. The statistical results show that the classification accuracy based on ODD features outperforms that of the state-of-the-art feature extraction methods.",2014,Knowledge and Information Systems volume 39 issue 2 pp 409-433,feature;multiclass classification;support vector machine;feature;feature extraction;k nearest neighbors algorithm;data mining;pattern recognition;machine learning;computer science;mathematics;
Sparse regression mixture modeling with the multi-kernel relevance vector machine,Konstantinos Blekas (University of Ioannina);Aristidis Likas (University of Ioannina);,"2297036864,2080877090","A regression mixture model is proposed where each mixture component is a multi-kernel version of the Relevance Vector Machine (RVM). This mixture model exploits the enhanced modeling capability of RVMs, due to their embedded sparsity enforcing properties. In order to deal with the selection problem of kernel parameters, a weighted multi-kernel scheme is employed, where the weights are estimated during training. The mixture model is trained using the maximum a posteriori approach, where the Expectation Maximization (EM) algorithm is applied offering closed form update equations for the model parameters. Moreover, an incremental learning methodology is also presented that tackles the parameter initialization problem of the EM algorithm along with a BIC-based model selection methodology to estimate the proper number of mixture components. We provide comparative experimental results using various artificial and real benchmark datasets that empirically illustrate the efficiency of the proposed mixture model.",2014,Knowledge and Information Systems volume 39 issue 2 pp 241-264,mixture model;data mining;pattern recognition;machine learning;statistics;computer science;
Peer-to-peer information retrieval using shared-content clustering,Irad Ben-Gal (Tel Aviv University);Yuval Shavitt (Tel Aviv University);Ela Weinsberg (Tel Aviv University);Udi Weinsberg (Technicolor);,"2068795980,2089766664,215671502,90917981","Peer-to-peer (p2p) networks are used by millions for searching and downloading content. Recently, clustering algorithms were shown to be useful for helping users find content in large networks. Yet, many of these algorithms overlook the fact that p2p networks follow graph models with a power-law node degree distribution. This paper studies the obtained clusters when applying clustering algorithms on power-law graphs and their applicability for finding content. Driven by the observed deficiencies, a simple yet efficient clustering algorithm is proposed, which targets a relaxed optimization of a minimal distance distribution of each cluster with a size balancing scheme. A comparative analysis using a song-similarity graph collected from 1.2 million Gnutella users reveals that commonly used efficiency measures often overlook search and recommendation applicability issues and provide the wrong impression that the resulting clusters are well suited for these tasks. We show that the proposed algorithm performs well on various measures that are well suited for the domain.",2014,Knowledge and Information Systems volume 39 issue 2 pp 383-408,canopy clustering algorithm;correlation clustering;constrained clustering;data stream clustering;cure data clustering algorithm;affinity propagation;fuzzy clustering;clustering high dimensional data;cluster analysis;recommender system;world wide web;data mining;database;machine learning;computer science;
Combining compound and single terms under language model framework,Arezki Hammache;Mohand Boughanem (University of Toulouse);Rachid Ahmed-Ouamer;,"2071292001,117014290,54975763","Most existing Information Retrieval model including probabilistic and vector space models are based on the term independence hypothesis. To go beyond this assumption and thereby capture the semantics of document and query more accurately, several works have incorporated phrases or other syntactic information in IR, such attempts have shown slight benefit, at best. Particularly in language modeling approaches this extension is achieved through the use of the bigram or n-gram models. However, in these models all bigrams/n-grams are considered and weighted uniformly. In this paper we introduce a new approach to select and weight relevant n-grams associated with a document. Experimental results on three TREC test collections showed an improvement over three strongest state-of-the-art model baselines, which are the original unigram language model, the Markov Random Field model, and the positional language model.",2014,Knowledge and Information Systems volume 39 issue 2 pp 329-349,divergence from randomness model;vector space model;language model;natural language processing;speech recognition;information retrieval;data mining;database;artificial intelligence;machine learning;computer science;
Robust sparsity-preserved learning with application to image visualization,Haixian Wang (Southeast University);Wenming Zheng (Southeast University);,"2632268431,2713915796","Linear subspace learning is of great importance for the purpose of visualization of high-dimensional observations. Sparsity-preserved learning (SPL) is a recently developed technique for linear subspace learning. Its objective function is formulated by using the \(\ell _2\)-norm, which implies that the obtained projection vectors are likely to be distorted by outliers. In this paper, we develop a new SPL algorithm called SPL-L1 based on the \(\ell _1\)-norm instead of the \(\ell _2\)-norm. The proposed approach seeks projection vectors by minimizing a reconstruction error subject to a constraint of samples dispersion, both of which are defined using the \(\ell _1\)-norm. As a robust alternative, SPL-L1 works well in the presence of atypical samples. We design an iterative algorithm under the framework of bound optimization to solve the projection vectors of SPL-L1. The experiments on image visualization demonstrate the superiority of the proposed method.",2014,Knowledge and Information Systems volume 39 issue 2 pp 287-304,pattern recognition;machine learning;mathematical optimization;mathematics;
Learning and clustering of fuzzy cognitive maps for travel behaviour analysis,Maikel León (University of Hasselt);Lusine Mkrtchyan (European Committee for Standardization);Benoît Depaire (University of Hasselt);Da Ruan (European Committee for Standardization);Koen Vanhoof (University of Hasselt);,"2163717982,2559891436,141832950,2126395520,2242613764","In modern society, more and more attention is given to the increase in public transportation or bike use. In this regard, one of the most important issues is to find and analyse the factors influencing car dependency and the attitudes of people in terms of preferred transport mode. Although the individuals’ transport behavioural modelling is a complex task, it has a notable social and economic impact. Thus, in this paper, fuzzy cognitive maps are explored to represent the behaviour and operation of such complex systems. This soft-computing technique allows modelling how the travellers make decisions based on their knowledge of different transport modes properties at different levels of abstraction. These levels correspond to the hierarchy perception including different scenarios of travelling, different benefits of choosing a specific travel mode, and different situations and attributes related to those benefits. We use learning and clustering of fuzzy cognitive maps to describe travellers’ behaviour and change trends in different abstraction levels. Cluster estimations are done before and after the learning of the maps, in order to compare people’s way of thinking if only considering an initial view of a transport mode decision for a daily activity, and when they really have a deeper reasoning process in view of benefits and consequences. The results of this study will help transportation policy decision makers in better understanding of people’s needs and consequently will help them actualizing different policy formulations and implementations.",2014,Knowledge and Information Systems volume 39 issue 2 pp 435-462,fuzzy cognitive map;cluster analysis;artificial intelligence;machine learning;simulation;computer science;
Analyzing topics and authors in chat logs for crime investigation,Abdur Rahman M. A. Basher (Concordia University Wisconsin);Benjamin C. M. Fung (Concordia University Wisconsin);,"2509769954,2015549731","Cybercriminals have been using the Internet to accomplish illegitimate activities and to execute catastrophic attacks. Computer-Mediated Communication such as online chat provides an anonymous channel for predators to exploit victims. In order to prosecute criminals in a court of law, an investigator often needs to extract evidence from a large volume of chat messages. Most of the existing search tools are keyword-based, and the search terms are provided by an investigator. The quality of the retrieved results depends on the search terms provided. Due to the large volume of chat messages and the large number of participants in public chat rooms, the process is often time-consuming and error-prone. This paper presents a topic search model to analyze archives of chat logs for segregating crime-relevant logs from others. Specifically, we propose an extension of the Latent Dirichlet Allocation-based model to extract topics, compute the contribution of authors in these topics, and study the transitions of these topics over time. In addition, we present a special model for characterizing authors-topics over time. This is crucial for investigation because it provides a view of the activity in which authors are involved in certain topics. Experiments on two real-life datasets suggest that the proposed approach can discover hidden criminal topics and the distribution of authors to these topics.",2014,Knowledge and Information Systems volume 39 issue 2 pp 351-381,topic model;gibbs sampling;data science;world wide web;data mining;database;artificial intelligence;machine learning;statistics;computer science;
A sparse \({\varvec{L}}_{2}\)-regularized support vector machines for efficient natural language learning,Yu-Chieh Wu (Ming Chuan University);,2681822611,"Linear kernel support vector machines (SVMs) using either \(L_{1}\)-norm or \(L_{2}\)-norm have emerged as an important and wildly used classification algorithm for many applications such as text chunking, part-of-speech tagging, information retrieval, and dependency parsing. \(L_{2}\)-norm SVMs usually provide slightly better accuracy than \(L_{1}\)-SVMs in most tasks. However, \(L_{2}\)-norm SVMs produce too many near-but-nonzero feature weights that are highly time-consuming when computing nonsignificant weights. In this paper, we present a cutting-weight algorithm to guide the optimization process of the \(L_{2}\)-SVMs toward a sparse solution. Before checking the optimality, our method automatically discards a set of near-but-nonzero feature weight. The final objects can then be achieved when the objective function is met by the remaining features and hypothesis. One characteristic of our cutting-weight algorithm is that it requires no changes in the original learning objects. To verify this concept, we conduct the experiments using three well-known benchmarks, i.e., CoNLL-2000 text chunking, SIGHAN-3 Chinese word segmentation, and Chinese word dependency parsing. Our method achieves 1–10 times feature parameter reduction rates in comparison with the original \(L_{2}\)-SVMs, slightly better accuracy with a lower training time cost. In terms of run-time efficiency, our method is reasonably faster than the original \(L_{2}\)-regularized SVMs. For example, our sparse \(L_{2}\)-SVMs is 2.55 times faster than the original \(L_{2}\)-SVMs with the same accuracy.",2014,Knowledge and Information Systems volume 39 issue 2 pp 305-328,dependency grammar;regularization;support vector machine;natural language processing;data mining;pattern recognition;machine learning;computer science;
Belief propagation algorithms for finding the probable configurations over factor graph models,Zheng Wang (Huazhong University of Science and Technology);Yunsheng Liu (Huazhong University of Science and Technology);Guangwei Wang;,"2648550004,2674650903,2690063529","In this article, we study the belief propagation algorithms for solving the multiple probable configurations (MPC) problem over graphical models. Based on the loopy max-product methodology, we first develop an iterative belief propagation mechanism (IBPM), which aims to find the most probable configurations facing with the existence of multiple solutions. In applications ranging from low-density parity-check codes to combinatorial optimization one would like to find not just the best configurations but rather than the summary of all possible explanations. Not only can this problem be solved by our proposed loopy message-passing algorithm (LMPA), we also prove that, for tree factor graph models, this LMPA guarantees fast convergence. Moveover, we subsequently present a low-complexity approach to simplifying the message integration operation throughout the whole belief propagation circulation. Simulations built on various settings demonstrate that both IBPM and LMPA can accurately and rapidly approximate the MPC in acyclic graph with hundreds of variables.",2014,Knowledge and Information Systems volume 39 issue 2 pp 265-285,factor graph;belief propagation;theoretical computer science;combinatorics;machine learning;computer science;mathematics;
Modeling and exploiting tag relevance for Web service mining,Liang Chen (Zhejiang University);Jian Wu (Zhejiang University);Zibin Zheng (The Chinese University of Hong Kong);Michael R. Lyu (The Chinese University of Hong Kong);Zhaohui Wu (Zhejiang University);,"2248621837,2525822525,2104906049,2227744130,2151930384","Web service tags, i.e., terms annotated by users to describe the functionality or other aspects of Web services, are being treated as collective user knowledge for Web service mining. Since user tagging is inherently uncontrolled, ambiguous, and overly personalized, a critical and fundamental problem is how to measure the relevance of a user-contributed tag with respect to the functionality of the annotated Web service. In this paper, we propose a hybrid mechanism by using Web Service Description Language documents and service-tag network information to compute the relevance scores of tags by employing semantic computation and Hyperlink-Induced Topic Search model, respectively. Further, we introduce tag relevance measurement mechanism into three applications of Web service mining: (1) Web service clustering; (2) Web service tag recommendation; and (3) tag-based Web service retrieval. To evaluate the accuracy of tag relevance measurement and its impact to Web service mining, experiments are implemented based on Titan which is a Web service search engine constructed based on 15,968 real Web services. Comprehensive experiments demonstrate the effectiveness of the proposed tag relevance measurement mechanism and its active promotion to the usage of tagging data in Web service mining.",2014,Knowledge and Information Systems volume 39 issue 1 pp 153-173,web 2 0;semantic web stack;web modeling;ws policy;social semantic web;web analytics;data web;web standards;web mapping;web application security;web development;web design;web navigation;web service;relevance;web page;web intelligence;web mining;world wide web;information retrieval;data mining;computer science;
Beyond one billion time series: indexing and mining very large time series collections with iSAX2+,"Alessandro Camerra (University of Trento);Jin Shieh (University of California, Riverside);Themis Palpanas (University of Trento);Thanawin Rakthanmanon (Kasetsart University);Eamonn J. Keogh (University of California, Riverside);","197289954,2101109093,2010554420,141314290,2170070822","There is an increasingly pressing need, by several applications in diverse domains, for developing techniques able to index and mine very large collections of time series. Examples of such applications come from astronomy, biology, the web, and other domains. It is not unusual for these applications to involve numbers of time series in the order of hundreds of millions to billions. However, all relevant techniques that have been proposed in the literature so far have not considered any data collections much larger than one-million time series. In this paper, we describe \(i\)SAX 2.0 and its improvements, \(i\)SAX 2.0 Clustered and \(i\)SAX2+, three methods designed for indexing and mining truly massive collections of time series. We show that the main bottleneck in mining such massive datasets is the time taken to build the index, and we thus introduce a novel bulk loading mechanism, the first of this kind specifically tailored to a time series index. We show how our methods allows mining on datasets that would otherwise be completely untenable, including the first published experiments to index one billion time series, and experiments in mining massive data from domains as diverse as entomology, DNA and web-scale image collections.",2014,Knowledge and Information Systems volume 39 issue 1 pp 123-151,search engine indexing;time series;data science;world wide web;data mining;database;statistics;computer science;
An interaction model between human and system for intuitive graphical search interface,Tomoko Kajiyama (Aoyama Gakuin University);Shin’ichi Satoh (National Institute of Informatics);,"1966483478,2146842387","Our objective was to propose a new model which provides real interactions like human conversations. In this paper, we defined “interaction” to mean not only superficial interactions between human and systems but also internal elements inspiring one another. We proposed a new interaction model by defining four user elements namely user knowledge, information needs, thinking, and feelings, and five system elements namely system knowledge, interaction algorithm, knowledge base, retrieval algorithm, and database. The key point is that users can understand inside the systems gradually and operate them flexibly in their own way to provide real interactions where users and systems inspire one another’s internal elements. We then defined system requirements to realize this model so that users can change and comprehend system knowledge and that users interact with the system constantly. We constructed an image retrieval system applying our proposed graphical search interface named Concentric Ring View and confirmed that all system requirements were satisfied. In a usability test with 12 university students, we confirmed that the proposed interaction model provided intuitive searches to users by inspiring internal elements between users and systems. Users could continuously change and comprehend system knowledge, synchronize user knowledge, shifting thinking and feeling, and changing information needs.",2014,Knowledge and Information Systems volume 39 issue 1 pp 41-60,knowledge management;human computer interaction;data mining;database;artificial intelligence;simulation;computer science;
Hubness-aware shared neighbor distances for high-dimensional k-nearest neighbor classification,Nenad Tomašev (University of Novi Sad);Dunja Mladenić (Carnegie Mellon University);,"2311756312,2282522104","Learning from high-dimensional data is usually quite challenging, as captured by the well-known phrase curse of dimensionality. Data analysis often involves measuring the similarity between different examples. This sometimes becomes a problem, as many widely used metrics tend to concentrate in high-dimensional feature spaces. The reduced contrast makes it more difficult to distinguish between close and distant points, which renders many traditional distance-based learning methods ineffective. Secondary distances based on shared neighbor similarities have recently been proposed as one possible solution to this problem. However, these initial metrics failed to take hubness into account. Hubness is a recently described aspect of the dimensionality curse, and it affects all sorts of \(k\)-nearest neighbor learning methods in severely negative ways. This paper is the first to discuss the impact of hubs on forming the shared neighbor similarity scores. We propose a novel, hubness-aware secondary similarity measure \(simhub_s\) and an extensive experimental evaluation shows it to be much more appropriate for high-dimensional data classification than the standard \(simcos_s\) measure. The proposed similarity changes the underlying \(k\)NN graph in such a way that it reduces the overall frequency of label mismatches in \(k\)-neighbor sets and increases the purity of occurrence profiles, which improves classifier performance. It is a hybrid measure, which takes into account both the supervised and the unsupervised hubness information. The analysis shows that both components are useful in their own ways and that the measure is therefore properly defined. This new similarity does not increase the overall computational cost, and the improvement is essentially ‘free’.",2014,Knowledge and Information Systems volume 39 issue 1 pp 89-122,data mining;pattern recognition;machine learning;mathematics;
Fuzzy set-oriented neural networks based on fuzzy polynomial inference and dynamic genetic optimization,Byoung-Jun Park (Electronics and Telecommunications Research Institute);Wook-Dong Kim (University of Suwon);Sung-Kwun Oh (University of Suwon);Witold Pedrycz (University of Alberta);,"2113458132,2128314703,2618016734,136859070","In this paper, we introduce a new topology and offer a comprehensive design methodology of fuzzy set-based neural networks (FsNNs). The proposed architecture of the FsNNs is based on the fuzzy polynomial neurons formed through a collection of ‘if-then’ fuzzy rules, fuzzy inference, and polynomials with the extended structure of the premise and the consequence parts of fuzzy rules being formed within the networks. Three different forms of regression polynomials (namely constant, linear, and quadratic) are used in the consequence part of the rules. In order to build an optimal FsNN, the underlying structural and parametric optimization is supported by a dynamic search-based genetic algorithm (GA), which forms an optimal solution through successive adjustments (refinements) of the search range. The structure optimization involves the determination of the input variables included in the premise part and the order of the polynomial forming the consequence part of the rules. In the study, we explore two types of optimization methodologies, namely a simultaneous tuning and a separate tuning. GAs are global optimizers; however, when being used in their generic version, they often lead to a significant computing overhead caused by the need to explore an excessively large search space. To eliminate this shortcoming and increase the effectiveness of the optimization itself, we introduce a dynamic search-based GA that results in a rapid convergence while narrowing down the search to a limited region of the search space. We exploit this optimization mechanism to be completed both at the structural as well as the parametric level. To evaluate the performance of the proposed FsNN, we offer a suite of several representative numerical examples.",2014,Knowledge and Information Systems volume 39 issue 1 pp 207-240,fuzzy associative matrix;fuzzy transportation;fuzzy set operations;defuzzification;adaptive neuro fuzzy inference system;fuzzy classification;fuzzy number;neuro fuzzy;data mining;artificial intelligence;machine learning;mathematical optimization;mathematics;
Taxonomic data integration from multilingual Wikipedia editions,Gerard de Melo (Max Planck Society);Gerhard Weikum (Max Planck Society);,"2134233121,514836396","Information systems are increasingly making use of taxonomic knowledge about words and entities. A taxonomic knowledge base may reveal that the Lago di Garda is a lake and that lakes as well as ponds, reservoirs, and marshes are all bodies of water. As the number of available taxonomic knowledge sources grows, there is a need for techniques to integrate such data into combined, unified taxonomies. In particular, the Wikipedia encyclopedia has been used by a number of projects, but its multilingual nature has largely been neglected. This paper investigates how entities from all editions of Wikipedia as well as WordNet can be integrated into a single coherent taxonomic class hierarchy. We rely on linking heuristics to discover potential taxonomic relationships, graph partitioning to form consistent equivalence classes of entities, and a Markov chain-based ranking approach to construct the final taxonomy. This results in MENTA (Multilingual Entity Taxonomy), a resource that describes 5.4 million entities and is one of the largest multilingual lexical knowledge bases currently available.",2014,Knowledge and Information Systems volume 39 issue 1 pp 1-39,ranking;graph;natural language processing;information retrieval;data mining;database;computer science;
A multi-colony ant algorithm for optimizing join queries in distributed database systems,Ladan Golshanara (Shahid Beheshti University);Seyed Mohammad Taghi Rouhani Rankoohi (Shahid Beheshti University);Hamed Shah-Hosseini (Shahid Beheshti University);,"2562328822,349883525,2132327583","Distributed database systems provide a new data processing and storage technology for decentralized organizations of today. Query optimization, the process to generate an optimal execution plan for the posed query, is more challenging in such systems due to the huge search space of alternative plans incurred by distribution. As finding an optimal execution plan is computationally intractable, using stochastic-based algorithms has drawn the attention of most researchers. In this paper, for the first time, a multi-colony ant algorithm is proposed for optimizing join queries in a distributed environment where relations can be replicated but not fragmented. In the proposed algorithm, four types of ants collaborate to create an execution plan. Hence, there are four ant colonies in each iteration. Each type of ant makes an important decision to find the optimal plan. In order to evaluate the quality of the generated plan, two cost models are used—one based on the total time and the other on the response time. The proposed algorithm is compared with two previous genetic-based algorithms on chain, tree and cyclic queries. The experimental results show that the proposed algorithm saves up to about 80 % of optimization time with no significant difference in the quality of generated plans compared with the best existing genetic-based algorithm.",2014,Knowledge and Information Systems volume 39 issue 1 pp 175-206,query optimization;ant colony optimization algorithms;distributed computing;data mining;database;computer science;
High-dimensional clustering: a clique-based hypergraph partitioning framework,Tianming Hu (East China Normal University);Chuanren Liu (Rutgers University);Yong Tang (South China Normal University);Jing Sun (University of Auckland);Hui Xiong (Rutgers University);Sam Yuan Sung (South Texas College);,"2270319918,2169554947,2644485431,2148688753,2153710278,2251526302","Hypergraph partitioning has been considered as a promising method to address the challenges of high-dimensional clustering. With objects modeled as vertices and the relationship among objects captured by the hyperedges, the goal of graph partitioning is to minimize the edge cut. Therefore, the definition of hyperedges is vital to the clustering performance. While several definitions of hyperedges have been proposed, a systematic understanding of desired characteristics of hyperedges is still missing. To that end, in this paper, we first provide a unified clique perspective of the definition of hyperedges, which serves as a guide to define hyperedges. With this perspective, based on the concepts of shared (reverse) nearest neighbors, we propose two new types of clique hyperedges and analyze their properties regarding purity and size issues. Finally, we present an extensive evaluation using real-world document datasets. The experimental results show that, with shared (reverse) nearest neighbor-based hyperedges, the clustering performance can be improved significantly in terms of various external validation measures without the need for fine tuning of parameters.",2014,Knowledge and Information Systems volume 39 issue 1 pp 61-88,clique;combinatorics;pattern recognition;machine learning;mathematics;
DFSP: a Depth-First SPelling algorithm for sequential pattern mining of biological sequences,Vance Chiang-Chi Liao (National Taiwan University);Ming-Syan Chen (National Taiwan University);,"2013228292,2122365371","Scientific progress in recent years has led to the generation of huge amounts of biological data, most of which remains unanalyzed. Mining the data may provide insights into various realms of biology, such as finding co-occurring biosequences, which are essential for biological data mining and analysis. Data mining techniques like sequential pattern mining may reveal implicitly meaningful patterns among the DNA or protein sequences. If biologists hope to unlock the potential of sequential pattern mining in their field, it is necessary to move away from traditional sequential pattern mining algorithms, because they have difficulty handling a small number of items and long sequences in biological data, such as gene and protein sequences. To address the problem, we propose an approach called Depth-First SPelling (DFSP) algorithm for mining sequential patterns in biological sequences. The algorithm’s processing speed is faster than that of PrefixSpan, its leading competitor, and it is superior to other sequential pattern mining algorithms for biological sequences.",2014,Knowledge and Information Systems volume 38 issue 3 pp 623-639,k optimal pattern discovery;concept mining;data stream mining;data science;bioinformatics;data mining;computer science;
Maritime abnormality detection using Gaussian processes,Mark Smith;Steven Reece (University of Oxford);Stephen J. Roberts (University of Oxford);Ioannis Psorakis (University of Oxford);Iead Rezek (University of Oxford);,"2347206327,2086994224,2132254683,1700414774,1977850406","Novelty, or abnormality, detection aims to identify patterns within data streams thatdonotconformtoexpectedbehaviour.Thispaperintroducesnoveltydetectiontechniques using a combination of Gaussian processes, extreme value theory and divergence measure- menttoidentifyanomalousbehaviourinbothstreamingandbatchdata.Theapproachistested on both synthetic and real data, showing itself to be effective in our primary application of maritime vessel track analysis.",2014,Knowledge and Information Systems volume 38 issue 3 pp 717-741,hellinger distance;extreme value theory;gaussian process;anomaly detection;pattern recognition;machine learning;statistics;mathematics;
Comparative news summarization using concept-based optimization,Xiaojiang Huang (Peking University);Xiaojun Wan (Peking University);Jianguo Xiao (Peking University);,"2163694995,2146508076,2131977055","Comparative news summarization aims to highlight the commonalities and differences between two comparable news topics by using human-readable sentences. The summary ought to focus on the salient comparative aspects of both topics, and at the same time, it should describe the representative properties of each topic appropriately. In this study, we propose a novel approach for generating comparative news summaries. We consider cross-topic pairs of semantic-related concepts as evidences of comparativeness and consider topic-related concepts as evidences of representativeness. The score of a summary is estimated by summing up the weights of evidences in the summary. We formalize the summarization task as an optimization problem of selecting proper sentences to maximize this score and address the problem by using a mixed integer programming model. The experimental results demonstrate the effectiveness of our proposed model.",2014,Knowledge and Information Systems volume 38 issue 3 pp 691-716,multi document summarization;automatic summarization;integer programming;data science;information retrieval;data mining;computer science;
A multi-phase correlation search framework for mining non-taxonomic relations from unstructured text,Mei Kuan Wong (Dalhousie University);Syed Sibte Raza Abidi (Dalhousie University);Ian D. Jonsen (Dalhousie University);,"2131868950,2167880352,1972987086","Over the last decade, ontology engineering has been pursued by “learning” the ontology from domain-specific electronic documents. Most of the research works are focused on extraction of concepts and taxonomic relations. The extraction of non-taxonomic relations is often neglected and not well researched. In this paper, we present a multi-phase correlation search framework to extract non-taxonomic relations from unstructured text. Our framework addresses the two main problems in any non-taxonomic relations extraction: (a) the discovery of non-taxonomic relations and (b) the labelling of non-taxonomic relations. First, our framework is capable of extracting correlated concepts beyond ordinary search window size of a single sentence. Interesting correlations are then filtered using association rule mining with lift interestingness measure. Next, our framework distinguishes non-taxonomic concept pairs from taxonomic concept pairs based on existing domain ontology. Finally, our framework features the usage of domain related verbs as labels for the non-taxonomic relations. Our proposed framework has been tested with the marine biology domain. Results have been validated by domain experts showing reliable results as well as demonstrate significant improvement over traditional association rule approach in search of non-taxonomic relations from unstructured text.",2014,Knowledge and Information Systems volume 38 issue 3 pp 641-667,association rule learning;digital library;data science;information retrieval;data mining;database;artificial intelligence;machine learning;computer science;
GUISE: a uniform sampler for constructing frequency histogram of graphlets,Mahmudur Rahman (Indiana University – Purdue University Indianapolis);Mansurul Alam Bhuiyan (Indiana University – Purdue University Indianapolis);Mahmuda Rahman (Syracuse University);Mohammad Al Hasan (Indiana University – Purdue University Indianapolis);,"2107991719,2114304832,2278213566,2430381672","Graphlet frequency distribution (GFD) has recently become popular for characterizing large networks. However, the computation of GFD for a network requires the exact count of embedded graphlets in that network, which is a computationally expensive task. As a result, it is practically infeasible to compute the GFD for even a moderately large network. In this paper, we propose Guise, which uses a Markov Chain Monte Carlo sampling method for constructing the approximate GFD of a large network. Our experiments on networks with millions of nodes show that Guise obtains the GFD with very low rate of error within few minutes, whereas the exhaustive counting-based approach takes several days.",2014,Knowledge and Information Systems volume 38 issue 3 pp 511-536,power graph analysis;theoretical computer science;mathematical optimization;statistics;mathematics;
An effective neural network and fuzzy time series-based hybridized model to handle forecasting problems of two factors,Pritpal Singh (Tezpur University);Bhogeswar Borah (Tezpur University);,"2310981868,2146556095","Fuzzy time series forecasting method has been applied in several domains, such as stock market price, temperature, sales, crop production and academic enrollments. In this paper, we introduce a model to deal with forecasting problems of two factors. The proposed model is designed using fuzzy time series and artificial neural network. In a fuzzy time series forecasting model, the length of intervals in the universe of discourse always affects the results of forecasting. Therefore, an artificial neural network- based technique is employed for determining the intervals of the historical time series data sets by clustering them into different groups. The historical time series data sets are then fuzzified, and the high-order fuzzy logical relationships are established among fuzzified values based on fuzzy time series method. The paper also introduces some rules for interval weighing to defuzzify the fuzzified time series data sets. From experimental results, it is observed that the proposed model exhibits higher accuracy than those of existing two-factors fuzzy time series models.",2014,Knowledge and Information Systems volume 38 issue 3 pp 669-690,type 2 fuzzy sets and systems;fuzzy set operations;defuzzification;adaptive neuro fuzzy inference system;fuzzy classification;fuzzy number;neuro fuzzy;temperature;artificial neural network;data mining;artificial intelligence;machine learning;computer science;
An effective dissimilarity measure for clustering of high-dimensional categorical data,Jeonghoon Lee (École Centrale Paris);Yoon-Joon Lee (KAIST);,"2309969281,2167172276","Clustering is to group similar data and find out hidden information about the characteristics of dataset for the further analysis. The concept of dissimilarity of objects is a decisive factor for good quality of results in clustering. When attributes of data are not just numerical but categorical and high dimensional, it is not simple to discriminate the dissimilarity of objects which have synonymous values or unimportant attributes. We suggest a method to quantify the level of difference between categorical values and to weigh the implicit influence of each attribute on constructing a particular cluster. Our method exploits distributional information of data correlated with each categorical value so that intrinsic relationship of values can be discovered. In addition, it measures significance of each attribute in constructing respective cluster dynamically. Experiments on real datasets show the propriety and effectiveness of the method, which improves the results considerably even with simple clustering algorithms. Our approach does not couple with a clustering algorithm tightly and can also be applied to various algorithms flexibly.",2014,Knowledge and Information Systems volume 38 issue 3 pp 743-757,k medians clustering;flame clustering;correlation clustering;constrained clustering;data stream clustering;cure data clustering algorithm;single linkage clustering;fuzzy clustering;clustering high dimensional data;similarity;categorical variable;cluster analysis;data mining;pattern recognition;machine learning;computer science;mathematics;
Outcome aware ranking in value creation networks,Sampath Kameshwaran (IBM);Vinayaka Pandit (IBM);Sameep Mehta (IBM);Ambika Agarwal (IBM);Kashyap Dixit (IBM);,"1186150813,2139650730,2121593309,2187247143,2137589547","In this paper, we consider a natural ranking problem that arises in settings in which a community of people (or agents) are engaged in regular interactions with an end goal of creating value. Examples of such scenarios are academic collaboration networks, creative collaborations, and interactions between agents of a service delivery organization. For instance, consider a service delivery organization which essentially resolves a sequence of service requests from its customers by deploying its agents to resolve the requests. Typically, resolving a request requires interaction between multiple agents and results in an outcome (or value). The outcome could be success or failure of problem resolution or an index of customer satisfaction. For this scenario, the ranking of the agents of the network should take into account two aspects: importance of the agents in the network structure that arises as a result of interactions and the value generated by the interactions involving the respective agents. Such a ranking can be used for several purposes such as identifying influential agents of the interaction network, effective and efficient spreading of messages in the network. In this paper, we formally model the above ranking problem and develop a novel algorithm for computing the ranking. The key aspect of our approach is creating special nodes in the interaction network corresponding to the outcomes and endowing them independent, external status. The algorithm then iteratively spreads the external status of the outcomes to the agents based on their interactions and the outcome of those interactions. This results in an eigenvector like formulation, which results in a method requiring computing the inverse of a matrix rather than the eigenvector. We present several theoretical characterizations of our algorithmic approach. We present experimental results on the public domain real-life datasets from the Internet Movie Database and a dataset constructed by retrieving impact and citation ratings for papers listed in the DBLP database.",2014,Knowledge and Information Systems volume 38 issue 3 pp 537-565,world wide web;data mining;database;artificial intelligence;machine learning;simulation;computer science;
Scalable sequential alternating proximal methods for sparse structural SVMs and CRFs,P Balamurugan (Indian Institute of Science);Shirish Krishnaj Shevade (Indian Institute of Science);T. Ravindra Babu (Infosys);,"2563353755,2054697475,2147474944","Structural Support Vector Machines (SSVMs) and Conditional Random Fields (CRFs) are popular discriminative methods used for classifying structured and complex objects like parse trees, image segments and part-of-speech tags. The datasets involved are very large dimensional, and the models designed using typical training algorithms for SSVMs and CRFs are non-sparse. This non-sparse nature of models results in slow inference. Thus, there is a need to devise new algorithms for sparse SSVM and CRF classifier design. Use of elastic net and L1-regularizer has already been explored for solving primal CRF and SSVM problems, respectively, to design sparse classifiers. In this work, we focus on dual elastic net regularized SSVM and CRF. By exploiting the weakly coupled structure of these convex programming problems, we propose a new sequential alternating proximal (SAP) algorithm to solve these dual problems. This algorithm works by sequentially visiting each training set example and solving a simple subproblem restricted to a small subset of variables associated with that example. Numerical experiments on various benchmark sequence labeling datasets demonstrate that the proposed algorithm scales well. Further, the classifiers designed are sparser than those designed by solving the respective primal problems and demonstrate comparable generalization performance. Thus, the proposed SAP algorithm is a useful alternative for sparse SSVM and CRF classifier design.",2014,Knowledge and Information Systems volume 38 issue 3 pp 599-621,elastic net regularization;data mining;pattern recognition;machine learning;algorithm;computer science;
Transfer dimensionality reduction by Gaussian process in parallel,Bin Tong (Kyushu University);Junbin Gao (Charles Sturt University);Thach Nguyen Huy (Kyushu University);Hao Shao (Kyushu University);Einoshin Suzuki (Kyushu University);,"2250175026,2118078281,2496866584,2515944123,2228069075","Dimensionality reduction has been considered as one of the most significant tools for data analysis. In general, supervised information is helpful for dimensionality reduction. However, in typical real applications, supervised information in multiple source tasks may be available, while the data of the target task are unlabeled. An interesting problem of how to guide the dimensionality reduction for the unlabeled target data by exploiting useful knowledge, such as label information, from multiple source tasks arises in such a scenario. In this paper, we propose a new method for dimensionality reduction in the transfer learning setting. Unlike traditional paradigms where the useful knowledge from multiple source tasks is transferred through distance metric, we attempt to learn a more informative mapping function between the original data and the reduced data by Gaussian process that behaves more appropriately than other parametric regression methods due to its less parametric characteristic. In our proposal, we firstly convert the dimensionality reduction problem into integral regression problems in parallel. Gaussian process is then employed to learn the underlying relationship between the original data and the reduced data. Such a relationship can be appropriately transferred to the target task by exploiting the prediction ability of the Gaussian process model and inventing different kinds of regularizers. Extensive experiments on both synthetic and real data sets show the effectiveness of our method.",2014,Knowledge and Information Systems volume 38 issue 3 pp 567-597,diffusion map;transfer of learning;gaussian process;dimensionality reduction;regression;data mining;pattern recognition;artificial intelligence;machine learning;statistics;computer science;mathematics;
A weighted voting framework for classifiers ensembles,Ludmila I. Kuncheva (Bangor University);Juan J. Rodríguez (University of Burgos);,"67480443,2111901707","We propose a probabilistic framework for classifier combination, which gives rigorous optimality conditions (minimum classification error) for four combination methods: majority vote, weighted majority vote, recall combiner and the naive Bayes combiner. The framework is based on two assumptions: class-conditional independence of the classifier outputs and an assumption about the individual accuracies. The four combiners are derived subsequently from one another, by progressively relaxing and then eliminating the second assumption. In parallel, the number of the trainable parameters increases from one combiner to the next. Simulation studies reveal that if the parameter estimates are accurate and the first assumption is satisfied, the order of preference of the combiners is: naive Bayes, recall, weighted majority and majority. By inducing label noise, we expose a caveat coming from the stability-plasticity dilemma. Experimental results with 73 benchmark data sets reveal that there is no definitive best combiner among the four candidates, giving a slight preference to naive Bayes. This combiner was better for problems with a large number of fairly balanced classes while weighted majority vote was better for problems with a small number of unbalanced classes.",2014,Knowledge and Information Systems volume 38 issue 2 pp 259-275,randomized weighted majority algorithm;recall;naive bayes classifier;data mining;pattern recognition;machine learning;statistics;computer science;mathematics;
Exploiting semantic annotations for open information extraction: an experience in the biomedical domain,Victoria Nebot (James I University);Rafael Berlanga (James I University);,"2153224506,1997356218","The increasing amount of unstructured text published on the Web is demanding new tools and methods to automatically process and extract relevant information. Traditional information extraction has focused on harvesting domain-specific, pre-specified relations, which usually requires manual labor and heavy machinery; especially in the biomedical domain, the main efforts have been directed toward the recognition of well-defined entities such as genes or proteins, which constitutes the basis for extracting the relationships between the recognized entities. The intrinsic features and scale of the Web demand new approaches able to cope with the diversity of documents, where the number of relations is unbounded and not known in advance. This paper presents a scalable method for the extraction of domain-independent relations from text that exploits the knowledge in the semantic annotations. The method is not geared to any specific domain (e.g., protein–protein interactions and drug–drug interactions) and does not require any manual input or deep processing. Moreover, the method uses the extracted relations to compute groups of abstract semantic relations characterized by their signature types and synonymous relation strings. This constitutes a valuable source of knowledge when constructing formal knowledge bases, as we enable seamless integration of the extracted relations with the available knowledge resources through the process of semantic annotation. The proposed approach has successfully been applied to a large text collection in the biomedical domain and the results are very encouraging.",2014,Knowledge and Information Systems volume 38 issue 2 pp 365-389,relationship extraction;information retrieval;data mining;database;artificial intelligence;machine learning;computer science;
Estimating Feature Ratings through an Effective Review Selection Approach,Chong Long (Yahoo!);Jie Zhang (Nanyang Technological University);Minlie Huang (Tsinghua University);Xiaoyan Zhu (Tsinghua University);Ming Li (University of Waterloo);Bin Ma (University of Waterloo);,"2718767988,2422123818,2162268045,2147746173,2439529108,2303816330","Most participatory web sites collect overall ratings (e.g., five stars) of products from their customers, reflecting the overall assessment of the products. However, it is more useful to present ratings of product features (such as price, battery, screen, and lens of digital cameras) to help customers make effective purchase decisions. Unfortunately, only a very few web sites have collected feature ratings. In this paper, we propose a novel approach to accurately estimate feature ratings of products. This approach selects user reviews that extensively discuss specific features of the products (called specialized reviews), using information distance of reviews on the features. Experiments on both annotated and real data show that overall ratings of the specialized reviews can be used to represent their feature ratings. The average of these overall ratings can be used by recommender systems to provide feature-specific recommendations that can better help users make purchasing decisions.",2014,Knowledge and Information Systems volume 38 issue 2 pp 419-446,feature extraction;text mining;data science;data mining;database;computer science;
On the adaptability of G3PARM to the extraction of rare association rules,José María Luna (University of Córdoba);José Raúl Romero (University of Córdoba);Sebastián Ventura (University of Córdoba);,"2130867717,2218717932,2098297204","To date, association rule mining has mainly focused on the discovery of frequent patterns. Nevertheless, it is often interesting to focus on those that do not frequently occur. Existing algorithms for mining this kind of infrequent patterns are mainly based on exhaustive search methods and can be applied only over categorical domains. In a previous work, the use of grammar-guided genetic programming for the discovery of frequent association rules was introduced, showing that this proposal was competitive in terms of scalability, expressiveness, flexibility and the ability to restrict the search space. The goal of this work is to demonstrate that this proposal is also appropriate for the discovery of rare association rules. This approach allows one to obtain solutions within specified time limits and does not require large amounts of memory, as current algorithms do. It also provides mechanisms to discard noise from the rare association rule set by applying four different and specific fitness functions, which are compared and studied in depth. Finally, this approach is compared with other existing algorithms for mining rare association rules, and an analysis of the mined rules is performed. As a result, this approach mines rare rules in a homogeneous and low execution time. The experimental study shows that this proposal obtains a small and accurate set of rules close to the size specified by the data miner.",2014,Knowledge and Information Systems volume 38 issue 2 pp 391-418,k optimal pattern discovery;association rule learning;evolutionary computation;data mining;artificial intelligence;machine learning;computer science;
Exploiting rich user information for one-class collaborative filtering,Yanen Li (University of Illinois at Urbana–Champaign);Cheng Xiang Zhai (University of Illinois at Urbana–Champaign);Ye Chen (Microsoft);,"2158342695,2152766206,2167005179","One-class collaborative filtering (OCCF) is an emerging setup in collaborative filtering in which only positive examples or implicit feedback can be observed. Compared with the traditional collaborative filtering setting where the data have ratings, OCCF is more realistic in many scenarios when no ratings are available. In this paper, we propose to improve OCCF accuracy by exploiting the rich user information that is often naturally available in community-based interactive information systems, including a user’s search query history, and purchasing and browsing activities. We propose two major strategies to incorporate such user information into the OCCF models: One is to linearly combine scores from different sources, and the other is to embed user information into collaborative filtering. Furthermore, we employ the MapReduce framework for similarity computation over millions of users and items. Experimental results on two large-scale retail datasets from a major e-commerce company show that the proposed methods are effective and can improve the performance of the OCCF over baseline methods through leveraging rich user information.",2014,Knowledge and Information Systems volume 38 issue 2 pp 277-301,information filtering system;collaborative filtering;ensembl;recommender system;world wide web;information retrieval;data mining;database;computer science;
Models of distributed data clustering in peer-to-peer environments,Khaled M. Hammouda (University of Waterloo);Mohamed S. Kamel (University of Waterloo);,"1268727595,2096364092","Distributed data mining applies techniques to mine distributed data sources by avoiding the need to first collect the data into a central site. This has a significant appeal when issues of communication cost and privacy put a restriction on traditional centralized methods. Although there has been development on many fronts in distributed data mining, we are still lacking models that abstract the process by showing similarities and contrasts between the different methods. In this paper, we introduce two abstract models for distributed clustering in peer-to-peer environments with different goals. The first is the Locally optimized Distributed Clustering (LDC) model, which aims toward achieving better local clusters at each node, and is facilitated by collaboration through sharing of summarized cluster information. The second is the Globally optimized Distributed Clustering (GDC) model, which aims toward achieving one global clustering solution that is an approximation of centralized clustering. We also report on concrete realizations of the two models that show their benefits, through application in text mining. The LDC model is realized through the Collaborative P2P Clustering algorithm, while the GDC model is realized through the Hierarchically distributed P2P Clustering algorithm. In the former, we show that peer collaboration results in significant increase in local clustering quality. The process utilizes cluster summarization to exchange information between peers. In the latter, we target scalability by structuring the P2P network hierarchically and devise a distributed variant of the k-means algorithm to compute one set of clusters across the hierarchy. We demonstrate through experimental results the effectiveness of both methods and make recommendation on when to use each method.",2014,Knowledge and Information Systems volume 38 issue 2 pp 303-329,flame clustering;brown clustering;correlation clustering;constrained clustering;data stream clustering;cure data clustering algorithm;affinity propagation;fuzzy clustering;clustering high dimensional data;cluster analysis;consensus clustering;conceptual clustering;world wide web;data mining;database;artificial intelligence;machine learning;computer science;
Securing SIP-based VoIP infrastructure against flooding attacks and Spam Over IP Telephony,Muhammad Ali Akbar (National University of Computer and Emerging Sciences);Muddassar Farooq (National University of Computer and Emerging Sciences);,"2659849748,2120593778","Security of session initiation protocol (SIP) servers is a serious concern of Voice over Internet (VoIP) vendors. The important contribution of our paper is an accurate and real-time attack classification system that detects: (1) application layer SIP flood attacks that result in denial of service (DoS) and distributed DoS attacks, and (2) Spam over Internet Telephony (SPIT). The major advantage of our framework over existing schemes is that it performs packet-based analysis using a set of spatial and temporal features. As a result, we do not need to transform network packet streams into traffic flows and thus save significant processing and memory overheads associated with the flow-based analysis. We evaluate our framework on a real-world SIP traffic—collected from the SIP server of a VoIP vendor—by injecting a number of application layer anomalies in it. The results of our experiments show that our proposed framework achieves significantly greater detection accuracy compared with existing state-of-the-art flooding and SPIT detection schemes.",2014,Knowledge and Information Systems volume 38 issue 2 pp 491-510,invite of death;sip trunking;intrusion detection system;denial of service attack;session initiation protocol;internet privacy;computer security;computer network;computer science;
Expansion and decentralized search in complex networks,Arun S. Maiya (Institute for Defense Analyses);Tanya Y. Berger-Wolf (University of Illinois at Chicago);,"2225019251,107058130","Borrowing from concepts in expander graphs, we study the expansion properties of real-world, complex networks (e.g., social networks, unstructured peer-to-peer, or P2P networks) and the extent to which these properties can be exploited to understand and address the problem of decentralized search. We first produce samples that concisely capture the overall expansion properties of an entire network, which we collectively refer to as the expansion signature. Using these signatures, we find a correspondence between the magnitude of maximum expansion and the extent to which a network can be efficiently searched. We further find evidence that standard graph-theoretic measures, such as average path length, fail to fully explain the level of “searchability” or ease of information diffusion and dissemination in a network. Finally, we demonstrate that this high expansion can be leveraged to facilitate decentralized search in networks and show that an expansion-based search strategy outperforms typical search methods.",2014,Knowledge and Information Systems volume 38 issue 2 pp 469-490,evolving networks;expander graph;peer to peer;social network analysis;complex network;theoretical computer science;data mining;machine learning;computer science;mathematics;
Composition pattern oriented tag extraction from short documents using a structural learning method,Yongwook Shin (Seoul National University);Sung Jun Lee (Seoul National University);Jonghun Park (Seoul National University);,"2104296863,2666841699,2163513841","With the rapid growth of web, automatic tagging that detects informative terms from a document becomes an important problem for information aggregation and sharing services. In particular, automatic tagging for short documents becomes more interesting as many users are increasingly publishing information through social media services which encourage users to create the documents of short length. In this paper, we propose a novel automatic tagging model for short text documents from social media services, following the framework of supervised learning. We redefine traditional frequency-based term features so that they can address the properties of the documents created under length limitation and consider sequential dependencies between successive terms in a document based on a structural support vector machine. In addition, our proposed approach incorporates composition patterns by which users put informative terms into their documents. Extensive experiments have been conducted to validate the presented approach, and it was found that the proposed term features were effective for extracting tags, and the tag extractor trained by considering the sequential dependencies and composition patterns achieved superior performance results over the existing alternative methods.",2014,Knowledge and Information Systems volume 38 issue 2 pp 447-468,composite pattern;social media;biological classification;world wide web;information retrieval;data mining;database;computer science;
Clustering semantically heterogeneous distributed aggregate databases,Shuai Zhang (Ulster University);Sally I. McClean (Ulster University);Bryan W. Scotney (Ulster University);,"2109311327,2006696304,2205299394","Databases developed independently in a common open distributed environment may be heterogeneous with respect to both data schema and the embedded semantics. Managing schema and semantic heterogeneities brings considerable challenges to learning from distributed data and to support applications involving cooperation between different organisations. In this paper, we are concerned mainly with heterogeneous databases that hold aggregates on a set of attributes, which are often the result of materialised views of native large-scale distributed databases. A model-based clustering algorithm is proposed to construct a mixture model where each component corresponds to a cluster which is used to capture the contextual heterogeneity among databases from different populations. Schema heterogeneity, which can be recast as incomplete information, is handled within the clustering process using Expectation-Maximisation estimation and integration is carried out within a clustering iteration. Our proposed algorithm resolves the schema heterogeneity as part of the clustering process, thus avoiding transformation of the data into a unified schema. Results of algorithm evaluation on classification, scalability and reliability, using both real and synthetic data, demonstrate that our algorithm can achieve good performance by incorporating all of the information from available heterogeneous data. Our clustering approach has great potential for scalable knowledge discovery from semantically heterogeneous databases and for applications in an open distributed environment, such as the Semantic Web.",2014,Knowledge and Information Systems volume 38 issue 2 pp 331-364,elasticity;correlation clustering;constrained clustering;data stream clustering;cure data clustering algorithm;fuzzy clustering;clustering high dimensional data;cluster analysis;expectation maximization algorithm;unsupervised learning;theoretical computer science;data mining;database;machine learning;computer science;
Clustering Web services to facilitate service discovery,Jian Wu (Zhejiang University);Liang Chen (Zhejiang University);Zibin Zheng (The Chinese University of Hong Kong);Michael R. Lyu (The Chinese University of Hong Kong);Zhaohui Wu (Zhejiang University);,"2525822525,2248621837,2104906049,2227744130,2151930384","Clustering Web services would greatly boost the ability of Web service search engine to retrieve relevant services. The performance of traditional Web service description language (WSDL)-based Web service clustering is not satisfied, due to the singleness of data source. Recently, Web service search engines such as Seekda! allow users to manually annotate Web services using tags, which describe functions of Web services or provide additional contextual and semantical information. In this paper, we cluster Web services by utilizing both WSDL documents and tags. To handle the clustering performance limitation caused by uneven tag distribution and noisy tags, we propose a hybrid Web service tag recommendation strategy, named WSTRec, which employs tag co-occurrence, tag mining, and semantic relevance measurement for tag recommendation. Extensive experiments are conducted based on our real-world dataset, which consists of 15,968 Web services. The experimental results demonstrate the effectiveness of our proposed service clustering and tag recommendation strategies. Specifically, compared with traditional WSDL-based Web service clustering approaches, the proposed approach produces gains in both precision and recall for up to 14 % in most cases.",2014,Knowledge and Information Systems volume 38 issue 1 pp 207-229,web 2 0;semantic web stack;ws i basic profile;web modeling;ws policy;social semantic web;web analytics;data web;web standards;web mapping;web application security;web development;web design;universal description discovery and integration;service discovery;web service;cluster analysis;web intelligence;world wide web;information retrieval;data mining;database;machine learning;computer science;
An efficient projection-based indexing approach for mining high utility itemsets,Guo-Cheng Lan (National Cheng Kung University);Tzung-Pei Hong (National University of Kaohsiung);Vincent S. Tseng (National Cheng Kung University);,"2150324475,2123991549,2023431847","Recently, utility mining has widely been discussed in the field of data mining. It finds high utility itemsets by considering both profits and quantities of items in transactional data sets. However, most of the existing approaches are based on the principle of levelwise processing, as in the traditional two-phase utility mining algorithm to find a high utility itemsets. In this paper, we propose an efficient utility mining approach that adopts an indexing mechanism to speed up the execution and reduce the memory requirement in the mining process. The indexing mechanism can imitate the traditional projection algorithms to achieve the aim of projecting sub-databases for mining. In addition, a pruning strategy is also applied to reduce the number of unpromising itemsets in mining. Finally, the experimental results on synthetic data sets and on a real data set show the superior performance of the proposed approach.",2014,Knowledge and Information Systems volume 38 issue 1 pp 85-107,data science;data mining;database;computer science;
Analyzing the presence of noise in multi-class problems: alleviating its influence with the One-vs-One decomposition,José A. Sáez (University of Granada);Mikel Galar (Universidad Pública de Navarra);Julián Luengo (LSI Corporation);Francisco Herrera (University of Granada);,"2096358471,2148878631,2583509827,2141382980","The presence of noise in data is a common problem that produces several negative consequences in classification problems. In multi-class problems, these consequences are aggravated in terms of accuracy, building time, and complexity of the classifiers. In these cases, an interesting approach to reduce the effect of noise is to decompose the problem into several binary subproblems, reducing the complexity and, consequently, dividing the effects caused by noise into each of these subproblems. This paper analyzes the usage of decomposition strategies, and more specifically the One-vs-One scheme, to deal with noisy multi-class datasets. In order to investigate whether the decomposition is able to reduce the effect of noise or not, a large number of datasets are created introducing different levels and types of noise, as suggested in the literature. Several well-known classification algorithms, with or without decomposition, are trained on them in order to check when decomposition is advantageous. The results obtained show that methods using the One-vs-One strategy lead to better performances and more robust classifiers when dealing with noisy data, especially with the most disruptive noise schemes.",2014,Knowledge and Information Systems volume 38 issue 1 pp 179-206,biological classification;data mining;pattern recognition;machine learning;statistics;computer science;
Efficiently spotting the starting points of an epidemic in a large graph,B. Aditya Prakash (Virginia Tech);Jilles Vreeken (University of Antwerp);Christos Faloutsos (Carnegie Mellon University);,"2124002246,1971070670,2198983026","Given a snapshot of a large graph, in which an infection has been spreading for some time, can we identify those nodes from which the infection started to spread? In other words, can we reliably tell who the culprits are? In this paper, we answer this question affirmatively and give an efficient method called NetSleuth for the well-known susceptible-infected virus propagation model. Essentially, we are after that set of seed nodes that best explain the given snapshot. We propose to employ the minimum description length principle to identify the best set of seed nodes and virus propagation ripple, as the one by which we can most succinctly describe the infected graph. We give an highly efficient algorithm to identify likely sets of seed nodes given a snapshot. Then, given these seed nodes, we show we can optimize the virus propagation ripple in a principled way by maximizing likelihood. With all three combined, NetSleuth can automatically identify the correct number of seed nodes, as well as which nodes are the culprits. Experimentation on our method shows high accuracy in the detection of seed nodes, in addition to the correct automatic identification of their number. Moreover, NetSleuth scales linearly in the number of nodes of the graph.",2014,Knowledge and Information Systems volume 38 issue 1 pp 35-59,diffusion;data mining;artificial intelligence;mathematics;
Multi-view constrained clustering with an incomplete mapping between views,"Eric Eaton (Bryn Mawr College);Marie desJardins (University of Maryland, Baltimore County);Sara Jacob (Lockheed Martin Advanced Technology Laboratories);","2125814586,2126990796,2486805141","Multi-view learning algorithms typically assume a complete bipartite mapping between the different views in order to exchange information during the learning process. However, many applications provide only a partial mapping between the views, creating a challenge for current methods. To address this problem, we propose a multi-view algorithm based on constrained clustering that can operate with an incomplete mapping. Given a set of pairwise constraints in each view, our approach propagates these constraints using a local similarity measure to those instances that can be mapped to the other views, allowing the propagated constraints to be transferred across views via the partial mapping. It uses co-EM to iteratively estimate the propagation within each view based on the current clustering model, transfer the constraints across views, and then update the clustering model. By alternating the learning process between views, this approach produces a unified clustering model that is consistent with all views. We show that this approach significantly improves clustering performance over several other methods for transferring constraints and allows multi-view clustering to be reliably applied when given a limited mapping between the views. Our evaluation reveals that the propagated constraints have high precision with respect to the true clusters in the data, explaining their benefit to clustering performance in both single- and multi-view learning scenarios.",2014,Knowledge and Information Systems volume 38 issue 1 pp 231-257,brown clustering;canopy clustering algorithm;correlation clustering;constrained clustering;data stream clustering;cure data clustering algorithm;fuzzy clustering;cluster analysis;consensus clustering;semi supervised learning;conceptual clustering;data mining;pattern recognition;machine learning;computer science;mathematics;
A general framework for scalable transductive transfer learning,Mohammad Taha Bahadori (University of Southern California);Yan Liu (University of Southern California);Dan Zhang (Purdue University);,"2032867848,2240541904,2585585268","Transductive transfer learning is one special type of transfer learning problem, in which abundant labeled examples are available in the source domain and only unlabeled examples are available in the target domain. It easily finds applications in spam filtering, microblogging mining, and so on. In this paper, we propose a general framework to solve the problem by mapping the input features in both the source domain and the target domain into a shared latent space and simultaneously minimizing the feature reconstruction loss and prediction loss. We develop one specific example of the framework, namely latent large-margin transductive transfer learning algorithm, and analyze its theoretic bound of classification loss via Rademacher complexity. We also provide a unified view of several popular transfer learning algorithms under our framework. Experiment results on one synthetic dataset and three application datasets demonstrate the advantages of the proposed algorithm over the other state-of-the-art ones.",2014,Knowledge and Information Systems volume 38 issue 1 pp 61-83,rademacher complexity;inductive transfer;transduction;stochastic gradient descent;semi supervised learning;data mining;pattern recognition;machine learning;computer science;
A Flexible Coupling Approach to Multi-Agent Planning under Incomplete Information,Alejandro Torreño (Polytechnic University of Valencia);Eva Onaindia (Polytechnic University of Valencia);Óscar Sapena (Polytechnic University of Valencia);,"118862845,1970255001,149976645","Multi-agent planning (MAP) approaches are typically oriented at solving loosely coupled problems, being ineffective to deal with more complex, strongly related problems. In most cases, agents work under complete information, building complete knowledge bases. The present article introduces a general-purpose MAP framework designed to tackle problems of any coupling levels under incomplete information. Agents in our MAP model are partially unaware of the information managed by the rest of agents and share only the critical information that affects other agents, thus maintaining a distributed vision of the task. Agents solve MAP tasks through the adoption of an iterative refinement planning procedure that uses single-agent planning technology. In particular, agents will devise refinements through the partial-order planning paradigm, a flexible framework to build refinement plans leaving unsolved details that will be gradually completed by means of new refinements. Our proposal is supported with the implementation of a fully operative MAP system and we show various experiments when running our system over different types of MAP problems, from the most strongly related to the most loosely coupled.",2014,Knowledge and Information Systems volume 38 issue 1 pp 141-178,multi agent system;data mining;database;artificial intelligence;simulation;computer science;
Context-based information analysis for the Web environment,Vesile Evrim (Cyprus International University);Dennis McLeod (University of Southern California);,"2565737718,2097719264","Finding the relevant set of information that satisfies an information request of a Web user in the availability of today's vast amount of digital data is becoming a challenging problem. Currently, available Information Retrieval (IR) Systems are designed to return long lists of results, only a few of which are relevant for a specific user. In this paper, an IR method called Context-Based Information Analysis (CONIA) that investigates the context informa- tion of the user and user's information request to provide relevant results for the given domain users is introduced. In this paper, relevance is measured by the semantics of the information provided in the documents. The information extracted from lexical and domain ontologies is integrated by the user's interest information to expand the terms entered in the request. The obtained set of terms is categorized by a novel approach, and the relations between the categories are obtained from the ontologies. This categorization is used to improve the quality of the document selection by going beyond checking the availability of the words in the document by analyzing the semantic composition of the mapped terms.",2014,Knowledge and Information Systems volume 38 issue 1 pp 109-140,information filtering system;concept search;information quality;relevance;relevance;ontology;world wide web;information retrieval;data mining;database;computer science;
Challenges for adaptation in agent societies,Juan M. Alberola (Polytechnic University of Valencia);Vicente Julian (Polytechnic University of Valencia);Ana Garcia-Fornes (Polytechnic University of Valencia);,"2148500854,2162501773,224486239","Adaptation in multiagent systems societies provides a paradigm for allowing these societies to change dynamically in order to satisfy the current requirements of the system. This support is especially required for the next generation of systems that focus on open, dynamic, and adaptive applications. In this paper, we analyze the current state of the art regarding approaches that tackle the adaptation issue in these agent societies. We survey the most relevant works up to now in order to highlight the most remarkable features according to what they support and how this support is provided. In order to compare these approaches, we also identify different characteristics of the adaptation process that are grouped in different phases. Finally, we discuss some of the most important considerations about the analyzed approaches, and we provide some interesting guidelines as open issues that should be required in future developments.",2014,Knowledge and Information Systems volume 38 issue 1 pp 1-34,adaptation;organization;management science;knowledge management;operations research;
Using Case-Based Reasoning and Principled Negotiation to provide decision support for dispute resolution,"Davide Rua Carneiro (University of Minho);Paulo Novais (University of Minho);Francisco Carneiro Pacheco Andrade (Law School Admission Council);John Zeleznikow (Victoria University, Australia);José Neves (University of Minho);","2139885325,2052635854,2471290492,76571739,2568267441","The growing use of Information Technology in the commercial arena leads to an urgent need to find alternatives to traditional dispute resolution. New tools from fields such as artificial intelligence (AI) should be considered in the process of developing novel online dispute resolution (ODR) platforms, in order to make the ligation process simpler, faster and conform with the new virtual environments. In this work, we describe UMCourt, a project built around two sub-fields of AI research: Multi-agent Systems and Case-Based Reasoning, aimed at fostering the development of tools for ODR. This is then used to accomplish several objectives, from suggesting solutions to new disputes based on the observation of past similar disputes, to the improvement of the negotiation and mediation processes that may follow. The main objective of this work is to develop autonomous tools that can increase the effectiveness of the dispute resolution processes, namely by increasing the amount of meaningful information that is available for the parties.",2013,Knowledge and Information Systems volume 36 issue 3 pp 789-826,best alternative to a negotiated agreement;online dispute resolution;alternative dispute resolution;expert system;knowledge management;data mining;database;artificial intelligence;computer science;
Hybrid Collaborative Filtering algorithm for bidirectional Web service recommendation,Jie Cao (Nanjing University of Finance and Economics);Zhiang Wu (Nanjing University of Finance and Economics);Youquan Wang (Nanjing University of Science and Technology);Yi Zhuang (Zhejiang Gongshang University);,"2703313050,2163261191,2498855876,2238339724","Web service recommendation has become a hot yet fundamental research topic in service computing. The most popular technique is the Collaborative Filtering (CF) based on a user-item matrix. However, it cannot well capture the relationship between Web services and providers. To address this issue, we first design a cube model to explicitly describe the relationship among providers, consumers and Web services. And then, we present a Standard Deviation based Hybrid Collaborative Filtering (SD-HCF) for Web Service Recommendation (WSRec) and an Inverse consumer Frequency based User Collaborative Filtering (IF-UCF) for Potential Consumers Recommendation (PCRec). Finally, the decision-making process of bidirectional recommendation is provided for both providers and consumers. Sets of experiments are conducted on real-world data provided by Planet-Lab. In the experiment phase, we show how the parameters of SD-HCF impact on the prediction quality as well as demonstrate that the SD-HCF is much better than extant methods on recommendation quality, including the CF based on user, the CF based on item and general HCF. Experimental comparison between IF-UCF and UCF indicates the effectiveness of adding inverse consumer frequency to UCF.",2013,Knowledge and Information Systems volume 36 issue 3 pp 607-627,collaborative filtering;web service;multimedia;world wide web;information retrieval;machine learning;computer science;
Transfer learning for activity recognition: a survey,Diane J. Cook (Washington State University);Kyle D. Feuz (Washington State University);Narayanan Chatapuram Krishnan (Washington State University);,"2106796750,1982570015,2020355954","Many intelligent systems that focus on the needs of a human require information about the activities being performed by the human. At the core of this capability is activity recognition, which is a challenging and well-researched problem. Activity recognition algorithms require substantial amounts of labeled training data yet need to perform well under very diverse circumstances. As a result, researchers have been designing methods to identify and utilize subtle connections between activity recognition datasets, or to perform transfer-based activity recognition. In this paper, we survey the literature to highlight recent advances in transfer learning for activity recognition. We characterize existing approaches to transfer-based activity recognition by sensor modality, by differences between source and target environments, by data availability, and by type of information that is transferred. Finally, we present some grand challenges for the community to consider as this field is further developed.",2013,Knowledge and Information Systems volume 36 issue 3 pp 537-556,transfer of learning;activity recognition;text mining;data science;information retrieval;data mining;database;artificial intelligence;machine learning;computer science;
Efficient planning for top-K Web service composition,Shuiguang Deng (Zhejiang University);Bin Wu (Zhejiang University);Jianwei Yin (Zhejiang University);Zhaohui Wu (Zhejiang University);,"2012690264,2620849689,2124632089,2151930384","This paper proposes a novel approach based on the planning-graph to solve the top-k QoS-aware automatic composition problem of semantic Web services. The approach includes three sequential stages: a forward search stage to generate a planning-graph to reduce the search space of the following two stages greatly, an optimal local QoS calculating stage to compute all the optimal local QoS values of services in the planning, and a backward search stage to find the top-K composed services with optimal QoS values according to the planning-graph and the optimal QoS value. In order to validate the approach, experiments are carried out based on the test sets offered by the WS-Challenge competition 2009. The results show that the approach can find the same optimal solution as the champion system from the competition but also can provide more alternative solutions with the optimal QoS for users.",2013,Knowledge and Information Systems volume 36 issue 3 pp 579-605,automated planning and scheduling;world wide web;data mining;database;artificial intelligence;computer science;
Multi-domain anomaly detection in spatial datasets,"Vandana Pursnani Janeja (University of Maryland, Baltimore County);Revathi Palanisamy (University of Maryland, Baltimore County);","2290556930,2480632041","A spatial anomaly captures a phenomenon occurring in a region which is vastly deviant in behavior with respect to the other normal observations. However, in reality this anomaly may impact other phenomena in the region across multiple domains, for example, crime is often linked to other sociopolitical factors or phenomenon such as poverty and education. Similarly, accidents in the region may be linked to other environmental factors such as weather and surface condition. So, finding anomalies across multiple domains is important in various applications. In this paper, we propose an approach for finding such a tangible anomalous window across multiple domains where window refers to the set of contiguous points in space, and since the window is multi-domain, there are several overlapping windows in the same space across domains. Our approach for finding anomalous window across the domains comprises the following steps: (1) single-domain anomaly detection: discovering anomalous window in each domain; (2) association rule mining: discovering relationship between the anomalous windows across domains using association rule mining; and (3) validation: validating the result using (a) Monte Carlo simulation, (b) correlation using lift and (c) ground truth evaluation. In addition, we also provide a probabilistic framework to evaluate the relationships between the spatial nodes as a postprocessing step. Finally, we provide a visualization technique for viewing the multi-domain anomalous window and the probabilistic relationships between the nodes. We provide detailed experimental results and comparisons with other approaches using real-world health ranking [51] and transportation datasets [50] with known ground truth windows. The results show that our approach is effective in finding the anomalies in multiple domains as compared to other approaches.",2013,Knowledge and Information Systems volume 36 issue 3 pp 749-788,co occurrence;association rule learning;data mining;artificial intelligence;machine learning;computer science;
A nonnegative matrix factorization framework for semi-supervised document clustering with dual constraints,Huifang Ma (Northwest Normal University);Weizhong Zhao (Xiangtan University);Zhongzhi Shi (Chinese Academy of Sciences);,"2636672251,2106927367,2728595356","In this paper, we propose a new semi-supervised co-clustering algorithm Orthogonal Semi-Supervised Nonnegative Matrix Factorization (OSS-NMF) for document clustering. In this new approach, the clustering process is carried out by incorporating both prior domain knowledge of data points (documents) in the form of pair-wise constraints and category knowledge of features (words) into the NMF co-clustering framework. Under this framework, the clustering problem is formulated as the problem of finding the local minimizer of objective function, taking into account the dual prior knowledge. The update rules are derived, and an iterative algorithm is designed for the co-clustering process. Theoretically, we prove the correctness and convergence of our algorithm and demonstrate its mathematical rigorous. Our experimental evaluations show that the proposed document clustering model presents remarkable performance improvements with those constraints.",2013,Knowledge and Information Systems volume 36 issue 3 pp 629-651,canopy clustering algorithm;correlation clustering;constrained clustering;cure data clustering algorithm;fuzzy clustering;non negative matrix factorization;cluster analysis;pattern recognition;machine learning;mathematical optimization;computer science;mathematics;
Customer credit scoring based on HMM/GMDH hybrid model,Ge-Er Teng (Sichuan University);Chang-Zheng He (Sichuan University);Jin Xiao (Sichuan University);Xiao-Yi Jiang (University of Münster);,"2162960923,2121932614,2227981784,2672056963","Hidden Markov model (HMM) has made great achievements in many fields such as speech recognition and engineering. However, due to its assumption of state conditional independence between observations, HMM has a very limited capacity for recognizing complex patterns involving more than first-order dependencies in customer relationships management. Group Method of Data Handling (GMDH) could overcome the drawbacks of HMM, so we propose a hybrid model by combining the HMM and GMDH to score customer credit. There are three phases in this model: training HMM with multiple observations, adding GMDH into HMM and optimizing the hybrid model. The proposed hybrid model is compared with other exiting methods in terms of average accuracy, Type I error, Type II error and AUC. Experimental results show that the proposed method has better performance than HMM/ANN in two credit scoring datasets. The implementation of HMM/GMDH hybrid model allows lenders and regulators to develop techniques to measure customer credit risk.",2013,Knowledge and Information Systems volume 36 issue 3 pp 731-747,aerospace;customer relationship management;hidden markov model;speech recognition;data mining;artificial intelligence;machine learning;computer science;
Query directed clustering,Daniel Crabtree (Victoria University of Wellington);Xiaoying Gao (Victoria University of Wellington);Peter Andreae (Victoria University of Wellington);,"2107186287,2160909171,1267919130","This paper identifies the conditions under which web page clustering algorithms are effective and identifies the problems that cause them to fail. It then presents Query Directed Clustering (QDC), a web page clustering algorithm that produces higher-quality clusterings than other clustering algorithms for easy ambiguous queries, while performing at least as well as other clustering algorithms on queries for which clustering is not well suited. QDC has the five key innovations: a new cluster quality guide that is based on the relationship between clusters and the query; an improved cluster merging method that considers both cluster overlap and cluster description similarity; a new cluster splitting method that addresses the cluster chaining (drifting) problem; an improved heuristic for selecting good clusters; a new method that improves the clusters by ranking the pages in each cluster. Our experiments evaluate QDC both quantitatively and qualitatively and show that QDC significantly improves clustering performance, while being substantially more efficient than existing approaches.",2013,Knowledge and Information Systems volume 36 issue 3 pp 693-729,flame clustering;k medians clustering;brown clustering;canopy clustering algorithm;complete linkage clustering;dbscan;correlation clustering;constrained clustering;data stream clustering;cure data clustering algorithm;single linkage clustering;affinity propagation;fuzzy clustering;clustering high dimensional data;hierarchical clustering;cluster analysis;consensus clustering;biclustering;conceptual clustering;world wide web;data mining;database;machine learning;computer science;
Revisiting the effect of history on learning performance: the problem of the demanding lord,George Giannakopoulos (University of Trento);Themis Palpanas (University of Trento);,"2106620877,2010554420","In a variety of settings ranging from recommendation systems to information filtering, approaches which take into account feedback have been introduced to improve services and user experience. However, as also indicated in the machine learning literature, there exist several settings where the requirements and target concept of a learning system changes over time, which consists a case of “concept drift”. In several systems, a sliding window over the training instances has been used to follow drifting concepts. However, no general analytic study has been performed on the relation between the size of the sliding window and the average performance of a learning system, since previous works have focused on instantaneous performance and specific underlying learners and data characteristics. This work proposes an analytic model that describes the effect of memory window size on the prediction performance of a learning system that is based on iterative feedback. The analysis considers target concepts changing over time, either periodically or randomly, using a formulation termed “the problem of the demanding lord”. Using a signal-to-noise approach to sketch learning ability of underlying machine learning algorithms, we estimate the average performance of a learning system regardless of its underlying algorithm and, as a corollary, propose a stepping stone toward finding the memory window that maximizes the average performance for a given drift setting and a specific modeling system. We experimentally support the proposed methodology with very promising results on three synthetic and four real datasets, using a variety of learning algorithms including Support Vector Machines, Naive Bayes, Nearest Neighbor and Decision Trees on classification and regression tasks. The results validate the analysis and indicate very good estimation performance in different settings.",2013,Knowledge and Information Systems volume 36 issue 3 pp 653-691,online machine learning;stability;multi task learning;concept drift;user modeling;generalization error;adaptive learning;active learning;learning classifier system;semi supervised learning;computational learning theory;instance based learning;unsupervised learning;data mining;database;artificial intelligence;machine learning;simulation;statistics;computer science;mathematics;
Machine learning and directional switching median-based filter for highly corrupted images,"Sohail Masood (International Islamic University, Islamabad);Ayyaz Hussain (National University of Computer and Emerging Sciences);M. Arfan Jaffar (International Islamic University, Islamabad);Anwar M. Mirza (King Saud University);Tae-Sun Choi (Gwangju Institute of Science and Technology);","2148662197,2172971535,2707761442,2106211688,2135349640","In this paper, two-stage machine learning-based noise detection scheme has been proposed for identification of salt-and- pepper impulse noise which gives excellent detection results for highly corrupted images. In the first stage, a window of size \(3\times 3\) is taken from image and some other features of this window are used as input to neural network. This scheme has distinction of having very low missed detection (MD) and false positives rates. In the second stage, decision tree-based algorithm (J48) is applied on some well-known statistical parameters to generate rules for noise detection. These noise detection methods give promising results for identification of noise from highly corrupted images. A modified version of switching median filter (directional weighted switching median filter) is proposed for noise removal. Performance of noise detector is measured using MD and false alarm FA. Filtering results are compared with state-of-the-art noise removal techniques in terms of peak signal-to-noise ratio and structural similarity index measure. Extensive experiments are performed to show that the proposed technique gives better results than state-of-the-art noise detection and filtering methods.",2013,Knowledge and Information Systems volume 36 issue 3 pp 557-577,value noise;gradient noise;salt and pepper noise;image noise;impulse noise;median filter;noise measurement;gaussian noise;speech recognition;machine learning;computer science;
Learning with multi-resolution overlapping communities,Xufei Wang (Arizona State University);Lei Tang (Walmart Labs);Huan Liu (Arizona State University);Lei Wang (University of Wollongong);,"2121550591,2141813676,2122391114,2441765901","A recent surge of participatory web and social media has created a new laboratory for studying human relations and collective behavior on an unprecedented scale. In this work, we study the predictive power of social connections to determine the preferences or behaviors of individuals such as whether a user supports a certain political view, whether one likes a product, whether she would like to vote for a presidential candidate, etc. Since an actor is likely to participate in multiple different communities with each regulating the actor’s behavior in varying degrees, and a natural hierarchy might exist between these communities, we propose to zoom into a network at multiple different resolutions and determine which communities reflect a targeted behavior. We develop an efficient algorithm to extract a hierarchy of overlapping communities. Empirical results on social media networks demonstrate the promising potential of the proposed approach in real-world applications.",2013,Knowledge and Information Systems volume 36 issue 2 pp 517-535,hierarchical clustering;resolution;world wide web;data mining;artificial intelligence;machine learning;simulation;statistics;computer science;
Toward structural sparsity: an explicit \ell _{2}/\ell _0 approach,Dijun Luo (University of Texas at Arlington);Chris H. Q. Ding (University of Texas at Arlington);Heng Huang (University of Texas at Arlington);,"2166799549,2119616764,2137533801","As powerful tools, machine learning and data mining techniques have been widely applied in various areas. However, in many real-world applications, besides establishing accurate black box predictors, we are also interested in white box mechanisms, such as discovering predictive patterns in data that enhance our understanding of underlying physical, biological and other natural processes. For these purposes, sparse representation and its variations have been one of the focuses. More recently, structural sparsity has attracted increasing attentions. In previous research, structural sparsity was often achieved by imposing convex but non-smooth norms such as \({\ell _{2}/\ell _{1}}\) and group \({\ell _{2}/\ell _{1}}\) norms. In this paper, we present the explicit \({\ell _2/\ell _0}\) and group \({\ell _2/\ell _0}\) norm to directly approach the structural sparsity. To tackle the problem of intractable \({\ell _2/\ell _0}\) optimizations, we develop a general Lipschitz auxiliary function that leads to simple iterative algorithms. In each iteration, optimal solution is achieved for the induced subproblem and a guarantee of convergence is provided. Furthermore, the local convergent rate is also theoretically bounded. We test our optimization techniques in the multitask feature learning problem. Experimental results suggest that our approaches outperform other approaches in both synthetic and real-world data sets.",2013,Knowledge and Information Systems volume 36 issue 2 pp 411-438,multi task learning;pattern recognition;machine learning;mathematical optimization;computer science;mathematics;
Minimally supervised question classification on fine-grained taxonomies,David Tomás (University of Alicante);José L. Vicedo (University of Alicante);,"2112102700,2097429769","This article presents a minimally supervised approach to question classification on fine-grained taxonomies. We have defined an algorithm that automatically obtains lists of weighted terms for each class in the taxonomy, thus identifying which terms are highly related to the classes and are highly discriminative between them. These lists have then been applied to the task of question classification. Our approach is based on the divergence of probability distributions of terms in plain text retrieved from the Web. A corpus of questions with which to train the classifier is not therefore necessary. As the system is based purely on statistical information, it does not require additional linguistic resources or tools. The experiments were performed on English questions and their Spanish translations. The results reveal that our system surpasses current supervised approaches in this task, obtaining a significant improvement in the experiments carried out.",2013,Knowledge and Information Systems volume 36 issue 2 pp 303-334,question answering;data mining;pattern recognition;machine learning;computer science;
An ensemble method for fuzzy rule-based classification systems,Basma Soua;Amel Borgi (National Institute of Applied Science and Technology);Moncef Tagina (Manouba University);,"2060933821,2716239162,1522294673","Fuzzy rule-based classification systems are very useful tools in the field of machine learning as they are able to build linguistic comprehensible models. However, these systems suffer from exponential rule explosion when the number of variables increases, degrading, therefore, the accuracy of these systems as well as their interpretability. In this article, we propose to improve the comprehensibility through a supervised learning method by automatic generation of fuzzy classification rules, designated SIFCO–PAF. Our method reduces the complexity by decreasing the number of rules and of antecedent conditions, making it thus adapted to the representation and the prediction of rather high-dimensional pattern classification problems. We perform, firstly, an ensemble methodology by combining a set of simple classification models. Subsequently, each model uses a subset of the initial attributes: In this case, we propose to regroup the attributes using linear correlation search among the training set elements. Secondly, we implement an optimal fuzzy partition thanks to supervised discretization followed by an automatic membership functions construction. The SIFCO–PAF method, analyzed experimentally on various data sets, guarantees an important reduction in the number of rules and of antecedents without deteriorating the classification rates, on the contrary accuracy is even improved.",2013,Knowledge and Information Systems volume 36 issue 2 pp 385-410,type 2 fuzzy sets and systems;fuzzy set operations;defuzzification;fuzzy classification;fuzzy number;neuro fuzzy;supervised learning;data mining;pattern recognition;machine learning;computer science;mathematics;
Improving clustering with pairwise constraints: a discriminative approach,Hong Zeng (Southeast University);Aiguo Song (Southeast University);Yiu Ming Cheung (Hong Kong Baptist University);,"2703553974,2676743142,2342408760","To obtain a user-desired and accurate clustering result in practical applications, one way is to utilize additional pairwise constraints that indicate the relationship between two samples, that is, whether these samples belong to the same cluster or not. In this paper, we put forward a discriminative learning approach which can incorporate pairwise constraints into the recently proposed two-class maximum margin clustering framework. In particular, a set of pairwise loss functions is proposed, which features robust detection and penalization for violating the pairwise constraints. Consequently, the proposed method is able to directly find the partitioning hyperplane, which can separate the data into two groups and satisfy the given pairwise constraints as much as possible. In this way, it makes fewer assumptions on the distance metric or similarity matrix for the data, which may be complicated in practice, than existing popular constrained clustering algorithms. Finally, an iterative updating algorithm is proposed for the resulting optimization problem. The experiments on a number of real-world data sets demonstrate that the proposed pairwise constrained two-class clustering algorithm outperforms several representative pairwise constrained clustering counterparts in the literature.",2013,Knowledge and Information Systems volume 36 issue 2 pp 489-515,correlation clustering;constrained clustering;fuzzy clustering;cluster analysis;pattern recognition;machine learning;mathematical optimization;mathematics;
A hybrid scheme for energy-efficient object tracking in sensor networks,Ming-Hua Hsieh (National Cheng Kung University);Kawuu Weicheng Lin (National Kaohsiung University of Applied Sciences);Vincent S. Tseng (National Cheng Kung University);,"2167380234,2121275728,2023431847","Energy saving is a critical issue in many sensor-network-based applications. Among the existing sensor-network-based applications, the surveillance application has attracted extensive attention. Object tracking in sensor networks (OTSNs) is a typical surveillance application. Previous studies on energy saving for OTSNs can be divided into two main approaches: (1) improvements in hardware design to lower the energy consumption of attached components and (2) improvements in software to predict the movement of objects. In this paper, we propose a novel scheme, namely hybrid tracking scheme (HTS), for tracking objects with energy efficiency. The scheme consists of the two parts: (1) adaptive schedule monitoring and (2) a recovery mechanism integrated with seamless temporal movement patterns and seeding-based flooding to relocate missing objects with the purpose of saving energy. Furthermore, we also propose a frequently visited periods mining algorithm, which discovers the corresponding frequently visited periods for adaptive schedule monitoring efficiently from the visitation information of sensor nodes. To decrease the number of sensor nodes activated in flooding, a seeding-based flooding mechanism is first proposed in our work. Empirical evaluations of various simulation conditions and real datasets show that the proposed HTS delivers excellent performance in terms of energy efficiency and low missing rates.",2013,Knowledge and Information Systems volume 36 issue 2 pp 359-384,computer security;data mining;real time computing;simulation;computer science;
Knowledge-aware identity services,Klaus-Dieter Schewe (Johannes Kepler University of Linz);Qing Wang (Australian National University);,"2194111320,2287519903","The identification problem is concerned with the question whether two objects in an application refer to the same real-world entity. In this paper, the identification problem is investigated from a knowledge modelling point of view. We develop a framework of establishing knowledge-aware identity services by abstracting identity knowledge into an additional identity layer. The knowledge model in the identity service layer provides a capability for combining declarative formulae with concrete data and thus allows us to capture domain-specific identity knowledge at flexible levels of abstraction. By adding validation constraints to the identity service, we are also able to reason about inconsistency of identity knowledge. In doing so, the accuracy of identity knowledge can be improved over time, especially when utilising identity services provided by different communities in a service-oriented architecture. Our experimental study shows the effectiveness of the proposed knowledge modelling approach and the effects of domain-specific identity knowledge on data quality control.",2013,Knowledge and Information Systems volume 36 issue 2 pp 335-357,knowledge management;data mining;database;computer science;
Pairwise ranking component analysis,Jean-François Pessiot (National Institute of Advanced Industrial Science and Technology);Hyeryung Kim (Tokyo Medical and Dental University);Wataru Fujibuchi (National Institute of Advanced Industrial Science and Technology);,"2254155332,2672396618,195903072","Uncovering the latent structure of the data is an active research topic in data mining. However, in the distance metric learning framework, previous studies have mainly focused on the classification performance. In this work, we consider the distance metric learning problem in the ranking setting, where predicting the order between the data vectors is more important than predicting the class labels. We focus on two problems: improving the ranking prediction accuracy and identifying the latent structure of the data. The core of our model consists of ranking the data using a Mahalanobis distance function. The additional use of non-negativity constraints and an entropy-based cost function allows us to simultaneously minimize the ranking error while identifying useful meta-features. To demonstrate its usefulness for information retrieval applications, we compare the performance of our method with four other methods on four UCI data sets, three text data sets, and four image data sets. Our approach shows good ranking accuracies, especially when few training data are available. We also use our model to extract and interpret the latent structure of the data sets. In addition, our approach is simple to implement and computationally efficient and can be used for data embedding and visualization.",2013,Knowledge and Information Systems volume 36 issue 2 pp 459-487,ranking;ranking svm;ranking;dimensionality reduction;data mining;pattern recognition;machine learning;computer science;mathematics;
Non-unique cluster numbers determination methods based on stability in spectral clustering,Sumuya Borjigin (Inner Mongolia University);Chonghui Guo (Dalian University of Technology);,"2137441160,2719303095","Recently, a large amount of work has been devoted to the study of spectral clustering—a simple yet powerful method for finding structure in a data set using spectral properties of an associated pairwise similarity matrix. Most of the existing spectral clustering algorithms estimate only one cluster number or estimate non-unique cluster numbers based on eigengap criterion. However, the number of clusters not always exists one, and eigengap criterion lacks theoretical justification. In this paper, we propose non-unique cluster numbers determination methods based on stability in spectral clustering (NCNDBS). We first utilize the multiway normalized cut spectral clustering algorithm to cluster data set for a candidate cluster number \(k\). Then the ratio value of the multiway normalized cut criterion of the obtained clusters and the sum of the leading eigenvalues (descending sort) of the stochastic transition matrix is chosen as a standard to decide whether the \(k\) is a reasonable cluster number. At last, by varying the scaling parameter in the Gaussian function, we judge whether the reasonable cluster number \(k\) is also a stability one. By three stages, we can determine non-unique cluster numbers of a data set. The Lumpability theorem concluded by Meil\(\breve{a}\) and Xu provides a theoretical base for our methods. NCNDBS can estimate non-unique cluster numbers of the data set successfully by illustrative experiments.",2013,Knowledge and Information Systems volume 36 issue 2 pp 439-458,k medians clustering;stability;discrete mathematics;combinatorics;statistics;mathematics;
Towards graphical models for text processing,Charu C. Aggarwal (IBM);Peixiang Zhao (Florida State University);,"2146335907,2169163720","The rapid proliferation of the World Wide Web has increased the importance and prevalence of text as a medium for dissemination of information. A variety of text mining and management algorithms have been developed in recent years such as clustering, classification, indexing, and similarity search. Almost all these applications use the well-known vector-space model for text representation and analysis. While the vector-space model has proven itself to be an effective and efficient representation for mining purposes, it does not preserve information about the ordering of the words in the representation. In this paper, we will introduce the concept of distance graph representations of text data. Such representations preserve information about the relative ordering and distance between the words in the graphs and provide a much richer representation in terms of sentence structure of the underlying data. Recent advances in graph mining and hardware capabilities of modern computers enable us to process more complex representations of text. We will see that such an approach has clear advantages from a qualitative perspective. This approach enables knowledge discovery from text which is not possible with the use of a pure vector-space representation, because it loses much less information about the ordering of the underlying words. Furthermore, this representation does not require the development of new mining and management techniques. This is because the technique can also be converted into a structural version of the vector-space representation, which allows the use of all existing tools for text. In addition, existing techniques for graph and XML data can be directly leveraged with this new representation. Thus, a much wider spectrum of algorithms is available for processing this representation. We will apply this technique to a variety of mining and management applications and show its advantages and richness in exploring the structure of the underlying text documents.",2013,Knowledge and Information Systems volume 36 issue 1 pp 1-21,co occurrence networks;noisy text analytics;full text search;text graph;document clustering;text mining;data science;information retrieval;data mining;database;machine learning;computer science;
Efficient and flexible anonymization of transaction data,Grigorios Loukides (Cardiff University);Aris Gkoulalas-Divanis (IBM);Jianhua Shao (Cardiff University);,"68021724,57288862,2157821589","Transaction data are increasingly used in applications, such as marketing research and biomedical studies. Publishing these data, however, may risk privacy breaches, as they often contain personal information about individuals. Approaches to anonymizing transaction data have been proposed recently, but they may produce excessively distorted and inadequately protected solutions. This is because these approaches do not consider privacy requirements that are common in real-world applications in a realistic and flexible manner, and attempt to safeguard the data only against either identity disclosure or sensitive information inference. In this paper, we propose a new approach that overcomes these limitations. We introduce a rule-based privacy model that allows data publishers to express fine-grained protection requirements for both identity and sensitive information disclosure. Based on this model, we also develop two anonymization algorithms. Our first algorithm works in a top-down fashion, employing an efficient strategy to recursively generalize data with low information loss. Our second algorithm uses sampling and a combination of top-down and bottom-up generalization heuristics, which greatly improves scalability while maintaining low information loss. Extensive experiments show that our algorithms significantly outperform the state-of-the-art in terms of retaining data utility, while achieving good protection and scalability.",2013,Knowledge and Information Systems volume 36 issue 1 pp 153-210,privacy software;transaction data;anonymity;scalability;privacy;efficiency;internet privacy;computer security;data mining;database;computer science;
A new approach for maximizing bichromatic reverse nearest neighbor search,Yubao Liu (Sun Yat-sen University);Raymond Chi-Wing Wong (Hong Kong University of Science and Technology);Ke Wang (Simon Fraser University);Zhijie Li (Sun Yat-sen University);Cheng Chen (Sun Yat-sen University);Zhitong Chen (Sun Yat-sen University);,"2307224481,2125027556,2626264286,2307236384,2620329262,2169818380","Maximizing bichromatic reverse nearest neighbor (MaxBRNN) is a variant of bichromatic reverse nearest neighbor (BRNN). The purpose of the MaxBRNN problem is to find an optimal region that maximizes the size of BRNNs. This problem has lots of real applications such as location planning and profile-based marketing. The best-known algorithm for the MaxBRNN problem is called MaxOverlap. In this paper, we study the MaxBRNN problem and propose a new approach called MaxSegment for a two-dimensional space when the \(L_2\)-norm is used. Then, we extend our algorithm to other variations of the MaxBRNN problem such as the MaxBRNN problem with other metric spaces, and a three-dimensional space. Finally, we conducted experiments on real and synthetic datasets to compare our proposed algorithm with existing algorithms. The experimental results verify the efficiency of our proposed approach.",2013,Knowledge and Information Systems volume 36 issue 1 pp 23-58,nearest neighbor chain algorithm;ball tree;fixed radius near neighbors;best bin first;cover tree;large margin nearest neighbor;nearest neighbor graph;nearest neighbor search;pattern recognition;machine learning;
Exploring heterogeneous information networks and random walk with restart for academic search,Meng-Fen Chiang (National Chiao Tung University);Jiun-Jiue Liou (National Chiao Tung University);Jen-Liang Wang (National Chengchi University);Wen-Chih Peng (National Chiao Tung University);Man-Kwan Shan (National Chengchi University);,"2696995032,2707783455,2126008518,2100961564,2007106882","In this paper, we explore heterogenous information networks in which each vertex represents one entity and the edges reflect linkage relationships. Heterogenous information networks contain vertices of several entity types, such as papers, authors and terms, and hence can fully reflect multiple linkage relationships among different entities. Such a heterogeneous information network is similar to a mixed media graph (MMG). By representing a bibliographic dataset as an MMG, the performance obtained when searching relevant entities (e.g., papers) can be improved. Furthermore, our academic search enables multiple-entity search, where a variety of entity search results are provided, such as relevant papers, authors and conferences, via a one-time query. Explicitly, given a bibliographic dataset, we propose a Global-MMG, in which a global heterogeneous information network is built. When a user submits a query keyword, we perform a random walk with restart (RWR) to retrieve papers or other types of entity objects. To reduce the query response time, algorithm Net-MMG (standing for NetClus-based MMG) is developed. Algorithm Net-MMG first divides a heterogeneous information network into a collection of sub-networks. Afterward, the Net-MMG performs a RWR on a set of selected relevant sub-networks. We implemented our academic search and conducted extensive experiments using the ACM Digital Library. The experimental results show that by exploring heterogeneous information networks and RWR, both the Global-MMG and Net-MMG achieve better search quality compared with existing academic search services. In addition, the Net-MMG has a shorter query response time while still guaranteeing good quality in search results.",2013,Knowledge and Information Systems volume 36 issue 1 pp 59-82,world wide web;information retrieval;data mining;database;artificial intelligence;machine learning;computer science;
DRAL: a tool for discovering relevant e-activities for learners,Amelia Zafra (University of Córdoba);Cristóbal Romero (University of Córdoba);Sebastián Ventura (University of Córdoba);,"2137006243,2107411838,2098297204","Web-based educational systems routinely collect vast quantities of data on students’ e-activity generating log files that offer researchers unique opportunities to apply data mining techniques and discover interesting information to improve the learning process. This paper proposes a friendly and intuitive tool called DRAL to detect the most relevant e-activities that a student needs to pass a course based on features extracted from logged data in an education web-based system. The method uses a more flexible representation of the available information based on multiple instance learning to prevent the appearance of a great number of missing values and is based on a multi-objective grammar guided genetic programming algorithm which obtains simple and clear classification rules which are markedly useful to identify the number, type and time of e-activities more relevant so that a student has a high probability to pass a course. To validate this approach, our proposal is compared with the most traditional proposals in multiple instance learning over the years. Experimental results demonstrate that the approach proposed successfully improves the accuracy of previous models by finding a balance between specificity and sensitivity values.",2013,Knowledge and Information Systems volume 36 issue 1 pp 211-250,genetic programming;web mining;data science;data mining;database;artificial intelligence;machine learning;computer science;
A new approach to radial basis function-based polynomial neural networks: analysis and design,Sung-Kwun Oh (University of Suwon);Ho-Sung Park (Wonkwang University);Wook-Dong Kim (University of Suwon);Witold Pedrycz (University of Alberta);,"2618016734,2166548767,2128314703,136859070","In this study, we introduce a new topology of radial basis function-based polynomial neural networks (RPNNs) that is based on a genetically optimized multi-layer perceptron with radial polynomial neurons (RPNs). This paper offers a comprehensive design methodology involving various mechanisms of optimization, especially fuzzy C-means (FCM) clustering and particle swarm optimization (PSO). In contrast to the typical architectures encountered in polynomial neural networks (PNNs), our main objective is to develop a topology and establish a comprehensive design strategy of RPNNs: (a) The architecture of the proposed network consists of radial polynomial neurons (RPN). These neurons are fully reflective of the structure encountered in numeric data, which are granulated with the aid of FCM clustering. RPN dwells on the concepts of a collection of radial basis function and the function-based nonlinear polynomial processing. (b) The PSO-based design procedure being applied to each layer of the RPNN leads to the selection of preferred nodes of the network whose local parameters (such as the number of input variables, a collection of the specific subset of input variables, the order of the polynomial, the number of clusters of FCM clustering, and a fuzzification coefficient of the FCM method) are properly adjusted. The performance of the RPNN is quantified through a series of experiments where we use several modeling benchmarks, namely a synthetic three-dimensional data and learning machine data (computer hardware data, abalone data, MPG data, and Boston housing data) already used in neuro-fuzzy modeling. A comparative analysis shows that the proposed RPNN exhibits higher accuracy in comparison with some previous models available in the literature.",2013,Knowledge and Information Systems volume 36 issue 1 pp 121-151,particle swarm optimization;data mining;artificial intelligence;machine learning;mathematical optimization;computer science;mathematics;
A feature-free and parameter-light multi-task clustering framework,Thach Nguyen Huy (Kyushu University);Hao Shao (Kyushu University);Bin Tong (Kyushu University);Einoshin Suzuki (Kyushu University);,"2496866584,2515944123,2250175026,2228069075","The two last decades have witnessed extensive research on multi-task learning algorithms in diverse domains such as bioinformatics, text mining, natural language processing as well as image and video content analysis. However, all existing multi-task learning methods require either domain-specific knowledge to extract features or a careful setting of many input parameters. There are many disadvantages associated with prior knowledge requirements for feature extraction or parameter-laden approaches. One of the most obvious problems is that we may find a wrong or non-existent pattern because of poorly extracted features or incorrectly set parameters. In this work, we propose a feature-free and parameter-light multi-task clustering framework to overcome these disadvantages. Our proposal is motivated by the recent successes of Kolmogorov-based methods on various applications. However, such methods are only defined for single-task problems because they lack a mechanism to share knowledge between different tasks. To address this problem, we create a novel dictionary-based compression dissimilarity measure that allows us to share knowledge across different tasks effectively. Experimental results with extensive comparisons demonstrate the generality and the effectiveness of our proposal.",2013,Knowledge and Information Systems volume 36 issue 1 pp 251-276,cluster analysis;data mining;database;artificial intelligence;machine learning;statistics;computer science;mathematics;
Out-of-core detection of periodicity from sequence databases,Faraz Rasheed (University of Calgary);Muhaimenul Adnan (University of Calgary);Reda Alhajj (University of Calgary);,"1969483463,2150072323,497422635","In this paper, we address the scalability problem of periodicity detection for time series and sequence databases. We present time and space efficient periodicity detection method that efficiently uses external memory (disk) when the series cannot be processed inside the available main memory. Our approach uses suffix tree to facilitate periodicity detection. We consider two cases, namely in-core and out of core. First, we optimize storage requirements of the suffix tree to be able to fit larger suffix trees in-core. This guarantees the ability to mine larger sequences when everything can be kept in-core, which is what the current periodicity detection approaches are able to mine. Second, when the data structures go out of core, we extend the suffix tree construction part to use external memory. We are able to achieve this while maintaining linear time complexity. As a result, when we go out of core, we can mine databases that require considerably larger space to keep the data structures compared to the available main memory. For the out-of-core periodicity detection part, the proposed method allows the required data structures to be kept on external memory partially when a memory overflow situation occurs. Various pruning strategies are also proposed to allow the proposed approach to process large sequences within reasonable amount of time. Additionally, we introduced the notion of “emulated tree traversal” for fast suffix tree traversal. Due to all these special considerations, we are able to mine much larger sequences compared to other existing periodicity detection algorithms. To demonstrate the applicability, power, and effectiveness of the proposed framework, we present results of periodicity detection up to 500 MB of time sequence data, which (to the best of our knowledge) is the largest reported sequence mined for periodicity detection ever.",2013,Knowledge and Information Systems volume 36 issue 1 pp 277-301,compressed suffix array;sequential pattern mining;theoretical computer science;data mining;algorithm;computer science;
Common understanding in a multi-agent system using ontology-guided learning,Mohsen Afsharchi (University of Zanjan);Arman Didandeh (University of Western Ontario);Nima Mirbakhsh (University of Western Ontario);Behrouz H. Far (University of Calgary);,"295635017,128475293,1758422170,2115501983","Traditionally, communication among agents has been established based on the group commitment to a common ontology which is unfortunately often too strong or unrealistic. In the real world of communicating agents, it is preferred to enable agents to exchange information while they keep their own individual ontology. While this assumption makes agents represent their knowledge more independently and give them more flexibility, it also adds to the complexity of communication. We believe that agents can overcome this complexity by using their learning capability. The agents can learn any concept they do not know but want to communicate about with other agents in the multi-agent system where they work in. Our goal in this paper is to present a general method for agents using ontologies to teach each other concepts to improve their communication, and therefore cooperation abilities. In our method, a particular agent that understands a concept only ambiguously intends to learn it by receiving positive and negative examples for that concept from the other agents. Then, utilizing one of the known concept learning methods, the agent learns the concept in question. In case of conflicts in the received set of examples, the learning agent asks other agents again to get involved in the learning process by taking votes. While this method allows agents not to share common ontologies, it enables agents to establish common grounds on the concepts known only by some of them if these common grounds are needed during cooperation. In fact, the learned concepts by an agent are compromised among the views of other agents the method improves the autonomy of agents using them significantly.",2013,Knowledge and Information Systems volume 36 issue 1 pp 83-120,embodied agent;feature;object;concept;probability;multi agent system;ontology;knowledge management;artificial intelligence;programming language;computer science;
CLOVER: a faster prior-free approach to rare-category detection,Hao Huang (Zhejiang University);Qinming He (Zhejiang University);Kevin Chiew;Feng Qian (Zhejiang University);Lianhang Ma (Zhejiang University);,"2230010131,2133238876,2162472101,2651270214,2157003340","Rare-category detection helps discover new rare classes in an unlabeled data set by selecting their candidate data examples for labeling. Most of the existing approaches for rare-category detection require prior information about the data set without which they are otherwise not applicable. The prior-free algorithms try to address this problem without prior information about the data set; though, the compensation is high time complexity, which is not lower than \(O(dN^2)\) where \(N\) is the number of data examples in a data set and \(d\) is the data set dimension. In this paper, we propose CLOVER a prior-free algorithm by introducing a novel rare-category criterion known as local variation degree (LVD), which utilizes the characteristics of rare classes for identifying rare-class data examples from other types of data examples and passes those data examples with maximum LVD values to CLOVER for labeling. A remarkable improvement is that CLOVER’s time complexity is \(O(dN^{2-1/d})\) for \(d > 1\) or \(O(N\log N)\) for \(d = 1\). Extensive experimental results on real data sets demonstrate the effectiveness and efficiency of our method in terms of new rare classes discovery and lower time complexity.",2013,Knowledge and Information Systems volume 35 issue 3 pp 713-736,data mining;artificial intelligence;machine learning;statistics;algorithm;mathematics;
Mining indirect antagonistic communities from social interactions,Kuan Zhang (Singapore Management University);David Lo (Singapore Management University);Ee-Peng Lim (Singapore Management University);Philips Kokoh Prasetyo (Singapore Management University);,"2170883979,2132927693,2130308643,2106800246","Antagonistic communities refer to groups of people with opposite tastes, opinions, and factions within a community. Given a set of interactions among people in a community, we develop a novel pattern mining approach to mine a set of antagonistic communities. In particular, based on a set of user-specified thresholds, we extract a set of pairs of communities that behave in opposite ways with one another. We focus on extracting a compact lossless representation based on the concept of closed patterns to prevent exploding the number of mined antagonistic communities. We also present a variation of the algorithm using a divide and conquer strategy to handle large datasets when main memory is inadequate. The scalability of our approach is tested on synthetic datasets of various sizes mined using various parameters. Case studies on Amazon, Epinions, and Slashdot datasets further show the efficiency and the utility of our approach in extracting antagonistic communities from social interactions.",2013,Knowledge and Information Systems volume 35 issue 3 pp 553-583,divide and conquer algorithms;social network;numerical analysis;social relation;bioinformatics;data mining;machine learning;computer science;
Quantifying explainable discrimination and removing illegal discrimination in automated decision making,F Faisal Kamiran (King Abdullah University of Science and Technology);I Indré Zliobaite (Bournemouth University);T Toon Calders (Eindhoven University of Technology);,"2072234693,2057986279,2064105222","Recently, the following discrimination-aware classification problem was introduced. Historical data used for supervised learning may contain discrimination, for instance, with respect to gender. The question addressed by discrimination-aware techniques is, given sensitive attribute, how to train discrimination-free classifiers on such historical data that are discriminative, with respect to the given sensitive attribute. Existing techniques that deal with this problem aim at removing all discrimination and do not take into account that part of the discrimination may be explainable by other attributes. For example, in a job application, the education level of a job candidate could be such an explainable attribute. If the data contain many highly educated male candidates and only few highly educated women, a difference in acceptance rates between woman and man does not necessarily reflect gender discrimination, as it could be explained by the different levels of education. Even though selecting on education level would result in more males being accepted, a difference with respect to such a criterion would not be considered to be undesirable, nor illegal. Current state-of-the-art techniques, however, do not take such gender-neutral explanations into account and tend to overreact and actually start reverse discriminating, as we will show in this paper. Therefore, we introduce and analyze the refined notion of conditional non-discrimination in classifier design. We show that some of the differences in decisions across the sensitive groups can be explainable and are hence tolerable. Therefore, we develop methodology for quantifying the explainable discrimination and algorithmic techniques for removing the illegal discrimination when one or more attributes are considered as explanatory. Experimental evaluation on synthetic and real-world classification datasets demonstrates that the new techniques are superior to the old ones in this new context, as they succeed in removing almost exclusively the undesirable discrimination, while leaving the explainable differences unchanged, allowing for differences in decisions as long as they are explainable.",2013,Knowledge and Information Systems volume 35 issue 3 pp 613-644,independence;biological classification;data mining;artificial intelligence;machine learning;statistics;
A study on semi-supervised FCM algorithm,Shan Zeng (Huazhong University of Science and Technology);Xiaojun Tong;Nong Sang (Huazhong University of Science and Technology);Rui Huang (Huazhong University of Science and Technology);,"2303555672,2704296471,2153253016,2669335317","Most variants of fuzzy c-means (FCM) clustering algorithms involving prior knowledge are generally based on the modification of the objective function or the clustering process. This paper proposes a new weighted semi-supervised FCM algorithm (SSFCM-HPR) that transforms the prior knowledge in the labeled samples into constraint conditions in terms of fuzzy membership degrees, assigns different weights according to the representativeness of the samples, and then uses the HPR multiplier to solve the clustering problem. The “representativeness” of the labeled samples is decided by their distances to the cluster centers they belong to. In this paper, we take the ratio of the largest to the second largest fuzzy membership degree from a labeled sample as its weight. This algorithm not only retains the fuzzy partition of the labeled samples, which guarantees the effective guidance on the clustering process, but also can detect whether a sample is an outlier or not. Moreover, when part of the supervised information of the labeled samples is wrong, this algorithm can reduce the influence of the incorrectly labeled samples on the final clustering results. The experimental evaluation on synthetic and real data sets demonstrates the efficiency and effectiveness of our approach.",2013,Knowledge and Information Systems volume 35 issue 3 pp 585-612,canopy clustering algorithm;fuzzy clustering;data mining;machine learning;statistics;mathematics;
DEMass: a new density estimator for big data,Kai Ming Ting (Monash University);Takashi Washio (Osaka University);Jonathan R. Wells (Monash University);Fei Tony Liu (Monash University);Sunil Aryal (Monash University);,"2121496889,1794038515,2136601004,2681393184,2172477944","Density estimation is the ubiquitous base modelling mechanism employed for many tasks including clustering, classification, anomaly detection and information retrieval. Commonly used density estimation methods such as kernel density estimator and \(k\)-nearest neighbour density estimator have high time and space complexities which render them inapplicable in problems with big data. This weakness sets the fundamental limit in existing algorithms for all these tasks. We propose the first density estimation method, having average case sub-linear time complexity and constant space complexity in the number of instances, that stretches this fundamental limit to an extent that dealing with millions of data can now be done easily and quickly. We provide an asymptotic analysis of the new density estimator and verify the generality of the method by replacing existing density estimators with the new one in three current density-based algorithms, namely DBSCAN, LOF and Bayesian classifiers, representing three different data mining tasks of clustering, anomaly detection and classification. Our empirical evaluation results show that the new density estimation method significantly improves their time and space complexities, while maintaining or improving their task-specific performances in clustering, anomaly detection and classification. The new method empowers these algorithms, currently limited to small data size only, to process big data—setting a new benchmark for what density-based algorithms can achieve.",2013,Knowledge and Information Systems volume 35 issue 3 pp 493-524,variable kernel density estimation;multivariate kernel density estimation;density estimation;data mining;machine learning;statistics;computer science;mathematics;
Reduced ordered binary decision diagram with implied literals: a new knowledge compilation approach,Yong Lai (Jilin University);Dayou Liu;Shengsheng Wang (Jilin University);,"2116967643,2720381407,2642165478","Reduced ordered binary decision diagram (ROBDD) is one of the most influential knowledge compilation languages. We generalize it by associating some implied literals with each node to propose a new language called ROBDD with implied literals (ROBDD-\(L\)) and show that ROBDD-\(L\) can meet most of the querying requirements involved in the knowledge compilation map. Then, we discuss a kind of subsets of ROBDD-\(L\) called ROBDD-\(L_i\) with precisely \(i\) implied literals \((0\le i\le \infty )\), where ROBDD-\(L_0\) is isomorphic to ROBDD. ROBDD-\(L_i\) has uniqueness over any given linear order of variables. We mainly focus on ROBDD-\(L_\infty \) and demonstrate that: (a) it is a canonical representation on any given variable order; (b) it is the most succinct subset in ROBDD-\(L\) and thus also meets most of the querying requirements; (c) given any logical operation ROBDD supports in polytime, ROBDD-\(L_\infty \) can also support it in time polynomial in the sizes of the equivalent ROBDDs. Moreover, we propose an ROBDD-\(L_i\) compilation algorithm for any \(i\) and an ROBDD-\(L_\infty \) compilation algorithm, and then we implement an ROBDD-\(L\) package called BDDjLu. Our preliminary experimental results indicate that: (a) the compilation results of ROBDD-\(L_\infty \) are significantly smaller than those of ROBDD; (b) the standard d-DNNF compiler c2d and our ROBDD-\(L_\infty \) compiler do not dominate the other, yet ROBDD-\(L_\infty \) has canonicity and supports more querying requirements and relatively efficient logical operations; and (c) the method that first compiles knowledge base into ROBDD-\(L_\infty \) and then converts ROBDD-\(L_\infty \) into ROBDD provides an efficient ROBDD compiler.",2013,Knowledge and Information Systems volume 35 issue 3 pp 665-712,knowledge base;theoretical computer science;data mining;database;artificial intelligence;machine learning;algorithm;computer science;mathematics;
"Gender, writing and ranking in review forums: a case study of the IMDb",Jahna Otterbacher (Illinois Institute of Technology);,2132348783,"Online review forums provide consumers with essential information about goods and services by facilitating word-of-mouth communication. Despite that preferences are correlated to demographic characteristics, reviewer gender is not often provided on user profiles. We consider the case of the internet movie database (IMDb), where users exchange views on movies. Like many forums, IMDb employs collaborative filtering such that by default, reviews are ranked by perceived utility. IMDb also provides a unique gender filter that displays an equal number of reviews authored by men and women. Using logistic classification, we compare reviews with respect to writing style, content and metadata features. We find salient differences in stylistic features and content between reviews written by men and women, as predicted by sociolinguistic theory. However, utility is the best predictor of gender, with women’s reviews perceived as being much less useful than those written by men. While we cannot observe who votes at IMDb, we do find that highly rated female-authored reviews exhibit “male” characteristics. Our results have implications for which contributions are likely to be seen, and to what extent participants get a balanced view as to “what others think” about an item.",2013,Knowledge and Information Systems volume 35 issue 3 pp 645-664,multimedia;world wide web;data mining;
Adapting ADtrees for improved performance on large datasets with high-arity features,Robert Van Dam (Brigham Young University);Irene Langkilde-Geary (Brigham Young University);Dan Ventura (Brigham Young University);,"2097242257,41800997,2117320897","The ADtree, a data structure useful for caching sufficient statistics, has been successfully adapted to grow lazily when memory is limited and to update sequentially with an incrementally updated dataset. However, even these modified forms of the ADtree still exhibit inefficiencies in terms of both space usage and query time, particularly on datasets with very high dimensionality and with high-arity features. We propose four modifications to the ADtree, each of which can be used to improve size and query time under specific types of datasets and features. These modifications also provide an increased ability to precisely control how an ADtree is built and to tune its size given external memory or speed requirements.",2013,Knowledge and Information Systems volume 35 issue 3 pp 525-552,treebank;sufficient statistic;data structure;data mining;database;machine learning;computer science;
A survey on instance selection for active learning,Yifan Fu (Information Technology University);Xingquan Zhu (Information Technology University);Bin Li (Information Technology University);,"2149640560,2618356905,2565587667","Active learning aims to train an accurate prediction model with minimum cost by labeling most informative instances. In this paper, we survey existing works on active learning from an instance-selection perspective and classify them into two categories with a progressive relationship: (1) active learning merely based on uncertainty of independent and identically distributed (IID) instances, and (2) active learning by further taking into account instance correlations. Using the above categorization, we summarize major approaches in the field, along with their technical strengths/weaknesses, followed by a simple runtime performance comparison, and discussion about emerging active learning applications and instance-selection challenges therein. This survey intends to provide a high-level summa- rization for active learning and motivates interested readers to consider instance-selection approaches for designing effective active learning solutions.",2013,Knowledge and Information Systems volume 35 issue 2 pp 249-283,active learning;proactive learning;semi supervised learning;instance based learning;data mining;pattern recognition;artificial intelligence;machine learning;computer science;
Efficient greedy feature selection for unsupervised learning,Ahmed K. Farahat (University of Waterloo);Ali Ghodsi (University of Waterloo);Mohamed S. Kamel (University of Waterloo);,"2107050089,734473377,2096364092","Reducing the dimensionality of the data has been a challenging task in data mining and machine learning applications. In these applications, the existence of irrelevant and redundant features negatively affects the efficiency and effectiveness of different learning algorithms. Feature selection is one of the dimension reduction techniques, which has been used to allow a better understanding of data and improve the performance of other learning tasks. Although the selection of relevant features has been extensively studied in supervised learning, feature selection in the absence of class labels is still a challenging task. This paper proposes a novel method for unsupervised feature selection, which efficiently selects features in a greedy manner. The paper first defines an effective criterion for unsupervised feature selection that measures the reconstruction error of the data matrix based on the selected subset of features. The paper then presents a novel algorithm for greedily minimizing the reconstruction error based on the features selected so far. The greedy algorithm is based on an efficient recursive formula for calculating the reconstruction error. Experiments on real data sets demonstrate the effectiveness of the proposed algorithm in comparison with the state-of-the-art methods for unsupervised feature selection.",2013,Knowledge and Information Systems volume 35 issue 2 pp 285-310,greedy algorithm;dimensionality reduction;feature;feature learning;semi supervised learning;feature selection;unsupervised learning;data mining;pattern recognition;machine learning;computer science;
Confidence in predictions from random tree ensembles,Siddhartha Bhattacharyya (College of Business Administration);,2518403447,"Obtaining an indication of confidence of predictions is desirable for many data mining applications. Predictions complemented with confidence levels can inform on the certainty or extent of reliability that may be associated with the prediction. This can be useful in varied application contexts where model outputs form the basis for potentially costly decisions, and in general across risk sensitive applications. The conformal prediction framework presents a novel approach for obtaining valid confidence measures associated with predictions from machine learning algorithms. Confidence levels are obtained from the underlying algorithm, using a non-conformity measure which indicates how ‘atypical’ a given example set is. The non-conformity measure is a key to determining the usefulness and efficiency of the approach. This paper considers inductive conformal prediction in the context of random tree ensembles like random forests, which have been noted to perform favorably across problems. Focusing on classification tasks, and considering realistic data contexts including class imbalance, we develop non-conformity measures for assessing the confidence of predicted class labels from random forests. We examine the performance of these measures on multiple data sets. Results demonstrate the usefulness and validity of the measures, their relative differences, and highlight the effectiveness of conformal prediction random forests for obtaining predictions with associated confidence.",2013,Knowledge and Information Systems volume 35 issue 2 pp 391-410,confidence and prediction bands;random forest;biological classification;data mining;machine learning;statistics;computer science;mathematics;
An efficient concept-based retrieval model for enhancing text retrieval quality,Shady Shehata (University of Waterloo);Fakhri Karray (University of Waterloo);Mohamed S. Kamel (University of Waterloo);,"2018912633,2011220966,2096364092","Most of the common techniques in text retrieval are based on the statistical analysis terms (words or phrases). Statistical analysis of term frequency captures the importance of the term within a document only. Thus, to achieve a more accurate analysis, the underlying model should indicate terms that capture the semantics of text. In this case, the model can capture terms that represent the concepts of the sentence, which leads to discovering the topic of the document. In this paper, a new concept-based retrieval model is introduced. The proposed concept-based retrieval model consists of conceptual ontological graph (COG) representation and concept-based weighting scheme. The COG representation captures the semantic structure of each term within a sentence. Then, all the terms are placed in the COG representation according to their contribution to the meaning of the sentence. The concept-based weighting analyzes terms at the sentence and document levels. This is different from the classical approach of analyzing terms at the document level only. The weighted terms are then ranked, and the top concepts are used to build a concept-based document index for text retrieval. The concept-based retrieval model can effectively discriminate between unimportant terms with respect to sentence semantics and terms which represent the concepts that capture the sentence meaning. Experiments using the proposed concept-based retrieval model on different data sets in text retrieval are conducted. The experiments provide comparison between traditional approaches and the concept-based retrieval model obtained by the combined approach of the conceptual ontological graph and the concept-based weighting scheme. The evaluation of results is performed using three quality measures, the preference measure (bpref), precision at 10 documents retrieved (P(10)) and the mean uninterpolated average precision (MAP). All of these quality measures are improved when the newly developed concept-based retrieval model is used, confirming that such model enhances the quality of text retrieval.",2013,Knowledge and Information Systems volume 35 issue 2 pp 411-434,divergence from randomness model;term discrimination;concept search;visual word;vector space model;document retrieval;natural language processing;information retrieval;data mining;computer science;
Structured feature selection and task relationship inference for multi-task learning,Hongliang Fei (University of Kansas);Jun Huan (University of Kansas);,"2125525699,2139058963","Multi-task learning (MTL) aims to enhance the generalization performance of supervised regression or classification by learning multiple related tasks simultaneously. In this paper, we aim to extend the current MTL techniques to high dimensional data sets with structured input and structured output (SISO), where the SI means the input features are structured and the SO means the tasks are structured. We investigate a completely ignored problem in MTL with SISO data: the interplay of structured feature selection and task relationship modeling. We hypothesize that combining the structure information of features and task relationship inference enables us to build more accurate MTL models. Based on the hypothesis, we have designed an efficient learning algorithm, in which we utilize a task covariance matrix related to the model parameters to capture the task relationship. In addition, we design a regularization formulation for incorporating the structured input features in MTL. We have developed an efficient iterative optimization algorithm to solve the corresponding optimization problem. Our algorithm is based on the accelerated first order gradient method in conjunction with the projected gradient scheme. Using two real-world data sets, we demonstrate the utility of the proposed learning methods.",2013,Knowledge and Information Systems volume 35 issue 2 pp 345-364,data mining;pattern recognition;artificial intelligence;machine learning;mathematical optimization;mathematics;
D-cores: measuring collaboration of directed graphs based on degeneracy,Christos Giatsidis (École Polytechnique);Dimitrios M. Thilikos (National and Kapodistrian University of Athens);Michalis Vazirgiannis (École Polytechnique);,"1958188848,1926143158,1914497179","Community detection and evaluation is an important task in graph mining. In many cases, a community is defined as a subgraph characterized by dense connections or interactions between its nodes. A variety of measures are proposed to evaluate different quality aspects of such communities—in most cases ignoring the directed nature of edges. In this paper, we introduce novel metrics for evaluating the collaborative nature of directed graphs—a property not captured by the single node metrics or by other established community evaluation metrics. In order to accomplish this objective, we capitalize on the concept of graph degeneracy and define a novel D-core framework, extending the classic graph-theoretic notion of \(k\)-cores for undirected graphs to directed ones. Based on the D-core, which essentially can be seen as a measure of the robustness of a community under degeneracy, we devise a wealth of novel metrics used to evaluate graph collaboration features of directed graphs. We applied the D-core approach on large synthetic and real-world graphs such as Wikipedia, DBLP, and ArXiv and report interesting results at the graph as well at the node level.",2013,Knowledge and Information Systems volume 35 issue 2 pp 311-343,implicit graph;degeneracy;voltage graph;forbidden graph characterization;tree depth;moral graph;comparability graph;feedback arc set;null graph;dependency graph;graph property;degeneracy;modular decomposition;graph;null model;directed acyclic graph;directed graph;management science;discrete mathematics;combinatorics;mathematics;
Anonymizing bag-valued sparse data by semantic similarity-based clustering,Junqiang Liu (Zhejiang Gongshang University);Ke Wang (Simon Fraser University);,"2338946638,2626264286","Web query logs provide a rich wealth of information, but also present serious privacy risks. We preserve privacy in publishing vocabularies extracted from a web query log by introducing vocabulary k-anonymity, which prevents the privacy attack of re-identification that reveals the real identities of vocabularies. A vocabulary is a bag of query-terms extracted from queries issued by a user at a specified granularity. Such bag-valued data are extremely sparse, which makes it hard to retain enough utility in enforcing k-anonymity. To the best of our knowledge, the prior works do not solve such a problem, among which some achieve a different privacy principle, for example, differential privacy, some deal with a different type of data, for example, set-valued data or relational data, and some consider a different publication scenario, for example, publishing frequent keywords. To retain enough data utility, a semantic similarity-based clustering approach is proposed, which measures the semantic similarity between a pair of terms by the minimum path distance over a semantic network of terms such as WordNet, computes the semantic similarity between two vocabularies by a weighted bipartite matching, and publishes the typical vocabulary for each cluster of semantically similar vocabularies. Extensive experiments on the AOL query log show that our approach can retain enough data utility in terms of loss metrics and in frequent pattern mining.",2013,Knowledge and Information Systems volume 35 issue 2 pp 435-461,anonymity;semantic similarity;privacy;information retrieval;data mining;database;computer science;
Extended MDL principle for feature-based inductive transfer learning,Hao Shao (Kyushu University);Bin Tong (Kyushu University);Einoshin Suzuki (Kyushu University);,"2515944123,2250175026,2228069075","Transfer learning provides a solution in real applications of how to learn a target task where a large amount of auxiliary data from source domains are given. Despite numerous research studies on this topic, few of them have a solid theoretical framework and are parameter-free. In this paper, we propose an Extended Minimum Description Length Principle (EMDLP) for feature-based inductive transfer learning, in which both the source and the target data sets contain class labels and relevant features are transferred from the source domain to the target one. Unlike conventional methods, our encoding measure is based on a theoretical background and has no parameter. To obtain useful features to be used in the target task, we design an enhanced encoding length by adopting a code book that stores useful information obtained from the source task. With the code book that builds connections between the source and the target tasks, our EMDLP is able to evaluate the inferiority of the results of transfer learning with the add sum of the code lengths of five components: those of the corresponding two hypotheses, the two data sets with the help of the hypotheses, and the set of the transferred features. The proposed method inherits the nice property of the MDLP that elaborately evaluates the hypotheses and balances the simplicity of the hypotheses and the goodness-of-the-fit to the data. Extensive experiments using both synthetic and real data sets show that the proposed method provides a better performance in terms of the classification accuracy and is robust against noise.",2013,Knowledge and Information Systems volume 35 issue 2 pp 365-389,codebook;case based reasoning;occam s razor;robustness;information theory;artificial intelligence;machine learning;statistics;algorithm;computer science;
"An order-sorted query system for sort, predicate, and meta-predicate hierarchies",Ken Kaneiwa (Iwate University);Philip H. P. Nguyen (Attorney-General's Department);,"1266120892,2304176968","This paper presents a decidable order-sorted query system for reasoning between ontologies and rules. We describe order-sorted logic programming with sort, predicate, and meta-predicate hierarchies (OSL3h), which derives predicate and meta-predicate assertions. Meta-level predicates (predicates of predicates) are useful for representing relationships between predicate formulas, and further, they conceptually yield a hierarchy similar to the hierarchies of sorts and predicates. By extending the order-sorted Horn-clause calculus, we develop a query-answering system in OSL3h that can answer queries such as atoms and meta-atoms generalized by containing predicate variables. We show that the expressive query-answering system computes every generalized query in single exponential time, that is, the complexity of our query system is equal to that of DATALOG.",2013,Knowledge and Information Systems volume 35 issue 2 pp 463-492,predicate variable;predicate;syntactic predicate;functional predicate;predicate transformer semantics;semantic web;logic programming;database;programming language;algorithm;computer science;mathematics;
A signed-distance-based approach to importance assessment and multi-criteria group decision analysis based on interval type-2 fuzzy set,Ting-Yu Chen (Chang Gung University);,2676748608,"Interval type-2 fuzzy sets are associated with greater imprecision and more ambiguities than ordinary fuzzy sets. This paper presents a signed-distance-based method for determining the objective importance of criteria and handling fuzzy, multiple criteria group decision-making problems in a flexible and intelligent way. These advantages arise from the method’s use of interval type-2 trapezoidal fuzzy numbers to represent alternative ratings and the importance of various criteria. An integrated approach to determine the overall importance of the criteria is also developed using the subjective information provided by decision-makers and the objective information delivered by the decision matrix. In addition, a linear programming model is developed to estimate criterion weights and to extend the proposed multiple criteria decision analysis method. Finally, the feasibility and effectiveness of the proposed methods are illustrated by a group decision-making problem of patient-centered medicine in basilar artery occlusion.",2013,Knowledge and Information Systems volume 35 issue 1 pp 193-231,fuzzy set operations;signed distance function;defuzzification;fuzzy classification;fuzzy number;linear programming;discrete mathematics;artificial intelligence;mathematical optimization;mathematics;
Supervised term weighting centroid-based classifiers for text categorization,Tam T. Nguyen (Nanyang Technological University);Kuiyu Chang (Nanyang Technological University);Siu Cheung Hui (Nanyang Technological University);,"2119014062,2147346151,2166513862","In this paper, we study the theoretical properties of the class feature centroid (CFC) classifier by considering the rate of change of each prototype vector with respect to individual dimensions (terms). We show that CFC is inherently biased toward the larger (dominant majority) classes, which invariably leads to poor performance on class-imbalanced data. CFC also aggressively prune terms that appear across all classes, discarding some non-exclusive but useful terms. To overcome these CFC limitations while retaining its intrinsic and worthy design goals, we propose an improved centroid-based classifier that uses precise term-class distribution properties instead of presence or absence of terms in classes. Specifically, terms are weighted based on the Kullback–Leibler (KL) divergence measure between pairs of class-conditional term probabilities; we call this the CFC–KL centroid classifier. We then generalize CFC–KL to handle multi-class data by replacing the KL measure with the multi-class Jensen–Shannon (JS) divergence, called CFC–JS. Our proposed supervised term weighting schemes have been evaluated on 5 datasets; KL and JS weighted classifiers consistently outperformed baseline CFC and unweighted support vector machines (SVM). We also devise a word cloud visualization approach to highlight the important class-specific words picked out by our KL and JS term weighting schemes, which were otherwise obscured by unsupervised term weighting. The experimental and visualization results show that KL and JS term weighting not only notably improve centroid-based classifiers, but also benefit SVM classifiers as well.",2013,Knowledge and Information Systems volume 35 issue 1 pp 61-85,jensen shannon divergence;kullback leibler divergence;support vector machine;data mining;pattern recognition;machine learning;statistics;computer science;mathematics;
Overlapping correlation clustering,Francesco Bonchi (Yahoo!);Aristides Gionis (Yahoo!);Antti Ukkonen (Yahoo!);,"2176652147,737311942,2635975366","We introduce a new approach for finding overlapping clusters given pairwise similarities of objects. In particular, we relax the problem of correlation clustering by allowing an object to be assigned to more than one cluster. At the core of our approach is an optimization problem in which each data point is mapped to a small set of labels, representing membership in different clusters. The objective is to find a mapping so that the given similarities between objects agree as much as possible with similarities taken over their label sets. The number of labels can vary across objects. To define a similarity between label sets, we consider two measures: (i) a 0–1 function indicating whether the two label sets have non-zero intersection and (ii) the Jaccard coefficient between the two label sets. The algorithm we propose is an iterative local-search method. The definitions of label set similarity give rise to two non-trivial optimization problems, which, for the measures of set-intersection and Jaccard, we solve using a greedy strategy and non-negative least squares, respectively. We also develop a distributed version of our algorithm based on the BSP model and implement it using a Pregel framework. Our algorithm uses as input pairwise similarities of objects and can thus be applied when clustering structured objects for which feature vectors are not available. As a proof of concept, we apply our algorithms on three different and complex application domains: trajectories, amino-acid sequences, and textual documents.",2013,Knowledge and Information Systems volume 35 issue 1 pp 1-32,data mining;pattern recognition;machine learning;mathematics;
Lightly supervised acquisition of named entities and linguistic patterns for multilingual text mining,César de Pablo-Sánchez (Charles III University of Madrid);Isabel Segura-Bedmar (Charles III University of Madrid);Paloma Martínez (Charles III University of Madrid);Ana Iglesias-Maqueda (Charles III University of Madrid);,"89549634,162655220,2099769895,2346796208","Named Entity Recognition and Classification (NERC) is an important component of applications like Opinion Tracking, Information Extraction, or Question Answering. When these applications require to work in several languages, NERC becomes a bottleneck because its development requires language-specific tools and resources like lists of names or annotated corpora. This paper presents a lightly supervised system that acquires lists of names and linguistic patterns from large raw text collections in western languages and starting with only a few seeds per class selected by a human expert. Experiments have been carried out with English and Spanish news collections and with the Spanish Wikipedia. Evaluation of NE classification on standard datasets shows that NE lists achieve high precision and reveals that contextual patterns increase recall significantly. Therefore, it would be helpful for applications where annotated NERC data are not available such as those that have to deal with several western languages or information from different domains.",2013,Knowledge and Information Systems volume 35 issue 1 pp 87-109,information extraction;natural language processing;information retrieval;data mining;database;computer science;
Formal foundations for RDF/S KB evolution,Giorgos Flouris (Foundation for Research & Technology – Hellas);George Konstantinidis (University of Southern California);Grigoris Antoniou (Foundation for Research & Technology – Hellas);Vassilis Christophides (Foundation for Research & Technology – Hellas);,"1912670343,2612890319,2048729876,55450124","There are ongoing efforts to provide declarative formalisms of integrity constraints over RDF/S data. In this context, addressing the evolution of RDF/S knowledge bases while respecting associated constraints is a challenging issue, yet to receive a formal treatment. We provide a theoretical framework for dealing with both schema and data change requests. We define the notion of a rational change operator as one that satisfies the belief revision principles of Success, Validity and Minimal Change. The semantics of such an operator are subject to customization, by tuning the properties that a rational change should adhere to. We prove some interesting theoretical results and propose a general-purpose algorithm for implementing rational change operators in knowledge bases with integrity constraints, which allows us to handle uniformly any possible change request in a provably rational and consistent manner. Then, we apply our framework to a well-studied RDF/S variant, for which we suggest a specific notion of minimality. For efficiency purposes, we also describe specialized versions of the general evolution algorithm for the RDF/S case, which provably have the same semantics as the general-purpose one for a limited set of (useful in practice) types of change requests.",2013,Knowledge and Information Systems volume 35 issue 1 pp 153-191,data integrity;data mining;database;artificial intelligence;algorithm;computer science;mathematics;
Finding best algorithmic components for clustering microarray data,Milan Vukićević (University of Belgrade);Kathrin Kirchner (Schiller International University);Boris Delibašić (University of Belgrade);Miloš Jovanović (University of Belgrade);Johannes Ruhland (Schiller International University);Milija Suknović (University of Belgrade);,"2012495783,2007835287,761827189,2434913434,2442073775,73791184","The analysis of microarray data is fundamental to microbiology. Although clustering has long been realized as central to the discovery of gene functions and disease diagnostic, researchers have found the construction of good algorithms a surprisingly difficult task. In this paper, we address this problem by using a component-based approach for clustering algorithm design, for class retrieval from microarray data. The idea is to break up existing algorithms into independent building blocks for typical sub-problems, which are in turn reassembled in new ways to generate yet unexplored methods. As a test, 432 algorithms were generated and evaluated on published microarray data sets. We found their top performers to be better than the original, component-providing ancestors and also competitive with a set of new algorithms recently proposed. Finally, we identified components that showed consistently good performance for clustering microarray data and that should be considered in further development of clustering algorithms.",2013,Knowledge and Information Systems volume 35 issue 1 pp 111-130,correlation clustering;cure data clustering algorithm;gene chip analysis;fuzzy clustering;microarray analysis techniques;clustering high dimensional data;cluster analysis;biclustering;data science;bioinformatics;data mining;computer science;
On measuring the performance of binary classifiers,Charles Parker (Oregon State University);,2128990607,"If one is given two binary classifiers and a set of test data, it should be straightforward to determine which of the two classifiers is the superior. Recent work, however, has called into question many of the methods heretofore accepted as standard for this task. In this paper, we analyze seven ways of determining whether one classifier is better than another, given the same test data. Five of these are long established, and two are relative newcomers. We review and extend work showing that one of these methods is clearly inappropriate and then conduct an empirical analysis with a large number of datasets to evaluate the real-world implications of our theoretical analysis. Both our empirical and theoretical results converge strongly toward one of the newer methods.",2013,Knowledge and Information Systems volume 35 issue 1 pp 131-152,binary classification;evaluation;supervised learning;data mining;pattern recognition;machine learning;statistics;computer science;
Dynamic evaluation of the development process of knowledge-based information systems,Ghassan Beydoun (University of Wollongong);Achim G. Hoffmann (University of New South Wales);,"1239481227,2121065413","Codifying expert domain knowledge is a difficult and expensive task. To evaluate the quality of the outcome, often the same domain expert or a colleague of similar expertise is relied on to undertake a direct evaluation of the knowledge-based system or indirectly by preparing appropriate test data. During an incremental knowledge acquisition process, a data stream is available, and the knowledge base is observed and amended by an expert each time it produces an error. Using the kept record of the system’s performance, we propose an evaluation process to estimate its effectiveness as it gets evolved. We instantiate this process for an incremental knowledge acquisition methodology, Ripple Down Rules. We estimate the added value in each knowledge base update. Using these values, the decision makers in the organisation employing the knowledge-based information system can apply a cost-benefit analysis of the continuation of the incremental knowledge acquisition process. They can then determine when this process, involving keeping an expert online, should be terminated. As a result, the expert is not kept on-line longer than it is absolutely necessary. Hence, a major expense in deploying the information system—the cost of keeping a domain expert on-line—is reduced.",2013,Knowledge and Information Systems volume 35 issue 1 pp 233-247,legal expert system;idef3;subject matter expert;domain knowledge;process;information;evaluation;knowledge;knowledge extraction;knowledge base;knowledge based systems;knowledge management;data mining;database;artificial intelligence;computer science;
Detection of cross-channel anomalies,Duc-Son Pham (Curtin University);Budhaditya Saha (Deakin University);Dinh Q. Phung (Deakin University);Svetha Venkatesh (Deakin University);,"2131203824,2125102313,2314522249,2146461601","The data deluge has created a great challenge for data mining applications wherein the rare topics of interest are often buried in the flood of major headlines. We identify and formulate a novel problem: cross-channel anomaly detection from multiple data channels. Cross-channel anomalies are common among the individual channel anomalies and are often portent of significant events. Central to this new problem is a development of theoretical foundation and methodology. Using the spectral approach, we propose a two-stage detection method: anomaly detection at a single-channel level, followed by the detection of cross-channel anomalies from the amalgamation of single-channel anomalies. We also derive the extension of the proposed detection method to an online settings, which automatically adapts to changes in the data over time at low computational complexity using incremental algorithms. Our mathematical analysis shows that our method is likely to reduce the false alarm rate by establishing theoretical results on the reduction of an impurity index. We demonstrate our method in two applications: document understanding with multiple text corpora and detection of repeated anomalies in large-scale video surveillance. The experimental results consistently demonstrate the superior performance of our method compared with related state-of-art methods, including the one-class SVM and principal component pursuit. In addition, our framework can be deployed in a decentralized manner, lending itself for large-scale data stream analysis.",2013,Knowledge and Information Systems volume 35 issue 1 pp 33-59,topic model;anomaly detection;data science;data mining;database;pattern recognition;machine learning;statistics;computer science;
A review of feature selection methods on synthetic data,Verónica Bolón-Canedo (University of A Coruña);Noelia Sánchez-Maroño (University of A Coruña);Amparo Alonso-Betanzos (University of A Coruña);,"2001306915,277494648,1961547176","With the advent of high dimensionality, adequate identification of relevant features of the data has become indispensable in real-world scenarios. In this context, the importance of feature selection is beyond doubt and different methods have been developed. However, with such a vast body of algorithms available, choosing the adequate feature selection method is not an easy-to-solve question and it is necessary to check their effectiveness on different situations. Nevertheless, the assessment of relevant features is difficult in real datasets and so an interesting option is to use artificial data. In this paper, several synthetic datasets are employed for this purpose, aiming at reviewing the performance of feature selection methods in the presence of a crescent number or irrelevant features, noise in the data, redundancy and interaction between attributes, as well as a small ratio between number of samples and number of features. Seven filters, two embedded methods, and two wrappers are applied over eleven synthetic datasets, tested by four classifiers, so as to be able to choose a robust method, paving the way for its application to real datasets.",2013,Knowledge and Information Systems volume 34 issue 3 pp 483-519,feature selection;data mining;pattern recognition;machine learning;computer science;
A semantic approach for the requirement-driven discovery of web resources in the Life Sciences,María Pérez-Catalán (James I University);Rafael Berlanga (James I University);Ismael Sanz (James I University);María José Aramburu (James I University);,"2488817285,1997356218,2165390784,2048576184","Research in the Life Sciences depends on the integration of large, distributed and heterogeneous web resources (e.g., data sources and web services). The discovery of which of these resources are the most appropriate to solve a given task is a complex research question, since there are many candidate resources and there is little, mostly unstructured, metadata to be able to decide among them. In this paper, we contribute to a semi-automatic approach, based on semantic techniques, to assist researchers in the discovery of the most appropriate web resources to fulfill a set of requirements. The main feature of our approach is that it exploits broad knowledge resources in order to annotate the unstructured texts that are available in the emerging web-based repositories of web resource metadata. The results show that the web resource discovery process benefits from a semantic-based approach in several important aspects. One of the advantages is that the user can express her requirements in natural language avoiding the use of specific vocabularies or query languages. Moreover, the discovery exploits not only the categories or tags of web resources, but also their description and documentation.",2013,Knowledge and Information Systems volume 34 issue 3 pp 671-690,web 2 0;semantic web stack;web modeling;ws policy;social semantic web;data web;web standards;web mapping;web application security;mashup;semantic grid;web development;web design;web navigation;semantic web;web service;web intelligence;web engineering;world wide web;information retrieval;data mining;database;computer science;
SVDD-based outlier detection on uncertain data,Bo Liu (Guangdong University of Technology);Yanshan Xiao (Guangdong University of Technology);Longbing Cao (Information Technology University);Zhifeng Hao (Guangdong University of Technology);Feiqi Deng (South China University of Technology);,"2504109675,2138906516,2115085568,2643446293,2679400882","Outlier detection is an important problem that has been studied within diverse research areas and application domains. Most existing methods are based on the assumption that an example can be exactly categorized as either a normal class or an outlier. However, in many real-life applications, data are uncertain in nature due to various errors or partial completeness. These data uncertainty make the detection of outliers far more difficult than it is from clearly separable data. The key challenge of handling uncertain data in outlier detection is how to reduce the impact of uncertain data on the learned distinctive classi- fier. This paper proposes a new SVDD-based approach to detect outliers on uncertain data. The proposed approach operates in two steps. In the first step, a pseudo-training set is gen- erated by assigning a confidence score to each input example, which indicates the likelihood of an example tending normal class. In the second step, the generated confidence score is incorporated into the support vector data description training phase to construct a global",2013,Knowledge and Information Systems volume 34 issue 3 pp 597-618,anomaly detection;data mining;pattern recognition;machine learning;statistics;computer science;mathematics;
A segment-based approach to clustering multi-topic documents,Andrea Tagarelli (University of Calabria);George Karypis (University of Minnesota);,"273425128,219814910","Document clustering has been recognized as a central problem in text data management. Such a problem becomes particularly challenging when document contents are characterized by subtopical discussions that are not necessarily relevant to each other. Existing methods for document clustering have traditionally assumed that a document is an indivisible unit for text representation and similarity computation, which may not be appropriate to handle documents with multiple topics. In this paper, we address the problem of multi-topic document clustering by leveraging the natural composition of documents in text segments that are coherent with respect to the underlying subtopics. We propose a novel document clustering framework that is designed to induce a document organization from the identification of cohesive groups of segment-based portions of the original documents. We empirically give evidence of the significance of our segment-based approach on large collections of multi-topic documents, and we compare it to conventional methods for document clustering.",2013,Knowledge and Information Systems volume 34 issue 3 pp 563-595,brown clustering;well formed document;document layout analysis;text segmentation;fuzzy clustering;document clustering;world wide web;information retrieval;data mining;database;computer science;
An automatic keyphrase extraction system for scientific documents,Wei You (Centre national de la recherche scientifique);Dominique Fontaine (Centre national de la recherche scientifique);Jean-Paul A. Barthès (Centre national de la recherche scientifique);,"2310487788,2252055779,2137014219","Automatic keyphrase extraction techniques play an important role for many tasks including indexing, categorizing, summarizing, and searching. In this paper, we develop and evaluate an automatic keyphrase extraction system for scientific documents. Compared with previous work, our system concentrates on two important issues: (1) more precise location for potential keyphrases: a new candidate phrase generation method is proposed based on the core word expansion algorithm, which can reduce the size of the candidate set by about 75% without increasing the computational complexity; (2) overlap elimination for the output list: when a phrase and its sub-phrases coexist as candidates, an inverse document frequency feature is introduced for selecting the proper granularity. Additional new features are added for phrase weighting. Experiments based on real-world datasets were carried out to evaluate the proposed system. The results show the efficiency and effectiveness of the refined candidate set and demonstrate that the new features improve the accuracy of the system. The overall performance of our system compares favorably with other state-of-the-art keyphrase extraction systems.",2013,Knowledge and Information Systems volume 34 issue 3 pp 691-724,information retrieval;data mining;pattern recognition;computer science;
Decision rules extraction from data stream in the presence of changing context for diabetes treatment,Jakub M. Tomczak (Wrocław University of Technology);Adam Gonczarek (Wrocław University of Technology);,"2170088414,1208566535","The knowledge extraction is an important element of the e-Health system. In this paper, we introduce a new method for decision rules extraction called Graph-based Rules Inducer to support the medical interview in the diabetes treatment. The emphasis is put on the capability of hidden context change tracking. The context is understood as a set of all factors affecting patient condition. In order to follow context changes, a forgetting mechanism with a forgetting factor is implemented in the proposed algorithm. Moreover, to aggregate data, a graph representation is used and a limitation of the search space is proposed to protect from overfitting. We demonstrate the advantages of our approach in comparison with other methods through an empirical study on the Electricity benchmark data set in the classification task. Subsequently, our method is applied in the diabetes treatment as a tool supporting medical interviews.",2013,Knowledge and Information Systems volume 34 issue 3 pp 521-546,forgetting;decision rule;data mining;database;artificial intelligence;machine learning;statistics;computer science;
A method based on interval-valued fuzzy soft set for multi-attribute group decision-making problems under uncertain environment,Zhi Xiao (Chongqing University);Weijie Chen (Chongqing University);Lingling Li (Chongqing University);,"2734465404,2443150194,2685606843","In this paper, we develop a new method for multiple attributes group decision-making problems under uncertain environment, in which the information about attribute weights is incompletely known or completely unknown, and each maker’s decision information is expressed by an interval-valued fuzzy soft set. Moreover, this paper takes account of the decision makers’ attitude toward risk. In order to get the weight vector of the attributes, we construct the score matrix of the final fuzzy soft set. From the score matrix and the given attribute weights information, we establish an optimization model to determine the weights of attributes. For the special situations where the information about attribute weights is completely unknown, we establish another optimization model. By solving this model, we get a simple and exact formula, which can be used to determine the attribute weights. According to these models, a method based on interval-valued fuzzy soft set, which considers the decision makers’ risk attitude under uncertain environment, is given to rank the alternatives. Finally, a numerical example is used to illustrate the applicability of the proposed approach.",2013,Knowledge and Information Systems volume 34 issue 3 pp 653-669,variable and attribute;fuzzy set operations;attribute domain;fuzzy number;data mining;machine learning;mathematical optimization;mathematics;
Relating ontologies with a fuzzy information model,Maria Angelica Andrade Leite (Empresa Brasileira de Pesquisa Agropecuária);Ivan Luiz Marques Ricarte (State University of Campinas);,"2287346422,360250396","More people than ever before have access to information with the World Wide Web; information volume and number of users both continue to expand. Traditional search methods based on keywords are not effective, resulting in large lists of documents, many of which unrelated to users’ needs. One way to improve information retrieval is to associate meaning to users’ queries by using ontologies, knowledge bases that encode a set of concepts about one domain and their relationships. Encoding a knowledge base using one single ontology is usual, but a document collection can deal with different domains, each organized into an ontology. This work presents a novel way to represent and organize knowledge, from distinct domains, using multiple ontologies that can be related. The model allows the ontologies, as well as the relationships between concepts from distinct ontologies, to be represented independently. Additionally, fuzzy set theory techniques are employed to deal with knowledge subjectivity and uncertainty. This approach to organize knowledge and an associated query expansion method are integrated into a fuzzy model for information retrieval based on multi-related ontologies. The performance of a search engine using this model is compared with another fuzzy-based approach for information retrieval, and with the Apache Lucene search engine. Experimental results show that this model improves precision and recall measures.",2013,Knowledge and Information Systems volume 34 issue 3 pp 619-651,idef5;concept search;query expansion;ontology;knowledge based systems;information retrieval;data mining;database;computer science;
A novel semi-supervised learning framework with simultaneous text representing,Yan Zhu (Beijing Jiaotong University);Jian Yu (Beijing Jiaotong University);Liping Jing (Beijing Jiaotong University);,"2428464399,2678826171,2096438337","Text representation has received extensive attention in text mining tasks. There are various text representation models. Among them, vector space model is the most commonly used one. For vector space model, the core technique is term weighting. To date, a great deal of different term-weighting methods have been proposed, which can be divided into supervised group and unsupervised group. However, it is not advisable to use these two groups of methods directly in semi-supervised applications. In semi-supervised applications, the majority of the supervised term-weighting methods are not applicable as the label information is insufficient; meanwhile, the unsupervised term-weighting methods cannot make use of the provided category labels. Thus, a semi-supervised learning framework for iteratively revising the text representation by an EM-like strategy is proposed in this paper. Furthermore, a new supervised term-weighting method t f.sd f is proposed. T f.sd f has the ability to emphasize the importance of terms that are unevenly distributed among all the classes and weaken the importance of terms that are uniformly distributed. Experimental results on real text data show that the proposed semi-supervised learning framework with the aid of t f.sd f performs well. Also, t f.sd f is shown to be efficient for supervised learning.",2013,Knowledge and Information Systems volume 34 issue 3 pp 547-562,classifier;semi supervised learning;unsupervised learning;data mining;pattern recognition;artificial intelligence;machine learning;computer science;
Weight-based consistent query answering over inconsistent {\mathcal {SHIQ}} knowledge bases,Jianfeng Du (Guangdong University of Foreign Studies);Guilin Qi (Southeast University);Yi-Dong Shen (Chinese Academy of Sciences);,"2138241615,2178685138,2129952813","Non-standard query mechanisms that work under inconsistency are required in some important description logic (DL)-based applications, including those involving an inconsistent DL knowledge base ( KB) whose intensional knowledge is consistent but is violated by its extensional knowledge. This paper proposes a weight-based semantics for querying such an inconsistent KB. This semantics defines an answer of a conjunctive query posed upon an inconsistent KB as a tuple of individuals whose substitution for the variables in the query head makes the query body entailed by any subbase of the KB consisting of the intensional knowledge and a weight-maximally consistent subset of the extensional knowledge. A novel computational method for this semantics is proposed, which works for extensionally reduced\({\mathcal {SHIQ}}\) KBs and conjunctive queries without non-distinguished variables. The method first compiles the given KB to a propositional program; then, for any given conjunctive query, it reduces the problem of computing all answers of the given query to a set of propositional satisfiability (SAT) problems with PB-constraints, which are then solved by SAT solvers. A decomposition-based framework for optimizing the method is also proposed. The feasibility of this method is demonstrated in our experiments.",2013,Knowledge and Information Systems volume 34 issue 2 pp 335-371,boolean conjunctive query;query optimization;description logic;semantic web;theoretical computer science;database;artificial intelligence;algorithm;computer science;mathematics;
Skyline distance: a measure of multidimensional competence,Jin Huang (South China Normal University);Bin Jiang (Facebook);Jian Pei (Simon Fraser University);Jian Chen (South China University of Technology);Yong Tang (South China Normal University);,"2706776654,2575765363,2126330539,2653318668,2644485431","Skyline has been widely recognized as being useful for multi-criteria decision-making applications. While most of the existing work computes skylines in various contexts, in this paper, we consider a novel problem: how far away a point is from the skyline? We propose a novel notion of skyline distance that measures the minimum cost of upgrading a point to the skyline given a cost function. Skyline distance can be regarded as a measure of multidimensional competence and can be used to rank possible choices in recommendation systems. Computing skyline distances efficiently is far from trivial and cannot be handled by any straightforward extension of the existing skyline computation methods. To tackle this problem, we systematically explore several directions. We first present a dynamic programming method. Then, we investigate the boundary of skylines and develop a sort-projection method that utilizes the skyline boundary in calculating skyline distances. Last, we develop a space partitioning method to further improve the performance. We report extensive experiment results which show that our methods are efficient and scalable.",2013,Knowledge and Information Systems volume 34 issue 2 pp 373-396,query optimization;world wide web;data mining;database;computer science;mathematics;
Building actions from classification rules,Ronan Trépos (Institut national de la recherche agronomique);Ansaf Salleb-Aouissi (Columbia University);Marie-Odile Cordier (University of Rennes);Véronique Masson (French Institute for Research in Computer Science and Automation);Chantal Gascuel-Odoux (Institut national de la recherche agronomique);,"2033655741,36948468,2070323180,2136884697,703864755","Rule induction has attracted a great deal of attention in Machine Learning and Data Mining. However, generating rules is not an end in itself because their applicability is not straightforward especially when their number is large. Ideally, the ultimate user would like to use these rules to decide which actions to undertake. In the literature, this notion is usually referred to as actionability. We propose a new framework to address actionability. Our goal is to lighten the burden of analyzing a large set of classification rules when the user is confronted to an “unsatisfactory situation” and needs help to decide about the appropriate actions to remedy to this situation. The method consists in comparing the situation to a set of classification rules. For this purpose, we propose a new framework for learning action recommendations dealing with complex notions of feasibility and quality of actions. Our approach has been motivated by an environmental application aiming at building a tool to help specialists in charge of the management of a catchment to preserve stream-water quality. The results show the utility of this methodology with regard to enhancing the actionability of a set of classification rules in a real-world application.",2013,Knowledge and Information Systems volume 34 issue 2 pp 267-298,decision support system;data mining;artificial intelligence;machine learning;computer science;mathematics;
Recommendations for two-way selections using skyline view queries,Jian Chen (South China University of Technology);Jin Huang (South China Normal University);Bin Jiang (Facebook);Jian Pei (Simon Fraser University);Jian Yin (Sun Yat-sen University);,"2653318668,2706776654,2575765363,2126330539,2119154050","We study a practical and novel problem of making recommendations between two parties such as applicants and job positions. We model the competent choices of each party using skylines. In order to make recommendations in various scenarios, we propose a series of skyline view queries. To make recommendations, we often need to answer skyline view queries for many entries in one or two parties in batch, such as for many applicants versus many jobs. However, the existing skyline computation algorithms focus on answering a single skyline query at a time and do not consider sharing computation when answering skyline view queries for many members in one party or both parties. To tackle the batch recommendation problem, we develop several efficient algorithms to process skyline view queries in batch. The experiment results demonstrate that our algorithms significantly out- perform the state-of-the-art methods.",2013,Knowledge and Information Systems volume 34 issue 2 pp 397-424,stable marriage problem;multi objective optimization;world wide web;data mining;database;computer science;
Non-negative Tri-factor tensor decomposition with applications,Zhongyuan Zhang (Central University of Finance and Economics);Tao Li (Florida International University);Chris H. Q. Ding (University of Texas at Arlington);,"2131155200,2472069284,2119616764","Non-negative matrix factorization (NMF) mainly focuses on the hidden pattern discovery behind a series of vectors for two-way data. Here, we propose a tensor decomposition model Tri-ONTD to analyze three-way data. The model aims to discover the common characteristics of a series of matrices and at the same time identify the peculiarity of each matrix, thus enabling the discovery of the cluster structure in the data. In particular, the Tri-ONTD model performs adaptive dimension reduction for tensors as it integrates the subspace identification (i.e., the low-dimensional representation with a common basis for a set of matrices) and the clustering process into a single process. The Tri-ONTD model can also be regarded as an extension of the Tri-factor NMF model. We present the detailed optimization algorithm and also provide the convergence proof. Experimental results on real-world datasets demonstrate the effectiveness of our proposed method in author clustering, image clustering, and image reconstruction. In addition, the results of our proposed model have sparse and localized structures.",2013,Knowledge and Information Systems volume 34 issue 2 pp 243-265,non negative matrix factorization;matrix decomposition;theoretical computer science;machine learning;mathematical optimization;computer science;mathematics;
Impact of query intent and search context on clickthrough behavior in sponsored search,Azin Ashkan (University of Waterloo);Charles L. A. Clarke (University of Waterloo);,"2075598693,2098618034","Implicit feedback techniques may be used for query intent detection, taking advantage of user behavior to understand their interests and preferences. In sponsored search, a primary concern is the user’s interest in purchasing or utilizing a commercial service, or what is called online commercial intent. In this paper, we develop a methodology for employing the content of search engine result pages (SERPs), along with the information obtained from query strings, to study characteristics of query intent, with a particular focus on sponsored search. Our work represents a step toward the development and evaluation of an ontology for commercial search, considering queries that reference specific products, brands, and retailers. Characteristics of query categories are studied with respect to aggregated user clickthrough behavior on advertising links. We present a model for clickthrough behavior that considers the influence of such factors as the location of ads and the rank of ads, along with query category. We evaluate our work using a large corpus of clickthrough data obtained from a major commercial search engine. In addition, the impact of query intent is studied on clickthrough rate, where a baseline model and the query intent model are compared for the purpose of calculating an expected ad clickthrough rate. Our findings suggest that query-based features, along with the content of SERPs, are effective in detecting query intent. Factors such as query category, the rank of an ad, and the total number of ads displayed on a result page relate to the context of the ad, rather than its content. We demonstrate that these context-related factors can have a major influence on expected clickthrough rate, suggesting that these factors should be taken into consideration when the performance of an ad is evaluated.",2013,Knowledge and Information Systems volume 34 issue 2 pp 425-452,web search query;web query classification;query expansion;world wide web;information retrieval;data mining;database;computer science;
Application of rough set classifiers for determining hemodialysis adequacy in ESRD patients,You-Shyang Chen;Ching-Hsue Cheng (National Yunlin University of Science and Technology);,"2690331973,2161354717","The incidence and the prevalence of end-stage renal disease (ESRD) in Taiwan are the highest in the world. Therefore, hemodialysis (HD) therapy is a major concern and an important challenge due to the shortage of donated organs for transplantation. Previous researchers developed various forecasting models based on statistical methods and artificial intelligence techniques to address the real-world problems of HD therapy that are faced by ESRD patients and their doctors in the healthcare services. Because the performance of these forecasting models is highly dependent on the context and the data used, it would be valuable to develop more suitable methods for applications in this field. This study presents an integrated procedure that is based on rough set classifiers and aims to provide an alternate method for predicting the urea reduction ratio for assessing HD adequacy for ESRD patients and their doctors. The proposed procedure is illustrated in practice by examining a dataset from a specific medical center in Taiwan. The experimental results reveal that the proposed procedure has better accuracy with a low standard deviation than the listed methods. The output created by the rough set LEM2 algorithm is a comprehensible decision rule set that can be applied in knowledge-based healthcare services as desired. The analytical results provide useful information for both academics and practitioners.",2013,Knowledge and Information Systems volume 34 issue 2 pp 453-482,rough set;management science;data mining;machine learning;computer science;
Robust personalizable spam filtering via local and global discrimination modeling,Khurum Nazir Junejo (Lahore University of Management Sciences);Asim Karim (Lahore University of Management Sciences);,"2144972756,2206695298","Content-based e-mail spam filtering continues to be a challenging machine learning problem. Usually, the joint distribution of e-mails and labels changes from user to user and from time to time, and the training data are poor representatives of the true distribution. E-mail service providers have two options for automatic spam filtering at the service-side: a single global filter for all users or a personalized filter for each user. The practical usefulness of these options, however, depends upon the robustness and scalability of the filter. In this paper, we address these challenges by presenting a robust personalizable spam filter based on local and global discrimination modeling. Our filter exploits highly discriminating content terms, identified by their relative risk, to transform the input space into a two-dimensional feature space. This transformation is obtained by linearly pooling the discrimination information provided by each term for spam or non-spam classification. Following this local model, a linear discriminant is learned in the feature space for classification. We also present a strategy for personalizing the local and global models using unlabeled e-mails, without requiring user’s feedback. Experimental evaluations and comparisons are presented for global and personalized spam filtering, for varying distribution shift, for handling the problem of gray e-mails, on unseen e-mails, and with varying filter size. The results demonstrate the robustness and effectiveness of our filter and its suitability for global and personalized spam filtering at the service-side.",2013,Knowledge and Information Systems volume 34 issue 2 pp 299-334,personalization;world wide web;data mining;machine learning;computer science;
Topic-aware social influence propagation models,Nicola Barbieri (Yahoo!);Francesco Bonchi (Yahoo!);Giuseppe Manco (Indian Council of Agricultural Research);,"2155070167,2176652147,2093732677","The study of influence-driven propagations in social networks and its exploitation for viral marketing purposes has recently received a large deal of attention. However, regardless of the fact that users authoritativeness, expertise, trust and influence are evidently topic-dependent, the research on social influence has surprisingly largely overlooked this aspect. In this article, we study social influence from a topic modeling perspective. We introduce novel topic-aware influence-driven propagation models that, as we show in our experiments, are more accurate in describing real-world cascades than the standard (i.e., topic-blind) propagation models studied in the literature. In particular, we first propose simple topic-aware extensions of the well-known Independent Cascade and Linear Threshold models. However, these propagation models have a very large number of parameters which could lead to overfitting. Therefore, we propose a different approach explicitly modeling authoritativeness, influence and relevance under a topic-aware perspective. Instead of considering user-to-user influence, the proposed model focuses on user authoritativeness and interests in a topic, leading to a drastic reduction in the number of parameters of the model. We devise methods to learn the parameters of the models from a data set of past propagations. Our experimentation confirms the high accuracy of the proposed models and learning schemes.",2013,Knowledge and Information Systems volume 37 issue 3 pp 555-584,data mining;artificial intelligence;machine learning;simulation;statistics;computer science;
A survey of queries over uncertain data,Yijie Wang (National University of Defense Technology);Xiaoyong Li (National University of Defense Technology);Xiaoling Li (National University of Defense Technology);Yuan Wang (National University of Defense Technology);,"2707257740,2304592297,2286727994,2723460814","Uncertain data have already widely existed in many practical applications recently, such as sensor networks, RFID networks, location-based services, and mobile object management. Query processing over uncertain data as an important aspect of uncertain data management has received increasing attention in the field of database. Uncertain query processing poses inherent challenges and demands non-traditional techniques, due to the data uncertainty. This paper surveys this interesting and still evolving research area in current database community, so that readers can easily obtain an overview of the state-of-the-art techniques. We first provide an overview of data uncertainty, including uncertainty types, probability representation models, and sources of probabilities. We next outline the current major types of uncertain queries and summarize the main features of uncertain queries. Particularly, we present and analyze several typical uncertain queries in detail, such as skyline queries, top-\(k\) queries, nearest-neighbor queries, aggregate queries, join queries, range queries, and threshold queries over uncertain data. Finally, we present many interesting research topics on uncertain queries that have not yet been explored.",2013,Knowledge and Information Systems volume 37 issue 3 pp 485-530,range query;spatial query;information retrieval;data mining;database;computer science;
Discernibility matrix simplification with new attribute dependency functions for incomplete information systems,Guangming Lang (Hunan University);Qingguo Li (Hunan University);Lankun Guo (Hunan University);,"2108200369,2102202704,2123256684","Recently, many researches have been done on attribute dependency degree models. In this work, we bring forward three attribute dependency functions for incomplete information systems and investigate their basic properties in detail. Afterward, we apply the proposed models to twelve data sets from the UCI repository of machine learning databases. Finally, using the proposed functions, we perform the discernibility matrix simplification of incomplete information systems. The experimental results show that our proposed functions are more flexible to calculate the degree of each conditional attribute related to the decision attribute for incomplete information systems.",2013,Knowledge and Information Systems volume 37 issue 3 pp 611-638,attribute domain;rough set;information system;data mining;database;pattern recognition;machine learning;computer science;mathematics;
Exchanging Data amongst Linked Data applications,Carlos R. Rivero (University of Seville);Inma Hernández (University of Seville);David Ruiz (University of Seville);Rafael Corchuelo (University of Seville);,"2053568818,2125967671,2118244431,197690615","The goal of data exchange is to populate the data model of a target application using data that come from one or more source applications. It is common to address data exchange building on correspondences that are transformed into executable mappings. The problem that we address in this article is how to generate executable mappings in the context of Linked Data applications, that is, applications whose data models are semantic-web ontologies. In the literature, there are many proposals to generate executable mappings. Most of them focus on relational or nested-relational data models, which cannot be applied to our context; unfortunately, the few proposals that focus on ontologies have important drawbacks, namely: they solely work on a subset of taxonomies, they require the target data model to be pre-populated or they interpret correspondences in isolation, not to mention the proposals that actually require the user to handcraft the executable mappings. In this article, we present MostoDE, a new automated proposal to generate SPARQL executable mappings in the context of Linked Data applications. Its salient features are that it does not have any of the previous drawbacks, it is computationally tractable and it has been validated using a series of experiments that prove that it is very efficient and effective in practice.",2013,Knowledge and Information Systems volume 37 issue 3 pp 693-729,sparql;data exchange;linked data;data mining;database;programming language;computer science;
Adaptive goal selection for agents in dynamic environments,"Huiliang Zhang (Nanyang Technological University);Xudong Luo (Sun Yat-sen University);Chun Yan Miao (Nanyang Technological University);Zhiqi Shen (School of Electrical and Electronic Engineering, University of Manchester);Jin You (University of Houston);","2238636641,2146718932,2154137932,2117055168,2630309704","In psychology, goal-setting theory, which has been studied by psychologists for over 35 years, reveals that goals play significant roles in incentive, action and performance for human beings. Based on this theory, a goal net model has been proposed to design intelligent agents that can be viewed as a soft copy of human being somehow. The goal net model has been successfully applied in many agents, specially, non-player-character agents in computer games. Such an agent selects the optimal solution in all possible solutions found by using a recursive algorithm. However, if a goal net is very complex, the time of selection could be too long for the agent to respond quickly when the agent needs to re-select a new solution against the world’s change. Moreover, in some dynamic environments, it is impossible to know the exact outcome of choosing a solution in advance, and so the possible solutions cannot be evaluated precisely. Thus, to address the problem, this paper applies learning algorithm into goal selection in dynamic environments. More specifically, we first develop a reorganization algorithm that can convert a goal net to its equivalent counterpart that a Q-learning algorithm can operate on; then, we define the key component of Q-learning, reward function, according to the feature of goal nets; and finally lots of experiments are conducted to show that, in dynamic environments, the agent with the learning algorithm significantly outperforms the one with the recursive searching algorithm. Therefore, our work suggests an agent model that can effectively be applied in dynamic time-sensitive domain, like computer games and the P2P systems of online movie watching.",2013,Knowledge and Information Systems volume 37 issue 3 pp 665-692,q learning;planning;knowledge management;data mining;database;artificial intelligence;machine learning;simulation;computer science;
A General Streaming Algorithm for Pattern Discovery,Debprakash Patnaik (Amazon.com);Srivatsan Laxman (Microsoft);Badrish Chandramouli (Microsoft);Naren Ramakrishnan (Virginia Tech);,"2140123976,2044569235,2105061512,2199255697","Discovering frequent patterns over event sequences is an important data mining problem. Existing methods typically require multiple passes over the data, rendering them unsuitable for streaming contexts. We present the first streaming algorithm for mining frequent patterns over a window of recent events in the stream. We derive approximation guarantees for our algorithm in terms of: (i) the separation of frequent patterns from the infrequent ones, and (ii) the rate of change of stream characteristics. Our parameterization of the problem provides a new sweet spot in the tradeoff between making distributional assumptions over the stream and algorithmic efficiencies of mining. We illustrate how this yields significant benefits when mining practical streams from neuroscience and telecommunications logs.",2013,Knowledge and Information Systems volume 37 issue 3 pp 585-610,streaming algorithm;data stream mining;approximation algorithm;theoretical computer science;data mining;real time computing;computer science;
The address connector: noninvasive synchronization of hierarchical data sources,Nikolaus Augsten (Free University of Bozen-Bolzano);Michael H. Böhlen (University of Zurich);Johann Gamper (Free University of Bozen-Bolzano);,"113607873,2300834035,1786981052","Different databases often store information about the same or related objects in the real world. To enable collaboration between these databases, data items that refer to the same object must be identified. Residential addresses are data of particular interest as they often provide the only link between related pieces of information in different databases. Unfortunately, residential addresses that describe the same location might vary considerably and hence need to be synchronized. Non-matching street names and addresses stored at different levels of granularity make address synchronization a challenging task. Common approaches assume an authoritative reference set and correct residential addresses according to the reference set. Often, however, no reference set is available, and correcting addresses with different granularity is not possible. We present the address connector, which links residential addresses that refer to the same location. Instead of correcting addresses according to an authoritative reference set, the connector defines a lookup function for residential addresses. Given a query address and a target database, the lookup returns all residential addresses in the target database that refer to the same location. The lookup supports addresses that are stored with different granularity. To align the addresses of two matching streets, we use a global greedy address-matching algorithm that guarantees a stable matching. We define the concept of address containment that allows us to correctly link addresses with different granularity. The evaluation of our solution on real-world data from a municipality shows that our solution is both effective and efficient.",2013,Knowledge and Information Systems volume 37 issue 3 pp 639-663,name resolution;record linkage;hierarchical database model;data quality;world wide web;data mining;database;computer science;
Evolutionary isotonic separation for classification: theory and experiments,B. Malar (PSG College of Technology);R. Nadarajan (PSG College of Technology);,"2280245752,2722067900","Isotonic separation is a supervised machine learning technique where classification is represented as a linear programming problem (LPP) with an objective of minimizing the number of misclassifications. It is computationally expensive to solve the LPP using traditional methods when the dataset grows. Evolutionary isotonic separation (EIS), a hybrid classification algorithm, is introduced to tackle this issue. Here, isotonic separation acts as a host architecture where evolutionary framework based on genetic algorithm is embedded in the training phase of the isotonic separation, to find an optimum or near-optimum solution for the LPP. Evolutionary framework deploys a newly introduced slack vector to find the feasible solution. It also employs a position-based crossover operator to obtain the optimum or near-optimum solution. Experimental studies are conducted on Wisconsin Breast Cancer dataset and a synthetic dataset. Experimental and statistical results show that EIS outperforms its predecessors and state of the art machine learning techniques in terms of accuracy.",2013,Knowledge and Information Systems volume 37 issue 3 pp 531-553,isotonic regression;genetic algorithm;pattern recognition;artificial intelligence;machine learning;mathematical optimization;computer science;mathematics;
How you move reveals who you are: understanding human behavior by analyzing trajectory data,Chiara Renso (Istituto di Scienza e Tecnologie dell'Informazione);Miriam Baglioni (University of Pisa);José Antônio Fernandes de Macêdo (Federal University of Ceará);Roberto Trasarti (Istituto di Scienza e Tecnologie dell'Informazione);Monica Wachowicz (University of New Brunswick);,"55173896,2120777025,2153389233,2193623800,1944864863","The widespread use of mobile devices is producing a huge amount of trajectory data, making the discovery of movement patterns possible, which are crucial for understanding human behavior. Significant advances have been made with regard to knowledge discovery, but the process now needs to be extended bearing in mind the emerging field of behavior informatics. This paper describes the formalization of a semantic-enriched KDD process for supporting meaningful pattern interpretations of human behavior. Our approach is based on the integration of inductive reasoning (movement pattern discovery) and deductive reasoning (human behavior inference). We describe the implemented Athena system, which supports such a process, along with the experimental results on two different application domains related to traffic and recreation management.",2013,Knowledge and Information Systems volume 37 issue 2 pp 331-362,ontology;data science;data mining;database;artificial intelligence;machine learning;computer science;
Efficient algorithms for discovering high utility user behavior patterns in mobile commerce environments,Bai-En Shie (National Cheng Kung University);Hui-Fang Hsiao (National Cheng Kung University);Vincent S. Tseng (National Cheng Kung University);,"1840976773,2665036302,2023431847","Mining user behavior patterns in mobile environments is an emerging topic in data mining fields with wide applications. By integrating moving paths with purchasing transactions, one can find the sequential purchasing patterns with the moving paths, which are called mobile sequential patterns of the mobile users. Mobile sequential patterns can be applied not only for planning mobile commerce environments but also for analyzing and managing online shopping websites. However, unit profits and purchased numbers of the items are not considered in traditional framework of mobile sequential pattern mining. Thus, the patterns with high utility (i.e., profit here) cannot be found. In view of this, we aim at integrating mobile data mining with utility mining for finding high-utility mobile sequential patterns in this study. Two types of algorithms, namely level-wise and tree-based methods, are proposed for mining high-utility mobile sequential patterns. A series of analyses and comparisons on the performance of the two different types of algorithms are conducted through experimental evaluations. The results show that the proposed algorithms outperform the state-of-the-art mobile sequential pattern algorithms and that the tree-based algorithms deliver better performance than the level-wise ones under various conditions.",2013,Knowledge and Information Systems volume 37 issue 2 pp 363-387,mobile search;data mining;simulation;computer science;
Special issue on behavior computing,"Longbing Cao (University of Technology, Sydney);Philip S. Yu (University of Illinois at Chicago);Hiroshi Motoda (Osaka University);Graham Williams (Australian Taxation Office);","2115085568,2125104194,323689644,2106893266",-,2013,Knowledge and Information Systems volume 37 issue 2 pp 245-249,computer science;
Event Extraction Using Behaviors of Sentiment Signals and Burst Structure in Social Media,Thin Nguyen (Deakin University);Dinh Q. Phung (Deakin University);Brett Adams (Curtin University);Svetha Venkatesh (Deakin University);,"2117594642,2314522249,2163867018,2146461601","Significant world events often cause the behavioral convergence of the expression of shared sentiment. This paper examines the use of the blogosphere as a framework to study user psychological behaviors, using their sentiment responses as a form of ‘sensor’ to infer real-world events of importance automatically. We formulate a novel temporal sentiment index function using quantitative measure of the valence value of bearing words in blog posts in which the set of affective bearing words is inspired from psychological research in emotion structure. The annual local minimum and maximum of the proposed sentiment signal function are utilized to extract significant events of the year and corresponding blog posts are further analyzed using topic modeling tools to understand their content. The paper then examines the correlation of topics discovered in relation to world news events reported by the mainstream news service provider, Cable News Network, and by using the Google search engine. Next, aiming at understanding sentiment at a finer granularity over time, we propose a stochastic burst detection model, extended from the work of Kleinberg, to work incrementally with stream data. The proposed model is then used to extract sentimental bursts occurring within a specific mood label (for example, a burst of observing ‘shocked’). The blog posts at those time indices are analyzed to extract topics, and these are compared to real-world news events. Our comprehensive set of experiments conducted on a large-scale set of 12 million posts from Livejournal shows that the proposed sentiment index function coincides well with significant world events while bursts in sentiment allow us to locate finer-grain external world events.",2013,Knowledge and Information Systems volume 37 issue 2 pp 279-304,sentiment analysis;world wide web;data mining;
An incremental algorithm for clustering spatial data streams: exploring temporal locality,Ling-Yin Wei (National Chiao Tung University);Wen-Chih Peng (National Chiao Tung University);,"2641095543,2100961564","Clustering sensor data discovers useful information hidden in sensor networks. In sensor networks, a sensor has two types of attributes: a geographic attribute (i.e, its spatial location) and non-geographic attributes (e.g., sensed readings). Sensor data are periodically collected and viewed as spatial data streams, where a spatial data stream consists of a sequence of data points exhibiting attributes in both the geographic and non-geographic domains. Previous studies have developed a dual clustering problem for spatial data by considering similarity-connected relationships in both geographic and non-geographic domains. However, the clustering processes in stream environments are time-sensitive because of frequently updated sensor data. For sensor data, the readings from one sensor are similar for a period, and the readings refer to temporal locality features. Using the temporal locality features of the sensor data, this study proposes an incremental clustering (IC) algorithm to discover clusters efficiently. The IC algorithm comprises two phases: cluster prediction and cluster refinement. The first phase estimates the probability of two sensors belonging to a cluster from the previous clustering results. According to the estimation, a coarse clustering result is derived. The cluster refinement phase then refines the coarse result. This study evaluates the performance of the IC algorithm using synthetic and real datasets. Experimental results show that the IC algorithm outperforms exiting approaches confirming the scalability of the IC algorithm. In addition, the effect of temporal locality features on the IC algorithm is analyzed and thoroughly examined in the experiments.",2013,Knowledge and Information Systems volume 37 issue 2 pp 453-483,k medians clustering;canopy clustering algorithm;correlation clustering;data stream clustering;cure data clustering algorithm;single linkage clustering;fuzzy clustering;clustering high dimensional data;cluster analysis;data mining;database;machine learning;computer science;
Semantic trajectory based event detection and event pattern mining,Xiaofeng Wang (Chinese Academy of Sciences);Gang Li (Deakin University);Guang Jiang (Chinese Academy of Sciences);Zhongzhi Shi (Chinese Academy of Sciences);,"2710385401,2098057600,2713742196,2728595356","Video event detection is an effective way to automatically understand the semantic content of the video. However, due to the mismatch between low-level visual features and high-level semantics, the research of video event detection encounters a number of challenges, such as how to extract the suitable information from video, how to represent the event, how to build up reasoning mechanism to infer the event according to video information. In this paper, we propose a novel event detection method. The method detects the video event based on the semantic trajectory, which is a high-level semantic description of the moving object’s trajectory in the video. The proposed method consists of three phases to transform low-level visual features to middle-level raw trajectory information and then to high-level semantic trajectory information. Event reasoning is then carried out with the assistance of semantic trajectory information and background knowledge. Additionally, to release the users’ burden in manual event definition, a method is further proposed to automatically discover the event-related semantic trajectory pattern from the sample semantic trajectories. Furthermore, in order to effectively use the discovered semantic trajectory patterns, the associative classification-based event detection framework is adopted to discover the possibly occurred event. Empirical studies show our methods can effectively and efficiently detect video events.",2013,Knowledge and Information Systems volume 37 issue 2 pp 305-329,semantic computing;complex event processing;video;reason;ontology;information retrieval;computer vision;data mining;computer science;
Bootstrapping trust of Web services based on trust patterns and Hidden Markov Models,Hamdi Yahyaoui (Kuwait University);Sami Zhioua (King Fahd University of Petroleum and Minerals);,"1986715945,2261731950","We propose in this paper a new approach for bootstrapping trust of Web services in which the interactions of a Web service with a user are observed during a certain time frame. The observations sequence is modeled as a Hidden Markov Model and matched against pre-defined trust patterns in order to assess the behavior of such Web service. The pre-defined trust patterns are specifications of possible behaviors of Web services such as trusted, malicious, betraying, oscillating, and redemptive. Based on the matching result, an initial trust value is assigned to the Web service. Our experimental results show that our approach enjoys good precision and recall values and provides a fair distribution of trust values. Besides, the proposed approach is applied on a dataset of real-world Web services. A comparative study with published bootstrapping approaches shows a better bootstrapping success rate for our new approach.",2013,Knowledge and Information Systems volume 37 issue 2 pp 389-416,bootstrapping;trustworthy computing;web service;pattern;hidden markov model;internet privacy;world wide web;data mining;computer science;
On formalizing social commitments in dialogue and argumentation models using temporal defeasible logic,Asma Moubaiddin (University of Jordan);Nadim Obeid (University of Jordan);,"300908301,2107019284","In this paper, we take the view that any formalization of commitments has to come together with a formalization of time, events/actions and change. We enrich a suitable formalism for reasoning about time, event/action and change in order to represent and reason about commitments. We employ a three-valued based temporal first-order non-monotonic logic (TFONL) that allows an explicit representation of time and events/action. TFONL subsumes the action languages presented in the literature and takes into consideration the frame, qualification and ramification problems, and incorporates to a domain description the set of rules governing change. It can handle protocols for the different types of dialogues such as information seeking, inquiry and negotiation. We incorporate commitments into TFONL to obtain Com-TFONL. Com-TFONL allows an agent to reason about its commitments and about other agents’ behaviour during a dialogue. Thus, agents can employ social commitments to act on, argue with and reason about during interactions with other agents. Agents may use their reasoning and argumentative capabilities in order to determine the appropriate communicative acts during conversations. Furthermore, Com-TFONL allows for an integration of commitments and arguments which helps in capturing the public aspects of a conversation and the reasoning aspects required in coherent conversations.",2013,Knowledge and Information Systems volume 37 issue 2 pp 417-452,complete information;reason;representation;knowledge management;artificial intelligence;computer science;
A time-interval sequence classification method,Chieh-Yuan Tsai (Yuan Ze University);Chih-Jung Chen (Yuan Ze University);Chun-Ju Chien;,"2168292834,2133613834,2516792941","Classification is one of the most popular behavior prediction tools in behavior informatics (behavior computing) to predict group membership for data instances. It has been greatly used to support customer relationship management (CRM) such as customer identification, one-to-one marketing, fraud detection, and lifetime value analysis. Although previous studies showed themselves efficient and accurate in certain CRM classification applications, most of them took demographic, RFM-type, or activity attributes as classification criteria and seldom took temporal relationship among these attributes into account. To bridge this gap, this study takes customer temporal behavior data, called time-interval sequences, as classification criteria and develops a two-stage classification framework. In the first stage, time-interval sequential patterns are discovered from customer temporal databases. Then, a time-interval sequence classifier optimized by the particle swam optimization (PSO) algorithm is developed to achieve high classification accuracy in the second stage. The experiment results indicate the proposed time-interval sequence classification framework is efficient and accurate to predict the class label of new customer temporal data.",2013,Knowledge and Information Systems volume 37 issue 2 pp 251-278,biological classification;data science;data mining;machine learning;computer science;
Geographic knowledge extraction and semantic similarity in OpenStreetMap,Andrea Ballatore (University College Dublin);Michela Bertolotto (University College Dublin);David C. Wilson (University of North Carolina at Charlotte);,"2134238597,2046795088,2443776490","In recent years, a web phenomenon known as Volunteered Geographic Information (VGI) has produced large crowdsourced geographic data sets. OpenStreetMap (OSM), the leading VGI project, aims at building an open-content world map through user contributions. OSM semantics consists of a set of properties (called ‘tags’) describing geographic classes, whose usage is defined by project contributors on a dedicated Wiki website. Because of its simple and open semantic structure, the OSM approach often results in noisy and ambiguous data, limiting its usability for analysis in information retrieval, recommender systems and data mining. Devising a mechanism for computing the semantic similarity of the OSM geographic classes can help alleviate this semantic gap. The contribution of this paper is twofold. It consists of (1) the development of the OSM Semantic Network by means of a web crawler tailored to the OSM Wiki website; this semantic network can be used to compute semantic similarity through co-citation measures, providing a novel semantic tool for OSM and GIS communities; (2) a study of the cognitive plausibility (i.e. the ability to replicate human judgement) of co-citation algorithms when applied to the computation of semantic similarity of geographic concepts. Empirical evidence supports the usage of co-citation algorithms—SimRank showing the highest plausibility—to compute concept similarity in a crowdsourced semantic network.",2013,Knowledge and Information Systems volume 37 issue 1 pp 61-81,semantic web stack;semantic computing;open content;volunteered geographic information;crowdsourcing;web crawler;semantic gap;semantic similarity;semantic network;empirical evidence;computational semantics;recommender system;knowledge extraction;world wide web;information retrieval;data mining;database;computer science;
Travel route recommendation using geotagged photos,Takeshi Kurashima (Nippon Telegraph and Telephone);Tomoharu Iwata (Nippon Telegraph and Telephone);Go Irie (Nippon Telegraph and Telephone);Ko Fujimura (Nippon Telegraph and Telephone);,"2090365413,2108993706,2144615606,2164460727","We propose a travel route recommendation method that makes use of the photographers’ histories as held by social photo-sharing sites. Assuming that the collection of each photographer’s geotagged photos is a sequence of visited locations, photo-sharing sites are important sources for gathering the location histories of tourists. By following their location sequences, we can find representative and diverse travel routes that link key landmarks. Recommendations are performed by our photographer behavior model, which estimates the probability of a photographer visiting a landmark. We incorporate user preference and present location information into the probabilistic behavior model by combining topic models and Markov models. Based on the photographer behavior model, proposed route recommendation method outputs a set of personalized travel plans that match the user’s preference, present location, spare time and transportation means. We demonstrate the effectiveness of the proposed method using an actual large-scale geotag dataset held by Flickr in terms of the prediction accuracy of travel behavior.",2013,Knowledge and Information Systems volume 37 issue 1 pp 37-60,geotagging;internet privacy;world wide web;data mining;computer science;
Model-based probabilistic frequent itemset mining,Thomas Bernecker (Ludwig Maximilian University of Munich);Reynold Cheng (University of Hong Kong);David W. Cheung (University of Hong Kong);Hans-Peter Kriegel (Ludwig Maximilian University of Munich);Sau Dan Lee (University of Hong Kong);Matthias Renz (Ludwig Maximilian University of Munich);Florian Verhein (Ludwig Maximilian University of Munich);Liang Wang (University of Hong Kong);Andreas Zuefle (Ludwig Maximilian University of Munich);,"1977319283,2138267588,1979772396,1919135125,2192435601,2150497105,2584603217,2707401168,1223877746","Data uncertainty is inherent in emerging applications such as location-based services, sensor monitoring systems, and data integration. To handle a large amount of impre- cise information, uncertain databases have been recently developed. In this paper, we study how to efficiently discover frequent itemsets from large uncertain databases, interpreted under the Possible World Semantics. This is technically challenging, since an uncertain data- base induces an exponential number of possible worlds. To tackle this problem, we propose a novel methods to capture the itemset mining process as a probability distribution func- tion taking two models into account: the Poisson distribution and the normal distribution. These model-based approaches extract frequent itemsets with a high degree of accuracy and",2013,Knowledge and Information Systems volume 37 issue 1 pp 181-217,data science;data mining;database;computer science;
Markov-Miml: A Markov chain-based multi-instance multi-label learning algorithm,Qingyao Wu (Harbin Institute of Technology);Michael K. Ng (Hong Kong Baptist University);Yunming Ye (Harbin Institute of Technology);,"2116674287,2110705915,2167640884","The main aim of this paper is to propose an efficient and novel Markov chain-based multi-instance multi-label (Markov-Miml) learning algorithm to evaluate the importance of a set of labels associated with objects of multiple instances. The algorithm computes ranking of labels to indicate the importance of a set of labels to an object. Our approach is to exploit the relationships between instances and labels of objects. The rank of a class label to an object depends on (i) the affinity metric between the bag of instances of this object and the bag of instances of the other objects, and (ii) the rank of a class label of similar objects. An object, which contains a bag of instances that are highly similar to bags of instances of the other objects with a high rank of a particular class label, receives a high rank of this class label. Experimental results on benchmark data have shown that the proposed algorithm is computationally efficient and effective in label ranking for MIML data. In the comparison, we find that the classification performance of the Markov-Miml algorithm is competitive with those of the three popular MIML algorithms based on boosting, support vector machine, and regularization, but the computational time required by the proposed algorithm is less than those by the other three algorithms.",2013,Knowledge and Information Systems volume 37 issue 1 pp 83-104,markov chain;data mining;pattern recognition;machine learning;statistics;mathematics;
TAPAS: Trustworthy privacy-aware participatory sensing,Leyla Kazemi (University of Southern California);Cyrus Shahabi (University of Southern California);,"1870077263,240820708","With the advent of mobile technology, a new class of applications, called participatory sensing (PS), is emerging, with which the ubiquity of mobile devices is exploited to collect data at scale. However, privacy and trust are the two significant barriers to the success of any PS system. First, the participants may not want to associate themselves with the collected data. Second, the validity of the contributed data is not verified, since the intention of the participants is not always clear. In this paper, we formally define the problem of privacy and trust in PS systems and examine its challenges. We propose a trustworthy privacy-aware framework for PS systems dubbed TAPAS, which enables the participation of the users without compromising their privacy while improving the trustworthiness of the collected data. Our experimental evaluations verify the applicability of our proposed approaches and demonstrate their efficiency.",2013,Knowledge and Information Systems volume 37 issue 1 pp 105-128,privacy by design;trustworthy computing;privacy;internet privacy;world wide web;computer security;data mining;database;computer science;
Adaptive semi-supervised learning on labeled and unlabeled data with different distributions,Akinori Fujino (Nippon Telegraph and Telephone);Naonori Ueda (Nippon Telegraph and Telephone);Masaaki Nagata (Nippon Telegraph and Telephone);,"2142558357,2147735823,2102463429","Developing methods for designing good classifiers from labeled samples whose distribution is different from that of test samples is an important and challenging research issue in the fields of machine learning and its application. This paper focuses on designing semi-supervised classifiers with a high generalization ability by using unlabeled samples drawn by the same distribution as the test samples and presents a semi-supervised learning method based on a hybrid discriminative and generative model. Although JESS-CM is one of the most successful semi-supervised classifier design frameworks based on a hybrid approach, it has an overfitting problem in the task setting that we consider in this paper. We propose an objective function that utilizes both labeled and unlabeled samples for the discriminative training of hybrid classifiers and then expect the objective function to mitigate the overfitting problem. We show the effect of the objective function by theoretical analysis and empirical evaluation. Our experimental results for text classification using four typical benchmark test collections confirmed that with our task setting in most cases, the proposed method outperformed the JESS-CM framework. We also confirmed experimentally that the proposed method was useful for obtaining better performance when classifying data samples into either known or unknown classes, which were included in given labeled samples or not, respectively.",2013,Knowledge and Information Systems volume 37 issue 1 pp 129-154,transfer of learning;semi supervised learning;data mining;pattern recognition;machine learning;computer science;
FacetCube: a general framework for non-negative tensor factorization,Yun Chi (NEC);Shenghuo Zhu (NEC);,"2664427969,2096537073","Non-negative tensor factorization (NTF) has been successfully used to extract significant characteristics from polyadic data, such as data in social networks. Because these polyadic data have multiple dimensions (e.g., the author, content, and timestamp of a blog post), NTF fits in naturally and extracts data characteristics jointly from different data dimensions. In the traditional NTF, all information comes from the observed data, and therefore, the end users have no control over the outcomes. However, in many applications very often, the end users have certain prior knowledge, such as the demographic information about individuals in a social network or a pre-constructed ontology on the contents and therefore prefer the data characteristics extracting by NTF being consistent with such prior knowledge. To allow users’ prior knowledge to be naturally incorporated into NTF, in this paper, we present a general framework—FacetCube—that extends the standard NTF. The new framework allows the end users to control the factorization outputs at three different levels for each of the data dimensions. The proposed framework is intuitively appealing in that it has a close connection to the probabilistic generative models. In addition to introducing the framework, we provide an iterative algorithm for computing the optimal solution to the framework. We also develop an efficient implementation of the algorithm that consists of several techniques to make our framework scalable to large data sets. Extensive experimental studies on a paper citation data set and a blog data set demonstrate that our new framework is able to effectively incorporate users’ prior knowledge, improves performance over the traditional NTF on the task of personalized recommendation, and is scalable to large data sets from real-life applications.",2013,Knowledge and Information Systems volume 37 issue 1 pp 155-179,iterative method;theoretical computer science;data mining;database;machine learning;computer science;
Hyper-structure mining of frequent patterns in uncertain data streams,Chandima HewaNadungodage (Indiana University – Purdue University Indianapolis);Yuni Xia (Indiana University – Purdue University Indianapolis);Jaehwan John Lee (Indiana University – Purdue University Indianapolis);Yi-cheng Tu (University of South Florida);,"2491573301,2334154954,2144393415,2306077690","Data uncertainty is inherent in many real-world applications such as sensor monitoring systems, location-based services, and medical diagnostic systems. Moreover, many real-world applications are now capable of producing continuous, unbounded data streams. During the recent years, new methods have been developed to find frequent patterns in uncertain databases; nevertheless, very limited work has been done in discovering frequent patterns in uncertain data streams. The current solutions for frequent pattern mining in uncertain streams take a FP-tree-based approach; however, recent studies have shown that FP-tree-based algorithms do not perform well in the presence of data uncertainty. In this paper, we propose two hyper-structure-based false-positive-oriented algorithms to efficiently mine frequent itemsets from streams of uncertain data. The first algorithm, UHS-Stream, is designed to find all frequent itemsets up to the current moment. The second algorithm, TFUHS-Stream, is designed to find frequent itemsets in an uncertain data stream in a time-fading manner. Experimental results show that the proposed hyper-structure-based algorithms outperform the existing tree-based algorithms in terms of accuracy, runtime, and memory usage.",2013,Knowledge and Information Systems volume 37 issue 1 pp 219-244,medical research;data science;data mining;database;computer science;
Spectral evolution in dynamic networks,Jérôme Kunegis (Technical University of Berlin);Damien Fay (University College Cork);Christian Bauckhage (Fraunhofer Society);,"2232072325,2203975768,2034409955","We introduce and study the spectral evolution model, which characterizes the growth of large networks in terms of the eigenvalue decomposition of their adjacency matrices: In large networks, changes over time result in a change of a graph’s spectrum, leaving the eigenvectors unchanged. We validate this hypothesis for several large social, collaboration, rating, citation, and communication networks. Following these observations, we introduce two link prediction algorithms based on the learning of the changes to a network’s spectrum. These new link prediction methods generalize several common graph kernels that can be expressed as spectral transformations. The first method is based on reducing the link prediction problem to a one-dimensional curve-fitting problem which can be solved efficiently. The second algorithm extrapolates a network’s spectrum to predict links. Both algorithms are evaluated on fifteen network datasets for which edge creation times are known.",2013,Knowledge and Information Systems volume 37 issue 1 pp 1-36,graph energy;random geometric graph;adjacency matrix;spectral graph theory;network dynamics;discrete mathematics;combinatorics;machine learning;mathematics;
In-network outlier detection in wireless sensor networks,Joel W. Branch (IBM);Chris Giannella (Mitre Corporation);Boleslaw K. Szymanski (Rensselaer Polytechnic Institute);Ran Wolff (Technion – Israel Institute of Technology);Hillol Kargupta (IBM);,"2109008287,2336552136,1974741261,2096432594,539587773","To address the problem of unsupervised outlier detection in wireless sensor networks, we develop an approach that (1) is flexible with respect to the outlier definition, (2) computes the result in-network to reduce both bandwidth and energy consumption, (3) uses only single-hop communication, thus permitting very simple node failure detection and message reliability assurance mechanisms (e.g., carrier-sense), and (4) seamlessly accommodates dynamic updates to data. We examine performance by simulation, using real sensor data streams. Our results demonstrate that our approach is accurate and imposes reasonable communication and power consumption demands.",2013,Knowledge and Information Systems volume 34 issue 1 pp 23-54,key distribution in wireless sensor networks;computer security;data mining;real time computing;computer science;
Mitigation of the ground reflection effect in real-time locating systems based on wireless sensor networks by using artificial neural networks,Juan Francisco de Paz (University of Salamanca);Dante I. Tapia (University of Salamanca);Ricardo S. Alonso (University of Salamanca);Cristian Pinzón (University of Salamanca);Javier Bajo (Pontifical University of Salamanca);Juan M. Corchado (University of Salamanca);,"2096670735,2100778581,2156007396,2121886138,2132377945,1978763565","Wireless sensor networks (WSNs) have become much more relevant in recent years, mainly because they can be used in a wide diversity of applications. Real-time locating systems (RTLSs) are one of the most promising applications based on WSNs and represent a currently growing market. Specifically, WSNs are an ideal alternative to develop RTLSs aimed at indoor environments where existing global navigation satellite systems, such as the global positioning system, do not work correctly due to the blockage of the satellite signals. However, accuracy in indoor RTLSs is still a problem requiring novel solutions. One of the main challenges is to deal with the problems that arise from the effects of the propagation of radiofrequency waves, such as attenuation, diffraction, reflection and scattering. These effects can lead to other undesired problems, such as multipath. When the ground is responsible for wave reflections, multipath can be modeled as the ground reflection effect. This paper presents an innovative mathematical model for improving the accuracy of RTLSs, focusing on the mitigation of the ground reflection effect by using multilayer perceptron artificial neural networks.",2013,Knowledge and Information Systems volume 34 issue 1 pp 193-217,real time locating system;wireless sensor network;artificial neural network;telecommunications;machine learning;computer science;
From raw publications to Linked Data,Tudor Groza (National University of Ireland);Gunnar AAstrand Grimnes (University of Aberdeen);Siegfried Handschuh (National University of Ireland);Stefan Decker (National University of Ireland);,"298611937,2035177666,2012478052,2182667553","The continuous development of the Linked Data Web depends on the advancement of the underlying extraction mechanisms. This is of particular interest for the scientific publishing domain, where currently most of the data sets are being created manually. In this article, we present a Machine Learning pipeline that enables the automatic extraction of heading metadata (i.e., title, authors, etc) from scientific publications. The experimental evaluation shows that our solution handles very well any type of publication format and improves the average extraction performance of the state of the art with around 4%, in addition to showing an increased versatility. Finally, we propose a flexible Linked Data-driven mechanism to be used both for refining and linking the automatically extracted metadata.",2013,Knowledge and Information Systems volume 34 issue 1 pp 1-21,metadata repository;linked data;conditional random field;support vector machine;data science;information retrieval;data mining;database;artificial intelligence;machine learning;computer science;
Intelligent data structures selection using neural networks,Gabriela Czibula;Istvan Gergely Czibula;Radu Dan Găceanu;,"148830721,424501783,2660233452","It is well known that abstract data types represent the core for any software application, and a proper use of them is an essential requirement for developing a robust and efficient system. Data structures are essential in obtaining efficient algorithms, having a major importance in the software development process. Selecting and creating the appropriate data structure for implementing an abstract data type can greatly impact the performance and the efficiency of the software systems. It is not a trivial problem for a software developer, as it is hard to anticipate all the use scenarios of the deployed application, and a static selection before the system’s execution is, generally, not accurate. In this paper, we are focusing on the problem of dynamic selection of efficient data structures for abstract data types implementation using a supervised learning approach. In order to dynamically select the most suitable representation for an aggregate according to the software system’s current execution context, a neural network will be used. We experimentally evaluate the proposed technique on a case study, emphasizing the advantages of the proposed model in comparison with existing similar approaches.",2013,Knowledge and Information Systems volume 34 issue 1 pp 171-192,test data generation;abstract data type;data structure;theoretical computer science;data mining;database;artificial intelligence;machine learning;computer science;
MOSubdue: a Pareto dominance-based multiobjective Subdue algorithm for frequent subgraph mining,Prakash Shelokar (Environmental Defense Fund);Arnaud Quirin (Environmental Defense Fund);Óscar Cordón (University of Granada);,"2497695276,2143141426,155549070","Graph-based data mining approaches have been mainly proposed to the task popularly known as frequent subgraph mining subject to a single user preference, like frequency, size, etc. In this work, we propose to deal with the frequent subgraph mining problem from multiobjective optimization viewpoint, where a subgraph (or solution) is defined by several user-defined preferences (or objectives), which are conflicting in nature. For example, mined subgraphs with high frequency are often of small size, and vice-versa. Use of such objectives in the multiobjective subgraph mining process generates Pareto-optimal subgraphs, where no subgraph is better than another subgraph in all objectives. We have applied a Pareto dominance approach for the evaluation and search subgraphs regarding to both proximity and diversity in multiobjective sense, which has incorporated in the framework of Subdue algorithm for subgraph mining. The method is called multiobjective subgraph mining by Subdue (MOSubdue) and has several advantages: (i) generation of Pareto-optimal subgraphs in a single run (ii) selection of subgraph-seeds from the candidate subgraphs based on all objectives (iii) search in the multiobjective subgraphs lattice space, and (iv) capability to deal with different multiobjective frequent subgraph mining tasks by customizing the tackled objectives. The good performance of MOSubdue is shown by performing multiobjective subgraph mining defined by two and three objectives on two real-life datasets.",2013,Knowledge and Information Systems volume 34 issue 1 pp 75-108,subgraph isomorphism problem;data mining;machine learning;mathematical optimization;mathematics;
Query-dependent cross-domain ranking in heterogeneous network,Bo Wang (Nanjing University of Aeronautics and Astronautics);Jie Tang (Tsinghua University);Wei Fan (IBM);Songcan Chen (Nanjing University of Aeronautics and Astronautics);Chenhao Tan (Tsinghua University);Zi Yang (Tsinghua University);,"2703712228,2158012360,2491264568,2113462484,2131562809,2136972039","Traditional learning-to-rank problem mainly focuses on one single type of objects. However, with the rapid growth of the Web 2.0, ranking over multiple interrelated and heterogeneous objects becomes a common situation, e.g., the heterogeneous academic network. In this scenario, one may have much training data for some type of objects (e.g. conferences) while only very few for the interested types of objects (e.g. authors). Thus, the two important questions are: (1) Given a networked data set, how could one borrow supervision from other types of objects in order to build an accurate ranking model for the interested objects with insufficient supervision? (2) If there are links between different objects, how can we exploit their relationships for improved ranking performance? In this work, we first propose a regularized framework called HCDRank to simultaneously minimize two loss functions related to these two domains. Then, we extend the approach by exploiting the link information between heterogeneous objects. We conduct a theoretical analysis to the proposed approach and derive its generalization bound to demonstrate how the two related domains could help each other in learning ranking functions. Experimental results on three different genres of data sets demonstrate the effectiveness of the proposed approaches.",2013,Knowledge and Information Systems volume 34 issue 1 pp 109-145,heterogeneous network;aerospace;learning to rank;theoretical computer science;data mining;database;artificial intelligence;machine learning;computer science;
Semi-supervised fuzzy co-clustering algorithm for document categorization,"Yang Yan (Nanyang Technological University);Lihui Chen (Nanyang Technological University);William-Chandra Tjhi (Agency for Science, Technology and Research);","2097347211,2430817880,2001211022","In this paper, we propose a new semi-supervised fuzzy co-clustering algorithm called SS-FCC for categorization of large web documents. In this new approach, the clustering process is carried out by incorporating some prior domain knowledge of a dataset in the form of pairwise constraints provided by users into the fuzzy co-clustering framework. With the help of those constraints, the clustering problem is formulated as the problem of maximizing a competitive agglomeration cost function with fuzzy terms, taking into account the provided domain knowledge. The constraint specifies whether a pair of objects “must” or “cannot” be clustered together. The update rules for fuzzy memberships are derived, and an iterative algorithm is designed for the soft co-clustering process. Our experimental studies show that the quality of clustering results can be improved significantly with the proposed approach. Simulations on 10 large benchmark datasets demonstrate the strength and potentials of SS-FCC in terms of performance evaluation criteria, stability and operating time, compared with some of the existing semi-supervised algorithms.",2013,Knowledge and Information Systems volume 34 issue 1 pp 55-74,flame clustering;canopy clustering algorithm;fuzzy set operations;correlation clustering;constrained clustering;data stream clustering;defuzzification;fuzzy classification;fuzzy number;fuzzy clustering;cluster analysis;data mining;pattern recognition;machine learning;mathematics;
Preference-oriented mining techniques for location-based store search,Jess Soo-Fong Tan (National Cheng Kung University);Eric Hsueh-Chan Lu (National Cheng Kung University);Vincent S. Tseng (National Cheng Kung University);,"2119571821,2005838078,2023431847","With the development of wireless telecommunication technologies, a number of studies have been done on the issues of location-based services due to wide applications. Among them, one of the active topics is the location-based search. Most of previous studies focused on the search of nearby stores, such as restaurants, hotels, or shopping malls, based on the user’s location. However, such search results may not satisfy the users well for their preferences. In this paper, we propose a novel data mining-based approach, named preference-oriented location-based search (POLS), to efficiently search for k nearby stores that are most preferred by the user based on the user’s location, preference, and query time. In POLS, we propose two preference learning algorithms to automatically learn user’s preference. In addition, we propose a ranking algorithm to rank the nearby stores based on user’s location, preference, and query time. To the best of our knowledge, this is the first work on taking temporal location-based search with automatic user preference learning into account simultaneously. Through experimental evaluations on the real dataset, the proposed approach is shown to deliver excellent performance.",2013,Knowledge and Information Systems volume 34 issue 1 pp 147-169,preference learning;collaborative filtering;feedback;world wide web;information retrieval;data mining;database;machine learning;computer science;
Knowledge-based interaction protocols for intelligent interactive environments,Flavio Soares Correa da Silva (University of São Paulo);,2149661963,"Intelligent interactive environments are systems whose main features are related to intelligence, interactivity and location. Intelligent interactive environments are implemented in a variety of platforms, such as distributed sensors, actuators and processors for ambient intelligence; distributed and mobile reasoning entities in virtual worlds and augmented reality; and distributed, multiuser information systems such as social networks and service-oriented systems directed to the dissemination of public services and retailing. In the present article, we introduce JamSession, a tool to specify and execute workflow choreographies, as well as to monitor and control heterogeneous resources, which can be specially useful for the specification and implementation of the coordination of real-time, multimodal and multimedia activities. JamSession is used to specify and execute knowledge-based interaction protocols, which are formally grounded and can be formally analysed and verified using a straightforward and user-friendly graphical language. A knowledge-based interaction protocol determines how disparate resources can work cooperatively to build complex behaviour in intelligent interactive environments.",2013,Knowledge and Information Systems volume 34 issue 1 pp 219-242,knowledge management;human computer interaction;distributed computing;database;computer science;
Information propagation in online social networks: a tie-strength perspective,Jichang Zhao (Beihang University);Junjie Wu (Beihang University);Xu Feng (Beihang University);Hui Xiong (Rutgers University);Ke Xu (Beihang University);,"2153657934,2149366604,2238315765,2153710278,2692917804","In this paper, we investigate the relationship between the tie strength and information propagation in online social networks (OSNs). Specifically, we propose a novel information diffusion model to simulate the information propagation in OSNs. Empirical studies through this model on various real-world online social network data sets reveal three interesting findings. First, it is the adoption of the information pushing mechanism that greatly facilitates the information propagation in OSNs. Second, some global but cost-intensive strategies, such as selecting the ties of higher betweenness centralities for information propagation, no longer have significant advantages. Third, the random selection strategy is more efficient than selecting the strong ties for information propagation in OSNs. Along this line, we provide further explanations by categorizing weak ties into positive and negative ones and reveal the special bridge effect of positive weak ties. The inverse quantitative relationship between weak ties and network clustering coefficients is also carefully studied, which finally gives reasonable explanations to the above findings. Finally, we give some business suggestions for the cost-efficient and secured information propagation in online social networks.",2012,Knowledge and Information Systems volume 32 issue 3 pp 589-608,data mining;artificial intelligence;machine learning;
IKARUS-Onto: a methodology to develop fuzzy ontologies from crisp ones,Panos Alexopoulos (International Metalworking Companies);Manolis Wallace (University of Peloponnese);Konstantinos Kafentzis (International Metalworking Companies);Dimitris Askounis (National Technical University of Athens);,"2136888638,2170043565,2017566523,2111578312","Fuzzy Ontologies comprise a relatively new knowledge representation paradigm that is being increasingly applied in application scenarios in which the treatment and utilization of vague or imprecise knowledge are important. However, the majority of research in the area has mostly focused on the development of conceptual formalisms for representing (and reasoning with) fuzzy ontologies, while the methodological issues entailed within the development process of such an ontology have been so far neglected. With that in mind, we present in this paper IKARUS-Onto, a comprehensive methodology for developing fuzzy ontologies from existing crisp ones that significantly enhances the effectiveness of the fuzzy ontology development process and the quality, in terms of accuracy, shareability and reusability, of the process’s output.",2012,Knowledge and Information Systems volume 32 issue 3 pp 667-695,ontology components;idef5;process ontology;semantic web;knowledge based systems;management science;knowledge management;data mining;artificial intelligence;computer science;
Large-scale analysis of self-disclosure patterns among online social networks users: a Russian context,Slava Kisilevich (University of Konstanz);Chee Siang Ang (University of Kent);,"2089069711,2128408153","Online social network services (SNS) provide an unprecedented rich source of information about millions of users worldwide. However, most existing studies of this emerging phenomenon are limited to relatively small data samples, with an emphasis on mostly “western” online communities (such as Facebook and MySpace users in Western countries). To understand the cultural characteristics of users of online social networks, this paper explores the behavioral patterns of more than 16 million users of a popular social network in the Russian segment of the Internet, namely, My.Mail.Ru (also known as “My World” or “Moj Mir” in Russian). Our main goal is to study the self-disclosure patterns of the site users as a function of their age and gender. We compare the findings of our analysis to the previous studies on Western users of SNS and discuss the culturally distinctive aspects. Our study highlights some important cultural differences in usage patterns among Russian users, which call for further studies in SNS in various cultural contexts.",2012,Knowledge and Information Systems volume 32 issue 3 pp 609-628,self disclosure;correspondence analysis;cultural diversity;multidimensional scaling;cluster analysis;machine learning;computer science;
Impulse noise filtering based on noise-free pixels using genetic programming,Abdul Majid (Pakistan Institute of Engineering and Applied Sciences);Choong-Hwan Lee;Muhammad Tariq Mahmood (Korea University of Technology and Education);Tae-Sun Choi (Gwangju Institute of Science and Technology);,"2162692702,2508226462,2140424718,2135349640","Generally, the impulse noise filtering schemes use all pixels within a neighborhood and increase the size of neighborhood with the increase in noise density. However, the estimate from all pixels within neighborhood may not be accurate. Moreover, the larger window may remove edges and fine details as well. In contrast, we propose a novel impulse noise removal scheme that emphasizes on few noise-free pixels and small neighborhood. The proposed scheme searches noise-free pixels within a small neighborhood. If at least three pixels are not found, then the noisy pixel is left unchanged in current iteration. This iterative process continues until all noisy pixels are replaced with estimated values. In order to estimate the optimal value of the noisy pixel, genetic programming-based estimator is developed. The estimator (function) is composed of useful pixel information and arithmetic functions. Experimental results show that the proposed scheme is capable of removing impulse noise effectively while preserving the fine image details. Especially, our approach has shown effectiveness against high impulse noise density.",2012,Knowledge and Information Systems volume 32 issue 3 pp 505-526,value noise;dark frame subtraction;gradient noise;salt and pepper noise;image noise;non local means;impulse noise;genetic programming;image restoration;filter;gaussian noise;computer vision;mathematical optimization;computer science;mathematics;
Interpretable and reconfigurable clustering of document datasets by deriving word-based rules,Vipin Balachandran (VMware);Vipin Deepak P Balachandran (IBM);Deepak Khemani (Indian Institute of Technology Madras);,"2435470934,2498281766,64770994","Clusters of text documents output by clustering algorithms are often hard to interpret. We describe motivating real-world scenarios that necessitate reconfigurability and high interpretability of clusters and outline the problem of generating clusterings with interpretable and reconfigurable cluster models. We develop two clustering algorithms toward the outlined goal of building interpretable and reconfigurable cluster models. They generate clusters with associated rules that are composed of conditions on word occurrences or nonoccurrences. The proposed approaches vary in the complexity of the format of the rules; RGC employs disjunctions and conjunctions in rule generation whereas RGC-D rules are simple disjunctions of conditions signifying presence of various words. In both the cases, each cluster is comprised of precisely the set of documents that satisfy the corresponding rule. Rules of the latter kind are easy to interpret, whereas the former leads to more accurate clustering. We show that our approaches outperform the unsupervised decision tree approach for rule-generating clustering and also an approach we provide for generating interpretable models for general clusterings, both by significant margins. We empirically show that the purity and f-measure losses to achieve interpretability can be as little as 3 and 5%, respectively using the algorithms presented herein.",2012,Knowledge and Information Systems volume 32 issue 3 pp 475-503,cluster analysis;data mining;pattern recognition;artificial intelligence;machine learning;computer science;mathematics;
Toward the use of Petri nets for the formalization of OWL-S choreographies,Juan Carlos Vidal (University of Santiago de Compostela);Manuel Lama (University of Santiago de Compostela);Alberto Bugarín (Grupo México);,"2155561016,2133985302,2472837819","In this paper, we present a Petri net-based approach for modeling the choreography of semantic Web services which are described following the OWL-S specification. In our approach, each control construct of the OWL-S choreography is represented through a Petri net pattern that captures formally its operational semantics. The main difference between our work and the main proposals that model the semantics of OWL-S services choreography is that, although both approaches represent the service choreography with Petri nets, our proposal is also concerned with the practical execution of the Petri nets by the client. Therefore we also represent the flow of data, the outputs transformations, the effects in the environment, in addition to the structures that control the choreography of the services in our Petri net models. The implementation of the OWL-S choreography is performed in a Petri net ontology-based engine. This is another difference with traditional approaches that only use Petri nets for the analysis of the service properties. Furthermore, the use of an underlying ontology engine for supporting both the domain models of OWL-S services and the Petri net models provides several advantages in terms of reasoning, extension, and reuse.",2012,Knowledge and Information Systems volume 32 issue 3 pp 629-665,process architecture;owl s;ontology;petri net;knowledge management;database;artificial intelligence;programming language;computer science;
PI-Join: Efficiently processing join queries on massive data,Xixian Han (Harbin Institute of Technology);Jianzhong Li (Harbin Institute of Technology);Donghua Yang (Harbin Institute of Technology);,"2113522248,2628215130,2630904155","The ratio of disk capacity to disk transfer rate typically increases by 10× per decade. As a result, disk is becoming slower from the view of applications because of the much larger data volume that they need to store and process. In database systems, the less the data volume that is involved in query processing, the better the performance that is achieved. Disk-based join operation is a common but time-consuming database operation, especially in an environment of massive data in which I/O cost dominates the execution time. However, current join algorithms are only suitable for moderate or small data volume. They will incur high I/O cost when performing on massive data because of multi-pass I/O operations on the joined tables and the insensitivity to join selectivity. This paper proposes PI-Join a novel disk-based join algorithm that can efficiently process join queries involving massive data. PI-Join consists of two stages: JPIPT construction stage (JCS) and result output stage (ROS). JCS performs a cache-conscious construction algorithm on join attributes which are kept in column-oriented model to obtain join positional index pair table (JPIPT) of join results faster. The obtained JPIPT is used in ROS to retrieve results in a one-pass sequential selective scan on each table. We provide the correctness proof and cost analysis of PI-Join. Our experimental results indicate that PI-Join has a significant advantage over the existing join algorithms.",2012,Knowledge and Information Systems volume 32 issue 3 pp 527-557,recursive join;data mining;database;real time computing;computer science;
Real time prediction of closing price and duration of B2B reverse auctions,Bayarmaa Dashnyam (National Central University);Yu-Chin Liu (Shih Hsin University);Ping-Yu Hsu (National Central University);Yun-Ting Tsai;,"2003310025,2153659379,2102555095,2485657530","Nowadays, online auctions have become the most successful business model in the electronic marketplace. To the best of the authors’ knowledge, no other work has been devoted to the prediction of closing price and duration of Business-to-Business (B2B) English reverse online auctions in which goods or service providers compete with each other to win contracts by lowering offering prices with each bid, which is conducted on a virtual platform hosted on the Internet. This research designs and proposes a new methodology to predict closing prices and duration within the first few bids of the corresponding auctions based on real time bidding information rather than static auction information. In this article, we employ real time information and prediction rules to forecast the behavior of live auctions. This is in contrast to the static prediction approach that takes into consideration only information available at the beginning of an auction such as products, item features, or the seller’s reputation. This simulation is based on discretized auction data derived from a B2B online auction marketplace over a two-year period. Three measurements including accuracy, coverage, and benefit are used to evaluate the methodology. Results show that after observing the first 4 bids, this methodology can predict closing prices and duration with 84.6 and 71.9% accuracy, respectively.",2012,Knowledge and Information Systems volume 32 issue 3 pp 697-716,unique bid auction;auto auction;forward auction;eauction;reservation price;reverse auction;english auction;combinatorial auction;auction theory;common value auction;e commerce;data mining;computer science;
An energy-conserving approach for data formatting and trusted document exchange in resource-constrained networks,P. P. Abdul Haleem (MES College of Engineering);M. P. Sebastian (Indian Institute of Management Kozhikode);,"2562265564,2232231699","Lightweight data formatting and document exchanging schemes are of interest in conserving energy while exchanging documents, particularly in the resource constrained networks (RCNs). This paper presents an energy-conserving, lightweight method for data representation and trusted document exchange. It is based on a format derived from the YAML Ain’t Markup Language (YAML), a lightweight data serialization language and includes a schema extraction process to separate data from its structure. The resultant schema is useful for thinning the data format and also for checking the rewriting attacks. The rewriting attack is checked using a two-tiered approach. It is observed that the proposed document format is less verbose and more energy conserving than XML and other popular non-binary formats. The format can be used to represent non-document data also.",2012,Knowledge and Information Systems volume 32 issue 3 pp 559-587,data exchange;xml;mobile computing;theoretical computer science;world wide web;data mining;database;computer science;
Decision trees for uplift modeling with single and multiple treatments,Piotr Rzepakowski;Szymon Jaroszewicz (University of Massachusetts Amherst);,"1907515892,115085028","Most classification approaches aim at achieving high prediction accuracy on a given dataset. However, in most practical cases, some action such as mailing an offer or treating a patient is to be taken on the classified objects, and we should model not the class probabilities themselves, but instead, the change in class probabilities caused by the action. The action should then be performed on those objects for which it will be most profitable. This problem is known as uplift modeling, differential response analysis, or true lift modeling, but has received very little attention in machine learning literature. An important modification of the problem involves several possible actions, when for each object, the model must also decide which action should be used in order to maximize profit. In this paper, we present tree-based classifiers designed for uplift modeling in both single and multiple treatment cases. To this end, we design new splitting criteria and pruning methods. The experiments confirm the usefulness of the proposed approaches and show significant improvement over previous uplift modeling techniques.",2012,Knowledge and Information Systems volume 32 issue 2 pp 303-327,randomized controlled trial;decision tree;information theory;data mining;artificial intelligence;machine learning;simulation;statistics;computer science;
Real-time spatio-temporal analysis of dynamic scenes,Tobias Warden (University of Bremen);Ubbo Visser (University of Miami);,"2147877293,196936156","We propose a set of tools for spatio-temporal real-time analysis of dynamic scenes. It is designed to improve the grounding situation of autonomous agents in (simulated) physical domains. We introduce a knowledge processing pipeline ranging from relevance-driven compilation of a qualitative scene description to a knowledge-based detection of complex event and action sequences, conceived as a spatio-temporal pattern-matching problem. A methodology for the formalization of motion patterns and their inner composition is introduced and applied to capture human expertise about domain-specific motion situations. We present extensive experimental results from a challenging environment: 3D soccer simulation. It substantiates real-time applicability of our approach under tournament conditions, based on a 5-Hz (a) precise and (b) noisy/incomplete perception. The approach is not limited to robot soccer. Instead, it can also be applied in other fields such as experimental biology and logistic processes.",2012,Knowledge and Information Systems volume 32 issue 2 pp 243-279,scene statistics;computer vision;data mining;artificial intelligence;machine learning;simulation;computer science;
Kingfisher: an efficient algorithm for searching for both positive and negative dependency rules with statistical significance measures,Wilhelmiina Hämäläinen (University of Eastern Finland);,2275152698,"Statistical dependency analysis is the basis of all empirical science. A commonly occurring problem is to find the most significant dependency rules, which describe either positive or negative dependencies between categorical attributes. In medical science, for example, one is interested in genetic factors, which can either predispose or prevent diseases. The requirement of statistical significance is essential, because the discoveries should hold also in future data. Typically, the significance is estimated either by Fisher’s exact test or the χ2-measure. The problem is computationally very difficult, because the number of all possible dependency rules increases exponentially with the number of attributes. As a solution, different kinds of restrictions and heuristics have been applied, but a general, scalable search method has been missing. In this paper, we introduce an efficient algorithm, called Kingfisher, for searching for the best non-redundant dependency rules with statistical significance measures. The rules can express either positive or negative dependencies between a set of positive attributes and a single consequent attribute. The algorithm itself is independent from the used goodness measure, but we concentrate on Fisher’s exact test and the χ2-measure. The algorithm is based on an application of the branch-and-bound search strategy, supplemented by several pruning properties. Especially, we prove a new lower bound for Fisher’s p and introduce a new effective pruning principle. According to our experiments on classical benchmark data, the algorithm is well scalable and can efficiently handle even dense and high-dimensional data sets. An interesting observation was that Fisher’s exact test did not only produce more reliable rules than the χ2-measure, but it also performed the search much faster.",2012,Knowledge and Information Systems volume 32 issue 2 pp 383-414,fisher s exact test;statistical significance;data mining;machine learning;statistics;mathematics;
Improving clustering by learning a bi-stochastic data similarity matrix,Fei Wang (Cornell University);Ping Li (Cornell University);Arnd Christian König (Microsoft);Muting Wan (Cornell University);,"2656417562,2721445647,2157050567,2411918979","An idealized clustering algorithm seeks to learn a cluster-adjacency matrix such that, if two data points belong to the same cluster, the corresponding entry would be 1; otherwise, the entry would be 0. This integer (1/0) constraint makes it difficult to find the optimal solution. We propose a relaxation on the cluster-adjacency matrix, by deriving a bi-stochastic matrix from a data similarity (e.g., kernel) matrix according to the Bregman divergence. Our general method is named the Bregmanian Bi-Stochastication (BBS) algorithm. We focus on two popular choices of the Bregman divergence: the Euclidean distance and the Kullback–Leibler (KL) divergence. Interestingly, the BBS algorithm using the KL divergence is equivalent to the Sinkhorn–Knopp (SK) algorithm for deriving bi-stochastic matrices. We show that the BBS algorithm using the Euclidean distance is closely related to the relaxed k-means clustering and can often produce noticeably superior clustering results to the SK algorithm (and other algorithms such as Normalized Cut), through extensive experiments on public data sets.",2012,Knowledge and Information Systems volume 32 issue 2 pp 351-382,cuthill mckee algorithm;k medians clustering;canopy clustering algorithm;correlation clustering;euclidean distance matrix;constrained clustering;eight point algorithm;cure data clustering algorithm;bregman divergence;adjacency matrix;spectral clustering;distance matrix;fuzzy clustering;non negative matrix factorization;stochastic matrix;k means clustering;euclidean distance;kullback leibler divergence;cluster analysis;combinatorics;machine learning;mathematical optimization;statistics;mathematics;
A sampling approach for skyline query cardinality estimation,Cheng Luo (Coppin State University);Zhewei Jiang (Frederick Community College);Wen Chi Hou (Southern Illinois University Carbondale);Shan He (Southwest Petroleum University);Qiang Zhu (University of Michigan);,"2208147014,2162146863,2119586575,2143774539,2292350935","A skyline query returns a set of candidate records that satisfy several preferences. It is an operation commonly performed to aid decision making. Since executing a skyline query is expensive and a query plan may combine skyline queries with other data operations such as join, it is important that the query optimizer can quickly yield an accurate cardinality estimate for a skyline query. Log Sampling (LS) and Kernel-Based ( KB) skyline cardinality estimation are the two state-of-the-art skyline cardinality estimation methods. LS is based on a hypothetical model A(log(n))B. Since this model is originally derived under strong assumptions like data independence between dimensions, it does not apply well to an arbitrary data set. Consequently, LS can yield large estimation errors. KB relies on the integration of the estimated probability density function (PDF) to derive the scale factor Ψds. As the estimation of PDF and the ensuing integration both involve complex mathematical calculations, KB is time consuming. In view of these problems, we propose an innovative purely sampling-based (PS) method for skyline cardinality estimation. PS is non-parametric. It does not assume any particular data distribution and is, thus, more robust than LS. PS does not require complex mathematical calculations. Therefore, it is much simpler to implement and much faster to yield the estimates than KB. Extensive empirical studies show that for a variety of real and synthetic data sets, PS outperforms LS in terms of estimation speed, estimation accuracy, and estimation variability under the same space budget. PS outperforms KB in terms of estimation speed and estimation variability under the same performance mark.",2012,Knowledge and Information Systems volume 32 issue 2 pp 281-301,query optimization;sampling;data mining;database;statistics;computer science;
Probabilistic joint models incorporating logic and learning via structured variational approximation for information extraction,Xiaofeng Yu (The Chinese University of Hong Kong);Wai Lam (The Chinese University of Hong Kong);,"2655560346,2119595446","Traditional information extraction systems for compound tasks adopt pipeline architectures, which are highly ineffective and suffer from several problems such as cascading accumulation of errors. In this paper, we propose a joint discriminative probabilistic framework to optimize all relevant subtasks simultaneously. This framework offers a great flexibility to incorporate the advantage of both uncertainty for sequence modeling and first-order logic for domain knowledge. The first-order logic model provides a more expressive formalism tackling the issue of limited expressiveness of traditional attribute-value representation. Our framework defines a joint probability distribution for both segmentations in sequence data and possible worlds of relations between segments in the form of an exponential family. Since exact parameter estimation and inference are prohibitively intractable in this model, a structured variational inference algorithm is developed to perform parameter estimation approximately. For inference, we propose a highly coupled, bi-directional Metropolis-Hastings (MH) algorithm to find the maximum a posteriori (MAP) assignments for both segmentations and relations. Extensive experiments on two real-world information extraction tasks, entity identification and relation extraction from Wikipedia, and citation matching show that (1) the proposed model achieves significant improvement on both tasks compared to state-of-the-art pipeline models and other joint models; (2) the bi-directional MH inference algorithm obtains boosted performance compared to the greedy, N-best list, and uni-directional MH sampling algorithms.",2012,Knowledge and Information Systems volume 32 issue 2 pp 415-444,graphical model;first order logic;data mining;database;pattern recognition;artificial intelligence;machine learning;statistics;computer science;mathematics;
Knowledge-empowered agent information system for privacy payoff in eCommerce,Abdulsalam Yassine (University of Ottawa);Ali Asghar Nazari Shirehjini (Sharif University of Technology);Shervin Shirmohammadi (University of Ottawa);Thomas T. Tran (University of Ottawa);,"2142866468,2307739268,2144923365,2108359352","Today, many online companies are gathering information and assembling sophisticated databases that know a great deal of information about many people, generally without the knowledge of those people. Such endeavor has resulted in the unprecedented attrition of individual’s right to informational self-determination. On the one hand, Consumers are powerless to prevent the unauthorized dissemination of their personal information, and on the other, they are excluded from its profitable commercial exchange. This paper focuses on developing knowledge-empowered agent information system for privacy payoff as a means of rewarding consumers for sharing their personal information with online businesses. The design of this system is driven by the following argument: if consumers’ personal information is a valuable asset, should they not be entitled to benefit from their asset as well? The proposed information system is a multi-agent system where several agents employ various knowledge and requirements for personal information valuation and interaction capabilities that most users cannot do on their own. The agents in the information system bear the responsibility of working on behalf of consumers to categorize their personal data objects, report to consumers on online businesses’ trust and reputation, determine the value of their compensation using risk-based financial models, and finally negotiate for a payoff value in return for the dissemination of users’ information. The details of the system as well as a proof-of-concept implementation using JADE (Java Agent Development Environment) are presented here.",2012,Knowledge and Information Systems volume 32 issue 2 pp 445-473,group information management;personal information management;privacy;information system;knowledge;management information systems;e commerce;data mining;database;computer science;
A relational hierarchical model for decision-theoretic assistance,Sriraam Natarajan (Oregon State University);Prasad Tadepalli (Oregon State University);Alan Fern (Oregon State University);,"2156930872,1993564419,2139785505","Building intelligent assistants has been a long-cherished goal of AI, and many were built and fine-tuned to specific application domains. In recent work, a domain-independent decision-theoretic model of assistance was proposed, where the task is to infer the user’s goal and take actions that minimize the expected cost of the user’s policy. In this paper, we extend this work to domains where the user’s policies have rich relational and hierarchical structure. Our results indicate that relational hierarchies allow succinct encoding of prior knowledge for the assistant, which in turn enables the assistant to start helping the user after a relatively small amount of experience.",2012,Knowledge and Information Systems volume 32 issue 2 pp 329-349,knowledge management;data mining;database;artificial intelligence;machine learning;simulation;computer science;
On the choice of the best imputation methods for missing values considering three groups of classification methods,Julián Luengo (University of Granada);Salvador García (University of Jaén);Francisco Herrera (University of Granada);,"2126985344,2145770163,2141382980","In real-life data, information is frequently lost in data mining, caused by the presence of missing values in attributes. Several schemes have been studied to overcome the drawbacks produced by missing values in data mining tasks; one of the most well known is based on preprocessing, formerly known as imputation. In this work, we focus on a classification task with twenty-three classification methods and fourteen different imputation approaches to missing values treatment that are presented and analyzed. The analysis involves a group-based approach, in which we distinguish between three different categories of classification methods. Each category behaves differently, and the evidence obtained shows that the use of determined missing values imputation methods could improve the accuracy obtained for these methods. In this study, the convenience of using imputation methods for preprocessing data sets with missing values is stated. The analysis suggests that the use of particular imputation methods conditioned to the groups is required.",2012,Knowledge and Information Systems volume 32 issue 1 pp 77-108,imputation;missing data;biological classification;data mining;pattern recognition;statistics;mathematics;
Design and behavior study of a grammar-guided genetic programming algorithm for mining association rules,José María Luna (University of Córdoba);José Raúl Romero (University of Córdoba);Sebastián Ventura (University of Córdoba);,"2130867717,2218717932,2098297204","This paper presents a proposal for the extraction of association rules called G3PARM (Grammar-Guided Genetic Programming for Association Rule Mining) that makes the knowledge extracted more expressive and flexible. This algorithm allows a context-free grammar to be adapted and applied to each specific problem or domain and eliminates the problems raised by discretization. This proposal keeps the best individuals (those that exceed a certain threshold of support and confidence) obtained with the passing of generations in an auxiliary population of fixed size n. G3PARM obtains solutions within specified time limits and does not require the large amounts of memory that the exhaustive search algorithms in the field of association rules do. Our approach is compared to exhaustive search (Apriori and FP-Growth) and genetic (QuantMiner and ARMGA) algorithms for mining association rules and performs an analysis of the mined rules. Finally, a series of experiments serve to contrast the scalability of our algorithm. The proposal obtains a small set of rules with high support and confidence, over 90 and 99% respectively. Moreover, the resulting set of rules closely satisfies all the dataset instances. These results illustrate that our proposal is highly promising for the discovery of association rules in different types of datasets.",2012,Knowledge and Information Systems volume 32 issue 1 pp 53-76,genetic programming;association rule learning;evolutionary algorithm;data mining;artificial intelligence;machine learning;algorithm;computer science;
A clustering approach for sampling data streams in sensor networks,Alzennyr da Silva (Télécom ParisTech);Raja Chiky (International Student Exchange Programs);Georges Hébrail (Télécom ParisTech);,"2305000569,2280415193,352435704","The growing usage of embedded devices and sensors in our daily lives has been profoundly reshaping the way we interact with our environment and our peers. As more and more sensors will pervade our future cities, increasingly efficient infrastructures to collect, process and store massive amounts of data streams from a wide variety of sources will be required. Despite the different application-specific features and hardware platforms, sensor network applications share a common goal: periodically sample and store data collected from different sensors in a common persistent memory. In this article, we present a clustering approach for rapidly and efficiently computing the best sampling rate which minimizes the Sum of Square Error for each particular sensor in a network. In order to evaluate the efficiency of the proposed approach, we carried out experiments on real electric power consumption data streams provided by EDF (Electricite de France).",2012,Knowledge and Information Systems volume 32 issue 1 pp 1-23,wireless sensor network;cluster analysis;sampling;data stream mining;embedded system;data mining;database;real time computing;machine learning;computer science;
MatchSim: a novel similarity measure based on maximum neighborhood matching,Zhenjiang Lin (The Chinese University of Hong Kong);Michael R. Lyu (The Chinese University of Hong Kong);Irwin King (The Chinese University of Hong Kong);,"2251411024,2227744130,2121363826","Measuring object similarity in a graph is a fundamental data- mining problem in various application domains, including Web linkage mining, social network analysis, information retrieval, and recommender systems. In this paper, we focus on the neighbor-based approach that is based on the intuition that “similar objects have similar neighbors” and propose a novel similarity measure called MatchSim. Our method recursively defines the similarity between two objects by the average similarity of the maximum-matched similar neighbor pairs between them. We show that MatchSim conforms to the basic intuition of similarity; therefore, it can overcome the counterintuitive contradiction in SimRank. Moreover, MatchSim can be viewed as an extension of the traditional neighbor-counting scheme by taking the similarities between neighbors into account, leading to higher flexibility. We present the MatchSim score computation process and prove its convergence. We also analyze its time and space complexity and suggest two accelerating techniques: (1) proposing a simple pruning strategy and (2) adopting an approximation algorithm for maximum matching computation. Experimental results on real-world datasets show that although our method is less efficient computationally, it outperforms classic methods in terms of accuracy.",2012,Knowledge and Information Systems volume 32 issue 1 pp 141-166,similarity heuristic;link analysis;matching;web mining;data mining;pattern recognition;machine learning;computer science;mathematics;
Density-preserving projections for large-scale local anomaly detection,Timothy de Vries (University of Sydney);Sanjay Chawla (University of Sydney);Michael E. Houle (National Institute of Informatics);,"2137430947,2201421368,2088104168","Outlier or anomaly detection is a fundamental data mining task with the aim to identify data points, events, transactions which deviate from the norm. The identification of outliers in data can provide insights about the underlying data generating process. In general, outliers can be of two kinds: global and local. Global outliers are distinct with respect to the whole data set, while local outliers are distinct with respect to data points in their local neighbourhood. While several approaches have been proposed to scale up the process of global outlier discovery in large databases, this has not been the case for local outliers. We tackle this problem by optimising the use of local outlier factor (LOF) for large and high-dimensional data. We propose projection-indexed nearest-neighbours (PINN), a novel technique that exploits extended nearest-neighbour sets in a reduced-dimensional space to create an accurate approximation for k-nearest-neighbour distances, which is used as the core density measurement within LOF. The reduced dimensionality allows for efficient sub-quadratic indexing in the number of items in the data set, where previously only quadratic performance was possible. A detailed theoretical analysis of random projection (RP) and PINN shows that we are able to preserve the density of the intrinsic manifold of the data set after projection. Experimental results show that PINN outperforms the standard projection methods RP and PCA when measuring LOF for many high-dimensional real-world data sets of up to 300,000 elements and 102,600 dimensions. A further investigation into the use of high-dimensionality-specific indexing such as spatial approximate sample hierarchy (SASH) shows that our novel technique holds benefits over even these types of highly efficient indexing. We cement the practical applications of our novel technique with insights into what it means to find local outliers in real data including image and text data, and include potential applications for this knowledge.",2012,Knowledge and Information Systems volume 32 issue 1 pp 25-52,dimensionality reduction;anomaly detection;data mining;machine learning;statistics;computer science;mathematics;
BM25t: a BM25 extension for focused information retrieval,Mathias Géry (University of Lyon);Christine Largeron (University of Lyon);,"2205491476,361319035","This paper addresses the integration of XML tags into a term-weighting function for focused XML information retrieval (IR). Our model allows us to consider a certain kind of structural information: tags that represent a logical structure (e.g., title, section, paragraph, etc.) as well as other tags (e.g., bold, italic, center, etc.). We take into account the influence of a tag by estimating the probability for this tag to distinguish relevant terms from the others. Then, these weights are integrated in a term-weighting function. Experiments on a large collection from the INEX 2008 XML IR evaluation campaign showed improvements on focused XML retrieval.",2012,Knowledge and Information Systems volume 32 issue 1 pp 217-241,divergence from randomness model;human computer information retrieval;xml validation;xml framework;okapi bm25;xml database;document structure description;xml schema;xml;world wide web;information retrieval;data mining;database;machine learning;programming language;computer science;
Using structural similarity for clustering XML documents,Ali Aïtelhadj;Mohand Boughanem (Paul Sabatier University);Mohamed Mezghiche;Fatiha Souam;,"319891300,117014290,2635278911,315340077","In this paper, we describe a method for clustering XML documents. Its goal is to group documents sharing similar structures. Our approach is two-step. We first automatically extract the structure from each XML document to be classified. This extracted structure is then used as a representation model to classify the corresponding XML document. The idea behind the clustering is that if XML documents share similar structures, they are more likely to correspond to the structural part of the same query. Finally, for the experimentation purpose, we tested our algorithms on both real (ACM SIGMOD Record corpus) and synthetic data. The results clearly demonstrate the interest of our approach.",2012,Knowledge and Information Systems volume 32 issue 1 pp 109-139,xml schema;xml validation;simple api for xml;xml schema;fuzzy clustering;node;similarity;tree;cluster analysis;information retrieval;data mining;database;machine learning;computer science;
Similarity assessment for removal of noisy end user license agreements,Niklas Lavesson (Blekinge Institute of Technology);Stefan Axelsson (Blekinge Institute of Technology);,"1922797874,1991321396","In previous work, we have shown the possibility to automatically discriminate between legitimate software and spyware-associated software by performing supervised learning of end user license agreements (EULAs). However, the amount of false positives (spyware classified as legitimate software) was too large for practical use. In this study, the false positives problem is addressed by removing noisy EULAs, which are identified by performing similarity analysis of the previously studied EULAs. Two candidate similarity analysis methods for this purpose are experimentally compared: cosine similarity assessment in conjunction with latent semantic analysis (LSA) and normalized compression distance (NCD). The results show that the number of false positives can be reduced significantly by removing noise identified by either method. However, the experimental results also indicate subtle performance differences between LSA and NCD. To improve the performance even further and to decrease the large number of attributes, the categorical proportional difference (CPD) feature selection algorithm was applied. CPD managed to greatly reduce the number of attributes while at the same time increase classification performance on the original data set, as well as on the LSA- and NCD-based data sets.",2012,Knowledge and Information Systems volume 32 issue 1 pp 167-189,normalized compression distance;latent semantic analysis;world wide web;data mining;database;artificial intelligence;machine learning;computer science;
Ontology driven search of compound IDs,Alberto Abelló (Polytechnic University of Catalonia);Oscar Romero (Polytechnic University of Catalonia);,"2148989113,2124168510","Object identification is a crucial step in most information systems. Nowadays, we have many different ways to identify entities such as surrogates, keys, and object identifiers. However, not all of them guarantee the entity identity. Many works have been introduced in the literature for discovering meaningful identifiers (i.e., guaranteeing the entity identity according to the semantics of the universe of discourse), but all of them work at the logical or data level and they share some constraints inherent to the kind of approach. Addressing it at the logical level, we may miss some important data dependencies, while the cost to identify data dependencies purely at the data level may not be affordable. In this paper, we propose an approach for discovering meaningful identifiers driven by domain ontologies. In our approach, we guide the process at the conceptual level and we introduce a set of pruning rules for improving the performance by reducing the number of identifier hypotheses generated and to be verified with data. Finally, we also introduce a simulation over a case study to show the feasibility of our method.",2012,Knowledge and Information Systems volume 32 issue 1 pp 191-216,identifier;business process reengineering;ontology;world wide web;data mining;database;artificial intelligence;computer science;
Text document clustering using global term context vectors,Argyris Kalogeratos (University of Ioannina);Aristidis Likas (University of Ioannina);,"325970166,2080877090","Despite the advantages of the traditional vector space model (VSM) representation, there are known deficiencies concerning the term independence assumption. The high dimensionality and sparsity of the text feature space and phenomena such as polysemy and synonymy can only be handled if a way is provided to measure term similarity. Many approaches have been proposed that map document vectors onto a new feature space where learning algorithms can achieve better solutions. This paper presents the global term context vector-VSM (GTCV-VSM) method for text document representation. It is an extension to VSM that: (i) it captures local contextual information for each term occurrence in the term sequences of documents; (ii) the local contexts for the occurrences of a term are combined to define the global context of that term; (iii) using the global context of all terms a proper semantic matrix is constructed; (iv) this matrix is further used to linearly map traditional VSM (Bag of Words—BOW) document vectors onto a ‘semantically smoothed’ feature space where problems such as text document clustering can be solved more efficiently. We present an experimental study demonstrating the improvement of clustering results when the proposed GTCV-VSM representation is used compared with traditional VSM-based approaches.",2012,Knowledge and Information Systems volume 31 issue 3 pp 455-474,document clustering;text mining;information retrieval;data mining;database;pattern recognition;machine learning;computer science;mathematics;
Finding associations and computing similarity via biased pair sampling,Andrea Campagna (IT University of Copenhagen);Rasmus Pagh (IT University of Copenhagen);,"2165713599,1864519460","Sampling-based methods have previously been proposed for the problem of finding interesting associations in data, even for low-support items. While these methods do not guarantee precise results, they can be vastly more efficient than approaches that rely on exact counting. However, for many similarity measures no such methods have been known. In this paper, we show how a wide variety of measures can be supported by a simple biased sampling method. The method also extends to find high-confidence association rules. We demonstrate theoretically that our method is superior to exact methods when the threshold for “interesting similarity/confidence” is above the average pairwise similarity/confidence, and the average support is not too low. Our method is particularly advantageous when transactions contain many items. We confirm in experiments on standard association mining benchmarks that we obtain a significant speedup on real data sets. Reductions in computation time of over an order of magnitude, and significant savings in space, are observed.",2012,Knowledge and Information Systems volume 31 issue 3 pp 505-526,association rule learning;sampling;data mining;machine learning;statistics;computer science;mathematics;
SumCR: A new subtopic-based extractive approach for text summarization,Jian-Ping Mei (Nanyang Technological University);Lihui Chen (Nanyang Technological University);,"2653915013,2430817880","In text summarization, relevance and coverage are two main criteria that decide the quality of a summary. In this paper, we propose a new multi-document summarization approach SumCR via sentence extraction. A novel feature called Exemplar is introduced to help to simultaneously deal with these two concerns during sentence ranking. Unlike conventional ways where the relevance value of each sentence is calculated based on the whole collection of sentences, the Exemplar value of each sentence in SumCR is obtained within a subset of similar sentences. A fuzzy medoid-based clustering approach is used to produce sentence clusters or subsets where each of them corresponds to a subtopic of the related topic. Such kind of subtopic-based feature captures the relevance of each sentence within different subtopics and thus enhances the chance of SumCR to produce a summary with a wider coverage and less redundancy. Another feature we incorporate in SumCR is Position, i.e., the position of each sentence appeared in the corresponding document. The final score of each sentence is a combination of the subtopic-level feature Exemplar and the document-level feature Position. Experimental studies on DUC benchmark data show the good performance of SumCR and its potential in summarization tasks.",2012,Knowledge and Information Systems volume 31 issue 3 pp 527-545,cluster analysis;automatic summarization;natural language processing;information retrieval;data mining;pattern recognition;machine learning;computer science;
Sequential latent Dirichlet allocation,Lan Du (Australian National University);Wray L. Buntine (Australian National University);Huidong Jin (Australian National University);Changyou Chen (Australian National University);,"2150389075,2282891647,2126895050,2156460610","Understanding how topics within a document evolve over the structure of the document is an interesting and potentially important problem in exploratory and predictive text analytics. In this article, we address this problem by presenting a novel variant of latent Dirichlet allocation (LDA): Sequential LDA (SeqLDA). This variant directly considers the underlying sequential structure, i.e. a document consists of multiple segments (e.g. chapters, paragraphs), each of which is correlated to its antecedent and subsequent segments. Such progressive sequential dependency is captured by using the hierarchical two-parameter Poisson–Dirichlet process (HPDP). We develop an efficient collapsed Gibbs sampling algorithm to sample from the posterior of the SeqLDA based on the HPDP. Our experimental results on patent documents show that by considering the sequential structure within a document, our SeqLDA model has a higher fidelity over LDA in terms of perplexity (a standard measure of dictionary-based compressibility). The SeqLDA model also yields a nicer sequential topic structure than LDA, as we show in experiments on several books such as Melville’s ‘Moby Dick’.",2012,Knowledge and Information Systems volume 31 issue 3 pp 475-503,dynamic topic model;topic model;document structure description;latent dirichlet allocation;speech recognition;data mining;database;pattern recognition;artificial intelligence;machine learning;statistics;computer science;
Improvement of neural network classifier using floating centroids,Lin Wang (University of Jinan);Bo Yang (University of Jinan);Yuehui Chen (University of Jinan);Ajith Abraham (Technical University of Ostrava);Hongwei Sun (University of Jinan);Zhenxiang Chen (University of Jinan);Haiyang Wang (Shandong University);,"2555644399,2495399568,2144134715,2345075623,2668219254,2135536934,2721069896","This paper presents a novel technique—Floating Centroids Method (FCM) designed to improve the performance of a conventional neural network classifier. Partition space is a space that is used to categorize data sample after sample is mapped by neural network. In the partition space, the centroid is a point, which denotes the center of a class. In a conventional neural network classifier, position of centroids and the relationship between centroids and classes are set manually. In addition, number of centroids is fixed with reference to the number of classes. The proposed approach introduces many floating centroids, which are spread throughout the partition space and obtained by using K-Means algorithm. Moreover, different classes labels are attached to these centroids automatically. A sample is predicted as a certain class if the closest centroid of its corresponding mapped point is labeled by this class. Experimental results illustrate that the proposed method has favorable performance especially with respect to the training accuracy, generalization accuracy, and average F-measures.",2012,Knowledge and Information Systems volume 31 issue 3 pp 433-454,biological classification;artificial neural network;data mining;pattern recognition;machine learning;computer science;mathematics;
Hybrid reasoning technique for improving context-aware applications,Matthias Strobbe (Ghent University);Olivier Van Laere (Ghent University);Bart Dhoedt (Ghent University);Filip De Turck (Ghent University);Piet Demeester (Ghent University);,"2223201854,1985108775,1990677562,264213217,2435511731","With the rapid adoption of GPS enabled smart phones and the fact that users are almost permanently connected to the Internet, an evolution is observed toward applications and services that adapt themselves using the user’s context, a.o. taking into account location information. To facilitate the development of such new intelligent applications, new enabling platforms are needed to collect, distribute, and exchange context information. An important aspect of such platforms is the derivation of new, high-level knowledge by combining different types of context information using reasoning techniques. In this paper, we present a new approach to derive context information by combining case-based and rule-based reasoning. Two use cases are detailed where both reasoners are used to derive extra useful information. For the desk sharing office use case, the combination of rule-based and case-based reasoning allows to automatically learn typical trajectories of a user and improve localization on such trajects with 42%. In both use cases, the hybrid approach is shown to provide a significant improvement.",2012,Knowledge and Information Systems volume 31 issue 3 pp 581-616,reasoning system;location based service;case based reasoning;system;information;rule based system;knowledge management;data mining;database;artificial intelligence;computer science;
A domain-specific decision support system for knowledge discovery using association and text mining,Dnyanesh Rajpathak (General Motors);Rahul Chougule (General Motors);Pulak Bandyopadhyay (General Motors);,"1243863422,2119302776,2115627377","We propose a novel association and text mining system for knowledge discovery (ASTEK) from the warranty and service data in the automotive domain. The complex architecture of modern vehicles makes fault diagnosis and isolation a non-trivial task. The association mining isolates anomaly cases from the millions of service and claims records. ASTEK has shown 86% accuracy in correctly identifying the anomaly cases. The text mining subscribes to the diagnosis and prognosis (D&P) ontology, which provides the necessary domain-specific knowledge. The root causes associated with the anomaly cases are identified by discovering frequent symptoms associated with the part failures along with the repair actions used to fix the part failures. The best-practice knowledge is disseminated to the dealers involved in the anomaly cases. ASTEK has been implemented as a prototype in the service and quality department of GM and its performance has been validated in the real life set up. On an average, the analysis time is reduced from few weeks to few minutes, which in real life industry are significant improvements.",2012,Knowledge and Information Systems volume 31 issue 3 pp 405-432,knowledge extraction;text mining;data science;data mining;database;computer science;
On processing continuous frequent K - N -match queries for dynamic data over networked data sources,Shih-Chuan Chiu (National Chiao Tung University);Jiun-Long Huang (National Chiao Tung University);Jen-He Huang (National Chiao Tung University);,"2144742676,2148510829,2581742738","Similarity search is one of the critical issues in many applications. When using all attributes of objects to determine their similarity, most prior similarity search algorithms are easily influenced by a few attributes with high dissimilarity. The frequent k-n-match query is proposed to overcome the above problem. However, the prior algorithm to process frequent k-n-match queries is designed for static data, whose attributes are fixed, and is not suitable for dynamic data. Thus, we propose in this paper two schemes to process continuous frequent k-n-match queries over dynamic data. First, the concept of safe region is proposed and four formulae are devised to compute safe regions. Then, scheme CFKNMatchAD-C is developed to speed up the process of continuous frequent k-n-match queries by utilizing safe regions to avoid unnecessary query re-evaluations. To reduce the amount of data transmitted by networked data sources, scheme CFKNMatchAD-C also uses safe regions to eliminate transmissions of unnecessary data updates which will not affect the results of queries. Moreover, for large-scale environments, we further propose scheme CFKNMatchAD-D by extending scheme CFKMatchAD-C to employ multiple servers to process continuous frequent k-n-match queries. Experimental results show that scheme CFKNMatchAD-C and scheme CFKNMatchAD-D outperform the prior algorithm in terms of average response time and the amount of produced network traffic.",2012,Knowledge and Information Systems volume 31 issue 3 pp 547-579,dynamic data;theoretical computer science;data mining;database;machine learning;computer science;
A unified view of the apriori-based algorithms for frequent episode discovery,Avinash Achar (Indian Institute of Science);Srivatsan Laxman (Microsoft);P. S. Sastry (Indian Institute of Science);,"343987496,2044569235,2520038081","Frequent episode discovery framework is a popular framework in temporal data mining with many applications. Over the years, many different notions of frequencies of episodes have been proposed along with different algorithms for episode discovery. In this paper, we present a unified view of all the apriori-based discovery methods for serial episodes under these different notions of frequencies. Specifically, we present a unified view of the various frequency counting algorithms. We propose a generic counting algorithm such that all current algorithms are special cases of it. This unified view allows one to gain insights into different frequencies, and we present quantitative relationships among different frequencies. Our unified view also helps in obtaining correctness proofs for various counting algorithms as we show here. It also aids in understanding and obtaining the anti-monotonicity properties satisfied by the various frequencies, the properties exploited by the candidate generation step of any apriori-based method. We also point out how our unified view of counting helps to consider generalization of the algorithm to count episodes with general partial orders.",2012,Knowledge and Information Systems volume 31 issue 2 pp 223-250,partially ordered set;satisfiability;theoretical computer science;data mining;database;artificial intelligence;machine learning;algorithm;mathematics;
gMLC: a multi-label feature selection framework for graph classification,Xiangnan Kong (University of Illinois at Chicago);Philip S. Yu (University of Illinois at Chicago);,"2204127537,2125104194","Graph classification has been showing critical importance in a wide variety of applications, e.g. drug activity predictions and toxicology analysis. Current research on graph classification focuses on single-label settings. However, in many applications, each graph data can be assigned with a set of multiple labels simultaneously. Extracting good features using multiple labels of the graphs becomes an important step before graph classification. In this paper, we study the problem of multi-label feature selection for graph classification and propose a novel solution, called gMLC, to efficiently search for optimal subgraph features for graph objects with multiple labels. Different from existing feature selection methods in vector spaces that assume the feature set is given, we perform multi-label feature selection for graph data in a progressive way together with the subgraph feature mining process. We derive an evaluation criterion to estimate the dependence between subgraph features and multiple labels of graphs. Then, a branch-and-bound algorithm is proposed to efficiently search for optimal subgraph features by judiciously pruning the subgraph search space using multiple labels. Empirical studies demonstrate that our feature selection approach can effectively boost multi-label graph classification performances and is more efficient by pruning the subgraph search space using multiple labels.",2012,Knowledge and Information Systems volume 31 issue 2 pp 281-305,complement graph;null graph;graph labeling;clique width;subgraph isomorphism problem;graph;feature selection;data mining;pattern recognition;machine learning;computer science;mathematics;
A survey of the state of the art in learning the kernels,M. Ehsan Abbasnejad (Universiti Sains Malaysia);Dhanesh Ramachandram (Universiti Sains Malaysia);Rajeswari Mandava (Universiti Sains Malaysia);,"666785602,2058322499,2136108751","In recent years, the machine learning community has witnessed a tremendous growth in the development of kernel-based learning algorithms. However, the performance of this class of algorithms greatly depends on the choice of the kernel function. Kernel function implicitly represents the inner product between a pair of points of a dataset in a higher dimensional space. This inner product amounts to the similarity between points and provides a solid foundation for nonlinear analysis in kernel-based learning algorithms. The most important challenge in kernel-based learning is the selection of an appropriate kernel for a given dataset. To remedy this problem, algorithms to learn the kernel have recently been proposed. These methods formulate a learning algorithm that finds an optimal kernel for a given dataset. In this paper, we present an overview of these algorithms and provide a comparison of various approaches to find an optimal kernel. Furthermore, a list of pivotal issues that lead to efficient design of such algorithms will be presented.",2012,Knowledge and Information Systems volume 31 issue 2 pp 193-221,kernel embedding of distributions;tree kernel;online machine learning;graph kernel;polynomial kernel;string kernel;radial basis function kernel;kernel method;active learning;semi supervised learning;instance based learning;data mining;pattern recognition;machine learning;computer science;
An unsupervised approach to modeling personalized contexts of mobile users,Tengfei Bao (University of Science and Technology of China);Huanhuan Cao (University of Science and Technology of China);Enhong Chen (University of Science and Technology of China);Jilei Tian (BMW);Hui Xiong (Rutgers–Newark);,"2112459816,2101313578,2136372366,2098156231,2153710278","Mobile context modeling is a process of recognizing and reasoning about contexts and situations in a mobile environment, which is critical for the success of context-aware mobile services. While there are prior works on mobile context modeling, the use of unsupervised learning techniques for mobile context modeling is still under-explored. Indeed, unsupervised techniques have the ability to learn personalized contexts, which are difficult to be predefined. To that end, in this paper, we propose an unsupervised approach to modeling personalized contexts of mobile users. Along this line, we first segment the raw context data sequences of mobile users into context sessions where a context session contains a group of adjacent context records which are mutually similar and usually reflect the similar contexts. Then, we exploit two methods for mining personalized contexts from context sessions. The first method is to cluster context sessions and then to extract the frequent contextual feature-value pairs from context session clusters as contexts. The second method leverages topic models to learn personalized contexts in the form of probabilistic distributions of raw context data from the context sessions. Finally, experimental results on real-world data show that the proposed approach is efficient and effective for mining personalized contexts of mobile users.",2012,Knowledge and Information Systems volume 31 issue 2 pp 345-370,multimedia;world wide web;data mining;computer science;
Effectiveness of NAQ -tree in handling reverse nearest-neighbor queries in high-dimensional metric space,Ming Zhang (University of Calgary);Reda Alhajj (University of Calgary);,"2428428385,497422635","Reverse nearest-neighbor (RNN) query processing is important for many applications such as decision-support systems, profile-based marketing and molecular biology; consequently, RNN query processing has attracted considerable attention in the research community in recent years. Most existing approaches for RNN query processing either rely on nearest-neighbor pre-computation or work for specific data space (e.g., the Euclidean space). The only method for RNN query processing in metric space is based on the M-tree. In this paper, we propose an approach for RNN query processing in high-dimensional metric space using distance-based index structure (in particular, NAQ-tree that outperforms the other distance-based index structures as we have already verified in a previous study). In high-dimensional space, the properties of distance-based index structure provide strong pruning rules than the M-tree. In addition, unlike the previous work, our approach integrates the filtering and verification steps and uses the information obtained in the verification stage to further improve the filtering rate. Our approach delivers results incrementally and hence well serves real-time applications. The reported experimental results demonstrate the applicability and effectiveness of the proposed NAQ-tree-based RNN approach.",2012,Knowledge and Information Systems volume 31 issue 2 pp 307-343,metric space;k nearest neighbors algorithm;data mining;database;pattern recognition;machine learning;computer science;mathematics;
Modeling collective blogging dynamics of popular incidental topics,Li Zhao (Tsinghua University);Xiaohong Guan (Tsinghua University);Ruixi Yuan (Tsinghua University);,"2720733142,2672103850,2157761868","An extended susceptible-infective (SI) epidemic model is presented in this paper to describe the collective blogging behavior on popular incidental topics. Our model has two major extensions over the classic SI model: in the new model, different blog writers get interested in a specific topic with different probabilities, while in a classic SI model, the infection probability of a disease between any two individuals is identical; the new model takes into consideration the impact of external mainstream media on blog writers, while in a classical SI model, spreading of diseases is merely based on personal contacts between individuals. The new model is capable of explaining the widely observed early burst and heavy tail of topic propagation velocity. The proposed model has a closed-form solution when the individual interest is of uniform distribution with the external influence assumed constant. We validate the proposed model using ten topics from two different data sets: Sina Blog and LiveJournal Blogspace, the results indicating that our model fits the topic propagation velocity and predicts the propagation trend very well.",2012,Knowledge and Information Systems volume 31 issue 2 pp 371-387,multimedia;data mining;artificial intelligence;machine learning;simulation;computer science;
An new immune genetic algorithm based on uniform design sampling,Ben-Da Zhou (Anhui University);Hong-Liang Yao (Hefei University of Technology);Ming-Hua Shi (Anhui University);Qin Yue (Anhui University);Hao Wang (Hefei University of Technology);,"2400175195,2663140481,2467264602,2647581645,2689686330","The deficiencies of keeping population diversity, prematurity and low success rate of searching the global optimal solution are the shortcomings of genetic algorithm (GA). Based on the bias of samples in the uniform design sampling (UDS) point set, the crossover operation in GA is redesigned. Using the concentrations of antibodies in artificial immune system (AIS), the chromosomes concentration in GA is defined and the clonal selection strategy is designed. In order to solve the maximum clique problem (MCP), an new immune GA (UIGA) is presented based on the clonal selection strategy and UDS. The simulation results show that the UIGA provides superior solution quality, convergence rate, and other various indices to those of the simple and good point GA when solving MCPs.",2012,Knowledge and Information Systems volume 31 issue 2 pp 389-403,machine learning;mathematical optimization;algorithm;mathematics;
Sleeved co-clustering of lagged data,Eran Shaham (Bar-Ilan University);David Sarne (Bar-Ilan University);Boaz Ben-Moshe (Ariel University);,"2158094253,2143023648,2224423478","The paper focuses on mining clusters that are characterized by a lagged relationship between the data objects. We call such clusters lagged co-clusters. A lagged co-cluster of a matrix is a submatrix determined by a subset of rows and their corresponding lag over a subset of columns. Extracting such subsets may reveal an underlying governing regulatory mechanism. Such a regulatory mechanism is quite common in real-life settings. It appears in a variety of fields: meteorology, seismic activity, stock market behavior, neuronal brain activity, river flow, and navigation, but a limited list of examples. Mining such lagged co-clusters not only helps in understanding the relationship between objects in the domain, but assists in forecasting their future behavior. For most interesting variants of this problem, finding an optimal lagged co-cluster is NP-complete problem. We present a polynomial-time Monte-Carlo algorithm for mining lagged co-clusters. We prove that, with fixed probability, the algorithm mines a lagged co-cluster which encompasses the optimal lagged co-cluster by a maximum 2 ratio columns overhead and completely no rows overhead. Moreover, the algorithm handles noise, anti-correlations, missing values, and overlapping patterns. The algorithm is extensively evaluated using both artificial and real-world test environments. The first enable the evaluation of specific, isolated properties of the algorithm. The latter (river flow and topographic data) enable the evaluation of the algorithm to efficiently mine relevant and coherent lagged co-clusters in environments that are temporal, i.e., time reading data and non-temporal.",2012,Knowledge and Information Systems volume 31 issue 2 pp 251-279,cluster analysis;biclustering;econometrics;data mining;machine learning;computer science;
Early classification on time series,Zhengzheng Xing (Amazon.com);Jian Pei (Simon Fraser University);Philip S. Yu (University of Illinois at Chicago);,"2152975621,2126330539,2125104194","In this paper, we formulate the problem of early classification of time series data, which is important in some time-sensitive applications such as health informatics. We introduce a novel concept of MPL (minimum prediction length) and develop ECTS (early classification on time series), an effective 1-nearest neighbor classification method. ECTS makes early predictions and at the same time retains the accuracy comparable with that of a 1NN classifier using the full-length time series. Our empirical study using benchmark time series data sets shows that ECTS works well on the real data sets where 1NN classification is effective.",2012,Knowledge and Information Systems volume 31 issue 1 pp 105-127,time series;biological classification;instance based learning;data mining;pattern recognition;machine learning;statistics;computer science;
Highly discriminative statistical features for email classification,Juan Carlos Gomez (Katholieke Universiteit Leuven);Erik Boiy (Katholieke Universiteit Leuven);Marie-Francine Moens (Katholieke Universiteit Leuven);,"2147203960,781537510,1931663571","This paper reports on email classification and filtering, more specifically on spam versus ham and phishing versus spam classification, based on content features. We test the validity of several novel statistical feature extraction methods. The methods rely on dimensionality reduction in order to retain the most informative and discriminative features. We successfully test our methods under two schemas. The first one is a classic classification scenario using a 10-fold cross-validation technique for several corpora, including four ground truth standard corpora: Ling-Spam, SpamAssassin, PU1, and a subset of the TREC 2007 spam corpus, and one proprietary corpus. In the second schema, we test the anticipatory properties of our extracted features and classification models with two proprietary datasets, formed by phishing and spam emails sorted by date, and with the public TREC 2007 spam corpus. The contributions of our work are an exhaustive comparison of several feature selection and extraction methods in the frame of email classification on different benchmarking corpora, and the evidence that especially the technique of biased discriminant analysis offers better discriminative features for the classification, gives stable classification results notwithstanding the amount of features chosen, and robustly retains their discriminative value over time and data setups. These findings are especially useful in a commercial setting, where short profile rules are built based on a limited number of features for filtering emails.",2012,Knowledge and Information Systems volume 31 issue 1 pp 23-53,dimensionality reduction;feature extraction;feature selection;information retrieval;data mining;pattern recognition;machine learning;computer science;
Enterprise knowledge management model: a knowledge tower,Ercan Oztemel (Marmara University);Seher Arslankaya (Sakarya University);,"2403907117,2235399365","One of the most important aspects of knowledge management (KM) is to create a system that is capable of providing mechanisms and methodologies allowing the right knowledge to be at the right place and at the right person as well as at the right time within an enterprise. There have been several models developed for this purpose. The main objectives of these models are to organize the knowledge activities to increase competitive advantage and turn the market share into a continuous and permanent superiority through utilizing the intellectual capital of the enterprise for better products and services. When existing models are carefully analyzed, it can be clearly seen that every model addresses different aspects of KM. While some of these models have been intensifying on the use of technology and production of knowledge, the others rather focus on the utilization of knowledge itself. Although these models point out the importance of managerial participation, they are mainly designed to be facilitated at operational levels. It is now obviously seen that there is a need for a new model that will deal with KM at strategic, tactic, and operational levels in an integrated manner. In this study, an enterprise knowledge management model (EKMM) is developed to facilitate this. The model is also called the “Knowledge Tower” due to its dynamics and tower-like infrastructure. EKMM does not only deal with utilizing the knowledge but also create KM strategies, knowledge culture as well as respective leveraging activities. It provides an extensive assessment capability to make sure that the KM practices are carried out as effectively as possible. This surely helps identifying the lack of implementations and areas requiring improvements.",2012,Knowledge and Information Systems volume 31 issue 1 pp 171-192,mathematical knowledge management;domain knowledge;knowledge;knowledge value chain;personal knowledge management;organizational learning;knowledge based systems;management science;knowledge management;data mining;database;artificial intelligence;computer science;
Scalable clustering methods for the name disambiguation problem,Byung-Won On (Pennsylvania State University);Ingyu Lee (Troy University);Dongwon Lee (Pennsylvania State University);,"2137831395,2501926320,2141172858","When non-unique values are used as the identifier of entities, due to their homonym, confusion can occur. In particular, when (part of) “names” of entities are used as their identifier, the problem is often referred to as a name disambiguation problem, where goal is to sort out the erroneous entities due to name homonyms (e.g., If only last name is used as the identifier, one cannot distinguish “Masao Obama” from “Norio Obama”). In this paper, in particular, we study the scalability issue of the name disambiguation problem—when (1) a small number of entities with large contents or (2) a large number of entities get un-distinguishable due to homonyms. First, we carefully examine two of the state-of-the-art solutions to the name disambiguation problem and point out their limitations with respect to scalability. Then, we propose two scalable graph partitioning algorithms known as multi-level graph partitioning and multi-level graph partitioning and merging to solve the large-scale name disambiguation problem. Our claim is empirically validated via experimentation—our proposal shows orders of magnitude improvement in terms of performance while maintaining equivalent or reasonable accuracy compared to competing solutions.",2012,Knowledge and Information Systems volume 31 issue 1 pp 129-151,identifier;graph partition;discrimination;scalability;cluster;graph theory;theoretical computer science;data mining;database;artificial intelligence;machine learning;programming language;algorithm;computer science;mathematics;
Clustering spatial data with a geographic constraint: exploring local search,Zhung-Xun Liao (National Chiao Tung University);Wen-Chih Peng (National Chiao Tung University);,"2138845760,2100961564","Spatial data objects that possess attributes in the optimization domain and the geographic domain are now widely available. For example, sensor data are one kind of spatial data objects. The location of a sensor is an attribute in the geographic domain, while its reading is an attribute in the optimization domain. Previous studies discuss dual clustering problems that attempt to partition spatial data objects into several groups, such that objects in the same group have similar values in their optimization attributes and form a compact region in the geographic domain. However, previous studies do not clearly define compact regions. Therefore, this paper formulates a connective dual clustering problem with an explicit connected constraint given. Objects with a geographic distance smaller than or equal to the connected constraint are connected. The goal of the connective dual clustering problem is to derive clusters that contain objects with similar values in the optimization domain and are connected in the geographic domain. This study further proposes an algorithm CLS (Clustering with Local Search) to efficiently derive clusters. This algorithm consists of two phases: the ConGraph (standing for Connective Graph) transformation phase and the clustering phase. In the ConGraph transformation phase, CLS first transforms the data objects into a ConGraph that captures geographic constraints among data objects and selects initial seeds for clustering. Then, the initial seeds selected nearby data objects and formed coarse clusters by exploring local search in the clustering phase. Moreover, coarse clusters are merged and finely turned. Experiments show that CLS algorithm is more efficient and scalable than existing methods.",2012,Knowledge and Information Systems volume 31 issue 1 pp 153-170,k medians clustering;flame clustering;canopy clustering algorithm;complete linkage clustering;correlation clustering;constrained clustering;data stream clustering;cure data clustering algorithm;single linkage clustering;fuzzy clustering;clustering high dimensional data;cluster analysis;consensus clustering;data mining;machine learning;mathematical optimization;mathematics;
Distribution-free bounds for relational classification,Amit Dhurandhar (IBM);Alin Dobra (University of Florida);,"2061123877,2074162063","Statistical relational learning (SRL) is a subarea in machine learning which addresses the problem of performing statistical inference on data that is correlated and not independently and identically distributed (i.i.d.)—as is generally assumed. For the traditional i.i.d. setting, distribution-free bounds exist, such as the Hoeffding bound, which are used to provide confidence bounds on the generalization error of a classification algorithm given its hold-out error on a sample size of N. Bounds of this form are currently not present for the type of interactions that are considered in the data by relational classification algorithms. In this paper, we extend the Hoeffding bounds to the relational setting. In particular, we derive distribution-free bounds for certain classes of data generation models that do not produce i.i.d. data and are based on the type of interactions that are considered by relational classification algorithms that have been developed in SRL. We conduct empirical studies on synthetic and real data which show that these data generation models are indeed realistic and the derived bounds are tight enough for practical use.",2012,Knowledge and Information Systems volume 31 issue 1 pp 55-78,biological classification;statistical relational learning;data mining;database;pattern recognition;machine learning;statistics;computer science;mathematics;
Conscience online learning: an efficient approach for robust kernel-based clustering,Chang-Dong Wang (Sun Yat-sen University);Jian-Huang Lai (Sun Yat-sen University);Jun-Yong Zhu (Sun Yat-sen University);,"2696508211,2168989598,2646853119","Kernel-based clustering is one of the most popular methods for partitioning nonlinearly separable datasets. However, exhaustive search for the global optimum is NP-hard. Iterative procedure such as k-means can be used to seek one of the local minima. Unfortunately, it is easily trapped into degenerate local minima when the prototypes of clusters are ill-initialized. In this paper, we restate the optimization problem of kernel-based clustering in an online learning framework, whereby a conscience mechanism is easily integrated to tackle the ill-initialization problem and faster convergence rate is achieved. Thus, we propose a novel approach termed conscience online learning (COLL). For each randomly taken data point, our method selects the winning prototype based on the conscience mechanism to bias the ill-initialized prototype to avoid degenerate local minima and efficiently updates the winner by the online learning rule. Therefore, it can more efficiently obtain smaller distortion error than k-means with the same initialization. The rationale of the proposed COLL method is experimentally analyzed. Then, we apply the COLL method to the applications of digit clustering and video clustering. The experimental results demonstrate the significant improvement over existing kernel-based clustering methods.",2012,Knowledge and Information Systems volume 31 issue 1 pp 79-104,correlation clustering;fuzzy clustering;k means clustering;cluster analysis;data mining;artificial intelligence;machine learning;algorithm;computer science;mathematics;
Two-layered Blogger identification model integrating profile and instance-based methods,Haytham Mohtasseb (University of Lincoln);Amr Ahmed (University of Lincoln);,"1520018819,2183690951","This paper introduces a two-layered framework that improves the result of authorship identification within larger sample numbers of bloggers as compared with earlier work. Previous studies are mainly divided into two categories: profile-based and instance-based methods. Each of these approaches has its advantages and limitations. The two-layered framework presented here integrates the two previous approaches and presents a new solution to a key problem in authorship identification, namely the drop in accuracy experienced as the number of authors increases. The paper begins by illustrating the regular instance-based core model and the investigated features. It then introduces a new psycholinguistic profile representation of authors, presents similarity grouping extraction over profiles, and applies blogger identification utilizing the two-layered approach. The results confirm the improvement introduced by the proposed two-layered approach against our regular classifier, as well as a selected baseline, for an extended number of users.",2012,Knowledge and Information Systems volume 31 issue 1 pp 1-21,data science;world wide web;data mining;database;artificial intelligence;machine learning;computer science;
Efficient discovery of influential nodes for SIS models in social networks,Kazumi Saito (University of Shizuoka);Masahiro Kimura (Ryukoku University);Kouzou Ohara (Aoyama Gakuin University);Hiroshi Motoda (Osaka University);,"2171394160,1989850375,2108019927,323689644","We address the problem of discovering the influential nodes in a social network under the susceptible/infected/susceptible model that allows multiple activation of the same node, by defining two influence maximization problems: final-time and integral-time. We solve this problem by constructing a layered graph from the original network with each layer added on top as the time proceeds and applying the bond percolation with two effective control strategies: pruning and burnout. We experimentally demonstrate that the proposed method gives much better solutions than the conventional methods that are based solely on the notion of centrality using two real-world networks. The pruning is most effective when searching for a single influential node, but burnout is more powerful in searching for multiple nodes which together are influential. We further show that the computational complexity is much smaller than the naive probabilistic simulation both by theory and experiment. The influential nodes discovered are substantially different from those identified by the centrality measures. We further note that the solutions of the two optimization problems are also substantially different, indicating the importance of distinguishing these two problem characteristics and using the right objective function that best suits the task in hand.",2012,Knowledge and Information Systems volume 30 issue 3 pp 613-635,artificial intelligence;machine learning;mathematical optimization;mathematics;
Improving pattern quality in web usage mining by using semantic information,Pinar Senkul (Middle East Technical University);Suleyman Salin (Middle East Technical University);,"1996282972,2227823204","Frequent Web navigation patterns generated by using Web usage mining techniques provide valuable information for several applications such as Web site restructuring and recommendation. In conventional Web usage mining, semantic information of the Web page content does not take part in the pattern generation process. In this work, we investigate the effect of semantic information on the patterns generated for Web usage mining in the form of frequent sequences. To this aim, we developed a technique and a framework for integrating semantic information into Web navigation pattern generation process, where frequent navigational patterns are composed of ontology instances instead of Web page addresses. The quality of the generated patterns is measured through an evaluation mechanism involving Web page recommendation. Experimental results show that more accurate recommendations can be obtained by including semantic information in navigation pattern generation, which indicates the increase in pattern quality.",2012,Knowledge and Information Systems volume 30 issue 3 pp 527-541,website parse template;semantic web stack;web modeling;social semantic web;web analytics;data web;web standards;web mapping;mashup;owl s;web design;web navigation;semantic web;web service;web intelligence;web mining;ontology;semantics;world wide web;information retrieval;data mining;computer science;
Effective semi-supervised document clustering via active learning with instance-level constraints,Weizhong Zhao (Chinese Academy of Sciences);Qing He (Chinese Academy of Sciences);Huifang Ma (Chinese Academy of Sciences);Zhongzhi Shi (Chinese Academy of Sciences);,"2106927367,2167314737,2114729544,2112067332","Semi-supervised document clustering, which takes into account limited supervised data to group unlabeled documents into clusters, has received significant interest recently. Because of getting supervised data may be expensive, it is important to get most informative knowledge to improve the clustering performance. This paper presents a semi-supervised document clustering algorithm and a new method for actively selecting informative instance-level constraints to get improved clustering performance. The semi- supervised document clustering algorithm is a Constrained DBSCAN (Cons-DBSCAN) algorithm, which incorporates instance-level constraints to guide the clustering process in DBSCAN. An active learning approach is proposed to select informative document pairs for obtaining user feedbacks. Experimental results show that Cons-DBSCAN with our proposed active learning approach can improve the clustering performance significantly when given a relatively small amount of constraints.",2012,Knowledge and Information Systems volume 30 issue 3 pp 569-587,flame clustering;subclu;brown clustering;canopy clustering algorithm;optics algorithm;determining the number of clusters in a data set;dbscan;correlation clustering;constrained clustering;data stream clustering;cure data clustering algorithm;affinity propagation;fuzzy clustering;active learning;clustering high dimensional data;cluster analysis;consensus clustering;conceptual clustering;document clustering;data mining;pattern recognition;machine learning;computer science;
How many performance measures to evaluate information retrieval systems,Alain Baccini (University of Toulouse);Sébastien Déjean (University of Toulouse);Laetitia Lafage (Institut Universitaire de Formation des Maîtres);Josiane Mothe (Institut Universitaire de Formation des Maîtres);,"2467591073,2246441375,2275465877,2141333472","Evaluating effectiveness of information retrieval systems is achieved by performing on a collection of documents, a search, in which a set of test queries are performed and, for each query, the list of the relevant documents. This evaluation framework also includes performance measures making it possible to control the impact of a modification of search parameters. The program trec_eval calculates a large number of measures, some being more used like the mean average precision or recall-precision curves. The motivation of our work is to compare all measures and to help the user to choose a small number of them when evaluating different information retrieval systems. In this paper, we present the study we carried out from a massive data analysis of TREC results. Relationships between the 130 measures calculated by trec_eval for individual queries are investigated, and we show that they can be clustered into homogeneous clusters.",2012,Knowledge and Information Systems volume 30 issue 3 pp 693-713,relevance;evaluation;information retrieval;data mining;database;computer science;
On exploiting static and dynamically mined metadata for exploratory web searching,Panagiotis Papadakos (Foundation for Research & Technology – Hellas);Nikos Armenatzoglou (Foundation for Research & Technology – Hellas);Stella Kopidaki (Foundation for Research & Technology – Hellas);Yannis Tzitzikas (Foundation for Research & Technology – Hellas);,"2149125156,252418274,1988552210,64415330","Most Web Search Engines (WSEs) are appropriate for focalized search, i.e., they make the assumption that users can accurately describe their information need using a small sequence of terms. However, as several user studies have shown, a high percentage of search tasks are exploratory, and focalized search very commonly leads to inadequate interactions and poor results. This paper proposes exploiting static and dynamically mined metadata for enriching web searching with exploration services. Online results clustering, which is a mining task of dynamic nature since it is based on query-dependent snippets, is useful for providing users with overviews of the top results and thus allowing them to restrict their focus to the desired parts. On the other hand, the various static metadata that are available to a search engine (e.g., domain, language, date, and filetype) are commonly exploited only through the advanced (form-based) search facilities that some WSEs offer (and users rarely use). We propose an approach that combines both kinds of metadata by adopting the interaction paradigm of dynamic taxonomies and faceted exploration, which allows the users to restrict their focus gradually using both static and dynamically derived metadata. Special focus is given on the design and analysis of incremental algorithms for speeding up the exploration process. The experimental evaluation over a real WSE shows that this combination results to an effective, flexible, and efficient exploration experience. Finally, we report the results of a user study indicating that this direction is promising in terms of user preference, satisfaction, and effort.",2012,Knowledge and Information Systems volume 30 issue 3 pp 493-525,world wide web;information retrieval;data mining;database;computer science;
Multi-objective frequent termset clustering,Katharina Morik (Technical University of Dortmund);Andreas Kaspari;Michael Wurst (IBM);Marcin Skirzynski;,"2070565061,2068846888,2156342499,2517186319","Large media collections rapidly evolve in the World Wide Web. In addition to the targeted retrieval as is performed by search engines, browsing and explorative navigation is an important issue. Since the collections grow fast and authors most often do not annotate their web pages according to a given ontology, automatic structuring is in demand as a prerequisite for any pleasant human–computer interface. In this paper, we investigate the problem of finding alternative high-quality structures for navigation in a large collection of high-dimensional data. We express desired properties of frequent termset clustering (FTS) in terms of objective functions. In general, these functions are conflicting. This leads to the formulation of FTS clustering as a multi-objective optimization problem. The optimization is solved by a genetic algorithm. The result is a set of Pareto-optimal solutions. Users may choose their favorite type of a structure for their navigation through a collection or explore the different views given by the different optimal solutions. We explore the capability of the new approach to produce structures that are well suited for browsing on a social bookmarking data set.",2012,Knowledge and Information Systems volume 30 issue 3 pp 715-738,cluster analysis;world wide web;data mining;database;artificial intelligence;machine learning;computer science;
ELF-Miner: using structural knowledge and data mining methods to detect new (Linux) malicious executables,Farrukh Shahzad (National University of Computer and Emerging Sciences);Muddassar Farooq (National University of Computer and Emerging Sciences);,"2119133870,2120593778","Linux malware can pose a significant threat—its (Linux) penetration is exponentially increasing—because little is known or understood about Linux OS vulnerabilities. We believe that now is the right time to devise non-signature based zero-day (previously unknown) malware detection strategies before Linux intruders take us by surprise. Therefore, in this paper, we first do a forensic analysis of Linux executable and linkable format (ELF) files. Our forensic analysis provides insight into different features that have the potential to discriminate malicious executables from benign ones. As a result, we can select a features’ set of 383 features that are extracted from an ELF headers. We quantify the classification potential of features using information gain and then remove redundant features by employing preprocessing filters. Finally, we do an extensive evaluation among classical rule-based machine learning classifiers—RIPPER, PART, C4.5 Rules, and decision tree J48—and bio-inspired classifiers—cAnt Miner, UCS, XCS, and GAssist—to select the best classifier for our system. We have evaluated our approach on an available collection of 709 Linux malware samples from vx heavens and offensive computing. Our experiments show that ELF-Miner provides more than 99% detection accuracy with less than 0.1% false alarm rate.",2012,Knowledge and Information Systems volume 30 issue 3 pp 589-612,constant false alarm rate;kullback leibler divergence;decision tree;cryptovirology;information security;internet privacy;world wide web;computer security;data mining;database;machine learning;computer science;
Boosted ranking models: a unifying framework for ranking predictions,Kevin Dela Rosa (Carnegie Mellon University);Vangelis Metsis (University of Texas at Arlington);Vassilis Athitsos (University of Texas at Arlington);,"2150117157,234221796,139967712","Ranking is an important functionality in a diverse array of applications, including web search, similarity-based multimedia retrieval, nearest neighbor classification, and recommendation systems. In this paper, we propose a new method, called Boosted Ranking Model (BRM), for learning how to rank from training data. An important feature of the proposed method is that it is domain-independent and can thus be applied to a wide range of ranking domains. The main contribution of the new method is that it reduces the problem of learning how to rank to the much more simple, and well-studied problem of constructing an optimized binary classifier from simple, weak classifiers. Using that reduction, our method constructs an optimized ranking model using multiple simple, easy-to-define ranking models as building blocks. The new method is a unifying framework that includes, as special cases, specific methods that we have proposed in earlier publications for specific ranking applications, such as nearest neighbor retrieval and classification. In this paper, we reformulate those earlier methods as special cases of the proposed BRM method, and we also illustrate a novel application of BRM, on the problem of making movie recommendations to individual users.",2012,Knowledge and Information Systems volume 30 issue 3 pp 543-568,ranking;ranking svm;boosting;learning to rank;recommender system;data mining;pattern recognition;machine learning;computer science;
Mining historical manuscripts with local color patches,"Qiang Zhu 0002 (University of California, Riverside);Eamonn J. Keogh (University of California, Riverside);","2312006013,2170070822","Initiatives such as the Google Print Library Project and the Million Book Project have already archived more than twelve million books in digital format, and within the next decade, the majority of world’s books will be online. Although most of the data will naturally be text, there will also be tens of millions of pages of images, many in color. While there is an active research community pursuing data mining of text from historical manuscripts, there has been very little work that exploits the rich color information which is often present. In this work, we introduce a simple color measure which both addresses and exploits typical features of historical manuscripts. To enable the efficient mining of massive archives, we propose a tight lower bound to the measure. Beyond the fast similarity search, we show how this lower bound allows us to build several higher-level data mining tools, including motif discovery and link analyses. We demonstrate our ideas in several data mining tasks on manuscripts dating back to the fifteenth century.",2012,Knowledge and Information Systems volume 30 issue 3 pp 637-665,data science;world wide web;data mining;computer science;
A spatial relation-based framework to perform visual information extraction,Giuseppe Della Penna (University of L'Aquila);Daniele Magazzeni (University of Chieti-Pescara);Sergio Orefice (University of L'Aquila);,"2138724275,145207510,1989025370","The Spatial Relation Query (SRQ) tool is a graphical software environment, supported by a SQL-like language, which enables users to perform information extraction driven by the visual appearance and the spatial arrangement of the information. The tool has been initially customised to work on specific application domains, like web pages and geospatial data. In this paper, we present the theoretical formalisation of the visual information extraction (VIE) task and accordingly the redesign of the SRQ tool, which is now a full-featured, general-purpose information extraction system. Moreover, we show a new application of the VIE framework to the analysis and visual information extraction from PDF files.",2012,Knowledge and Information Systems volume 30 issue 3 pp 667-692,spatial relation;relationship extraction;information extraction;information retrieval;computer vision;data mining;database;computer science;
Stream mining: a novel architecture for ensemble-based classification,Valerio Grossi (University of Pisa);Franco Turini (University of Pisa);,"2144534063,2241902680","Mining data streams has become an important and challenging task for a wide range of applications. In these scenarios, data tend to arrive in multiple, rapid and time-varying streams, thus constraining data mining algorithms to look at data only once. Maintaining an accurate model, e.g. a classifier, while the stream goes by requires a smart way of keeping track of the data already passed away. Such a synthetic structure has to serve two purposes: distilling the most of information out of past data and allowing a fast reaction to concept drifting, i.e. to the change of the data trend that necessarily affects the model. The paper outlines novel data structures and algorithms to tackle the above problem, when the model mined out of the data is a classifier. The introduced model and the overall ensemble architecture are presented in details, even considering how the approach can be extended for treating numerical attributes. A large part of the paper discusses the experiments and the comparisons with several existing systems. The comparisons show that the performance of our system in general, and in particular with respect to the reaction to concept drifting, is at the top level.",2012,Knowledge and Information Systems volume 30 issue 2 pp 247-281,data aggregator;biological classification;data stream mining;data science;data mining;database;artificial intelligence;machine learning;computer science;
A new vertical fragmentation algorithm based on ant collective behavior in distributed database systems,Mehdi Goli (Shahid Beheshti University);Seyed Mohammad Taghi Rouhani Rankoohi (Shahid Beheshti University);,"2134538047,349883525","Considering the existing massive volumes of data processed nowadays and the distributed nature of many organizations, there is no doubt how vital the need is for distributed database systems. In such systems, the response time to a transaction or a query is highly affected by the distribution design of the database system, particularly its methods for fragmentation, replication, and allocation data. According to the relevant literature, from the two approaches to fragmentation, namely horizontal and vertical fragmentation, the latter requires the use of heuristic methods due to it being NP-Hard. Currently, there are a number of different methods of providing vertical fragmentation, which normally introduce a relatively high computational complexity or do not yield optimal results, particularly for large-scale problems. In this paper, because of their distributed and scalable nature, we apply swarm intelligence algorithms to present an algorithm for finding a solution to vertical fragmentation problem, which is optimal in most cases. In our proposed algorithm, the relations are tried to be fragmented in such a way so as not only to make transaction processing at each site as much localized as possible, but also to reduce the costs of operations. Moreover, we report on the experimental results of comparing our algorithm with several other similar algorithms to show that ours outperforms the other algorithms and is able to generate a better solution in terms of the optimality of results and computational complexity.",2012,Knowledge and Information Systems volume 30 issue 2 pp 435-455,fragmentation;distributed database;distributed computing;data mining;database;computer science;
BicFinder: a biclustering algorithm for microarray data analysis,Wassim Ayadi (Tunis University);Mourad Elloumi (Tunis University);Jin-Kao Hao (University of Angers);,"2170533752,988120282,2110538291","In the context of microarray data analysis, biclustering allows the simultaneous identification of a maximum group of genes that show highly correlated expression patterns through a maximum group of experimental conditions (samples). This paper introduces a heuristic algorithm called BicFinder (The BicFinder software is available at: http://www.info.univ-angers.fr/pub/hao/BicFinder.html) for extracting biclusters from microarray data. BicFinder relies on a new evaluation function called Average Correspondence Similarity Index (ACSI) to assess the coherence of a given bicluster and utilizes a directed acyclic graph to construct its biclusters. The performance of BicFinder is evaluated on synthetic and three DNA microarray datasets. We test the biological significance using a gene annotation web-tool to show that our proposed algorithm is able to produce biologically relevant biclusters. Experimental results show that BicFinder is able to identify coherent and overlapping biclusters.",2012,Knowledge and Information Systems volume 30 issue 2 pp 341-358,gene chip analysis;evaluation function;heuristics;biclustering;data science;bioinformatics;data mining;artificial intelligence;computer science;
Knowledge management in image-based analysis of blood vessel structures,Iván Macía (Grupo México);Manuel Graña (University of the Basque Country);Celine Paloc (University of London);,"2573021243,2171936949,1968934753","We have detected the lack of a widely accepted knowledge representation model in the area of Blood Vessel analysis. We find that such a tool is needed for the future development of the field and our own research efforts. It will allow easy reuse of software pieces through appropriate abstractions, facilitating the development of innovative methods, procedures and applications. We include a thorough review of vascular morphology image analysis. After the identification of the key representation elements and operations, we propose a Vessel Knowledge Representation (VKR) model that would fill this gap. We give insights into its implementation based on standard Object-Oriented Programming tools and paradigms. The VKR would easily integrate with existing medical imaging and visualization software platforms, such as the Insight ToolKit (ITK) and Visualization Toolkit (VTK).",2012,Knowledge and Information Systems volume 30 issue 2 pp 457-491,knowledge representation and reasoning;computer vision;data mining;database;artificial intelligence;computer science;
Privacy-preserving hybrid collaborative filtering on cross distributed data,Ibrahim Yakut (Anadolu University);Huseyin Polat (Anadolu University);,"2050396335,2111227318","Data collected for collaborative filtering (CF) purposes might be cross distributed between two online vendors, even competing companies. Such corporations might want to integrate their data to provide more precise and reliable recommendations. However, due to privacy, legal, and financial concerns, they do not desire to disclose their private data to each other. If privacy-preserving measures are introduced, they might decide to generate predictions based on their distributed data collaboratively. In this study, we investigate how to offer hybrid CF-based referrals with decent accuracy on cross distributed data (CDD) between two e-commerce sites while maintaining their privacy. Our proposed schemes should prevent data holders from learning true ratings and rated items held by each other while still allowing them to provide accurate CF services efficiently. We perform real data-based experiments to evaluate our proposals in terms of accuracy. The results show that the proposed methods are able to provide precise predictions. Moreover, we analyze our schemes in terms of privacy and supplementary costs. We demonstrate that our schemes are secure, and online overhead costs due to privacy concerns are insignificant.",2012,Knowledge and Information Systems volume 30 issue 2 pp 405-433,accuracy and precision;privacy;performance;world wide web;computer security;data mining;database;computer science;
What is the difference? A cognitive dissimilarity measure for information retrieval result sets,Carsten Keßler (University of Münster);,2260767591,"Result rankings from context-aware information retrieval are inherently dynamic, as the same query can lead to significantly different outcomes in different contexts. For example, the search term Digital Camera will lead to different—albeit potentially overlapping—results in the contexts customer reviews and shops, respectively. The comparison of such result rankings can provide useful insights into the effects of context changes on the information retrieval results. In particular, the impact of single aspects of the context in complex applications can be analyzed to identify the most (and least) influential context parameters. While a multitude of methods exists for assessing the relevance of a result ranking with respect to a given query, the question how different two result rankings are from a user’s point of view has not been tackled so far. This paper introduces DIR, a cognitively plausible dissimilarity measure for information retrieval result sets that is based solely on the results and thus applicable independently of the retrieval method. Unlike statistical correlation measures, this dissimilarity measure reflects how human users quantify the changes in information retrieval result rankings. The DIR measure supports cognitive engineering tasks for information retrieval, such as work flow and interface design: using the measure, developers can identify which aspects of context heavily influence the outcome of the retrieval task and should therefore be in the focus of the user’s interaction with the system. The cognitive plausibility of DIR has been evaluated in two human participants tests, which demonstrate a strong correlation with user judgments.",2012,Knowledge and Information Systems volume 30 issue 2 pp 319-340,cognitive models of information retrieval;term discrimination;human computer information retrieval;relevance;information retrieval;data mining;database;artificial intelligence;machine learning;computer science;
Aggregate keyword search on large relational databases,Bin Zhou (Simon Fraser University);Jian Pei (Simon Fraser University);,"2714597203,2126330539","Keyword search has been recently extended to relational databases to retrieve information from text-rich attributes. However, all the existing methods focus on finding individual tuples matching a set of query keywords from one table or the join of multiple tables. In this paper, we motivate a novel problem of aggregate keyword search: finding minimal group-bys covering a set of query keywords well, which is useful in many applications. We develop two interesting approaches to tackle the problem. We further extend our methods to allow partial matches and matches using a keyword ontology. An extensive empirical evaluation using both real data sets and synthetic data sets is reported to verify the effectiveness of aggregate keyword search and the efficiency of our methods.",2012,Knowledge and Information Systems volume 30 issue 2 pp 283-318,data cube;relational database;information retrieval;data mining;database;computer science;
Extraction of independent discriminant features for data with asymmetric distribution,Chandra Shekhar Dhir (KAIST);Jaehyung Lee (KAIST);Soo-Young Lee (KAIST);,"1003661316,2294525842,2168986862","Standard unsupervised linear feature extraction methods find orthonormal (PCA) or statistically independent (ICA) latent variables that are good for data representation. These representative features may not be optimal for the classification tasks, thus requiring a search of linear projections that can give a good discriminative model. A semi-supervised linear feature extraction method, namely dICA, had recently been proposed which jointly maximizes the Fisher linear discriminant (FLD) and negentropy of the extracted features [Dhir and Lee in Discriminant independent component analysis. In: Proceedings of the international conference intelligent data engineering and automated learning, LNCS 5788:219–225 (Full paper is submitted to IEEE Trans. NN) 2009]. Motivated by the independence and unit covariance of the extracted dICA features, maximizing the determinant of between-class scatter of the features matrix is theoretically the same as the maximization of FLD. This also reduces the computational complexity of the algorithm. In this paper, we concentrate on text databases that follow inherent exponential distribution. Approximation and the maximization of negentropy for data with asymmetric distribution is discussed. Experiments on the text categorization problem show improvements in classification performance and data reconstruction.",2012,Knowledge and Information Systems volume 30 issue 2 pp 359-375,feature extraction;linear discriminant analysis;data mining;pattern recognition;machine learning;statistics;computer science;mathematics;
New possibilistic method for discovering linear local behavior using hyper-Gaussian distributed membership function,Fátima Barceló-Rico (Polytechnic University of Valencia);José-Luis Díez (Polytechnic University of Valencia);Jorge Bondia (Polytechnic University of Valencia);,"2190378309,2193757282,2014349191","This paper presents a method to find a model of a system based on the integration of a set of local models. Mainly, properties are sought for the local models: independence of clusters and interpretability of their validity. This has been achieved through the introduction of a possibilistic clustering for the first property and a pre-fixed shape of the membership functions for the second one. A new cost index for the clustering optimization problem has been defined consisting of two terms: one for global error and another for local errors. By giving higher importance to the local errors term, local models valid regionally can be found. To avoid local optima and numerical issues, the parameters of the models are found using global optimization. This new method has been applied to several data sets, and results show how the desired characteristics can be achieved in the resulting models.",2012,Knowledge and Information Systems volume 30 issue 2 pp 377-403,gaussian;cluster analysis;global optimization;data mining;machine learning;mathematical optimization;statistics;computer science;mathematics;
Algorithms for mining the evolution of conserved relational states in dynamic networks,Rezwan Ahmed (University of Minnesota);George Karypis (University of Minnesota);,"2140255430,219814910",Dynamic networks have recently being recognized as a powerful abstraction to model and represent the temporal changes and dynamic aspects of the data underlying many complex systems. Significant insights regarding the stable relational patterns among the entities can be gained by analyzing temporal evolution of the complex entity relations. This can help identify the transitions from one conserved state to the next and may provide evidence to the existence of external factors that are responsible for changing the stable relational patterns in these networks. This paper presents a new data mining method that analyzes the time-persistent relations or states between the entities of the dynamic networks and captures all maximal non-redundant evolution paths of the stable relational states. Experimental results based on multiple datasets from real-world applications show that the method is efficient and scalable.,2012,Knowledge and Information Systems volume 33 issue 3 pp 603-630,evolution;data mining;artificial intelligence;machine learning;computer science;
Analyzing collective behavior from blogs using swarm intelligence,"Soumya Banerjee (Birla Institute of Technology, Mesra);Nitin Agarwal (University of Arkansas at Little Rock);","2125276951,2153957209","With the rapid growth of the availability and popularity of interpersonal and behavior-rich resources such as blogs and other social media avenues, emerging opportunities and challenges arise as people now can, and do, actively use computational intelligence to seek out and understand the opinions of others. The study of collective behavior of individuals has implications to business intelligence, predictive analytics, customer relationship management, and examining online collective action as manifested by various flash mobs, the Arab Spring (2011) and other such events. In this article, we introduce a nature-inspired theory to model collective behavior from the observed data on blogs using swarm intelligence, where the goal is to accurately model and predict the future behavior of a large population after observing their interactions during a training phase. Specifically, an ant colony optimization model is trained with behavioral trend from the blog data and is tested over real-world blogs. Promising results were obtained in trend prediction using ant colony based pheromone classier and CHI statistical measure. We provide empirical guidelines for selecting suitable parameters for the model, conclude with interesting observations, and envision future research directions.",2012,Knowledge and Information Systems volume 33 issue 3 pp 523-547,collective behavior;ant colony;social network;sentiment analysis;supervised learning;swarm intelligence;data science;data mining;artificial intelligence;machine learning;simulation;computer science;
Threshold conditions for arbitrary cascade models on arbitrary networks,"B. Aditya Prakash (Carnegie Mellon University);Deepayan Chakrabarti (Yahoo!);Nicholas C. Valler (University of California, Berkeley);Michalis Faloutsos (University of California, Berkeley);Christos Faloutsos (Carnegie Mellon University);","2124002246,2189524569,1975627327,166771321,2198983026","Given a network of who-contacts-whom or who-links-to-whom, will a contagious virus/product/meme spread and ‘take over’ (cause an epidemic) or die out quickly? What will change if nodes have partial, temporary or permanent immunity? The epidemic threshold is the minimum level of virulence to prevent a viral contagion from dying out quickly and determining it is a fundamental question in epidemiology and related areas. Most earlier work focuses either on special types of graphs or on specific epidemiological/cascade models. We are the first to show the G2-threshold (twice generalized) theorem, which nicely de-couples the effect of the topology and the virus model. Our result unifies and includes as special case older results and shows that the threshold depends on the first eigenvalue of the connectivity matrix, (a) for any graph and (b) for all propagation models in standard literature (more than 25, including H.I.V.). Our discovery has broad implications for the vulnerability of real, complex networks and numerous applications, including viral marketing, blog dynamics, influence propagation, easy answers to ‘what-if’ questions, and simplified design and evaluation of immunization policies. We also demonstrate our result using extensive simulations on real networks, including on one of the biggest available social-contact graphs containing more than 31 million interactions among more than 1 million people representing the city of Portland, Oregon, USA.",2012,Knowledge and Information Systems volume 33 issue 3 pp 549-575,data mining;artificial intelligence;machine learning;simulation;statistics;computer science;mathematics;
Novel approaches to crawling important pages early,Md. Hijbul Alam (Korea University);JongWoo Ha (Korea University);SangKeun Lee (Korea University);,"2321410875,2143951465,2130279651","Web crawlers are essential to many Web applications, such as Web search engines, Web archives, and Web directories, which maintain Web pages in their local repositories. In this paper, we study the problem of crawl scheduling that biases crawl ordering toward important pages. We propose a set of crawling algorithms for effective and efficient crawl ordering by prioritizing important pages with the well-known PageRank as the importance metric. In order to score URLs, the proposed algorithms utilize various features, including partial link structure, inter-host links, page titles, and topic relevance. We conduct a large-scale experiment using publicly available data sets to examine the effect of each feature on crawl ordering and evaluate the performance of many algorithms. The experimental results verify the efficacy of our schemes. In particular, compared with the representative RankMass crawler, the FPR-title-host algorithm reduces computational overhead by a factor as great as three in running time while improving effectiveness by 5 % in cumulative PageRank.",2012,Knowledge and Information Systems volume 33 issue 3 pp 707-734,focused crawler;web crawler;world wide web;information retrieval;data mining;database;computer science;
Efficient algorithms for influence maximization in social networks,Yi-Cheng Chen (National Chiao Tung University);Wen-Chih Peng (National Chiao Tung University);Suh-Yin Lee (National Chiao Tung University);,"2652487327,2100961564,2251835087","In recent years, due to the surge in popularity of social-networking web sites, considerable interest has arisen regarding influence maximization in social networks. Given a social network structure, the problem of influence maximization is to determine a minimum set of nodes that could maximize the spread of influences. With a large-scale social network, the efficiency and practicability of such algorithms are critical. Although many recent studies have focused on the problem of influence maximization, these works in general are time-consuming when a social network is large-scale. In this paper, we propose two novel algorithms, CDH-Kcut and Community and Degree Heuristic on Kcut/SHRINK, to solve the influence maximization problem based on a realistic model. The algorithms utilize the community structure, which significantly decreases the number of candidates of influential nodes, to avoid information overlap. The experimental results on both synthetic and real datasets indicate that our algorithms not only significantly outperform the state-of-the-art algorithms in efficiency but also possess graceful scalability.",2012,Knowledge and Information Systems volume 33 issue 3 pp 577-601,social network;data mining;artificial intelligence;machine learning;mathematical optimization;computer science;
Graph mining for discovering infrastructure patterns in configuration management databases,Pranay Anchuri (Rensselaer Polytechnic Institute);Mohammed J. Zaki (Rensselaer Polytechnic Institute);Omer Barkol (HP Labs);Ruth Bergman (HP Labs);Yifat Felder (HP Labs);Shahar Golan (HP Labs);Arik Sityon (HP Labs);,"73046339,2165917828,160540224,2097048889,1975755874,1972300174,223802074","A configuration management database (CMDB) can be considered to be a large graph representing the IT infrastructure entities and their interrelationships. Mining such graphs is challenging because they are large, complex, and multi-attributed and have many repeated labels. These characteristics pose challenges for graph mining algorithms, due to the increased cost of subgraph isomorphism (for support counting) and graph isomorphism (for eliminating duplicate patterns). The notion of pattern frequency or support is also more challenging in a single graph, since it has to be defined in terms of the number of its (potentially, exponentially many) embeddings. We present CMDB-Miner, a novel two-step method for mining infrastructure patterns from CMDB graphs. It first samples the set of maximal frequent patterns and then clusters them to extract the representative infrastructure patterns. We demonstrate the effectiveness of CMDB-Miner on real-world CMDB graphs, as well as synthetic graphs.",2012,Knowledge and Information Systems volume 33 issue 3 pp 491-522,graph operations;graph database;graph;theoretical computer science;combinatorics;data mining;mathematics;
Mining frequent conjunctive queries in relational databases through dependency discovery,Bart Goethals (University of Antwerp);Dominique Laurent (Centre national de la recherche scientifique);Wim Le Page (University of Antwerp);Cheikh Tidiane Dieng (Centre national de la recherche scientifique);,"1992071743,2125812555,2160829272,2503578283","We present an approach for mining frequent conjunctive in arbitrary relational databases. Our pattern class is the simple, but appealing subclass of simple conjunctive queries. Our algorithm, called Conqueror\(^+\), is capable of detecting previously unknown functional and inclusion dependencies that hold on the database relations as well as on joins of relations. These newly detected dependencies are then used to prune redundant queries. We propose an efficient database-oriented implementation of our algorithm using SQL and provide several promising experimental results.",2012,Knowledge and Information Systems volume 33 issue 3 pp 655-684,boolean conjunctive query;conjunctive query;relational database;functional dependency;information retrieval;data mining;database;computer science;
Wrapper positive Bayesian network classifiers,Borja Calvo (University of the Basque Country);Iñaki Inza (University of the Basque Country);Pedro Larrañaga (Technical University of Madrid);José Antonio Lozano (University of the Basque Country);,"2132699034,714011566,2305155066,2223079850","In the information retrieval framework, there are problems where the goal is to recover objects of a particular class from big sets of unlabelled objects. In some of these problems, only examples from the class we want to recover are available. For such problems, the machine learning community has developed algorithms that are able to learn binary classifiers in the absence of negative examples. Among them, we can find the positive Bayesian network classifiers, algorithms that induce Bayesian network classifiers from positive and unlabelled examples. The main drawback of these algorithms is that they require some previous knowledge about the a priori probability distribution of the class. In this paper, we propose a wrapper approach to tackle the learning when no such information is available, setting this probability at the optimal value in terms of the recovery of positive examples. The evaluation of classifiers in positive unlabelled learning problems is a non-trivial question. We have also worked on this problem, and we have proposed a new guiding metric to be used in the search for the optimal a priori probability of the positive class that we have called the pseudo F. We have empirically tested the proposed metric and the wrapper classifiers on both synthetic and real-life datasets. The results obtained in this empirical comparison show that the wrapper Bayesian network classifiers provide competitive results, particularly when the actual a priori probability of the positive class is high.",2012,Knowledge and Information Systems volume 33 issue 3 pp 631-654,random subspace method;data mining;pattern recognition;machine learning;computer science;
Structuring persistent chat conversations: experimental results of the chatsistance tool,Mohsen Rahimifar (Information Technology University);Siti Salwah Salim (Information Technology University);,"2318355173,2102325482","This research targets persistency by introducing new features in ‘Chatsistance’, a persistent chat tool that employs speech act theory and an explicit referencing technique to structure the chat log, and by providing further options for retrieving information. Earlier theory is related to the linguistics and characteristics of user statements. The later technique shows the relation of artefacts in an interconnected context named ‘explicit referencing’. Although speech acts were previously used in the field of chat, it is the first time that this theory has been used for the purpose of enhancing persistency combined with an explicit referencing technique, which together provide a well-established structure that provides greater flexibility for the analysis of content. In this paper, we show how this structuring approach could be useful, by evaluating the results of previous experiments that have used Chatsistance.",2012,Knowledge and Information Systems volume 33 issue 3 pp 685-705,multimedia;simulation;computer science;
SMOTE-RSB*: a hybrid preprocessing approach based on oversampling and undersampling for high imbalanced data-sets using SMOTE and rough sets theory,"Enislay Ramentol (University of Camagüey);Yailé Caballero (University of Camagüey);Rafael Bello (Central University, India);Francisco Herrera (University of Granada);","105840890,2104754391,2248511097,2141382980","Imbalanced data is a common problem in classification. This phenomenon is growing in importance since it appears in most real domains. It has special relevance to highly imbalanced data-sets (when the ratio between classes is high). Many techniques have been developed to tackle the problem of imbalanced training sets in supervised learning. Such techniques have been divided into two large groups: those at the algorithm level and those at the data level. Data level groups that have been emphasized are those that try to balance the training sets by reducing the larger class through the elimination of samples or increasing the smaller one by constructing new samples, known as undersampling and oversampling, respectively. This paper proposes a new hybrid method for preprocessing imbalanced data-sets through the construction of new samples, using the Synthetic Minority Oversampling Technique together with the application of an editing technique based on the Rough Set Theory and the lower approximation of a subset. The proposed method has been validated by an experimental study showing good results using C4.5 as the learning algorithm.",2012,Knowledge and Information Systems volume 33 issue 2 pp 245-265,oversampling and undersampling in data analysis;undersampling;oversampling;skewness;rough set;relevance;biological classification;data analysis;supervised learning;artificial intelligence;machine learning;statistics;algorithm;computer science;mathematics;
MDL-based time series clustering,"Thanawin Rakthanmanon (University of California, Riverside);Eamonn J. Keogh (University of California, Riverside);Stefano Lonardi (University of California, Riverside);Scott Evans (GE Global Research);","141314290,2170070822,301234865,2297125128","Time series data are pervasive across all human endeavors, and clustering is arguably the most fundamental data mining application. Given this, it is somewhat surprising that the problem of time series clustering from a single stream remains largely unsolved. Most work on time series clustering considers the clustering of individual time series that have been carefully extracted from their original context, for example, gene expression profiles, individual heartbeats, or individual gait cycles. The few attempts at clustering time series streams have been shown to be objectively incorrect in some cases, and in other cases shown to work only on the most contrived synthetic datasets by carefully adjusting a large set of parameters. In this work, we make two fundamental contributions that allow for the first time, the meaningful clustering of subsequences from a time series stream. First, we show that the problem definition for time series clustering from streams currently used is inherently flawed, and a new definition is necessary. Second, we show that the minimum description length framework offers an efficient, effective, and essentially parameter-free method for time series clustering. We show that our method produces objectively correct results on a wide variety of datasets from medicine, speech recognition, zoology, gesture recognition, and industrial process analyses.",2012,Knowledge and Information Systems volume 33 issue 2 pp 371-399,brown clustering;canopy clustering algorithm;dbscan;correlation clustering;constrained clustering;data stream clustering;cure data clustering algorithm;fuzzy clustering;clustering high dimensional data;cluster analysis;time series;consensus clustering;biclustering;conceptual clustering;data mining;artificial intelligence;machine learning;statistics;computer science;
Spatial co-location pattern discovery without thresholds,Feng Qian (Zhejiang University);Qinming He (Zhejiang University);Kevin Chiew;Jiangfeng He (Zhejiang University);,"2651270214,2133238876,2162472101,2113611368","Spatial co-location pattern mining discovers the subsets of features whose events are frequently located together in geographic space. The current research on this topic adopts a threshold-based approach that requires users to specify in advance the thresholds of distance and prevalence. However, in practice, it is not easy to specify suitable thresholds. In this article, we propose a novel iterative mining framework that discovers spatial co-location patterns without predefined thresholds. With the absolute and relative prevalence of spatial co-locations, our method allows users to iteratively select informative edges to construct the neighborhood relationship graph until every significant co-location has enough confidence and eventually to discover all spatial co-location patterns. The experimental results on real world data sets indicate that our framework is effective for prevalent co-locations discovery.",2012,Knowledge and Information Systems volume 33 issue 2 pp 419-445,data science;data mining;machine learning;statistics;
A countably infinite mixture model for clustering and feature selection,Nizar Bouguila (Concordia University);Djemel Ziou (Université de Sherbrooke);,"2221646335,283729413","Mixture modeling is one of the most useful tools in machine learning and data mining applications. An important challenge when applying finite mixture models is the selection of the number of clusters which best describes the data. Recent developments have shown that this problem can be handled by the application of non-parametric Bayesian techniques to mixture modeling. Another important crucial preprocessing step to mixture learning is the selection of the most relevant features. The main approach in this paper, to tackle these problems, consists on storing the knowledge in a generalized Dirichlet mixture model by applying non-parametric Bayesian estimation and inference techniques. Specifically, we extend finite generalized Dirichlet mixture models to the infinite case in which the number of components and relevant features do not need to be known a priori. This extension provides a natural representation of uncertainty regarding the challenging problem of model selection. We propose a Markov Chain Monte Carlo algorithm to learn the resulted infinite mixture. Through applications involving text and image categorization, we show that infinite mixture models offer a more powerful and robust performance than classic finite mixtures for both clustering and feature selection.",2012,Knowledge and Information Systems volume 33 issue 2 pp 351-370,hierarchical dirichlet process;categorization;markov chain monte carlo;mixture model;cluster analysis;feature selection;pattern recognition;machine learning;statistics;computer science;mathematics;
Parsimonious unsupervised and semi-supervised domain adaptation with good similarity functions,Emilie Morvant (Centre national de la recherche scientifique);Amaury Habrard (Centre national de la recherche scientifique);Stéphane Ayache (Centre national de la recherche scientifique);,"2294027368,1915993033,2570462787","In this paper, we address the problem of domain adaptation for binary classification. This problem arises when the distributions generating the source learning data and target test data are somewhat different. From a theoretical standpoint, a classifier has better generalization guarantees when the two domain marginal distributions of the input space are close. Classical approaches try mainly to build new projection spaces or to reweight the source data with the objective of moving closer the two distributions. We study an original direction based on a recent framework introduced by Balcan et al. enabling one to learn linear classifiers in an explicit projection space based on a similarity function, not necessarily symmetric nor positive semi-definite. We propose a well founded general method for learning a low-error classifier on target data which is effective with the help of an iterative procedure compatible with Balcan et al.'s framework. A reweighting scheme of the similarity function is then introduced in order to move closer the distri- butions in a new projection space. The hyperparameters and the reweighting quality are controlled by a reverse validation procedure. Our approach is based on a linear programming formulation and shows good adaptation performances with very sparse models. We first consider the challenging unsupervised case where no target label is accessible, which can be helpful when no manual annotation is possible. We also propose a generalization to the semi-supervised case allowing us to consider some few target labels when available. Finally, we evaluate our method on a synthetic problem and on a real image annotation task.",2012,Knowledge and Information Systems volume 33 issue 2 pp 309-349,transfer of learning;data mining;pattern recognition;artificial intelligence;machine learning;statistics;computer science;mathematics;
Dynamic classifier ensemble for positive unlabeled text stream classification,Shirui Pan (Northwest A&F University);Yang Zhang (Northwest A&F University);Xue Li (University of Queensland);,"2661154544,2303437990,2239470812","Most of studies on streaming data classification are based on the assumption that data can be fully labeled. However, in real-life applications, it is impractical and time-consuming to manually label the entire stream for training. It is very common that only a small part of positive data and a large amount of unlabeled data are available in data stream environments. In this case, applying the traditional streaming algorithms with straightforward adaptation to positive unlabeled stream may not work well or lead to poor performance. In this paper, we propose a Dynamic Classifier Ensemble method for Positive and Unlabeled text stream (DCEPU) classification scenarios. We address the problem of classifying positive and unlabeled text stream with various concept drift by constructing an appropriate validation set and designing a novel dynamic weighting scheme in the classification phase. Experimental results on benchmark dataset RCV1-v2 demonstrate that the proposed method DCEPU outperforms the existing LELC (Li et al. 2009b), DVS (with necessary adaption) (Tsymbal et al. in Inf Fusion 9(1):56–68, 2008), and Stacking style ensemble-based algorithm (Zhang et al. 2008b).",2012,Knowledge and Information Systems volume 33 issue 2 pp 267-287,concept drift;data mining;pattern recognition;machine learning;computer science;
Batch incremental processing for FP-tree construction using FP-Growth algorithm,Shashikumar G. Totad (GMR Institute of Technology);R. B. Geeta (GMR Institute of Technology);P. V. G. D. Prasad Reddy (Andhra University);,"1977361375,2019790961,2198974767","In the present scenario of global economy and World Wide Web, large sets of evolving and distributed data can be handled efficiently by incremental data mining. Frequent patterns are very important in knowledge discovery and data mining process, such as mining of association rules, correlations. FP-tree is a very versatile data structure used for mining of frequent patterns in knowledge discovery and data mining process. FP-tree is a compact representation of transaction database that contains frequency information of all relevant frequent patterns (FP) of the database. All of the existing incremental frequent pattern mining algorithms, such as AFPIM, CATS, CanTree, CP-tree, and SPO-tree, perform incremental mining by processing one transaction of the incremental part of database at a time and updating it to the FP-tree of initial (original) database. Here, in this paper, we propose a novel method that takes advantage of FP-tree representation of incremental transaction database for incremental mining. We propose a batch incremental processing algorithm BIT_FPGrowth that restructures and merges two small consecutive duration FP-trees to obtain a FP-tree of the FP-Growth algorithm. Our BIT_FPGrowth uses FP-tree as preprocessed data repository to get transactions (i.e., item-sets), unlike other sequential incremental algorithms that read transactions from database. BIT_FPGrowth algorithm takes less time for constructing FP-tree. Our experimental results show that, as the size of the database increases, increase in runtime of BIT_FPGrowth is much less and is least of all the other algorithms.",2012,Knowledge and Information Systems volume 33 issue 2 pp 475-490,gsp algorithm;data stream mining;data science;data mining;database;computer science;
Diverse dimension decomposition for itemset spaces,Mikalai Tsytsarau (University of Trento);Francesco Bonchi (Yahoo!);Aristides Gionis (Yahoo!);Themis Palpanas (University of Trento);,"18896700,2176652147,737311942,2010554420","We introduce the problem of diverse dimension decomposition in transactional databases, where a dimension is a set of mutually exclusive itemsets. The problem we consider requires to find a decomposition of the itemset space into dimensions, which are orthogonal to each other and which provide high coverage of the input database. The mining framework we propose can be interpreted as a dimensionality-reducing transformation from the space of all items to the space of orthogonal dimensions. Relying on information-theoretic concepts, we formulate the diverse dimension decomposition problem with a single objective function that simultaneously captures constraints on coverage, exclusivity, and orthogonality. We show that our problem is NP-hard, and we propose a greedy algorithm exploiting the well-known FP-tree data structure. Our algorithm is equipped with strategies for pruning the search space deriving directly from the objective function. We also prove a property that allows assessing the level of informativeness for newly added dimensions, thus allowing to define criteria for terminating the decomposition. We demonstrate the effectiveness of our solution by experimental evaluation on synthetic datasets with known dimension and three real-world datasets, flickr, del.icio.us and dblp. The problem we study is largely motivated by applications in the domain of collaborative tagging; however, the mining task we introduce in this paper is useful in other application domains as well.",2012,Knowledge and Information Systems volume 33 issue 2 pp 447-473,discrete mathematics;data mining;database;machine learning;mathematical optimization;mathematics;
Restoring coverage to the Bayesian false discovery rate control procedure,David L. Gold (Roswell Park Cancer Institute);,2320321995,"Principal among knowledge discovery tasks is recognition of insightful patterns or features from data that can inform otherwise challenging decisions. For the costly future decisions, there is little room for error. Features must provide substantial evidence to be robust for classification and dependable for important decisions. Here we seek statistical evidence for feature selection, that feature signals are of sufficient magnitude and frequency to be generalizable for classification. The Bayesian false discovery rate (bFDR) error control procedure is powerfully suited for this task. In realistic situations often encountered in practice, the bFDR procedure is biased, yielding a greater than desired FDR. In other less typical cases, the FDR is less than desired. We investigate the sources of bias in the bFDR procedure, and predict the direction of bias. A new algorithm has been developed to recover the bias in the bFDR control procedure. In simulation and real data mining examples, the new bFDR control algorithm shows promise. The strengths and limitations of the new approach are presented with examples and discussed.",2012,Knowledge and Information Systems volume 33 issue 2 pp 401-417,false discovery rate;multiple comparisons problem;bayesian probability;econometrics;data mining;machine learning;statistics;computer science;
Density-based weighting multi-surface least squares classification with its applications,Qiaolin Ye (Nanjing Forestry University);Ning Ye (Nanjing Forestry University);Shangbing Gao;,"2302906170,2028547736,2635808400","Traditionally, multi-plane support vector machines (SVM), including twin support vector machine (TWSVM) and least squares twin support vector machine (LSTSVM), consider all of points and view them as equally important points. In real cases, most of the samples of a dataset are highly correlated. These samples generally lie in the high-density regions and may be important for performances of classifiers. This motivates the rush toward new classifiers that can sufficiently take advantage of the points in the high-density regions. Illuminated by several new geometrically motivated algorithms, we propose density-based weighting multi-surface least squares classification (DWLSC) method, which is designed for classification. Considering the special features of multi-plane SVMs, DWLSC can measure the importance of points sharing the same labels by density weighting method and sufficiently make the full use of margin point information between pairs of points from different classes. It also includes naturally an extension of the non-linear case. In addition to keeping the respective advantages of both TWSVM and LSTSVM, our method improves the separation of the points sharing different classes and is shown to be better than other multi-plane classifiers in favor of reduction in space complexity, especially when confronted with the non-linear case. In addition, experimental evidence suggests that our method is effective in performing classification tasks.",2012,Knowledge and Information Systems volume 33 issue 2 pp 289-308,least squares support vector machine;data mining;pattern recognition;machine learning;statistics;mathematics;
Facing the reality of data stream classification: coping with scarcity of labeled data,Mohammad M. Masud (University of Texas at Dallas);Clay Woolam (University of Texas at Dallas);Jing Gao (University of Illinois at Urbana–Champaign);Latifur Khan (University of Texas at Dallas);Jiawei Han (University of Illinois at Urbana–Champaign);Kevin W. Hamlen (University of Texas at Dallas);Nikunj C. Oza (Ames Research Center);,"2146873716,2031924902,2096731881,2155983610,2121939561,2029339368,2047114418","Recent approaches for classifying data streams are mostly based on supervised learning algorithms, which can only be trained with labeled data. Manual labeling of data is both costly and time consuming. Therefore, in a real streaming environment where large volumes of data appear at a high speed, only a small fraction of the data can be labeled. Thus, only a limited number of instances will be available for training and updating the classification models, leading to poorly trained classifiers. We apply a novel technique to overcome this problem by utilizing both unlabeled and labeled instances to train and update the classification model. Each classification model is built as a collection of micro-clusters using semi-supervised clustering, and an ensemble of these models is used to classify unlabeled data. Empirical evaluation of both synthetic and real data reveals that our approach outperforms state-of-the-art stream classification algorithms that use ten times more labeled data than our approach.",2012,Knowledge and Information Systems volume 33 issue 1 pp 213-244,data stream clustering;concept drift;semi supervised learning;data mining;pattern recognition;machine learning;computer science;
Online active multi-field learning for efficient email spam filtering,Wuying Liu (National University of Defense Technology);Ting Wang (National University of Defense Technology);,"2104267691,2483396188","Email spam causes a serious waste of time and resources. This paper addresses the email spam filtering problem and proposes an online active multi-field learning approach, which is based on the following ideas: (1) Email spam filtering is an online application, which suggests an online learning idea; (2) Email document has a multi-field text structure, which suggests a multi-field learning idea; and (3) It is costly to obtain a label for a real-world email spam filter, which suggests an active learning idea. The online learner regards the email spam filtering as an incremental supervised binary streaming text classification. The multi-field learner combines multiple results predicted by field classifiers in a novel compound weight schema, and each field classifier calculates the arithmetical average of multiple conditional probabilities calculated from feature strings according to a data structure of string-frequency index. Comparing the current variance of field classifying results with the historical variance, the active learner evaluates the classifying confidence and takes the more uncertain email as the more informative sample for which to request a label. The experimental results show that the proposed approach can achieve the state-of-the-art performance with greatly reduced label requirements and very low space-time costs. The performance of our online active multi-field learning, the standard (1-ROCA)% measurement, even exceeds the full feedback performance of some advanced individual text classification algorithms.",2012,Knowledge and Information Systems volume 33 issue 1 pp 117-136,bag of words model;active learning;world wide web;data mining;machine learning;computer science;
Data preprocessing techniques for classification without discrimination,F Faisal Kamiran (Information Technology University);T Toon Calders (Université libre de Bruxelles);,"2072234693,2064105222","Recently, the following Discrimination-Aware Classification Problem was introduced: Suppose we are given training data that exhibit unlawful discrimination; e.g., toward sensitive attributes such as gender or ethnicity. The task is to learn a classifier that optimizes accuracy, but does not have this discrimination in its predictions on test data. This problem is relevant in many settings, such as when the data are generated by a biased decision process or when the sensitive attribute serves as a proxy for unobserved features. In this paper, we concentrate on the case with only one binary sensitive attribute and a two-class classification problem. We first study the theoretically optimal trade-off between accuracy and non-discrimination for pure classifiers. Then, we look at algorithmic solutions that preprocess the data to remove discrimination before a classifier is learned. We survey and extend our existing data preprocessing techniques, being suppression of the sensitive attribute, massaging the dataset by changing class labels, and reweighing or resampling the data to remove discrimination without relabeling instances. These preprocessing techniques have been implemented in a modified version of Weka and we present the results of experiments on real-life data.",2012,Knowledge and Information Systems volume 33 issue 1 pp 1-33,preprocessor;biological classification;data mining;pattern recognition;machine learning;computer science;
Web user clustering and Web prefetching using Random Indexing with weight functions,Miao Wan (Linköping University);Arne Jönsson (Linköping University);Cong Wang (Beijing University of Posts and Telecommunications);Lixiang Li (Beijing University of Posts and Telecommunications);Yixian Yang (Beijing University of Posts and Telecommunications);,"2167954854,2112592349,2676649739,2701244112,2723264050","Users of a Web site usually perform their interest-oriented actions by clicking or visiting Web pages, which are traced in access log files. Clustering Web user access patterns may capture common u ...",2012,Knowledge and Information Systems volume 33 issue 1 pp 89-115,web 2 0;web log analysis software;web analytics;static web page;web api;data web;site map;web mapping;web development;web design;web navigation;web server;web service;web page;web mining;multimedia;world wide web;data mining;computer science;
Compression and aggregation of Bayesian estimates for data intensive computing,Ruibin Xi (Harvard University);Nan Lin (Washington University in St. Louis);Yixin Chen (Washington University in St. Louis);Youngjin Kim (Google);,"2413907813,2610664851,2295009377,2305050297","Bayesian estimation is a major and robust estimator for many advanced statistical models. Being able to incorporate prior knowledge in statistical inference, Bayesian methods have been successfully applied in many different fields such as business, computer science, economics, epidemiology, genetics, imaging, and political science. However, due to its high computational complexity, Bayesian estimation has been deemed difficult, if not impractical, for large-scale databases, stream data, data warehouses, and data in the cloud. In this paper, we propose a novel compression and aggregation schemes (C&A) that enables distributed, parallel, or incremental computation of Bayesian estimates. Assuming partitioning of a large dataset, the C&A scheme compresses each partition into a synopsis and aggregates the synopsis into an overall Bayesian estimate without accessing the raw data. Such a C&A scheme can find applications in OLAP for data cubes, stream data mining, and cloud computing. It saves tremendous computing time since it processes each partition only once, enabling fast incremental update, and allows parallel processing. We prove that the compression is asymptotically lossless in the sense that the aggregated estimator deviates from the true model by an error that is bounded and approaches to zero when the data size increases. The results show that the proposed C&A scheme can make feasible OLAP of Bayesian estimates in a data cube. Further, it supports real-time Bayesian analysis of stream data, which can only be scanned once and cannot be permanently retained. Experimental results validate our theoretical analysis and demonstrate that our method can dramatically save time and space costs with almost no degradation of the modeling accuracy.",2012,Knowledge and Information Systems volume 33 issue 1 pp 191-212,bayesian average;online analytical processing;data cube;bayes estimator;compression;data stream mining;data science;theoretical computer science;data mining;database;machine learning;statistics;computer science;
Data classification through an evolutionary approach based on multiple criteria,Alvaro Garcia-Piquer (Intel);Albert Fornells (Intel);Albert Orriols-Puig (Intel);Guiomar Corral (Intel);Elisabet Golobardes (Intel);,"2400386425,2304243050,195131418,2401193283,1779716832","Real-world problems usually present a huge volume of imprecise data. These types of problems may challenge case-based reasoning systems because the knowledge extracted from data is used to identify analogies and solve new problems. Many authors have focused on organizing case memory in patterns to minimize the computational burden and deal with uncertainty. The organization is usually determined by a single criterion, but in some problems, a single criterion can be insufficient to find accurate clusters. This work describes an approach to organize the case memory in patterns based on multiple criteria. This new approach uses the searching capabilities of multiobjective evolutionary algorithms to build a Pareto set of solutions, where each one is a possible organization based on the relevance of objectives. The system shows promising capabilities when it is compared with a successful system based on self-organizing maps. Due to the data set geometry influences, the clustering building process results are analyzed taking into account it. For this reason, some complexity measures are used to categorize data sets according to their topology.",2012,Knowledge and Information Systems volume 33 issue 1 pp 35-56,case based reasoning;data mining;artificial intelligence;machine learning;computer science;
A software trustworthiness evaluation model using objective weight based evidential reasoning approach,Shuai Ding (Hefei University of Technology);Xi-Jun Ma (Hefei University of Technology);Shan-Lin Yang (Hefei University of Technology);,"2657880387,2719714100,2674522585","Evaluation of software trustworthiness is an extensively investigated topic. A lot of software trustworthiness evaluation (STE) models and systems have been proposed in the literature. While scholars have concentrated their efforts on the design and development of trustworthiness metrics, other STE characteristics have been neglected, such as various types of uncertainties and objective of the attribute weights determination. This paper focuses on the STE problems that include both quantitative and qualitative indicators with uncertainties. We propose an objective weight based evidential reasoning approach that employs total uncertainty measure to solve STE problems with specific trustworthiness requirements. A complex software is evaluated by the proposed STE model to demonstrate its applicability and effectiveness.",2012,Knowledge and Information Systems volume 33 issue 1 pp 171-189,evidential reasoning approach;data mining;artificial intelligence;computer science;
Exponential family tensor factorization: an online extension and applications,Kohei Hayashi (Nara Institute of Science and Technology);Takashi Takenouchi (Nara Institute of Science and Technology);Tomohiro Shibata (Nara Institute of Science and Technology);Yuki Kamiya (NEC);Daishi Kato (NEC);Kazuo Kunieda (NEC);Keiji Yamada (NEC);Kazushi Ikeda (Nara Institute of Science and Technology);,"2260348883,1904534369,2146437674,2013147939,2067174492,2030377576,2141889024,2132901609","In this paper, we propose a new probabilistic model of heterogeneously attributed multi-dimensional arrays. The model can manage heterogeneity by employing individual exponential family distributions for each attribute of the tensor array. Entries of the tensor are connected by latent variables and share information across the different attributes through the latent variables. The assumption of heterogeneity makes a Bayesian inference intractable, and we cast the EM algorithm approximated by the Laplace method and Gaussian process. We also extended the proposal algorithm for online learning. We apply our method to missing-values prediction and anomaly detection problems and show that our method outperforms conventional approaches that do not consider heterogeneity.",2012,Knowledge and Information Systems volume 33 issue 1 pp 57-88,sensor fusion;anomaly detection;pattern recognition;machine learning;statistics;computer science;mathematics;
From global to local and viceversa: uses of associative rule learning for classification in imprecise environments,Gianni Costa (Indian Council of Agricultural Research);Giuseppe Manco (Indian Council of Agricultural Research);Riccardo Ortale (Indian Council of Agricultural Research);Ettore Ritacco (Indian Council of Agricultural Research);,"2157428172,2093732677,1974615881,1993379685","We propose two models for improving the performance of rule-based classification under unbalanced and highly imprecise domains. Both models are probabilistic frameworks aimed to boost the performance of basic rule-based classifiers. The first model implements a global-to-local scheme, where the response of a global rule-based classifier is refined by performing a probabilistic analysis of the coverage of its rules. In particular, the coverage of the individual rules is used to learn local probabilistic models, which ultimately refine the predictions from the corresponding rules of the global classifier. The second model implements a dual local-to-global strategy, in which single classification rules are combined within an exponential probabilistic model in order to boost the overall performance as a side effect of mutual influence. Several variants of the basic ideas are studied, and their performances are thoroughly evaluated and compared with state-of-the-art algorithms on standard benchmark datasets.",2012,Knowledge and Information Systems volume 33 issue 1 pp 137-169,principle of maximum entropy;probabilistic classification;data mining;pattern recognition;artificial intelligence;machine learning;statistics;computer science;
Cluster-based instance selection for machine classification,Ireneusz Czarnowski (California Maritime Academy);,149233959,"Instance selection in the supervised machine learning, often referred to as the data reduction, aims at deciding which instances from the training set should be retained for further use during the learning process. Instance selection can result in increased capabilities and generalization properties of the learning model, shorter time of the learning process, or it can help in scaling up to large data sources. The paper proposes a cluster-based instance selection approach with the learning process executed by the team of agents and discusses its four variants. The basic assumption is that instance selection is carried out after the training data have been grouped into clusters. To validate the proposed approach and to investigate the influence of the clustering method used on the quality of the classification, the computational experiment has been carried out.",2012,Knowledge and Information Systems volume 30 issue 1 pp 113-133,online machine learning;stability;generalization error;active learning;semi supervised learning;instance based learning;multi agent system;data mining;pattern recognition;machine learning;computer science;
Mining top − k frequent patterns without minimum support threshold,"Abdus Salam (International Islamic University, Islamabad);M. Sikandar Hayat Khayal (Fatima Jinnah Women University);","2476199669,2223292564","Finding frequent patterns play an important role in mining association rules, sequences, episodes, Web log mining and many other interesting relationships among data. Frequent pattern mining methods often produce a huge number of frequent itemsets that is not feasible for effective usage. The number of highly correlated patterns is usually very small and may even be one. Most of the existing frequent pattern mining techniques often require the setting of many input parameters and may involve multiple passes over the database. Minimum support is the widely used parameter in frequent pattern mining to discover statistically significant patterns. Specifying appropriate minimum support is a challenging task for a data analyst as the choice of minimum support value is somewhat arbitrary. Generally, it is required to repeatedly execute an algorithm, heuristically tuning the value of minimum support over a wide range, until the desired result is obtained, certainly, a very time-consuming process. Setting up an inappropriate minimum support may also cause an algorithm to fail in finding the true patterns. We present a novel method to efficiently retrieve top few maximal frequent patterns in order of significance without use of the minimum support parameter. Instead, we are only required to specify a more human understandable parameter, namely the desired number itemsets k. Our technique requires only a single pass over the database and generation of length two itemsets. The association ratio graph is proposed as a compact structure containing concise information, which is created in time quadratic to the size of the database. Algorithms are described for using this graph structure to discover top-most and top-k maximal frequent itemsets without minimum support threshold. To effectively achieve this, the method employs construction of an all path source-to-destination tree to discover all maximal cycles in the graph. The results can be ranked in decreasing order of significance. Results are presented demonstrating the performance advantages to be gained from the use of this approach.",2012,Knowledge and Information Systems volume 30 issue 1 pp 57-86,data mining;pattern recognition;machine learning;mathematics;
A segment-based framework for modeling and mining data streams,Charu C. Aggarwal (IBM);,2146335907,"Data Streams have become ubiquitous in recent years because of advances in hardware technology which have enabled automated recording of large amounts of data. The primary constraint in the effective mining of streams is the large volume of data which must be processed in real time. In many cases, it is desirable to store a summary of the data stream segments in order to perform data mining tasks. Since density estimation provides a comprehensive overview of the probabilistic data distribution of a stream segment, it is a natural choice for this purpose. A direct use of density distributions can however turn out to be an inefficient storage and processing mechanism in practice. In this paper, we introduce the concept of cluster histograms, which provides an efficient way to estimate and summarize the most important data distribution profiles over different stream segments. These profiles can be constructed in a supervised or unsupervised way depending upon the nature of the underlying application. The profiles can also be used for change detection, anomaly detection, segmental nearest neighbor search, or supervised stream segment classification. Furthermore, these techniques can also be used for modeling other kinds of data such as text and categorical data. The flexibility of the tasks which can be performed from the cluster histogram framework follows from its generality in storing the historical density profile of the data stream. As a result, this method provides a holistic framework for density-based mining of data streams. We discuss and test the application of the cluster histogram framework to a variety of interesting data mining applications.",2012,Knowledge and Information Systems volume 30 issue 1 pp 1-29,data stream clustering;cluster analysis;data stream mining;data science;data mining;database;machine learning;computer science;
Correcting evaluation bias of relational classifiers with network cross validation,Jennifer Neville (Purdue University);Brian Gallagher (Lawrence Livermore National Laboratory);Tina Eliassi-Rad (Rutgers University);Tao Wang (Purdue University);,"2124572662,2188853992,218538652,2244818045","Recently, a number of modeling techniques have been developed for data mining and machine learning in relational and network domains where the instances are not independent and identically distributed (i.i.d.). These methods specifically exploit the statistical dependencies among instances in order to improve classification accuracy. However, there has been little focus on how these same dependencies affect our ability to draw accurate conclusions about the performance of the models. More specifically, the complex link structure and attribute dependencies in relational data violate the assumptions of many conventional statistical tests and make it difficult to use these tests to assess the models in an unbiased manner. In this work, we examine the task of within-network classification and the question of whether two algorithms will learn models that will result in significantly different levels of performance. We show that the commonly used form of evaluation (paired t-test on overlapping network samples) can result in an unacceptable level of Type I error. Furthermore, we show that Type I error increases as (1) the correlation among instances increases and (2) the size of the evaluation set increases (i.e., the proportion of labeled nodes in the network decreases). We propose a method for network cross-validation that combined with paired t-tests produces more acceptable levels of Type I error while still providing reasonable levels of statistical power (i.e., 1−Type II error).",2012,Knowledge and Information Systems volume 30 issue 1 pp 31-55,node;statistical inference;correlation;accuracy and precision;bias;prediction;statistical hypothesis testing;biological classification;methodology;statistical relational learning;data mining;database;artificial intelligence;machine learning;statistics;computer science;
Combining case-based reasoning systems and support vector regression to evaluate the atmosphere–ocean interaction,Juan Francisco de Paz (University of Salamanca);Javier Bajo (Pontifical University of Salamanca);Angélica González (University of Salamanca);Sara Rodríguez (University of Salamanca);Juan M. Corchado (University of Salamanca);,"2096670735,2132377945,2166586253,2148795135,1978763565","This work presents a system for automatically evaluating the interaction that exists between the atmosphere and the ocean’s surface. Monitoring and evaluating the ocean’s carbon exchange process is a function that requires working with a great amount of data: satellite images and in situ vessel’s data. The system presented in this study focuses on computational intelligence. The study presents an intelligent system based on the use of case-based reasoning (CBR) systems and offers a distributed model for such an interaction. Moreover, the system takes into account the fact that the working environment is dynamic and therefore it requires autonomous models that evolve over time. In order to resolve this problem, an intelligent environment has been developed, based on the use of CBR systems, which are capable of handling several goals, by constructing plans from the data obtained through satellite images and research vessels, acquiring knowledge and adapting to environmental changes. The artificial intelligence system has been successfully tested in the North Atlantic Ocean, and the results obtained will be presented in this study.",2012,Knowledge and Information Systems volume 30 issue 1 pp 155-177,case based reasoning;support vector machine;decision support system;operations research;data mining;database;artificial intelligence;machine learning;simulation;computer science;
Argument mining based on a structured database and its usage in an intelligent tutoring environment,Safia Abbas (Ain Shams University);Hajime Sawamura (Niigata University);,"2154588774,2278404382","Argumentation theory is a new research area that concerns mainly with reaching a mutually acceptable conclusion using logical reasoning. Argumentation can be defined as a proof of dynamic nature and is considered as an ill-defined domain that typically lacks clear distinctions between “right” and “wrong” answers. Instead, there are often competing reasonable answers. Recently, a number of argument mapping tools have been developed to diagram, articulate, and comprehend different arguments. Despite the fact, these methods are of complementary nature, and the efforts for integrating these tools are missing. The purpose of this paper is threefold: (1) revealing a novel approach for argument representation using a structured relational argument database “RADB”, which has been designed, developed, and implemented in order to represent different argument analyses and diagrams, (2) presenting a classifier agent that utilizes the RADB repository by using different mining techniques in order to retrieve the most relevant arguments to the subject of search, and (3) proposing an agent-based educational environment (ALES) that utilizes the RABD together with the classifier agent to teach argument analysis.",2012,Knowledge and Information Systems volume 30 issue 1 pp 213-246,argument;argumentation theory;data mining;database;artificial intelligence;machine learning;algorithm;computer science;mathematics;
Microeconomic analysis using dominant relationship analysis,Ling Zhu;Cuiping Li (Renmin University of China);Anthony K. H. Tung (National University of Singapore);Shan Wang (Renmin University of China);,"2436646856,2129719679,121171588,2640266581","The concept of dominance has recently attracted much interest in the context of skyline computation. Given an N-dimensional data set S, a point p is said to dominate q if p is better than q in at least one dimension and equal to or better than it in the remaining dimensions. In this article, we propose extending the concept of dominance for business analysis from a microeconomic perspective. More specifically, we propose a new form of analysis, called Dominant Relationship Analysis (DRA), which aims to provide insight into the dominant relationships between products and potential buyers. By analyzing such relationships, companies can position their products more effectively while remaining profitable. To support DRA, we propose a novel data cube called DADA (Data Cube for Dominant Relationship Analysis), which captures the dominant relationships between products and customers. Three types of queries called Dominant Relationship Queries (DRQs) are consequently proposed for analysis purposes: (1) Linear Optimization Queries (LOQ), (2) Subspace Analysis Queries (SAQ), and (3) Comparative Dominant Queries (CDQ). We designed efficient algorithms for computation, compression and incremental maintenance of DADA as well as for answering the DRQs using DADA. We conducted extensive experiments on various real and synthetic data sets to evaluate the technique of DADA and report results demonstrating the effectiveness and efficiency of DADA and its associated query-processing strategies.",2012,Knowledge and Information Systems volume 30 issue 1 pp 179-211,dominance;data mining;database;artificial intelligence;computer science;
Application-independent feature construction based on almost-closedness properties,Nazha Selmaoui-Folcher (University of New Caledonia);Jean-François Boulicaut (Institut national des sciences Appliquées de Lyon);,"1967950925,1971530415","Feature construction has been studied extensively, including for 0/1 data samples. Given the recent breakthroughs in closedness-related constraint-based mining, we are considering its impact on feature construction for classification tasks. We investigate the use of condensed representations of frequent itemsets based on closedness properties as new features. These itemset types have been proposed to avoid set counting in difficult association rule mining tasks, i.e. when data are noisy and/or highly correlated. However, our guess is that their intrinsic properties (say the maximality for the closed itemsets and the minimality for the δ-free itemsets) should have an impact on feature quality. Understanding this remains fairly open, and we discuss these issues thanks to itemset properties on the one hand and an experimental validation on various data sets (possibly noisy) on the other hand.",2012,Knowledge and Information Systems volume 30 issue 1 pp 87-111,data mining;pattern recognition;machine learning;mathematics;
A general insight into the effect of neuron structure on classification,Hadi Sadoghi Yazdi (Ferdowsi University of Mashhad);Alireza Rowhanimanesh (Ferdowsi University of Mashhad);Hamidreza Modares (Ferdowsi University of Mashhad);,"2159796335,1971970505,2631590739","This paper gives a general insight into how the neuron structure in a multilayer perceptron (MLP) can affect the ability of neurons to deal with classification. Most of the common neuron structures are based on monotonic activation functions and linear input mappings. In comparison, the proposed neuron structure utilizes a nonmonotonic activation function and/or a nonlinear input mapping to increase the power of a neuron. An MLP of these high power neurons usually requires a less number of hidden nodes than conventional MLP for solving classification problems. The fewer number of neurons is equivalent to the smaller number of network weights that must be optimally determined by a learning algorithm. The performance of learning algorithm is usually improved by reducing the number of weights, i.e., the dimension of the search space. This usually helps the learning algorithm to escape local optimums, and also, the convergence speed of the algorithm is increased regardless of which algorithm is used for learning. Several 2-dimensional examples are provided manually to visualize how the number of neurons can be reduced by choosing an appropriate neuron structure. Moreover, to show the efficiency of the proposed scheme in solving real-world classification problems, the Iris data classification problem is solved using an MLP whose neurons are equipped by nonmonotonic activation functions, and the result is compared with two well-known monotonic activation functions.",2012,Knowledge and Information Systems volume 30 issue 1 pp 135-154,multilayer perceptron;biological classification;artificial intelligence;machine learning;algorithm;mathematics;
An overview on subgroup discovery: foundations and applications,Franciso Herrera (University of Granada);Cristóbal José Carmona (University of Jaén);Pedro González (University of Jaén);María José del Jesus (University of Jaén);,"2412789748,2171158112,2110853531,2102672766","Subgroup discovery is a data mining technique which extracts interesting rules with respect to a target variable. An important characteristic of this task is the combination of predictive and descriptive induction. An overview related to the task of subgroup discovery is presented. This review focuses on the foundations, algorithms, and advanced studies together with the applications of subgroup discovery presented throughout the specialised bibliography.",2011,Knowledge and Information Systems volume 29 issue 3 pp 495-525,knowledge extraction;information retrieval;bioinformatics;data mining;computer science;mathematics;
Geometric data perturbation for privacy preserving outsourced data mining,Keke Chen (Wright State University);Ling Liu (Georgia Institute of Technology);,"2125330238,2125988131","Data perturbation is a popular technique in privacy-preserving data mining. A major challenge in data perturbation is to balance privacy protection and data utility, which are normally considered as a pair of conflicting factors. We argue that selectively preserving the task/model specific information in perturbation will help achieve better privacy guarantee and better data utility. One type of such information is the multidimensional geometric information, which is implicitly utilized by many data-mining models. To preserve this information in data perturbation, we propose the Geometric Data Perturbation (GDP) method. In this paper, we describe several aspects of the GDP method. First, we show that several types of well-known data-mining models will deliver a comparable level of model quality over the geometrically perturbed data set as over the original data set. Second, we discuss the intuition behind the GDP method and compare it with other multidimensional perturbation methods such as random projection perturbation. Third, we propose a multi-column privacy evaluation framework for evaluating the effectiveness of geometric data perturbation with respect to different level of attacks. Finally, we use this evaluation framework to study a few attacks to geometrically perturbed data sets. Our experimental study also shows that geometric data perturbation can not only provide satisfactory privacy guarantee but also preserve modeling accuracy well.",2011,Knowledge and Information Systems volume 29 issue 3 pp 657-695,theoretical computer science;data mining;database;computer science;
Non-derivable itemsets for fast outlier detection in large high-dimensional categorical data,Anna Koufakou (University of Central Florida);Jimmy Secretan (University of Central Florida);Michael Georgiopoulos (University of Central Florida);,"239421741,2077392856,673942444","Detecting outliers in a dataset is an important data mining task with many applications, such as detection of credit card fraud or network intrusions. Traditional methods assume numerical data and compute pair-wise distances among points. Recently, outlier detection methods were proposed for categorical and mixed-attribute data using the concept of Frequent Itemsets (FIs). These methods face challenges when dealing with large high-dimensional data, where the number of generated FIs can be extremely large. To address this issue, we propose several outlier detection schemes inspired by the well-known condensed representation of FIs, Non-Derivable Itemsets (NDIs). Specifically, we contrast a method based on frequent NDIs, FNDI-OD, and a method based on the negative border of NDIs, NBNDI-OD, with their previously proposed FI-based counterparts. We also explore outlier detection based on Non-Almost Derivable Itemsets (NADIs), which approximate the NDIs in the data given a δ parameter. Our proposed methods use a far smaller collection of sets than the FI collection in order to compute an anomaly score for each data point. Experiments on real-life data show that, as expected, methods based on NDIs and NADIs offer substantial advantages in terms of speed and scalability over FI-based Outlier Detection method. What is significant is that NDI-based methods exhibit similar or better detection accuracy compared to the FI-based methods, which supports our claim that the NDI representation is especially well-suited for the task of detecting outliers. At the same time, the NDI approximation scheme, NADIs is shown to exhibit similar accuracy to the NDI-based method for various δ values and further runtime performance gains. Finally, we offer an in-depth discussion and experimentation regarding the trade-offs of the proposed algorithms and the choice of parameter values.",2011,Knowledge and Information Systems volume 29 issue 3 pp 697-725,anomaly detection;data science;data mining;pattern recognition;machine learning;computer science;
Class separation through variance: a new application of outlier detection,Andrew Foss (University of Alberta);Osmar R. Zaïane (University of Alberta);,"2011760006,2308328903","This paper introduces a new outlier detection approach and discusses and extends a new concept, class separation through variance. We show that even for balanced and concentric classes differing only in variance, accumulating information about the outlierness of points in multiple subspaces leads to a ranking in which the classes naturally tend to separate. Exploiting this leads to a highly effective and efficient unsupervised class separation approach. Unlike typical outlier detection algorithms, this method can be applied beyond the ‘rare classes’ case with great success. The new algorithm FASTOUT introduces a number of novel features. It employs sampling of subspaces points and is highly efficient. It handles arbitrarily sized subspaces and converges to an optimal subspace size through the use of an objective function. In addition, two approaches are presented for automatically deriving the class of the data points from the ranking. Experiments show that FASTOUT typically outperforms other state-of-the-art outlier detection methods on high-dimensional data such as Feature Bagging, SOE1, LOF, ORCA and Robust Mahalanobis Distance, and competes even with the leading supervised classification methods for separating classes.",2011,Knowledge and Information Systems volume 29 issue 3 pp 565-596,linear subspace;one class classification;concentration of measure;biological classification;anomaly detection;data mining;pattern recognition;machine learning;statistics;computer science;mathematics;
A high-performing comprehensive learning algorithm for text classification without pre-labeled training set,Lizhen Liu (Capital Normal University);Qianhui Liang (HP Labs);,"2651848974,2713582002","In this paper, we investigate a comprehensive learning algorithm for text classification without pre-labeled training set based on incremental learning. In order to overcome the high cost in getting labeled training examples, this approach reforms fuzzy partition clustering to obtain a small quantity of labeled training data. Then the incremental learning of Bayesian classifier is applied. The model of the proposed classifier is composed of a Naive-Bayes-based incremental learning algorithm and a modified fuzzy partition clustering method. For improved efficiency, a feature reduction is designed based on the Quadratic Entropy in Mutual Information. We perform experiments to demonstrate the performance of the approach, and the results show that our approach is feasible and effective.",2011,Knowledge and Information Systems volume 29 issue 3 pp 727-738,population based incremental learning;stability;correlation clustering;fuzzy clustering;dimensionality reduction;cluster analysis;semi supervised learning;conceptual clustering;data mining;pattern recognition;machine learning;computer science;mathematics;
Obtaining scalable and accurate classification in large-scale spatio-temporal domains,Igor Vainer (Bar-Ilan University);Sarit Kraus (Bar-Ilan University);Gal A. Kaminka (Bar-Ilan University);Hamutal Slovin (Bar-Ilan University);,"2599342060,2122497617,595631857,1634296115","We present an approach for learning models that obtain accurate classification of data objects, collected in large-scale spatio-temporal domains. The model generation is structured in three phases: spatial dimension reduction, spatio-temporal features extraction, and feature selection. Novel techniques for the first two phases are presented, with two alternatives for the middle phase. We explore model generation based on the combinations of techniques from each phase. We apply the introduced methodology to data-sets from the Voltage-Sensitive Dye Imaging (VSDI) domain, where the resulting classification models successfully decode neuronal population responses in the visual cortex of behaving animals. VSDI is currently the best technique enabling simultaneous high spatial (10,000 points) and temporal (10 ms or less) resolution imaging from neuronal population in the cortex. We demonstrate that not only our approach is scalable enough to handle computationally challenging data, but it also contributes to the neuroimaging field of study with its decoding abilities. The effectiveness of our methodology is further explored on a data-set from the hurricanes domain, and a promising direction, based on the preliminary results of hurricane severity classification, is revealed.",2011,Knowledge and Information Systems volume 29 issue 3 pp 527-564,neural decoding;biological classification;neuroimaging;computer vision;data mining;artificial intelligence;machine learning;
GAC-GEO: a generic agglomerative clustering framework for geo-referenced datasets,Rachsuda Jiamthapthaksin (Assumption University);Christoph F. Eick (University of Houston);Seungchan Lee (University of Houston);,"2310798681,715851671,2482496765","Major challenges of clustering geo-referenced data include identifying arbitrarily shaped clusters, properly utilizing spatial information, coping with diverse extrinsic characteristics of clusters and supporting region discovery tasks. The goal of region discovery is to identify interesting regions in geo-referenced datasets based on a domain expert’s notion of interestingness. Almost all agglomerative clustering algorithms only focus on the first challenge. The goal of the proposed work is to develop agglomerative clustering frameworks that deal with all four challenges. In particular, we propose a generic agglomerative clustering framework for geo-referenced datasets (GAC-GEO) generalizing agglomerative clustering by allowing for three plug-in components. GAC-GEO agglomerates neighboring clusters maximizing a plug-in fitness function that capture the notion of interestingness of clusters. It enhances typical agglomerative clustering algorithms in two ways: fitness functions support task-specific clustering, whereas generic neighboring relationships increase the number of merging candidates. We also demonstrate that existing agglomerative clustering algorithms can be considered as specific cases of GAC-GEO. We evaluate the proposed framework on an artificial dataset and two real-world applications involving region discovery. The experimental results show that GAC-GEO is capable of identifying arbitrarily shaped hotspots for different data mining tasks.",2011,Knowledge and Information Systems volume 29 issue 3 pp 597-628,hierarchical clustering of networks;flame clustering;brown clustering;canopy clustering algorithm;correlation clustering;constrained clustering;cure data clustering algorithm;single linkage clustering;fuzzy clustering;clustering high dimensional data;hierarchical clustering;cluster analysis;biclustering;conceptual clustering;bioinformatics;data mining;machine learning;statistics;computer science;mathematics;
Adaptation and user modeling in hypermedia learning environments using the SEM-HP model and the JSEM-HP tool,Nuria Medina-Medina (University of Granada);Fernando Molina-Ortiz (University of Granada);Lina García-Cabrera (University of Jaén);,"112268058,87831636,9517766","This paper presents a model, called SEM-HP, which enables the development of evolutionary hypermedia systems that can adapt their functioning to each user and their structure to a particular group of users. The individual adaptation feature manages a user model that includes the personal data, experience, preferences, knowledge, and interests of each user. It applies knowledge-based adaptation techniques to the following tasks: (1) personalized selection of the navigation structure, (2) hiding and disabling of links to inappropriate information, (3) positive annotation of interesting links, (4) generation of guided routes, and (5) building of personalized conceptual summaries. The group adaptation feature uses transition matrices to model the navigational activities of a group of users and, based on these, suggests modifications to evolve the navigation structures defined by the author to bring them closer to the mental concept of the majority of users. The paper also introduces a general taxonomy of user models that makes it possible to classify any model according to various features of its structure and management. In addition, the taxonomy is used to classify the UM managed in SEM-HP, thus revealing its strengths and weaknesses. The last part of the paper describes a teaching experiment performed using the JSEM-HP tool, which is based on the SEM-HP model. This description includes the educational system created with the tool, the usage of the tool in the classroom, the evaluation performed after the tool was used, and the results obtained.",2011,Knowledge and Information Systems volume 29 issue 3 pp 629-656,software evolution;user modeling;adaptation;knowledge management;multimedia;data mining;database;artificial intelligence;machine learning;simulation;computer science;
The ClusTree: indexing micro-clusters for anytime stream mining,Philipp Kranen (RWTH Aachen University);Ira Assent (Aarhus University);Corinna Baldauf (RWTH Aachen University);Thomas Seidl (RWTH Aachen University);,"2008431275,145164693,2103339817,2140301036","Clustering streaming data requires algorithms that are capable of updating clustering results for the incoming data. As data is constantly arriving, time for processing is limited. Clustering has to be performed in a single pass over the incoming data and within the possibly varying inter-arrival times of the stream. Likewise, memory is limited, making it impossible to store all data. For clustering, we are faced with the challenge of maintaining a current result that can be presented to the user at any given time. In this work, we propose a parameter-free algorithm that automatically adapts to the speed of the data stream. It makes best use of the time available under the current constraints to provide a clustering of the objects seen up to that point. Our approach incorporates the age of the objects to reflect the greater importance of more recent data. For efficient and effective handling, we introduce the ClusTree, a compact and self-adaptive index structure for maintaining stream summaries. Additionally we present solutions to handle very fast streams through aggregation mechanisms and propose novel descent strategies that improve the clustering result on slower streams as long as time permits. Our experiments show that our approach is capable of handling a multitude of different stream characteristics for accurate and scalable anytime stream clustering.",2011,Knowledge and Information Systems volume 29 issue 2 pp 249-272,elasticity;constrained clustering;data stream clustering;cluster analysis;data stream mining;world wide web;data mining;database;machine learning;computer science;
An abstract architecture for virtual organizations: The THOMAS approach,Estefania Argente (Polytechnic University of Valencia);Vicente J. Botti (Polytechnic University of Valencia);Carlos Carrascosa (Polytechnic University of Valencia);Adriana Giret (Polytechnic University of Valencia);Vicente Julián (Polytechnic University of Valencia);Miguel Rebollo (Polytechnic University of Valencia);,"2206613986,157915158,2054966800,2142495341,2162501773,2128467596","Today, the need for architectures and computational models for large-scale open multi-agent systems is considered to be a key issue for the success of agent technology in real-world scenarios. This paper analyzes the significant unsolved problems that must be taken into account in order to develop real, open multi-agent systems. It identifies requirements and related open issues, discusses how some of these requirements have been tackled by current technologies, and explains how the THOMAS architecture is able to give support to these open issues. This paper also describes the THOMAS abstract architecture and computational model for large-scale open multi-agent systems based on a service-oriented approach that specifically addresses the design of virtual organizations. An application example for the management of a travel agency system, which demonstrates the new features of the proposal, is also presented.",2011,Knowledge and Information Systems volume 29 issue 2 pp 379-403,multi agent system;artificial intelligence;simulation;computer science;
Using OWL ontologies for adaptive patient information modelling and preoperative clinical decision support,Matt-Mouley Bouamrane (University of Glasgow);Alan L. Rector (University of Manchester);Martin Hurrell;,"2015746387,2013949216,1966507570","We here present our research and experience regarding the design and implementation of a knowledge-based preoperative assessment decision support system. We discuss generic design considerations as well as the practical system implementation. We developed the system using semantic web technology, including modular ontologies developed in the OWL web ontology language, the OWL Java application programming interface and an automated logic reasoner. We discuss how the system enables to tailor patient information collection according to personalized medical context. The use of ontologies at the core of the system’s architecture permits to efficiently manage a vast repository of preoperative assessment domain knowledge, including classification of surgical procedures, classification of morbidities and guidelines for routine preoperative tests. Logical inference on the domain knowledge according to individual patient’s medical context enables personalized patients’ reports consisting of a risk assessment and clinical recommendations such as relevant preoperative tests.",2011,Knowledge and Information Systems volume 29 issue 2 pp 405-418,knowledge representation and reasoning;management science;knowledge management;data mining;artificial intelligence;computer science;
A hybrid decision tree training method using data streams,Michal Wozniak (Wrocław University of Technology);,2141334231,"Classical classification methods usually assume that pattern recognition models do not depend on the timing of the data. However, this assumption is not valid in cases where new data frequently become available. Such situations are common in practice, for example, spam filtering or fraud detection, where dependencies between feature values and class numbers are continually changing. Unfortunately, most classical machine learning methods (such as decision trees) do not take into consideration the possibility of the model changing, as a result of so-called concept drift and they cannot adapt to a new classification model. This paper focuses on the problem of concept drift, which is a very important issue, especially in data mining methods that use complex structures (such as decision trees) for making decisions. We propose an algorithm that is able to co-train decision trees using a modified NGE (Nested Generalized Exemplar) algorithm. The potential for adaptation of the proposed algorithm and the quality thereof are evaluated through computer experiments, carried out on benchmark datasets from the UCI Machine Learning Repository.",2011,Knowledge and Information Systems volume 29 issue 2 pp 335-347,incremental decision tree;id3 algorithm;concept drift;decision tree learning;decision tree;data mining;database;pattern recognition;artificial intelligence;machine learning;statistics;computer science;
Convex non-negative matrix factorization for massive datasets,Christian Thurau (Fraunhofer Society);Kristian Kersting (Fraunhofer Society);Mirwaes Wahabzada (Fraunhofer Society);Christian Bauckhage (Fraunhofer Society);,"2012185565,2252032993,336206225,2034409955","Non-negative matrix factorization (NMF) has become a standard tool in data mining, information retrieval, and signal processing. It is used to factorize a non-negative data matrix into two non-negative matrix factors that contain basis elements and linear coefficients, respectively. Often, the columns of the first resulting factor are interpreted as “cluster centroids” of the input data, and the columns of the second factor are understood to contain cluster membership indicators. When analyzing data such as collections of gene expressions, documents, or images, it is often beneficial to ensure that the resulting cluster centroids are meaningful, for instance, by restricting them to be convex combinations of data points. However, known approaches to convex-NMF suffer from high computational costs and therefore hardly apply to large-scale data analysis problems. This paper presents a new framework for convex-NMF that allows for an efficient factorization of data matrices of millions of data points. Triggered by the simple observation that each data point can be expressed as a convex combination of vertices of the data convex hull, we require the basic factors to be vertices of the data convex hull. The benefits of convex-hull NMF are twofold. First, for a growing number of data points the expected size of the convex hull, i.e. the number of its vertices, grows much slower than the dataset. Second, distance preserving low-dimensional embeddings allow us to efficiently sample the convex hull and hence to quickly determine candidate vertices. Our extensive experimental evaluation on large datasets shows that convex-hull NMF compares favorably to convex-NMF in terms of both speed and reconstruction quality. We demonstrate that our method can easily be applied to large-scale, real-world datasets, in our case consisting of 750,000 DBLP entries, 4,000,000 digital images, and 150,000,000 votes on World of Warcraft ®guilds, respectively.",2011,Knowledge and Information Systems volume 29 issue 2 pp 457-478,low rank approximation;matrix decomposition;data mining;machine learning;mathematical optimization;statistics;computer science;mathematics;
Discovering excitatory relationships using dynamic Bayesian networks,Debprakash Patnaik (Virginia Tech);Srivatsan Laxman (Microsoft);Naren Ramakrishnan (Virginia Tech);,"2140123976,2044569235,2199255697","Mining temporal network models from discrete event streams is an important problem with applications in computational neuroscience, physical plant diagnostics, and human–computer interaction modeling. In this paper, we introduce the notion of excitatory networks which are essentially temporal models where all connections are stimulative, rather than inhibitive. The emphasis on excitatory connections facilitates learning of network models by creating bridges to frequent episode mining. Specifically, we show that frequent episodes help identify nodes with high mutual information relationships and that such relationships can be summarized into a dynamic Bayesian network (DBN). This leads to an algorithm that is significantly faster than state-of-the-art methods for inferring DBNs, while simultaneously providing theoretical guarantees on network optimality. We demonstrate the advantages of our approach through an application in neuroscience, where we show how strong excitatory networks can be efficiently inferred from both mathematical models of spiking neurons and several real neuroscience datasets.",2011,Knowledge and Information Systems volume 29 issue 2 pp 273-303,variable order bayesian network;dynamic bayesian network;computational neuroscience;data mining;artificial intelligence;machine learning;statistics;computer science;
Multi-resolution boosting for classification and regression problems,Chandan K. Reddy (Wayne State University);Jin Hyeong Park (Siemens);,"2100435683,2562172152","Various forms of additive modeling techniques have been successfully used in many data mining and machine learning–related applications. In spite of their great success, boosting algorithms still suffer from a few open-ended problems that require closer investigation. The efficiency of any additive modeling technique relies significantly on the choice of the weak learners and the form of the loss function. In this paper, we propose a novel multi-resolution approach for choosing the weak learners during additive modeling. Our method applies insights from multi-resolution analysis and chooses the optimal learners at multiple resolutions during different iterations of the boosting algorithms, which are simple yet powerful additive modeling methods. We demonstrate the advantages of this novel framework in both classification and regression problems and show results on both synthetic and real-world datasets taken from the UCI machine learning repository. Though demonstrated specifically in the context of boosting algorithms, our framework can be easily accommodated in general additive modeling techniques. Similarities and distinctions of the proposed algorithm with the popularly used methods like radial basis function networks are also discussed.",2011,Knowledge and Information Systems volume 29 issue 2 pp 435-456,gradient boosting;gaussian function;boosting;regression;biological classification;data mining;pattern recognition;artificial intelligence;machine learning;statistics;computer science;mathematics;
Classifier-based acronym extraction for business documents,Pierre André Ménard (École de technologie supérieure);Sylvie Ratté (École de technologie supérieure);,"2644677280,1949072360","Acronym extraction for business documents has been neglected in favor of acronym extraction for biomedical documents. Although there are overlapping challenges, the semi-structured and non-predictive nature of business documents hinder the effectiveness of the extraction methods used on biomedical documents and fail to deliver the expected performance. A classifier-based extraction subsystem is presented as part of the wider project, Binocle, for the analysis of French business corpora. Explicit and implicit acronym presentation cases are identified using textual and syntactical hints. Among the 7 features extracted from each candidate instance, we introduce “similarity” features, which compare a candidate’s characteristics with average length-related values calculated from a generic acronym repository. Commonly used rules for evaluating the candidate (matching first letters, ordered instances, etc.) are scored and aggregated in a single composite feature that permits a supple classification. One hundred and thirty-eight French business documents from 14 public organizations were used for the training and evaluation corpora, yielding a recall of 90.9% at a precision level of 89.1% for a search space size of 3 sentences.",2011,Knowledge and Information Systems volume 29 issue 2 pp 305-334,furps;biological classification;information extraction;data science;information retrieval;data mining;database;machine learning;computer science;
Knowledge visualization for evaluation tasks,Joachim Baumeister (University of Würzburg);Martina Freiberg (University of Würzburg);,"2059066017,2132499636","Although various methods for the evaluation of intelligent systems have been proposed in the past, almost no techniques are present that support the manual inspection of knowledge bases by the domain specialist. Manual knowledge base inspection is an important and frequently applied method in knowledge engineering. Since it can hardly be performed in an automated manner, it is a time-consuming and costly task. In this paper, we discuss a collection of appropriate visualization techniques that help developers to interactively browse and analyze the knowledge base in order to find deficiencies and semantic errors in their implementation. We describe standard visualization methods adapted to specifically support the analysis of the static knowledge base structure, but also of the usage of knowledge base objects such as questions or solutions. Additionally, we introduce a novel visualization technique that supports the validation of the derivation and interview behavior of a knowledge system in a semi-automatic manner. The application of the presented methods was motivated by the daily practice of knowledge base development.",2011,Knowledge and Information Systems volume 29 issue 2 pp 349-378,open knowledge base connectivity;domain knowledge;verification;knowledge engineering;knowledge extraction;knowledge base;knowledge based systems;knowledge management;data mining;database;artificial intelligence;machine learning;simulation;computer science;
Layered hidden Markov models for real-time daily activity monitoring using body sensor networks,Hongxing Wei (Beihang University);Jin He (Ohio State University);Jindong Tan (Michigan Technological University);,"2673779877,2693544475,2142416891","This paper presents an inferring and training architecture for long-term and continuous daily activity monitoring using a wearable body sensor network. Energy efficiency and system adaptivity to wearers are two of the most important requirements of a body sensor network. This paper discusses a two-layered hidden Markov model (HMM) architecture for in-network data processing to achieve energy efficiency and model individualization. The bottom-layer HMM is used to process sensory data locally at each wireless sensor node to significantly reduce data transmissions. The top-layer HMM is utilized to find the activity sequence from the result of the local processing. This approach is energy efficient in that only the results of the decoding procedure in each node need to be transmitted rather than raw sensing data. Therefore, the volume of data are significantly reduced. When the algorithm is applied in online monitoring systems, the results of local processing are transmitted only upon hidden state changes. The top-layer processing uses “old data” of one sensor node when it does not receive a “new” result sequence of the local processing from that sensor node. The adaption to various wearers is also discussed, and the robustness of this classification system is depicted. Experiments of 19 activity sequences to be classified are taken by 5 subjects to evaluate the performance of this system.",2011,Knowledge and Information Systems volume 29 issue 2 pp 479-494,efficient energy use;biological classification;hidden markov model;data mining;real time computing;machine learning;simulation;computer science;
A general framework for designing a fuzzy rule-based classifier,Antanas Verikas (Halmstad University);Jonas Guzaitis (Kaunas University of Technology);Adas Gelzinis (Kaunas University of Technology);Marija Bacauskiene (Kaunas University of Technology);,"2680788459,2017991367,1866159138,78318780","This paper presents a general framework for designing a fuzzy rule-based classifier. Structure and parameters of the classifier are evolved through a two-stage genetic search. To reduce the search space, the classifier structure is constrained by a tree created using the evolving SOM tree algorithm. Salient input variables are specific for each fuzzy rule and are found during the genetic search process. It is shown through computer simulations of four real world problems that a large number of rules and input variables can be eliminated from the model without deteriorating the classification accuracy. By contrast, the classification accuracy of unseen data is increased due to the elimination.",2011,Knowledge and Information Systems volume 29 issue 1 pp 203-221,margin classifier;quadratic classifier;classifier;genetic algorithm;knowledge extraction;data mining;pattern recognition;machine learning;computer science;
Mining fastest path from trajectories with multiple destinations in road networks,Eric Hsueh-Chan Lu (National Cheng Kung University);Wang-Chien Lee (Pennsylvania State University);Vincent S. Tseng (National Cheng Kung University);,"2005838078,2143778659,2023431847","Nowadays, research on Intelligent Transportation System (ITS) has received many attentions due to its broad applications, such as path planning, which has become a common activity in our daily life. Besides, with the advances of Web 2.0 technologies, users are willing to share their trajectories, thus providing good resources for ITS applications. To the best of our knowledge, there is no study on the fastest path planning with multiple destinations in the literature. In this paper, we develop a novel framework, called Trajectory-based Path Finding (TPF), which is built upon a novel algorithm named Mining-based Algorithm for Travel time Evaluation (MATE) for evaluating the travel time of a navigation path and a novel index structure named Efficient Navigation Path Search Tree (ENS-Tree) for efficiently retrieving the fastest path. With MATE and ENS-tree, an efficient fastest path finding algorithm for single destination is derived. To find the path for multiple destinations, we propose a novel strategy named Cluster-Based Approximation Strategy (CBAS), to determine the fastest visiting order from specified multiple destinations. Through a comprehensive set of experiments, we evaluate the proposed techniques employed in the design of TPF and show that MATE, ENS-tree and CBAS produce excellent performance under various system conditions.",2011,Knowledge and Information Systems volume 29 issue 1 pp 25-53,intelligent transportation system;motion planning;trajectory;data mining;artificial intelligence;simulation;computer science;
Information retrieval with concept-based pseudo-relevance feedback in MEDLINE,Vahid Jalali (Amirkabir University of Technology);Mohammad Reza Matash Borujerdi (Amirkabir University of Technology);,"1986862157,1973751828","Although using domain specific knowledge sources for information retrieval yields more accurate results compared to pure keyword-based methods, more improvements can be achieved by considering both relations between concepts in an ontology and also their statistical dependencies over the corpus. In this paper, an innovative approach named concept-based pseudo-relevance feedback is introduced for improving accuracy of biomedical retrieval systems. Proposed method uses a hybrid retrieval algorithm for discovering relevancy between queries and documents which is based on a combination of keyword- and concept-based approaches. It also uses a pseudo-relevance feedback mechanism for expanding initial queries with auxiliary biomedical concepts extracted from top-ranked results of hybrid information retrieval. Using concept-based similarities makes it possible for the system to detect related documents to users’ queries, which are semantically close to each other while not necessarily sharing common keywords. In addition, expanding initial queries with concepts introduced by pseudo-relevance feedback captures those relations between queries and documents, which rely on statistical dependencies between concepts they contain. As a matter of fact, these relations may remain undetected, examining merely existing links between concepts in an external knowledge source. Proposed approach is evaluated using OHSUMED test collection and standard evaluation methods from text retrieval conference (TREC). Experimental results on MEDLINE documents (in OHSUMED collection) show 21% improvement over keyword-based approach in terms of mean average precision, which is a noticeable gain.",2011,Knowledge and Information Systems volume 29 issue 1 pp 237-248,information retrieval;data mining;database;computer science;
Rule induction for uncertain data,Biao Qin (Renmin University of China);Yuni Xia (Indiana University – Purdue University Indianapolis);Sunil Prabhakar (Purdue University);,"2671038787,2334154954,2119765128","Data uncertainty are common in real-world applications and it can be caused by many factors such as imprecise measurements, network latency, outdated sources and sampling errors. When mining knowledge from these applications, data uncertainty need to be handled with caution. Otherwise, unreliable or even wrong mining results would be obtained. In this paper, we propose a rule induction algorithm, called uRule, to learn rules from uncertain data. The key problem in learning rules is to efficiently identify the optimal cut points from training data. For uncertain numerical data, we propose an optimization mechanism which merges adjacent bins that have equal classifying class distribution and prove its soundness. For the uncertain categorical data, we also propose a new method to select cut points based on possible world semantics. We then present the uRule algorithm in detail. Our experimental results show that the uRule algorithm can generate rules from uncertain numerical data with potentially higher accuracies, and the proposed optimization method is effective in the cut point selection for both certain and uncertain numerical data. Furthermore, uRule has quite stable performance when mining uncertain categorical data.",2011,Knowledge and Information Systems volume 29 issue 1 pp 103-130,possible world;data mining;machine learning;algorithm;mathematics;
Efficient mining of all margin-closed itemsets with applications in temporal knowledge discovery and classification by compression,Fabian Moerchen (Siemens);Michael Thies;Alfred Ultsch (University of Marburg);,"1991190249,2555468260,1959477005","Margin-closed itemsets have previously been proposed as a subset of the closed itemsets with a minimum margin constraint on the difference in support to supersets. The constraint reduces redundancy in the set of reported patterns favoring longer, more specific patterns. A variety of patterns ranging from rare specific itemsets to frequent general itemsets is reported to support exploratory data analysis and understandable classification models. We present DCI_Margin, a new efficient algorithm that mines the complete set of margin-closed itemsets. We modified the DCI_Closed algorithm that has low memory requirements and can be parallelized. The margin constraint is checked on-the-fly reusing information already computed by DCI_Closed. We thoroughly analyzed the behavior on many datasets and show how other data mining algorithms can benefit from the redundancy reduction.",2011,Knowledge and Information Systems volume 29 issue 1 pp 55-80,compression;data mining;database;pattern recognition;mathematics;
Finding best evidence for evidence-based best practice recommendations in health care: the initial decision support system design,Nick Cercone (York University);Xiangdong An (York University);Jiye Li (York University);Zhenmei Gu (York University);Aijun An (York University);,"76047928,2405816937,2628747094,2306726033,2156668854","A major problem for Canadian health organizations is finding best evidence for evidence-based best practice recommendations. Medications are not always effectively used and misuse may harm patients. Drugs are the fastest-growing element of Canadian health care spending, second only to hospital spending. Three hundred million prescriptions are filled annually. Prescription drugs accounted for 5.8% of total health care spending in 1980 and close to 18% today. A primary long-term goal of this research is to develop a decision support system for evidence-based management, quality control and best practice recommendations for medical prescriptions. Our results will improve accessibility and management of information by: (1) building an prototype for adaptive information extraction, text and data mining from (online) documents to find evidence on which to base best practices; and (2) employing multiply sectioned Bayesian networks (MSBNs) to infer a probabilistic interpretation to validate evidence for recommendations; MSBNs provide this structure. Best practices to improve drug-related health outcomes; patients’ quality of life; and cost-effective use of medications by changing knowledge and behavior. This research will support next generation eHealth decision support systems, which routinely find and verify evidence from multiple sources, leading to cost-effective use of drugs, improve patients’ quality of life and optimize drug-related health outcomes.",2011,Knowledge and Information Systems volume 29 issue 1 pp 159-201,reduct;naive bayes classifier;association rule learning;information extraction;data science;knowledge management;data mining;database;computer science;
Merging local patterns using an evolutionary approach,Maria Cruz Gaya (European University of Madrid);J. Ignacio Giráldez (European University of Madrid);,"2132613856,2307006163","This paper describes a Decentralized Agent-based model for Theory Synthesis (DATS) implemented by MASETS, a Multi-Agent System for Evolutionary Theory Synthesis. The main contributions are the following: first, a method for the synthesis of a global theory from distributed local theories. Second, a conflict resolution mechanism, based on genetic algorithms, that deals with collision/contradictions in the knowledge discovered by different agents at their corresponding locations. Third, a system-level classification procedure that improves the results obtained from both: the monolithic classifier and the best local classifier. And fourth, a method for mining very large datasets that allows for divide-and-conquer mining followed by merging of discoveries. The model is validated with an experimental application run on 15 datasets. Results show that the global theory outperforms all the local theories, and the monolithic theory (obtained from mining the concatenation of all the available distributed data), in a statistically significant way.",2011,Knowledge and Information Systems volume 29 issue 1 pp 1-24,biological classification;genetic algorithm;multi agent system;data science;data mining;database;artificial intelligence;machine learning;computer science;
Product selection for promotion planning,"Yinghui Yang (University of California, Davis);Chunhui Hao (Chinese Academy of Sciences);","2139308806,2621523842","This paper addresses a very important question—how to select the right products to promote in order to maximize promotional benefit. We set up a framework to incorporate promotion decisions into the data-mining process, formulate the profit maximization problem as an optimization problem, and propose a heuristic search solution to discover the right products to promote. Moreover, we are able to get access to real supermarket data and apply our solution to help achieve higher profits. Our experimental results on both synthetic data and real supermarket data demonstrate that our framework and method are highly effective and can potentially bring huge profit gains to a marketing campaign.",2011,Knowledge and Information Systems volume 29 issue 1 pp 223-236,affinity analysis;data mining;computer science;
D-Search : an efficient and exact search algorithm for large distribution sets,Yasuko Matsubara (Kyoto University);Yasushi Sakurai (Nippon Telegraph and Telephone);Masatoshi Yoshikawa (Kyoto University);,"2106416586,1989929707,2307105024","Distribution data naturally arise in countless domains, such as meteorology, biology, geology, industry and economics. However, relatively little attention has been paid to data mining for large distribution sets. Given n distributions of multiple categories and a query distribution Q, we want to find similar clouds (i.e., distributions) to discover patterns, rules and outlier clouds. For example, consider the numerical case of sales of items, where, for each item sold, we record the unit price and quantity; then, each customer is represented as a distribution of 2-d points (one for each item he/she bought). We want to find similar users, e.g., for market segmentation or anomaly/fraud detection. We propose to address this problem and present D-Search, which includes fast and effective algorithms for similarity search in large distribution datasets. Our main contributions are (1) approximate KL divergence, which can speed up cloud-similarity computations, (2) multistep sequential scan, which efficiently prunes a significant number of search candidates and leads to a direct reduction in the search cost. We also introduce an extended version of D-Search: (3) time-series distribution mining, which finds similar subsequences in time-series distribution datasets. Extensive experiments on real multidimensional datasets show that our solution achieves a wall clock time up to 2,300 times faster than the naive implementation without sacrificing accuracy.",2011,Knowledge and Information Systems volume 29 issue 1 pp 131-157,likelihood function;kullback leibler divergence;singular value decomposition;combinatorics;data mining;database;machine learning;mathematical optimization;statistics;mathematics;
On ontology-driven document clustering using core semantic features,Samah Fodeh (Yale University);Bill Punch (Michigan State University);Pang Ning Tan (Michigan State University);,"2007437749,2005448271,2113230973","Incorporating semantic knowledge from an ontology into document clustering is an important but challenging problem. While numerous methods have been developed, the value of using such an ontology is still not clear. We show in this paper that an ontology can be used to greatly reduce the number of features needed to do document clustering. Our hypothesis is that polysemous and synonymous nouns are both relatively prevalent and fundamentally important for document cluster formation. We show that nouns can be efficiently identified in documents and that this alone provides improved clustering. We next show the importance of the polysemous and synonymous nouns in clustering and develop a unique approach that allows us to measure the information gain in disambiguating these nouns in an unsupervised learning setting. In so doing, we can identify a core subset of semantic features that represent a text corpus. Empirical results show that by using core semantic features for clustering, one can reduce the number of features by 90% or more and still produce clusters that capture the main themes in a text corpus.",2011,Knowledge and Information Systems volume 28 issue 2 pp 395-421,fuzzy clustering;dimensionality reduction;cluster analysis;document clustering;ontology;unsupervised learning;semantics;natural language processing;data mining;pattern recognition;machine learning;computer science;mathematics;
COAT: COnstraint-based anonymization of transactions,Grigorios Loukides (Vanderbilt University);Aris Gkoulalas-Divanis (IBM);Bradley Malin (Vanderbilt University);,"68021724,57288862,2145238237","Publishing transactional data about individuals in an anonymous form is increasingly required by organizations. Recent approaches ensure that potentially identifying information cannot be used to link published transactions to individuals’ identities. However, these approaches are inadequate to anonymize data that is both protected and practically useful in applications because they incorporate coarse privacy requirements, do not integrate utility requirements, and tend to explore a small portion of the solution space. In this paper, we propose the first approach for anonymizing transactional data under application-specific privacy and utility requirements. We model such requirements as constraints, investigate how these constraints can be specified, and propose COnstraint-based Anonymization of Transactions, an algorithm that anonymizes transactions using a flexible anonymization scheme to meet the specified constraints. Experiments with benchmark datasets verify that COAT significantly outperforms the current state-of-the-art algorithm in terms of data utility, while being comparable in terms of efficiency. Our approach is also shown to be effective in preserving both privacy and utility in a real-world scenario that requires disseminating patients’ information.",2011,Knowledge and Information Systems volume 28 issue 2 pp 251-282,transaction data;internet privacy;data mining;database;computer science;
An efficient graph-mining method for complicated and noisy data with real-world applications,Yi Jia (University of Kansas);Jintao Zhang (University of Kansas);Jun Huan (University of Kansas);,"2308242117,2309818640,2139058963","In this paper, we present a novel graph database-mining method called APGM (APproximate Graph Mining) to mine useful patterns from noisy graph database. In our method, we designed a general framework for modeling noisy distribution using a probability matrix and devised an efficient algorithm to identify approximate matched frequent subgraphs. We have used APGM to both synthetic data set and real-world data sets on protein structure pattern identification and structure classification. Our experimental study demonstrates the efficiency and efficacy of the proposed method.",2011,Knowledge and Information Systems volume 28 issue 2 pp 423-447,synthetic data;protein structure;theoretical computer science;data mining;machine learning;statistics;mathematics;
Statistical semantics for enhancing document clustering,Ahmed K. Farahat (University of Waterloo);Mohamed S. Kamel (University of Waterloo);,"2107050089,2096364092",Document clustering algorithms usually use vector space model (VSM) as their underlying model for document representation. VSM assumes that terms are independent and accordingly ignores any semantic relations between them. This results in mapping documents to a space where the proximity between document vectors does not reflect their true semantic similarity. This paper proposes new models for document representation that capture semantic similarity between documents based on measures of correlations between their terms. The paper uses the proposed models to enhance the effectiveness of different algorithms for document clustering. The proposed representation models define a corpus-specific semantic similarity by estimating measures of term–term correlations from the documents to be clustered. The corpus of documents accordingly defines a context in which semantic similarity is calculated. Experiments have been conducted on thirteen benchmark data sets to empirically evaluate the effectiveness of the proposed models and compare them to VSM and other well-known models for capturing semantic similarity.,2011,Knowledge and Information Systems volume 28 issue 2 pp 365-393,semantic compression;semantic computing;explicit semantic analysis;semantic integration;semantic similarity;vector space model;statistical semantics;document clustering;information retrieval;data mining;pattern recognition;computer science;
Fuzzy emerging patterns for classifying hard domains,"Milton García-Borroto (Unica Corporation);José Fco Martínez-Trinidad (National Institute of Astrophysics, Optics and Electronics);Jesús Ariel Carrasco-Ochoa (National Institute of Astrophysics, Optics and Electronics);","2309636950,2044182321,432174343","Emerging pattern–based classification is an ongoing branch in Pattern Recognition. However, despite its simplicity and accurate results, this classification includes an a priori discretization step that may degrade the classification accuracy. In this paper, we introduce fuzzy emerging patterns as an extension of emerging patterns to deal with numerical attributes using fuzzy discretization. Based on fuzzy emerging patterns, we propose a new classifier that uses a novel graph organization of patterns. The new classifier outperforms some popular and state of the art classifiers on several UCI repository databases. In a pairwise comparison, it significantly beats every other single classifier.",2011,Knowledge and Information Systems volume 28 issue 2 pp 473-489,data mining;pattern recognition;machine learning;computer science;
Sensor data analysis for equipment monitoring,Ana Cristina Bicharra Garcia (Federal Fluminense University);Cristiana Bentes (Rio de Janeiro State University);Rafael Heitor C. de Melo (Federal Fluminense University);Bianca Zadrozny (Federal Fluminense University);Thadeu J. P. Penna (National Institute of Standards and Technology);,"2171914995,2030911775,2657111106,54648453,2683969370","Sensors play a key role in modern industrial plant operations. Nevertheless, the information they provide is still underused. Extracting information from the raw data generated by the sensors is a complicated task, and it is usually used to help the operator react to undesired events, other than preventing them. This paper presents SDAEM (Sensor Data Analysis for Equipment Monitoring), an oil process plant monitoring model that covers three main goals: mining the sensor time series data to understand plant operation status and predict failures, interpreting correlated data from different sensors to verify sensors interdependence, and adjusting equipments working set points that leads to a more stable plant operation and avoids an excessive number of alarms. In addition, as time series data generated by sensors grow at an extremely fast rate, SDAEM uses parallel processing to provide real-time feedback. We have applied our model to monitor a process plant of a Brazilian offshore platform. Initial results were promising since some undesired events were recognized and operators adopted the tool to assist them finding good set points for the oil processing equipments.",2011,Knowledge and Information Systems volume 28 issue 2 pp 333-364,time series;parallel processing;computer security;data mining;real time computing;statistics;computer science;
An AHP-based approach toward enterprise architecture analysis based on enterprise architecture quality attributes,"Mahsa Razavi (Islamic Azad University, Science and Research Branch, Tehran);Fereidoon Shams Aliee (Shahid Beheshti University);Kambiz Badie;","2566726729,92113186,2509347522","Enterprise Architecture (EA) as a discipline that manages large amount of models and information about different aspects of the enterprise, can support decision making on enterprise-wide issues. In order to provide such support, EA information should be amenable to analysis of various utilities and quality attributes. In this regard, we have proposed the idea of characterizing and using enterprise architecture quality attributes. And this paper provides a quantitative AHP-based method toward expert-based EA analysis. Our method proposes a step-by-step process of assessing quality attribute achievement of different scenarios using AHP. By this method, most suitable EA scenarios are selected according to prioritized enterprise utilities and this selection has an important affect on decision making in enterprises. The proposed method also introduces a data structure that contains required information about quality attribute achievement of different EA scenarios in enterprises. The stored asset can be used for further decision making and progress assessment in future. Sensitivity analysis is also part of the process to identify sensitive points in the decision process. The applicability of the proposed method is demonstrated using a practical case study.",2011,Knowledge and Information Systems volume 28 issue 2 pp 449-472,functional software architecture;enterprise architecture management;enterprise architecture;analytic hierarchy process;enterprise information security architecture;enterprise information system;data structure;knowledge management;data mining;computer science;
Discovering frequent behaviors: time is an essential element of the context,Bashar Saleh (French Institute for Research in Computer Science and Automation);Florent Masseglia (French Institute for Research in Computer Science and Automation);,"2137017210,2440503352","One of the most popular problems in usage mining is the discovery of frequent behaviors. It relies on the extraction of frequent itemsets from usage databases. However, those databases are usually considered as a whole, and therefore, itemsets are extracted over the entire set of records. Our claim is that possible subsets, hidden within the structure of the data and containing relevant itemsets, may exist. These subsets, as well as the itemsets they contain, depend on the context. Time is an essential element of the context. The users’ intents will differ from one period to another. Behaviors over Christmas will be different from those extracted during the summer. Unfortunately, these periods might be lost because of arbitrary divisions of the data. The goal of our work is to find itemsets that are frequent over a specific period, but would not be extracted by traditional methods since their support is very low over the whole dataset. We introduce the definition of solid itemsets, which represent coherent and compact behaviors over specific periods, and we propose Sim, an algorithm for their extraction.",2011,Knowledge and Information Systems volume 28 issue 2 pp 311-331,period;world wide web;data mining;database;mathematics;
CAS- Mine : providing personalized services in context-aware applications by means of generalized rules,Elena Maria Baralis (Polytechnic University of Turin);Luca Cagliero (Polytechnic University of Turin);Tania Cerquitelli (Polytechnic University of Turin);Paolo Garza (Instituto Politécnico Nacional);M. Marchetti (Telecom Italia);,"1978690322,2079565741,102100071,2170193594,2295784931","Context-aware systems acquire and exploit information on the user context to tailor services to a particular user, place, time, and/or event. Hence, they allow service providers to adapt their services to actual user needs, by offering personalized services depending on the current user context. Service providers are usually interested in profiling users both to increase client satisfaction and to broaden the set of offered services. Novel and efficient techniques are needed to tailor service supply to the user (or the user category) and to the situation in which he/she is involved. This paper presents the CAS-Mine framework to efficiently discover relevant relationships between user context data and currently asked services for both user and service profiling. CAS-Mine efficiently extracts generalized association rules, which provide a high-level abstraction of both user habits and service characteristics depending on the context. A lazy (analyst-provided) taxonomy evaluation performed on different attributes (e.g., a geographic hierarchy on spatial coordinates, a classification of provided services) drives the rule generalization process. Extracted rules are classified into groups according to their semantic meaning and ranked by means of quality indices, thus allowing a domain expert to focus on the most relevant patterns. Experiments performed on three context-aware datasets, obtained by logging user requests and context information for three real applications, show the effectiveness and the efficiency of the CAS-Mine framework in mining different valuable types of correlations between user habits, context information, and provided services.",2011,Knowledge and Information Systems volume 28 issue 2 pp 283-310,computer user satisfaction;service delivery framework;user modeling;service provider;world wide web;data mining;database;computer science;
Special issue on “context-aware data mining (CADM)”,Chiara Renso (Istituto di Scienza e Tecnologie dell'Informazione);Vania Bogorny (Universidade Federal de Santa Catarina);Hui Xiong (Rutgers University);,"55173896,126682076,2153710278",-,2011,Knowledge and Information Systems volume 28 issue 2 pp 249-250,information retrieval;data mining;database;computer science;
The k -anonymity and l -diversity approaches for privacy preservation in social networks against neighborhood attacks,Bin Zhou (Simon Fraser University);Jian Pei (Simon Fraser University);,"2714597203,2126330539","Recently, more and more social network data have been published in one way or another. Preserving privacy in publishing social network data becomes an important concern. With some local knowledge about individuals in a social network, an adversary may attack the privacy of some victims easily. Unfortunately, most of the previous studies on privacy preservation data publishing can deal with relational data only, and cannot be applied to social network data. In this paper, we take an initiative toward preserving privacy in social network data. Specifically, we identify an essential type of privacy attacks: neighborhood attacks. If an adversary has some knowledge about the neighbors of a target victim and the relationship among the neighbors, the victim may be re-identified from a social network even if the victim’s identity is preserved using the conventional anonymization techniques. To protect privacy against neighborhood attacks, we extend the conventional k-anonymity and l-diversity models from relational data to social network data. We show that the problems of computing optimal k-anonymous and l-diverse social networks are NP-hard. We develop practical solutions to the problems. The empirical study indicates that the anonymized social network data by our methods can still be used to answer aggregate network queries with high accuracy.",2011,Knowledge and Information Systems volume 28 issue 1 pp 47-77,privacy software;relational database;empirical research;information privacy;computer security;data mining;database;artificial intelligence;computer science;
Clustering uncertain trajectories,Nikos Pelekis (University of Piraeus);Ioannis Kopanakis (Technological Educational Institute of Crete);Evangelos E. Kotsifakos (University of Piraeus);Elias Frentzos (University of Piraeus);Yannis Theodoridis (University of Piraeus);,"2020175353,94740397,329981208,2698289080,31261168","Knowledge discovery in Trajectory Databases (TD) is an emerging field which has recently gained great interest. On the other hand, the inherent presence of uncertainty in TD (e.g., due to GPS errors) has not been taken yet into account during the mining process. In this paper, we study the effect of uncertainty in TD clustering and introduce a three-step approach to deal with it. First, we propose an intuitionistic point vector representation of trajectories that encompasses the underlying uncertainty and introduce an effective distance metric to cope with uncertainty. Second, we devise CenTra, a novel algorithm which tackles the problem of discovering the Centroid Trajectory of a group of movements taking into advantage the local similarity between portions of trajectories. Third, we propose a variant of the Fuzzy C-Means (FCM) clustering algorithm, which embodies CenTra at its update procedure. Finally, we relax the vector representation of the Centroid Trajectories by introducing an algorithm that post-processes them, as such providing these mobility patterns to the analyst with a more intuitive representation. The experimental evaluation over synthetic and real world TD demonstrates the efficiency and effectiveness of our approach.",2011,Knowledge and Information Systems volume 28 issue 1 pp 117-147,metric;knowledge base;data mining;artificial intelligence;machine learning;algorithm;computer science;mathematics;
Generalized sparse metric learning with relative comparisons,Kaizhu Huang (Chinese Academy of Sciences);Yiming Ying (University of Exeter);Colin Campbell (University of Bristol);,"2142894599,2216095205,2291705727","The objective of sparse metric learning is to learn a distance measure from a set of data in addition to finding a low-dimensional representation. Despite demonstrated success, the performance of existing sparse metric learning approaches is usually limited because the methods assumes certain problem relaxations or they target the SML objective indirectly. In this paper, we propose a Generalized Sparse Metric Learning method. This novel framework offers a unified view for understanding many existing sparse metric learning algorithms including the Sparse Metric Learning framework proposed in (Rosales and Fung ACM International conference on knowledge discovery and data mining (KDD), pp 367–373, 2006), the Large Margin Nearest Neighbor (Weinberger et al. in Advances in neural information processing systems (NIPS), 2006; Weinberger and Saul in Proceedings of the twenty-fifth international conference on machine learning (ICML-2008), 2008), and the D-ranking Vector Machine (D-ranking VM) (Ouyang and Gray in Proceedings of the twenty-fifth international conference on machine learning (ICML-2008), 2008). Moreover, GSML also establishes a close relationship with the Pairwise Support Vector Machine (Vert et al. in BMC Bioinform, 8, 2007). Furthermore, the proposed framework is capable of extending many current non-sparse metric learning models to their sparse versions including Relevant Component Analysis (Bar-Hillel et al. in J Mach Learn Res, 6:937–965, 2005) and a state-of-the-art method proposed in (Xing et al. Advances in neural information processing systems (NIPS), 2002). We present the detailed framework, provide theoretical justifications, build various connections with other models, and propose an iterative optimization method, making the framework both theoretically important and practically scalable for medium or large datasets. Experimental results show that this generalized framework outperforms six state-of-the-art methods with higher accuracy and significantly smaller dimensionality for seven publicly available datasets.",2011,Knowledge and Information Systems volume 28 issue 1 pp 25-45,vector processor;sparse approximation;asynchronous communication;curse of dimensionality;support vector machine;metric;information system;iterative method;k nearest neighbors algorithm;systems modeling;information processing;data mining;artificial intelligence;machine learning;statistics;computer science;
Improving SVM classification on imbalanced time series data sets with ghost points,Suzan Köknar-Tezel (Temple University);Longin Jan Latecki (Temple University);,"1095099590,214878226","Imbalanced data sets present a particular challenge to the data mining community. Often, it is the rare event that is of interest and the cost of misclassifying the rare event is higher than misclassifying the usual event. When the data is highly skewed toward the usual, it can be very difficult for a learning system to accurately detect the rare event. There have been many approaches in recent years for handling imbalanced data sets, from under-sampling the majority class to adding synthetic points to the minority class in feature space. However, distances between time series are known to be non-Euclidean and non-metric, since comparing time series requires warping in time. This fact makes it impossible to apply standard methods like SMOTE to insert synthetic data points in feature spaces. We present an innovative approach that augments the minority class by adding synthetic points in distance spaces. We then use Support Vector Machines for classification. Our experimental results on standard time series show that our synthetic points significantly improve the classification rate of the rare events, and in most cases also improves the overall accuracy of SVMs. We also show how adding our synthetic points can aid in the visualization of time series data sets.",2011,Knowledge and Information Systems volume 28 issue 1 pp 1-23,visualization;support vector machine;time series;non euclidean geometry;data mining;pattern recognition;artificial intelligence;machine learning;statistics;computer science;mathematics;
Banded structure in binary matrices,Gemma C. Garriga (French Institute for Research in Computer Science and Automation);Esa Junttila (University of Helsinki);Heikki Mannila (Aalto University);,"698135892,2041457156,310734946","A binary matrix has a banded structure if both rows and columns can be permuted so that the non-zero entries exhibit a staircase pattern of overlapping rows. The concept of banded matrices has its origins in numerical analysis, where entries can be viewed as descriptions between the problem variables; the bandedness corresponds to variables that are coupled over short distances. Banded data occurs also in other applications, for example in the physical mapping problem of the human genome, in paleontological data, in network data and in the discovery of overlapping communities without cycles. We study the banded structure of binary matrices, give a formal definition of the concept and discuss its theoretical properties. We consider the algorithmic problems of computing how far a matrix is from being banded, and of finding a good submatrix of the original data that exhibits approximate bandedness. Finally, we show by experiments on real data from ecology and other applications the usefulness of the concept. Our results reveal that bands exist in real datasets and that the final obtained orderings of rows and columns have natural interpretations.",2011,Knowledge and Information Systems volume 28 issue 1 pp 197-226,band matrix;order theory;permutation;formal specification;electronic band structure;human genome;data transmission;numerical analysis;algorithmics;calculus;algorithm;computer science;mathematics;
Composite kernels for semi-supervised clustering,Carlotta Domeniconi (George Mason University);Jing Peng (Montclair State University);Bojun Yan (George Mason University);,"45678088,2140174629,2103112977","A critical problem related to kernel-based methods is how to select optimal kernels. A kernel function must conform to the learning target in order to obtain meaningful results. While solutions to the problem of estimating optimal kernel functions and corresponding parameters have been proposed in a supervised setting, it remains a challenge when no labeled data are available, and all we have is a set of pairwise must-link and cannot-link constraints. In this paper, we address the problem of optimizing the kernel function using pairwise constraints for semi-supervised clustering. We propose a new optimization criterion for automatically estimating the optimal parameters of composite Gaussian kernels, directly from the data and given constraints. We combine our proposal with a semi-supervised kernel-based algorithm to demonstrate experimentally the effectiveness of our approach. The results show that our method is very effective for kernel-based semi-supervised clustering.",2011,Knowledge and Information Systems volume 28 issue 1 pp 99-116,kernel embedding of distributions;tree kernel;variable kernel density estimation;polynomial kernel;string kernel;kernel smoother;radial basis function kernel;kernel method;kernel;supervised learning;machine learning;mathematical optimization;statistics;computer science;mathematics;
Single pass text classification by direct feature weighting,Hassan H. Malik (Thomson Reuters);Dmitriy Fradkin (Siemens);Fabian Moerchen (Siemens);,"2292051180,2028188512,1991190249","The Feature Weighting Classifier (FWC) is an efficient multi-class classification algorithm for text data that uses Information Gain to directly estimate per-class feature weights in the classifier. This classifier requires only a single pass over the dataset to compute the feature frequencies per class, is easy to implement, and has memory usage that is linear in the number of features. Results of experiments performed on 128 binary and multi-class text and web datasets show that FWC’s performance is at least comparable to, and often better than that of Naive Bayes, TWCNB, Winnow, Balanced Winnow and linear SVM. On a large-scale web dataset with 12,294 classes and 135,973 training instances, FWC trained in 13 s and yielded comparable classification performance to a state of the art multi-class SVM implementation, which took over 15 min to train.",2011,Knowledge and Information Systems volume 28 issue 1 pp 79-98,winnow;naive bayes classifier;multiclass classification;natural language;kullback leibler divergence;the internet;biological classification;content analysis;data mining;pattern recognition;machine learning;computer science;
Distance approximation techniques to reduce the dimensionality for multimedia databases,Yongkwon Kim (KAIST);Chin-Wan Chung (KAIST);Seok-Lyong Lee (Hankuk University of Foreign Studies);Deok-Hwan Kim (Inha University);,"2147578605,2107685843,2103622432,2100599789","Recently, databases have been used to store multimedia data such as images, maps, video clips, and music clips. In order to search them, they should be represented by various features, which are composed of high-dimensional vectors. As a result, the dimensionality of data is increased considerably, which causes ‘the curse of dimensionality’. The increase of data dimensionality causes poor performance of index structures. To overcome the problem, the research on the dimensionality reduction has been conducted. However, some reduction methods do not guarantee no false dismissal, while others incur high computational cost. This paper proposes dimensionality reduction techniques that guarantee no false dismissal while providing efficiency considerable by approximating distances with a few values. To provide the no false dismissal property, approximated distances should always be smaller than original distances. The Cauchy–Schwarz inequality and two trigonometrical equations are used as well as the dimension partitioning technique is applied to approximate distances in such a way to reduce the difference between the approximated distance and the original distance. As a result, the proposed techniques reduce the candidate set of a query result for efficient query processing.",2011,Knowledge and Information Systems volume 28 issue 1 pp 227-248,curse of dimensionality;dimensionality reduction;data structure;theoretical computer science;data mining;database;artificial intelligence;machine learning;statistics;algorithm;computer science;mathematics;
Record-level peculiarity-based data analysis and classifications,Jian Yang (Beijing University of Technology);Ning Zhong (Beijing University of Technology);Yiyu Yao (Beijing University of Technology);Jue Wang (Chinese Academy of Sciences);,"2136481912,2157949701,2134033583,2143671012","Peculiarity-oriented mining is a data mining method consisting of peculiar data identification and peculiar data analysis. Peculiarity factor and local peculiarity factor are important concepts employed to describe the peculiarity of a data point in the identification step. One can study the notions at both attribute and record levels. In this paper, a new record LPF called distance-based record LPF (D-record LPF) is proposed, which is defined as the sum of distances between a point and its nearest neighbors. The authors prove that D-record LPF can characterize the probability density of a continuous m-dimensional distribution accurately. This provides a theoretical basis for some existing distance-based anomaly detection techniques. More importantly, it also provides an effective method for describing the class-conditional probabilities in a Bayesian classifier. The result enables us to apply D-record LPF to solve classification problems. A novel algorithm called LPF-Bayes classifier and its kernelized implementation are proposed, which have some connection to the Bayesian classifier. Experimental results on several benchmark datasets demonstrate that the proposed classifiers are competitive to some excellent classifiers such as AdaBoost, support vector machines and kernel Fisher discriminant.",2011,Knowledge and Information Systems volume 28 issue 1 pp 149-173,bayes classifier;outlier;fisher information;kernel method;naive bayes classifier;conditional probability;support vector machine;k nearest neighbors algorithm;probability density function;data analysis;anomaly detection;supervised learning;pattern recognition;machine learning;statistics;computer science;mathematics;
Generational analysis of tension and entropy in data structures: impact on automatic data integration and on the semantic web,Eli Rohn (New Jersey Institute of Technology);,2197313197,"The move toward automatic data integration from autonomous and heterogeneous sources is viewed as a transition from a closed to an open system, which is in essence an adaptive information processing system. Data definition languages from various computing eras spanning almost 50 years to date are examined, assessing if they have moved from closed systems to open systems paradigm. The study proves that contemporary data definition languages are indistinguishable from older ones using measurements of Variety, Tension and Entropy, three characteristics of complex adaptive systems (CAS). The conclusion is that even contemporary data definition languages designed for such integration exhibit closed systems characteristics along with open systems aspirations only. Plenty of good will is insufficient to make them more suitable for automatic data integration than their oldest predecessors. A previous report and these new findings set the stage for the development and proposal of a mathematically sound data definition language based on CAS, thus potentially making it better suited for automatic data integration from autonomous heterogeneous sources.",2011,Knowledge and Information Systems volume 28 issue 1 pp 175-196,complex adaptive system;variety;closed system;open system;data structure;adaptive system;information processing;information theory;database;artificial intelligence;algorithm;computer science;
Content annotation for the semantic web: an automatic web-based approach,David Sánchez (Intel);David Isern (Intel);Miquel Millan;,"2263546791,1976511566,2107702945","Semantic Annotation is required to add machine-readable content to natural language text. A global initiative such as the Semantic Web directly depends on the annotation of massive amounts of textual Web resources. However, considering the amount of those resources, a manual semantic annotation of their contents is neither feasible nor scalable. In this paper we introduce a methodology to partially annotate textual content of Web resources in an automatic and unsupervised way. It uses several well-established learning techniques and heuristics to discover relevant entities in text and to associate them to classes of an input ontology by means of linguistic patterns. It also relies on the Web information distribution to assess the degree of semantic co-relation between entities and classes of the input domain ontology. Special efforts have been put in minimizing the amount of Web accesses required to evaluate entities in order to ensure the scalability of the approach. A manual evaluation has been carried out to test the methodology for several domains showing promising results.",2011,Knowledge and Information Systems volume 27 issue 3 pp 393-418,website parse template;semantic web stack;web modeling;social semantic web;semantic analytics;data web;web standards;semantic grid;owl s;semantic web rule language;entity relationship model;ontology;semantic web;web service;scalability;natural language;the internet;image retrieval;ontology;semantics;world wide web;information retrieval;data mining;database;computer science;
Short text clustering by finding core terms,Xingliang Ni (University of Science and Technology of China);Xiaojun Quan (City University of Hong Kong);Zhi Lu (City University of Hong Kong);Liu Wenyin (University of Science and Technology of China);Bei Hua (University of Science and Technology of China);,"2107341104,2105633990,2222089121,347798632,2659979365","A new clustering strategy, TermCut, is presented to cluster short text snippets by finding core terms in the corpus. We model the collection of short text snippets as a graph in which each vertex represents a piece of short text snippet and each weighted edge between two vertices measures the relationship between the two vertices. TermCut is then applied to recursively select a core term and bisect the graph such that the short text snippets in one part of the graph contain the term, whereas those snippets in the other part do not. We apply the proposed method on different types of short text snippets, including questions and search results. Experimental results show that the proposed method outperforms state-of-the-art clustering algorithms for clustering short text snippets.",2011,Knowledge and Information Systems volume 27 issue 3 pp 345-365,cluster analysis;document clustering;information retrieval;data mining;pattern recognition;machine learning;computer science;
User-centric query refinement and processing using granularity-based strategies,Yi Zeng (Beijing University of Technology);Ning Zhong (Beijing University of Technology);Yan Wang (Beijing University of Technology);Yulin Qin (Beijing University of Technology);Zhisheng Huang (VU University Amsterdam);Haiyan Zhou (Beijing University of Technology);Yiyu Yao (Beijing University of Technology);Frank van Harmelen (VU University Amsterdam);,"2685163904,2157949701,2543101278,2136011459,2141621278,2712590354,2134033583,333553561","Under the context of large-scale scientific literatures, this paper provides a user-centric approach for refining and processing incomplete or vague query based on cognitive- and granularity-based strategies. From the viewpoints of user interests retention and granular information processing, we examine various strategies for user-centric unification of search and reasoning. Inspired by the basic level for human problem-solving in cognitive science, we refine a query based on retained user interests. We bring the multi-level, multi-perspective strategies from human problem-solving to large-scale search and reasoning. The power/exponential law-based interests retention modeling, network statistics–based data selection, and ontology-supervised hierarchical reasoning are developed to implement these strategies. As an illustration, we investigate some case studies based on a large-scale scientific literature dataset, DBLP. The experimental results show that the proposed strategies are potentially effective.",2011,Knowledge and Information Systems volume 27 issue 3 pp 419-450,granularity;unification;grain size;power law;systems modeling;ontology;information processing;data mining;database;artificial intelligence;statistics;algorithm;computer science;
A system for relevance analysis of performance indicators in higher education using Bayesian networks,Antonio Fernández (University of Almería);María Morales (University of Almería);Carmelo Rodríguez (University of Almería);Antonio Salmerón (University of Almería);,"2143909263,2169773022,2343680203,2112611937","In this paper, we propose a methodology for relevance analysis of performance indicators in higher education based on the use of Bayesian networks. These graphical models provide, at first glance, a snapshot of the relevant relationships among the variables under consideration. We analyse the behaviour of the proposed methodology in a practical case, showing that it is a useful tool to help decision making when elaborating policies based on performance indicators. The methodology has been implemented in a software that interacts with the Elvira package for graphical models, and that is available to the administration board at the University of Almeria (Spain) through a web interface. The software also implements a new method for constructing composite indicators by using a Bayesian network regression model.",2011,Knowledge and Information Systems volume 27 issue 3 pp 327-344,bayesian network;decision support system;operations research;artificial intelligence;machine learning;computer science;
RP-Miner: a relaxed prune algorithm for frequent similar pattern mining,"Ansel Yoan Rodríguez-González (National Institute of Astrophysics, Optics and Electronics);José Francisco Martínez-Trinidad (National Institute of Astrophysics, Optics and Electronics);Jesús Ariel Carrasco-Ochoa (National Institute of Astrophysics, Optics and Electronics);José Ruiz-Shulcloper;","1982500294,1995606883,432174343,120811424","Most of the current algorithms for mining frequent patterns assume that two object subdescriptions are similar if they are equal, but in many real-world problems some other ways to evaluate the similarity are used. Recently, three algorithms (ObjectMiner, STreeDC-Miner and STreeNDC-Miner) for mining frequent patterns allowing similarity functions different from the equality have been proposed. For searching frequent patterns, ObjectMiner and STreeDC-Miner use a pruning property called Downward Closure property, which should be held by the similarity function. For similarity functions that do not meet this property, the STreeNDC-Miner algorithm was proposed. However, for searching frequent patterns, this algorithm explores all subsets of features, which could be very expensive. In this work, we propose a frequent similar pattern mining algorithm for similarity functions that do not meet the Downward Closure property, which is faster than STreeNDC-Miner and loses fewer frequent similar patterns than ObjectMiner and STreeDC-Miner. Also we show the quality of the set of frequent similar patterns computed by our algorithm with respect to the quality of the set of frequent similar patterns computed by the other algorithms, in a supervised classification context.",2011,Knowledge and Information Systems volume 27 issue 3 pp 451-471,closure;search algorithm;data analysis;data mining;machine learning;algorithm;computer science;mathematics;
Locality preserving multimodal discriminative learning for supervised feature selection,Zhao Zhang (Nanjing Forestry University);Ning Ye (Nanjing Forestry University);,"2645826295,2671378662","Feature selection has been an important preprocessing step in high-dimensional data analysis and pattern recognition. In this paper, we propose a locality preserving multimodal discriminative learning method called LPMDL for supervised feature selection, which arises by solving two standard eigenvalue problems and seeks to find a pair of optimal transformations for two sets of multivariate data in different classes. This topic can optimally discover the local structure information of the given data hided in the original space and aims at structuring an effective low-dimensional embedding space, under which LPMDL keeps nearby data pairs in the same class close and between-class data pairs apart, and the projections of the original data in different classes can be appropriately separated from each other. LPMDL can be performed either in the input space or the reproducing kernel Hilbert space which gives rise to the kernelized version of LPMDL. We also evaluate the feasibility and efficiency of the LPMDL approach by conducting extensive data visualization and classification tasks. Experimental results on a broad range of data sets show LPMDL tends to capture the intrinsic structure characteristics of the samples data due to the effective representation of the points and achieves similar or even better performance than the conventional PCA, NPE, LPP and LFDA methods.",2011,Knowledge and Information Systems volume 27 issue 3 pp 473-490,news;reproducing kernel hilbert space;discrimination learning;kernel method;hilbert space;principal component analysis;data visualization;supervised learning;feature selection;pattern recognition;artificial intelligence;machine learning;computer science;mathematics;
Pronouncibility index (Π): a distance-based and confusion-based speech quality measure for dysarthric speakers,Prakasith Kayasith (Thammasat University);Thanaruk Theeramunkong (Thammasat University);,"2502064518,202049940","Recently, there have been many modern speech technologies, including those of speech synthesis and recognition, developed to help people with disabilities. While most of such technologies have successfully been applied to process speech of normal speakers, they may not be effective for speakers with speech disorder, depending on their severity. This paper proposes an automated method to preliminarily assess the ability of a speaker in pronouncing a word. Based on signal features, an indicator called pronouncibility index (Π) is introduced to express speech quality with two complementary measures, called distance-based and confusion-based factors. In the distance-based factor, the 1-norm, 2-norm and 3-norm distance are investigated while boundary-based and Gaussian-based approaches are introduced for confusion-based factors. The Π is used to estimate performance of speech recognition when it is applied to recognize speech of a dysarthric speaker. Three measures are applied to evaluate the effectiveness of Π, rank-order inconsistency, correlation coefficient, and root-mean-square of difference. The evaluations had been done by comparing its predicted recognition rates with ones predicted by the standard methods called the articulatory and intelligibility tests based on the two recognition systems (HMM and ANN). For the phoneme-test set (the training set), Π outperforms the articulatory and intelligibility tests in all three evaluations. The performance of Π decreases for the device-control set (the test set), and the intelligibility test becomes the best method followed by Π and the articulatory test. In general, Π is a promising indicator for predicting recognition rate with comparison to the standard assessments.",2011,Knowledge and Information Systems volume 27 issue 3 pp 367-391,manner of articulation;sound quality;intelligibility;loudspeaker;root mean square;markov model;gaussian process;central nervous system;quality control;correlation;phonetics;speech synthesis;speech processing;hidden markov model;speech recognition;artificial intelligence;statistics;computer science;
Approximate data instance matching: a survey,Carina Friedrich Dorneles (Universidade Federal de Santa Catarina);Rodrigo Gonçalves (Universidade Federal de Santa Catarina);Ronaldo dos Santos Mello (Universidade Federal de Santa Catarina);,"2275791825,2492525119,2135226501","Approximate data matching is a central problem in several data management processes, such as data integration, data cleaning, approximate queries, similarity search and so on. An approximate matching process aims at defining whether two data represent the same real-world object. For atomic values (strings, dates, etc), similarity functions have been defined for several value domains (person names, addresses, and so on). For matching aggregated values, such as relational tuples and XML trees, approaches alternate from the definition of simple functions that combine values of similarity of record attributes to sophisticated techniques based on machine learning, for example. For complex data comparison, including structured and semistructured documents, existing approaches use both structure and data for the comparison, by either considering or not considering data semantics. This survey presents terminology and concepts that base approximated data matching, as well as discusses related work on the use of similarity functions in such a subject.",2011,Knowledge and Information Systems volume 27 issue 1 pp 1-21,name resolution;record linkage;complex data type;data integrity;data management;data mining;database;pattern recognition;machine learning;computer science;
Strategies for contextual reasoning with conflicts in ambient intelligence,Antonis Bikakis (University College London);Grigoris Antoniou (University of Huddersfield);Panayiotis Hasapis (Athens University of Economics and Business);,"1979583523,2048729876,2525720945","Ambient Intelligence environments host various agents that collect, process, change and share the available context information. The imperfect nature of context, the open and dynamic nature of such environments and the special characteristics of ambient agents have introduced new research challenges in the study of Distributed Artificial Intelligence. This paper proposes a solution based on the Multi-Context Systems paradigm, according to which local knowledge of ambient agents is encoded in rule theories (contexts), and information flow between agents is achieved through mapping rules that associate concepts used by different contexts. To resolve potential inconsistencies that may arise from the interaction of contexts through their mappings (global conflicts), we use a preference ordering on the system contexts, which may express the confidence that an agent has in the knowledge imported by other agents. On top of this model, we have developed four alternative strategies for global conflicts resolution, which mainly differ in the type and extent of context and preference information that is used to resolve potential conflicts. The four strategies have been respectively implemented in four versions of a distributed algorithm for query evaluation and evaluated in a simulated P2P system.",2011,Knowledge and Information Systems volume 27 issue 1 pp 45-84,ambient intelligence;information flow;system;conflict resolution;distributed algorithm;semantics;knowledge management;data mining;database;artificial intelligence;machine learning;computer science;
A general framework for measuring inconsistency through minimal inconsistent sets,Kedian Mu (Peking University);Weiru Liu (Queen's University Belfast);Zhi Jin (Peking University);,"2126191483,2148409606,2155874124","Hunter and Konieczny explored the relationships between measures of inconsistency for a belief base and the minimal inconsistent subsets of that belief base in several of their papers. In particular, an inconsistency value termed MIV C , defined from minimal inconsistent subsets, can be considered as a Shapley Inconsistency Value. Moreover, it can be axiomatized completely in terms of five simple axioms. MinInc, one of the five axioms, states that each minimal inconsistent set has the same amount of conflict. However, it conflicts with the intuition illustrated by the lottery paradox, which states that as the size of a minimal inconsistent belief base increases, the degree of inconsistency of that belief base becomes smaller. To address this, we present two kinds of revised inconsistency measures for a belief base from its minimal inconsistent subsets. Each of these measures considers the size of each minimal inconsistent subset as well as the number of minimal inconsistent subsets of a belief base. More specifically, we first present a vectorial measure to capture the inconsistency for a belief base, which is more discriminative than MIV C . Then we present a family of weighted inconsistency measures based on the vectorial inconsistency measure, which allow us to capture the inconsistency for a belief base in terms of a single numerical value as usual. We also show that each of the two kinds of revised inconsistency measures can be considered as a particular Shapley Inconsistency Value, and can be axiomatically characterized by the corresponding revised axioms presented in this paper.",2011,Knowledge and Information Systems volume 27 issue 1 pp 85-114,shapley value;dynamic inconsistency;discrete mathematics;mathematics;
Study of select items in different data sources by grouping,Animesh Adhikari (Smart Technologies);Pralhad Ramachandrarao (Goa University);Witold Pedrycz (University of Alberta);,"2311247618,1921169193,136859070","Many large organizations have multiple large databases as they transact from multiple branches. Many important decisions are based on a set of specific items called the select items. Thus, the analysis of select items in multiple databases is an important issue. For the purpose of studying select items in multiple databases, one might need true global patterns of select items. Thus, we propose a model of mining global patterns of select items from multiple databases. A measure of overall association between two items in a database is proposed. We have extended the proposed measure for a database whose transactions contain items along with the quantities purchased. We have designed an algorithm based on proposed measure for the purpose of grouping the frequent items in multiple databases. In addition, we have studied properties of different measures proposed in this paper. Experimental results are presented for both real and synthetic databases.",2011,Knowledge and Information Systems volume 27 issue 1 pp 23-43,item bank;information retrieval;data mining;database;computer science;
Architecturing large integrated complex information systems: an application to healthcare,Daniel Pascot (Laval University);Faouzi Bouslama (Laval University);Sehl Mellouli (Laval University);,"67249547,2042375407,2305567807","The global enterprise-wide approaches help organizations to model and understand the enterprise key components and their relationships and manage the organizations’ transformations and change. However, many of these approaches lack of insights into how to manage complexities related to the multitude of applications developed in silos such as the various systems in health organizations that were designed independently from each other. This paper contributes to the solutions addressing this issue by proposing a methodology and tools to create foundations based on key components to help develop the information architecture at the heart of the enterprise architecture that can guarantee the evolution of the organization. These core components are a set of reusable Field Actions representing the non-contextual persistent information, a common canonical Corporate Conceptual Data Model capturing all the vital data in the organization, and Views or sub-schema of this global data model that represent information for different stakeholders in the organization. To show the effectiveness of the proposed approach and to gain more insights into its practical value, the architecturing approach is applied in the healthcare domain to create the information architecture and the enterprise architecture for the Quebec healthcare network.",2011,Knowledge and Information Systems volume 27 issue 1 pp 115-140,functional software architecture;three schema approach;enterprise life cycle;information technology architecture;enterprise architecture management;enterprise architecture framework;business architecture;solution architecture;applications architecture;architecture domain;data architecture;enterprise architecture;view model;information architecture;service oriented modeling;reference architecture;business process;enterprise information security architecture;process modeling;enterprise information system;health informatics;management science;knowledge management;data mining;database;computer science;
"Extraction, selection and ranking of Field Association (FA) Terms from domain-specific corpora for building a comprehensive FA terms dictionary",Tshering Cigay Dorji (University of Tokushima);El-sayed Atlam (University of Tokushima);Susumu Yata (University of Tokushima);Masao Fuketa (University of Tokushima);Kazuhiro Morita (University of Tokushima);Jun-ichi Aoe (University of Tokushima);,"2498398867,2006150659,1986631373,2059996369,2171470526,2037412677","Field Association (FA) Terms—words or phrases that serve to identify document fields are effective in document classification, similar file retrieval and passage retrieval. But the problem lies in the lack of an effective method to extract and select relevant FA Terms to build a comprehensive dictionary of FA Terms. This paper presents a new method to extract, select and rank FA Terms from domain-specific corpora using part-of-speech (POS) pattern rules, corpora comparison and modified tf-idf weighting. Experimental evaluation on 21 fields using 306 MB of domain-specific corpora obtained from English Wikipedia dumps selected up to 2,517 FA Terms (single and compound) per field at precision and recall of 74–97 and 65–98. This is better than the traditional methods. The FA Terms dictionary constructed using this method achieved an average accuracy of 97.6% in identifying the fields of 10,077 test documents collected from Wikipedia, Reuters RCV1 corpus and 20 Newsgroup data set.",2011,Knowledge and Information Systems volume 27 issue 1 pp 141-161,part of speech;natural language processing;speech recognition;information retrieval;computer science;
"Location privacy: going beyond K-anonymity, cloaking and anonymizers",Ali Khoshgozaran (University of Southern California);Cyrus Shahabi (University of Southern California);Houtan Shirani-Mehr (University of Southern California);,"243340819,240820708,2115267903","With many location-based services, it is implicitly assumed that the location server receives actual users locations to respond to their spatial queries. Consequently, information customized to their locations, such as nearest points of interest can be provided. However, there is a major privacy concern over sharing such sensitive information with potentially malicious servers, jeopardizing users’ private information. The anonymity- and cloaking-based approaches proposed to address this problem cannot provide stringent privacy guarantees without incurring costly computation and communication overhead. Furthermore, they require a trusted intermediate anonymizer to protect user locations during query processing. This paper proposes a fundamental approach based on private information retrieval to process range and K-nearest neighbor queries, the prevalent queries used in many location-based services, with stronger privacy guarantees compared to those of the cloaking and anonymity approaches. We performed extensive experiments on both real-world and synthetic datasets to confirm the effectiveness of our approaches.",2011,Knowledge and Information Systems volume 26 issue 3 pp 435-465,location based service;private information retrieval;internet privacy;world wide web;data mining;database;computer science;
A two-stage gene selection scheme utilizing MRMR filter and GA wrapper,Ali El Akadi (Centre national de la recherche scientifique);Aouatif Amine (Centre national de la recherche scientifique);Abdeljalil El Ouardighi (Centre national de la recherche scientifique);Driss Aboutajdine (Centre national de la recherche scientifique);,"770574213,2100090592,2076967920,2473449644","Gene expression data usually contain a large number of genes, but a small number of samples. Feature selection for gene expression data aims at finding a set of genes that best discriminates biological samples of different types. In this paper, we propose a two-stage selection algorithm for genomic data by combining MRMR (Minimum Redundancy–Maximum Relevance) and GA (Genetic Algorithm). In the first stage, MRMR is used to filter noisy and redundant genes in high-dimensional microarray data. In the second stage, the GA uses the classifier accuracy as a fitness function to select the highly discriminating genes. The proposed method is tested for tumor classification on five open datasets: NCI, Lymphoma, Lung, Leukemia and Colon using Support Vector Machine (SVM) and Naive Bayes (NB) classifiers. The comparison of the MRMR-GA with MRMR filter and GA wrapper shows that our method is able to find the smallest gene subset that gives the most classification accuracy in leave-one-out cross-validation (LOOCV).",2011,Knowledge and Information Systems volume 26 issue 3 pp 487-500,bayes classifier;gene centered view of evolution;fitness function;microarray analysis techniques;naive bayes classifier;cross validation;support vector machine;genetic algorithm;feature selection;data mining;pattern recognition;machine learning;computer science;
An information gain-based approach for recommending useful product reviews,Richong Zhang (University of Ottawa);Thomas T. Tran (University of Ottawa);,"2300652393,2108359352","Recently, many e-commerce Web sites, such as Amazon.com, provide platforms for users to review products and share their opinions, in order to help consumers make their best purchase decisions. However, the quality and the level of helpfulness of different product reviews are not disclosed to consumers unless they carefully analyze an immense number of lengthy reviews. Considering the large amount of available online product reviews, this is an impossible task for any consumer. Therefore, it is of vital importance to develop recommender systems that can evaluate online product reviews effectively to recommend the most useful ones to consumers. This paper proposes an information gain-based model to predict the helpfulness of online product reviews, with the aim of suggesting the most suitable products and vendors to consumers. Reviews are analyzed and ranked by our scoring model and reviews that help consumers better than others will be found. In addition, we also compare our model with several machine learning algorithms. Our experimental results show that our approach is effective in ranking and classifying online product reviews.",2011,Knowledge and Information Systems volume 26 issue 3 pp 419-434,ranking;kullback leibler divergence;recommender system;world wide web;data mining;machine learning;statistics;computer science;
Counting triangles in real-world networks using projections,Charalampos E. Tsourakakis (Carnegie Mellon University);,750472553,"Triangle counting is an important problem in graph mining. Two frequently used metrics in complex network analysis that require the count of triangles are the clustering coefficients and the transitivity ratio of the graph. Triangles have been used successfully in several real-world applications, such as detection of spamming activity, uncovering the hidden thematic structure of the web and link recommendation in online social networks. Furthermore, the count of triangles is a frequently used network statistic in exponential random graph models. However, counting the number of triangles in a graph is computationally expensive. In this paper, we propose the EigenTriangle and EigenTriangleLocal algorithms to estimate the number of triangles in a graph. The efficiency of our algorithms is based on the special spectral properties of real-world networks, which allow us to approximate accurately the number of triangles. We verify the efficacy of our method experimentally in almost 160 experiments using several Web Graphs, social, co-authorship, information, and Internet networks where we obtain significant speedups with respect to a straightforward triangle counting algorithm. Furthermore, we propose an algorithm based on Fast SVD which allows us to apply the core idea of the EigenTriangle algorithm on graphs which do not fit in the main memory. The main idea is a simple node-sampling process according to which node i is selected with probability $${\frac{d_i}{2m}}$$ where d i is the degree of node i and m is the total number of edges in the graph. Our theoretical contributions also include a theorem that gives a closed formula for the number of triangles in Kronecker graphs, a model of networks which mimics several properties of real-world networks.",2011,Knowledge and Information Systems volume 26 issue 3 pp 501-520,nested triangles graph;simplex graph;strength of a graph;voltage graph;geometric graph theory;complement graph;graph bandwidth;coxeter graph;graph power;butterfly graph;null graph;clique width;cubic graph;random geometric graph;line graph;clustering coefficient;degree;null model;planar graph;complex network;singular value decomposition;random graph;network analysis;discrete mathematics;combinatorics;data mining;machine learning;statistics;computer science;mathematics;
Temporal relation co-clustering on directional social network and author-topic evolution,Wei Peng (Xerox);Tao Li (Florida International University);,"2110579051,2472069284","Analyzing three-way data has attracted a lot of attention recently because such data have intrinsic rich structures and naturally appear in many real-world applications. One typical type of three-way data is multiple two-way data/matrices with different time periods, for example, authors’ publication key terms and people’s email correspondence varying with the time. We propose to use the PARATUCKER model to analyze three-way data. The PARATUCKER model combines the axis capabilities of the Parafac model and the structural generality of the Tucker model and thus can be viewed as the combination of Tucker and Parafac. It does not require the symmetry of the data nor the same dimensionality of mode 1 and mode 2. However, no algorithms have been developed for fitting the PARATUCKER model, especially for obtaining non-negative solutions that are intuitive to understand and explain. In this paper, we propose TANPT: a three-way alternating non-negative algorithm to fit the PARATUCKER model. We apply the algorithm to temporal relation co-clustering on directional social network and author-topic evolution. Experiments on real-world datasets (DBLP and Enron Email datasets) demonstrate that our proposed algorithm achieves better clustering performance than other well-known methods and also discovers some interesting patterns.",2011,Knowledge and Information Systems volume 26 issue 3 pp 467-486,public key cryptography;social network;biclustering;data mining;database;artificial intelligence;machine learning;statistics;computer science;mathematics;
Symbolic data analysis tools for recommendation systems,Byron Leite Dantas Bezerra (Universidade de Pernambuco);Francisco de Assis Tenorio de Carvalho (Federal University of Pernambuco);,"2711637404,2614928430","Recommender systems have become an important tool to cope with the information overload problem by acquiring data about user behavior. After tracing the user’s behavior, through actions or rates, computational recommender systems use information- filtering techniques to recommend items. In order to recommend new items, one of the three major approaches is generally adopted: content-based filtering, collaborative filtering, or hybrid filtering. This paper presents three information-filtering methods, each of them based on one of these approaches. In our methods, the user profile is built up through symbolic data structures and the user and item correlations are computed through dissimilarity functions adapted from the symbolic data analysis (SDA) domain. The use of SDA tools has improved the performance of recommender systems, particularly concerning the find good items task measured by the half-life utility metric, when there is not much information about the user.",2011,Knowledge and Information Systems volume 26 issue 3 pp 385-418,information filtering system;cold start;symbolic data analysis;information overload;collaborative filtering;recommender system;world wide web;information retrieval;data mining;database;machine learning;computer science;
Inductive database languages: requirements and examples,Andrea Romei (University of Pisa);Franco Turini (University of Pisa);,"1240669206,2241902680","Inductive databases (IDBs) represent a database perspective on Knowledge discovery in databases (KDD). In an IDB, the KDD application can express both queries capable of accessing and manipulating data, and queries capable of generating, manipulating, and applying patterns allowing to formalize the notion of mining process. The feature that makes them different from other data mining applications is exactly the idea of looking at the support for knowledge discovery as an extension of the query process. This paper draws a list of desirable properties to be taken into account in the definition of an IDB framework. They involve several dimensions, such as the expressiveness of the language in representing data and models, the closure principle, the capability to provide a support for an efficient algorithm programming. These requirements are a basis for a comparative study that highlights strengths and weaknesses of existing IDB approaches. The paper focuses on the SQL-based ATLaS language/system, on the logic-based $${\mathcal{LDL}++}$$ language/system, and on the XML-based KDDML language/system.",2011,Knowledge and Information Systems volume 26 issue 3 pp 351-384,data definition language;association rule learning;knowledge extraction;information retrieval;data mining;database;computer science;
Statistical outlier detection using direct density ratio estimation,Shohei Hido (IBM);Yuta Tsuboi (IBM);Hisashi Kashima (IBM);Masashi Sugiyama (Tokyo Institute of Technology);Takafumi Kanamori (Nagoya University);,"1893736562,2159264924,2428201863,2130456104,2162369918","We propose a new statistical approach to the problem of inlier-based outlier detection, i.e., finding outliers in the test set based on the training set consisting only of inliers. Our key idea is to use the ratio of training and test data densities as an outlier score. This approach is expected to have better performance even in high-dimensional problems since methods for directly estimating the density ratio without going through density estimation are available. Among various density ratio estimation methods, we employ the method called unconstrained least-squares importance fitting (uLSIF) since it is equipped with natural cross-validation procedures, allowing us to objectively optimize the value of tuning parameters such as the regularization parameter and the kernel width. Furthermore, uLSIF offers a closed-form solution as well as a closed-form formula for the leave-one-out error, so it is computationally very efficient and is scalable to massive datasets. Simulations with benchmark and real-world datasets illustrate the usefulness of the proposed approach.",2011,Knowledge and Information Systems volume 26 issue 2 pp 309-336,multidimensional analysis;outlier;ratio estimator;cross validation;density estimation;curve fitting;systems modeling;least squares;data analysis;anomaly detection;econometrics;statistics;
Application of correlation coefficient to interval-valued intuitionistic fuzzy multiple attribute decision-making with incomplete weight information,Gui-wu Wei (Chongqing University);Hong-Jun Wang (Chongqing University);Rui Lin (Chongqing University);,"2125167772,2114161940,2162119888","With respect to multiple attribute decision-making problems with interval-valued intuitionistic fuzzy information, some operational laws of interval-valued intuitionistic fuzzy numbers, correlation and correlation coefficient of interval-valued intuitionistic fuzzy sets are introduced. An optimization model based on the negative ideal solution and max-min operator, by which the attribute weights can be determined, is established. We utilize the interval-valued intuitionistic fuzzy weighted averaging operator proposed by Xu (Control Decis 22(2):215–219, 2007) to aggregate the interval-valued intuitionistic fuzzy information corresponding to each alternative, and then rank the alternatives and select the most desirable one(s) according to the correlation coefficient. Finally, an illustrative example is given to verify the developed approach and to demonstrate its practicality and effectiveness.",2011,Knowledge and Information Systems volume 26 issue 2 pp 337-349,type 2 fuzzy sets and systems;fuzzy set operations;fuzzy number;intuitionistic logic;complete information;fuzzy logic;systems modeling;discrete mathematics;artificial intelligence;mathematical optimization;computer science;mathematics;
Keyword search in relational databases,Jaehui Park (Seoul National University);Sang-goo Lee (Seoul National University);,"2139797254,2126044724","This paper surveys research on enabling keyword search in relational databases. We present fundamental characteristics and discuss research dimensions, including data representation, ranking, efficient processing, query representation, and result presentation. Various approaches for developing the search system are described and compared within a common framework. We discuss the evolution of new research strategies to resolve the issues associated with probabilistic models, efficient top-k query processing, and schema analysis in relational databases.",2011,Knowledge and Information Systems volume 26 issue 2 pp 175-193,sargable;information schema;web search query;conjunctive query;query by example;query optimization;external data representation;relational database;statistical model;systems modeling;statistical relational learning;survey research;evolutionary algorithm;data processing;information retrieval;data mining;database;statistics;computer science;
Learning to detect spyware using end user license agreements,Niklas Lavesson (Blekinge Institute of Technology);Martin Boldt (Blekinge Institute of Technology);Paul Davidsson (Blekinge Institute of Technology);Andreas Jacobsson (Malmö University);,"1922797874,2106107285,2094453071,2126577075","The amount of software that hosts spyware has increased dramatically. To avoid legal repercussions, the vendors need to inform users about inclusion of spyware via end user license agreements (EULAs) during the installation of an application. However, this information is intentionally written in a way that is hard for users to comprehend. We investigate how to automatically discriminate between legitimate software and spyware associated software by mining EULAs. For this purpose, we compile a data set consisting of 996 EULAs out of which 9.6% are associated to spyware. We compare the performance of 17 learning algorithms with that of a baseline algorithm on two data sets based on a bag-of-words and a meta data model. The majority of learning algorithms significantly outperform the baseline regardless of which data representation is used. However, a non-parametric test indicates that bag-of-words is more suitable than the meta model. Our conclusion is that automatic EULA classification can be applied to assist users in making informed decisions about whether to install an application without having read the EULA. We therefore outline the design of a spyware prevention tool and suggest how to select suitable learning algorithms for the tool by using a multi-criteria evaluation approach.",2011,Knowledge and Information Systems volume 26 issue 2 pp 285-307,adware;metamodeling;world wide web;data mining;database;artificial intelligence;machine learning;computer science;
A knowledge encapsulation approach to ontology modularization,Faezeh Ensan (University of New Brunswick);Weichang Du (University of New Brunswick);,"344693850,2134667061","The development of monolithic ontologies for complex domains may face various challenges in reasoning and implementation. The notion of modularity can be employed for developing more efficient ontologies, especially in distributed environments. In this paper, we introduce a framework for developing ontologies in a modular manner. We describe the interface-based modular ontology formalism, (IBF), which theoretically supports the framework. The main feature of the framework is its support for knowledge encapsulation, i.e., it allows ontologies to define their main content using well-defined interfaces, such that their knowledge bases can only be accessed by other ontologies through these interfaces. An important implication of the proposed framework is that ontology modules can be developed completely independent of each other’s signature and languages. Such modules are free to only utilize the required knowledge segments of the others. We also investigate the issues of inconsistency in the proposed modular ontology framework. We provide solutions for isolating inconsistent ontology modules from the other parts of a modular ontology and also resolve inconsistencies which may be arisen by integrating consistent knowledge bases.",2011,Knowledge and Information Systems volume 26 issue 2 pp 249-283,ontology components;idef5;ontology based data integration;process ontology;upper ontology;ontology language;suggested upper merged ontology;description logic;ontology;ontology;knowledge base;knowledge management;data mining;database;artificial intelligence;computer science;
Evaluation of two heuristic approaches to solve the ontology meta-matching problem,Jorge Martinez-Gil (University of Málaga);José F. Aldana-Montes (University of Málaga);,"1566942672,2257021128","Nowadays many techniques and tools are available for addressing the ontology matching problem, however, the complex nature of this problem causes existing solutions to be unsatisfactory. This work aims to shed some light on a more flexible way of matching ontologies. Ontology meta-matching, which is a set of techniques to configure optimum ontology matching functions. In this sense, we propose two approaches to automatically solve the ontology meta-matching problem. The first one is called maximum similarity measure, which is based on a greedy strategy to compute efficiently the parameters which configure a composite matching algorithm. The second approach is called genetics for ontology alignments and is based on a genetic algorithm which scales better for a large number of atomic matching algorithms in the composite algorithm and is able to optimize the results of the matching process.",2011,Knowledge and Information Systems volume 26 issue 2 pp 225-247,ontology based data integration;3 dimensional matching;ontology alignment;knowledge engineering;ontology;data mining;artificial intelligence;algorithm;computer science;mathematics;
Supervised inductive learning with Lotka–Volterra derived models,Karen Hovsepian (Emory University);Peter Anselmo (New Mexico Institute of Mining and Technology);Subhasish Mazumdar (New Mexico Institute of Mining and Technology);,"2643312124,305358035,2215170169","We present a classification algorithm built on our adaptation of the Generalized Lotka–Volterra model, well-known in mathematical ecology. The training algorithm itself consists only of computing several scalars, per each training vector, using a single global user parameter and then solving a linear system of equations. Construction of the system matrix is driven by our model and based on kernel functions. The model allows an interesting point of view of kernels’ role in the inductive learning process. We describe the model through axiomatic postulates. Finally, we present the results of the preliminary validation experiments.",2011,Knowledge and Information Systems volume 26 issue 2 pp 195-223,generalizability theory;axiom;kernel method;kernel;system of linear equations;systems modeling;biological classification;data analysis;supervised learning;artificial intelligence;machine learning;algorithm;computer science;mathematics;
D2S: Document-to-sentence framework for novelty detection,Flora S. Tsai (Nanyang Technological University);Yi Zhang (Nanyang Technological University);,"2159372297,2683528774","Novelty detection aims at identifying novel information from an incoming stream of documents. In this paper, we propose a new framework for document-level novelty detection using document-to-sentence (D2S) annotations and discuss the applicability of this method. D2S first segments a document into sentences, determines the novelty of each sentence, then computes the document-level novelty score based on a fixed threshold. Experimental results on APWSJ data show that D2S outperforms standard document-level novelty detection in terms of redundancy-precision (RP) and redundancy-recall (RR). We applied D2S on the document-level data from the TREC 2004 and TREC 2003 Novelty Track and find that D2S is useful in detecting novel information in data with a high percentage of novel documents. However, D2S shows a strong capability to detect redundant information regardless of the percentage of novel documents. D2S has been successfully integrated in a real-world novelty detection system.",2011,Knowledge and Information Systems volume 29 issue 2 pp 419-433,redundancy;text mining;information retrieval;data mining;pattern recognition;computer science;
Measuring gene similarity by means of the classification distance,Elena Maria Baralis (Polytechnic University of Turin);Giulia Bruno (Polytechnic University of Turin);Alessandro Fiori (Polytechnic University of Turin);,"1978690322,2108508946,2150389794","Microarray technology provides a simple way for collecting huge amounts of data on the expression level of thousands of genes. Detecting similarities among genes is a fundamental task, both to discover previously unknown gene functions and to focus the analysis on a limited set of genes rather than on thousands of genes. Similarity between genes is usually evaluated by analyzing their expression values. However, when additional information is available (e.g., clinical information), it may be beneficial to exploit it. In this paper, we present a new similarity measure for genes, based on their classification power, i.e., on their capability to separate samples belonging to different classes. Our method exploits a new gene representation that measures the classification power of each gene and defines the classification distance as the distance between gene classification powers. The classification distance measure has been integrated in a hierarchical clustering algorithm, but it may be adopted also by other clustering algorithms. The result of experiments runs on different microarray datasets supports the intuition of the proposed approach.",2011,Knowledge and Information Systems volume 29 issue 1 pp 81-101,gene chip analysis;microarray;cluster analysis;bioinformatics;data mining;pattern recognition;machine learning;computer science;mathematics;
Methods for mining frequent items in data streams: an overview,Hongyan Liu (Tsinghua University);Yuan Lin (University of Washington);Jiawei Han (University of Illinois at Urbana–Champaign);,"2618347989,2587762026,2121939561","In many real-world applications, information such as web click data, stock ticker data, sensor network data, phone call records, and traffic monitoring data appear in the form of data streams. Online monitoring of data streams has emerged as an important research undertaking. Estimating the frequency of the items on these streams is an important aggregation and summary technique for both stream mining and data management systems with a broad range of applications. This paper reviews the state-of-the-art progress on methods of identifying frequent items from data streams. It describes different kinds of models for frequent items mining task. For general models such as cash register and Turnstile, we classify existing algorithms into sampling-based, counting-based, and hashing-based categories. The processing techniques and data synopsis structure of each algorithm are described and compared by evaluation measures. Accordingly, as an extension of the general data stream model, four more specific models including time-sensitive model, distributed model, hierarchical and multi-dimensional model, and skewed data model are introduced. The characteristics and limitations of the algorithms of each model are presented, and open issues waiting for study and improvement are discussed.",2011,Knowledge and Information Systems volume 26 issue 1 pp 1-30,counting;active traffic management;hash function;data modeling;skewness;hierarchical control system;sensor array;wireless sensor network;data model;the internet;systems modeling;data analysis;sampling;data stream mining;data processing;world wide web;data mining;database;computer science;
Schema-based Web wrapping,Bettina Fazzinga (University of Calabria);Sergio Flesca (University of Calabria);Andrea Tagarelli (University of Calabria);,"23014645,2071564828,273425128","An effective solution to automate information extraction from Web pages is represented by wrappers. A wrapper associates a Web page with an XML document that represents part of the information in that page in a machine-readable format. Most existing wrapping approaches have traditionally focused on how to generate extraction rules, while they have ignored potential benefits deriving from the use of the schema of the information being extracted in the wrapper evaluation. In this paper, we investigate how the schema of extracted information can be effectively used in both the design and evaluation of a Web wrapper. We define a clean declarative semantics for schema-based wrappers by introducing the notion of (preferred) extraction model, which is essential to compute a valid XML document containing the information extracted from a Web page. We developed the SCRAP (SChema-based wRAPper for web data) system for the proposed schema-based wrapping approach, which also provides visual support tools to the wrapper designer. Moreover, we present a wrapper generalization framework to profitably speed up the design of schema-based wrappers. Experimental evaluation has shown that SCRAP wrappers are not only able to successfully extract the required data, but also they are robust to changes that may occur in the source Web pages.",2011,Knowledge and Information Systems volume 26 issue 1 pp 127-173,xml schema editor;document structure description;xml;knowledge extraction;information extraction;semantics;world wide web;data mining;database;computer science;
Online learning with social computing based interest sharing,Dennis Muhlestein (Utah State University);SeungJin Lim (Marshall University);,"2091450897,2109819818","Communities on the Internet are highly self-organizing, dynamic, and ubiquitous. One objective of peers in such a community is sharing common interests, even when compromising privacy. This paper presents a model for peers on the Internet that allows them to discover their common interests in terms of sets of frequently visited URLs. This model assists online learning by automatically presenting users with URLs related to what they are currently browsing, thus saving users’ time searching for additional information and helping to educate them on the current topic. To implement the model and collect test data, FireShare was developed as a plugin for the popular Web browser Firefox. Data was collected and analyzed on the number of discovered frequently visited URL sets, relevancy of mined association rules, and the overhead FireShare imposes on a network. While FireShare favorably validated the proposed model, analysis of the submitted test data shows high potential for success with future versions.",2011,Knowledge and Information Systems volume 26 issue 1 pp 31-58,news;social learning;confidentiality;social network;social computing;data science;world wide web;data mining;database;artificial intelligence;machine learning;computer science;
A decentralized search engine for dynamic Web communities,Daze Wang (University of Sydney);Quincy Chi Kwan Tse (University of Sydney);Ying Zhou (University of Sydney);,"2689688448,2067154876,2236630750","Currently, most Web search engines perform search on corpus comprising nearly entire content of the Web. The same centralized search service can be performed on a single site as well. Nonetheless, there is little research on community-wide search. This paper presents a peer-to-peer search engine ComSearch. ComSearch is designed to provide small- and middle-scale online communities—the ability to perform text search within the community. Communities are formed in a self-organizing style. P2P IR system may suffer unnecessary internal traffic in answering a multi-term query. In this paper, we propose several techniques to optimize the multi-term query process. The simulation results show that our proposed algorithms have good scalability. Compared with baseline approach, our improved algorithm can reduce the communication cost by about two orders of magnitude in the best case. We also deploy the system in a small-scale network and conduct a series of experiments to estimate the actual query response time as well as to investigate the data movement effect caused by node joining. Experimental results show that multiple data movements are quite common during network expansion. However, the percentage of multiple data movements decreases when a network is getting stable after the initial frequent joining activities. This provides possibilities for improvement on P2P data movement management.",2011,Knowledge and Information Systems volume 26 issue 1 pp 105-125,search analytics;beam search;web query classification;bloom filter;peer to peer;chord;query expansion;hash function;response time;scalability;search engine;self organization;the internet;social network;dynamical system;web search engine;world wide web;data mining;database;artificial intelligence;simulation;computer science;
Association patterns for data modeling and definition,Bryon K. Ehlmann (Southern Illinois University Edwardsville);,1321433975,"Association patterns provide guidance for modeling the associations that occur among objects within both the real world and the solution domains of computer applications. The patterns help the designer better understand and more precisely define the semantics of these associations, which allows them to be more easily and properly implemented. This paper describes a number of association patterns using Object Relationship Notation (ORN) and by doing so provides evidence for the effectiveness of this notation. It also shows how the development of database systems can be improved by an approach that uses association patterns to build a database model and then implements the model by mapping it to an ORN-extended database definition that is supported by a DBMS. The feasibility of this approach and the applicability of our association patterns have been validated by DBMS research prototypes and by the modeling, implementing, and testing of numerous associations.",2011,Knowledge and Information Systems volume 26 issue 1 pp 59-86,behavioral pattern;data modeling;unified modeling language;semantics;data mining;database;algorithm;computer science;
Restrictive partially blind signature for resource-constrained information systems,Weidong Qiu (Shanghai Jiao Tong University);Zheng Gong (University of Twente);Bozhong Liu (Shanghai Jiao Tong University);Yu Long (Shanghai Jiao Tong University);Kefei Chen (Shanghai Jiao Tong University);,"2232736356,2677335848,2121960048,2133266373,2120227044","Restrictive partially blind signature, which is designed for privacy-oriented information systems, allows a user to obtain a blind signature from a signer while the blind message must obey some certain rules. In order to reduce storage and communication costs, several public-key cryptosystems are constructed using characteristic sequences generated by linear feedback shift register (LFSR). In this paper, we present a new partially blind signature scheme with the restrictive property, which is based on nth order characteristic sequences generated by LFSR. By assuming the intractability of the discrete logarithm problem, our sequence-based schemes are provably secure in the random oracle model. We also present a practical e-cash application based on our restrictive partially blind signature. Due to the reduced representation of finite field elements and feasible sequence operations from LFSR, our scheme is time- and storage-efficient on both of signer and user sides. The advantages will make privacy-oriented applications more practical for resource-constrained devices.",2011,Knowledge and Information Systems volume 26 issue 1 pp 87-103,schnorr signature;blind signature;discrete logarithm;resource management;telecommunications;computer security;algorithm;computer science;mathematics;
Rule-based composite event queries: the language XChange EQ and its semantics,Michael Eckert (Ludwig Maximilian University of Munich);François Bry (Ludwig Maximilian University of Munich);,"2121111018,870525885","Web systems, Web services, and Web-based publish/subscribe systems communicate events as XML messages and in many cases, require composite event detection: it is not sufficient to react to single event messages, but events have to be considered in relation to other events that are received over time. This entails a need for expressive, high-level languages for querying composite events. Emphasizing language design and formal semantics, we describe the rule-based composite event query language XChangeEQ. XChangeEQ is designed to completely cover and integrate the four complementary querying dimensions: event data, event composition, temporal relationships, and event accumulation. Semantics are provided as a model theory with accompanying fixpoint theory, an approach that is established for rule languages but has not been applied to event queries so far. Because they are highly declarative, thus easy to understand and well suited for query optimization, such semantics are desirable for event queries.",2010,Knowledge and Information Systems volume 25 issue 3 pp 551-573,complex event processing;temporal database;formal semantics;high level programming language;web service;query language;xml;reactive system;formal language;systems modeling;expert system;data mining;database;programming language;computer science;
Extension of TOPSIS method for 2-tuple linguistic multiple attribute group decision making with incomplete weight information,Gui-Wu Wei (Chongqing University);,2628020716,"With respect to linguistic multiple attribute group decision making problems with incomplete weight information, a new method is proposed. In the method, the 2-tuple linguistic representation developed in recent years is used to aggregate the linguistic assessment information. In order to get the weight vector of the attribute, we establish an optimization model based on the basic ideal of traditional technique for order performance by similarity to ideal solution, by which the attribute weights can be determined. Then, the optimal alternative(s) is determined by calculating the shortest distance from the 2-tuple linguistic positive ideal solution, and on the other side, the farthest distance of the 2-tuple linguistic negative ideal solution. The method has exact characteristic in linguistic information processing. It avoided information distortion and losing, which occur formerly in the linguistic information processing. Finally, a numerical example is used to illustrate the use of the proposed method. The result shows the approach is simple, effective, and easy to calculate.",2010,Knowledge and Information Systems volume 25 issue 3 pp 623-634,group decision making;method of characteristics;complete information;systems modeling;information processing;artificial intelligence;machine learning;algorithm;mathematics;
Short text similarity based on probabilistic topics,Xiaojun Quan (City University of Hong Kong);Gang Liu (City University of Hong Kong);Zhi Lu (City University of Hong Kong);Xingliang Ni (University of Science and Technology of China);Liu Wenyin (City University of Hong Kong);,"2105633990,2305641634,2222089121,2107341104,347798632","In this paper, we propose a new method for measuring the similarity between two short text snippets by comparing each of them with the probabilistic topics. Specifically, our method starts by firstly finding the distinguishing terms between the two short text snippets and comparing them with a series of probabilistic topics, extracted by Gibbs sampling algorithm. The relationship between the distinguishing terms of the short text snippets can be discovered by examining their probabilities under each topic. The similarity between two short text snippets is calculated based on their common terms and the relationship of their distinguishing terms. Extensive experiments on paraphrasing and question categorization show that the proposed method can calculate the similarity of short text snippets more accurately than other methods including the pure TF-IDF measure.",2010,Knowledge and Information Systems volume 25 issue 3 pp 473-491,news;query expansion;gibbs sampling;categorization;similitude;similarity;metric;question answering;text mining;information retrieval;data mining;pattern recognition;computer science;mathematics;
Ontology-based multiperspective requirements traceability framework,Namfon Assawamekin (Mahidol University);Thanwadee Sunetnanta (Mahidol University);Charnyote Pluempitiwiriyawej (Mahidol University);,"221000721,2086629781,1731305290","Large-scaled software development inevitably involves a group of stakeholders, each of whom may express their requirements differently in their own terminology and representation depending on their perspectives or perceptions of their shared problems. In view of that, the heterogeneity must be well handled and resolved in tracing and managing changes of such requirements. This paper presents our multiperspective requirements traceability (MUPRET) framework which deploys ontology as a knowledge management mechanism to intervene mutual “understanding” without restricting the freedom in expressing requirements differently. Ontology matching is applied as a reasoning mechanism in automatically generating traceability relationships. The relationships are identified by deriving semantic analogy of ontology concepts representing requirements elements. The precision and recall of traceability relationships generated by the framework are verified by comparing with a set of traceability relationships manually identified by users as a proof-of-concept of this framework.",2010,Knowledge and Information Systems volume 25 issue 3 pp 493-522,traceability matrix;requirements traceability;traceability;tracing;proof of concept;software development;heterogeneity;terminology;knowledge engineering;ontology;semantics;data mining;database;computer science;
Efficient monitoring of skyline queries over distributed data streams,Shengli Sun (Peking University);Zhenghua Huang (Tongji University);Hao Zhong (Chinese Academy of Sciences);Dongbo Dai (Fudan University);Hongbin Liu (State Grid Corporation of China);Jinjiu Li;,"2318646501,2314454490,2425534787,2120615122,2311849347,2700799351","Data management and data mining over distributed data streams have received considerable attention within the database community recently. This paper is the first work to address skyline queries over distributed data streams, where streams derive from multiple horizontally split data sources. Skyline query returns a set of interesting objects which are not dominated by any other objects within the base dataset. Previous work is concentrated on skyline computations over static data or centralized data streams. We present an efficient and an effective algorithm called BOCS to handle this issue under a more challenging environment of distributed streams. BOCS consists of an efficient centralized algorithm GridSky and an associated communication protocol. Based on the strategy of progressive refinement in BOCS, the skyline is incrementally computed by two phases. In the first phase, local skylines on remote sites are maintained by GridSky. At each time, only skyline increments on remote sites are sent to the coordinator. In the second phase, a global skyline is obtained by integrating remote increments with the latest global skyline. A theoretical analysis shows that BOCS is communication-optimal among all algorithms which use a share-nothing strategy. Extensive experiments demonstrate that our proposals are efficient, scalable, and stable.",2010,Knowledge and Information Systems volume 25 issue 3 pp 575-606,cielo;sky;multiplicity;pareto distribution;communications protocol;data analysis;distributed database;data management;world wide web;data mining;database;computer science;
"Immune algorithm with orthogonal design based initialization, cloning, and selection for global optimization",Maoguo Gong (Xidian University);Licheng Jiao (Xidian University);Fang Liu (Xidian University);Wenping Ma (Xidian University);,"2139563917,2696268999,2421134446,2208751570","In this study, an orthogonal immune algorithm (OIA) is proposed for global optimization by incorporating orthogonal initialization, a novel neighborhood orthogonal cloning operator, a static hypermutation operator, and a novel diversity-based selection operator. The orthogonal initialization scans the feasible solution space once to locate good points for further exploration in subsequent iterations. Meanwhile, each row of the orthogonal array defines a sub-domain. The neighborhood orthogonal cloning operator uses orthogonal arrays to scan uniformly the neighborhood around each antibody. Then the new algorithm explores each clone by using hypermutation. The improved maturated progenies are selectively added to an external population by the diversity-based selection, which retains one and only one external antibody in each sub-domain. The OIA is unique in three aspects: First, a new selection method based on orthogonal arrays is provided in order to preserve diversity in the population. Second, the orthogonal design with a modified quantization technique is introduced to generate initial population. Third, the orthogonal design is introduced into the cloning operator. The performance comparisons of OIA with two known immune algorithms and three evolutionary algorithms in optimizing eight benchmark functions and six composition functions indicate that OIA is an effective algorithm for solving global optimization problems.",2010,Knowledge and Information Systems volume 25 issue 3 pp 523-549,orthogonal array;domain decomposition methods;global optimum;initialization;clone;principle of orthogonal design;immune system;evolutionary algorithm;global optimization;artificial intelligence;mathematical optimization;algorithm;computer science;mathematics;
Spectral clustering in multi-agent systems,Balint Takacs (Imperial College London);Yiannis Demiris (Imperial College London);,"2128649641,250698822","We examine the application of spectral clustering for breaking up the behavior of a multi-agent system in space and time into smaller, independent elements. We propose clustering observations of individual entities in order to identify significant changes in the parameter space (like spatial position) and detect temporal alterations of behavior within the same framework. Available knowledge of important interactions (events) between entities is also considered. We describe a novel algorithm utilizing iterative subdivisions where clusters are pre-processed at each step to counter spatial scaling, rotation, replay speed, and varying sampling frequency. A method is presented to balance spatial and temporal segmentation based on the expected group size, and a validity measure is introduced to determine the optimal number of clusters. We demonstrate our results by analyzing the outcomes of computer games and compare our algorithm to K-means and traditional spectral clustering.",2010,Knowledge and Information Systems volume 25 issue 3 pp 607-622,k medians clustering;canopy clustering algorithm;correlation clustering;constrained clustering;data stream clustering;cure data clustering algorithm;fuzzy clustering;k means clustering;cluster analysis;multi agent system;artificial intelligence;machine learning;algorithm;computer science;mathematics;
Ontological reasoning for improving the treatment of emotions in text,Virginia Francisco (Complutense University of Madrid);Pablo Gervás (Complutense University of Madrid);Federico Peinado (Complutense University of Madrid);,"2156667208,184266003,2123484986","With the advent of affective computing, the task of adequately identifying, representing and processing the emotional connotations of text has acquired importance. Two problems facing this task are addressed in this paper: the composition of sentence emotion from word emotion, and a representation of emotion that allows easy conversion between existing computational representations. The emotion of a sentence of text should be derived by composition of the emotions of the words in the sentence, but no method has been proposed so far to model this compositionality. Of the various existing approaches for representing emotions, some are better suited for some problems and some for others, but there is no easy way of converting from one to another. This paper presents a system that addresses these two problems by reasoning with two ontologies implemented with Semantic Web technologies: one designed to represent word dependency relations within a sentence, and one designed to represent emotions. The ontology of word dependency relies on roles to represent the way emotional contributions project over word dependencies. By applying automated classification of mark-up results in terms of the emotion ontology the system can interpret unrestricted input in terms of a restricted set of concepts for which particular rules are provided. The rules applied at the end of the process provide configuration parameters for a system for emotional voice synthesis.",2010,Knowledge and Information Systems volume 25 issue 3 pp 421-443,principle of compositionality;voice;ontology;semantic web;reason;user interface;systems modeling;affective computing;ontology;natural language processing;speech recognition;data mining;database;artificial intelligence;machine learning;computer science;
WiKi’mantics: interpreting ontologies with WikipediA,Bo Hu (Career Education Corporation);,2681402291,"In the context of the Semantic Web, many ontology-related operations can be boiled down to one fundamental task: finding as accurately as possible the semantics hiding beneath the superficial representation of ontological entities. This, however, is not an easy task due to the ambiguous nature of semantics and a lack of systematic engineering method to guide how we comprehend semantics. We acknowledge the gap between human cognition and knowledge representation formalisms: even though precise logic formulae can be used as the canonical representation of ontological entities, understanding of such formulae may vary. A feasible solution to juxtaposing semantics interpretation, therefore, is to reflect such cognitive variations. In this paper, we propose an approximation of semantics using sets of words/phrases, referred to as WɪKɪmantic vectors. These vectors are emerged through a set of well-tuned methods gradually surfacing the semantics that remain implicit otherwise. Given a concept, we first identify its conceptual niche amongst its neighbours in the graph representation of the ontology. We generate a natural language paraphrases of the isolated sub-graph and project this textual description upon a large document repository. WɪKɪmantic vectors are then drawn from the document repository. We evaluated each of the aforementioned steps by way of user study.",2010,Knowledge and Information Systems volume 25 issue 3 pp 445-472,well founded semantics;operational semantics;cognition;ontology;semantics;graph theory;natural language processing;data mining;database;artificial intelligence;computer science;
Boosting support vector machines for imbalanced data sets,Benjamin X. Wang (University of Ottawa);Nathalie Japkowicz (University of Ottawa);,"2695387881,195975349","Real world data mining applications must address the issue of learning from imbalanced data sets. The problem occurs when the number of instances in one class greatly outnumbers the number of instances in the other class. Such data sets often cause a default classifier to be built due to skewed vector spaces or lack of information. Common approaches for dealing with the class imbalance problem involve modifying the data distribution or modifying the classifier. In this work, we choose to use a combination of both approaches. We use support vector machines with soft margins as the base classifier to solve the skewed vector spaces problem. We then counter the excessive bias introduced by this approach with a boosting algorithm. We found that this ensemble of SVMs makes an impressive improvement in prediction performance, not only for the majority class, but also for the minority class.",2010,Knowledge and Information Systems volume 25 issue 1 pp 1-20,margin classifier;support vector machine;supervised learning;data mining;pattern recognition;machine learning;computer science;mathematics;
Knowledge-based vector space model for text clustering,Liping Jing (Beijing Jiaotong University);Michael K. Ng (Hong Kong Baptist University);Joshua Zhexue Huang (University of Hong Kong);,"2096438337,2110705915,2111159692","This paper presents a new knowledge-based vector space model (VSM) for text clustering. In the new model, semantic relationships between terms (e.g., words or concepts) are included in representing text documents as a set of vectors. The idea is to calculate the dissimilarity between two documents more effectively so that text clustering results can be enhanced. In this paper, the semantic relationship between two terms is defined by the similarity of the two terms. Such similarity is used to re-weight term frequency in the VSM. We consider and study two different similarity measures for computing the semantic relationship between two terms based on two different approaches. The first approach is based on the existing ontologies like WordNet and MeSH. We define a new similarity measure that combines the edge-counting technique, the average distance and the position weighting method to compute the similarity of two terms from an ontology hierarchy. The second approach is to make use of text corpora to construct the relationships between terms and then calculate their semantic similarities. Three clustering algorithms, bisecting k-means, feature weighting k-means and a hierarchical clustering algorithm, have been used to cluster real-world text data represented in the new knowledge-based VSM. The experimental results show that the clustering performance based on the new model was much better than that based on the traditional term-based VSM.",2010,Knowledge and Information Systems volume 25 issue 1 pp 35-55,correlation clustering;constrained clustering;fuzzy clustering;semantic similarity;k means clustering;cluster analysis;ontology;knowledge base;data mining;pattern recognition;machine learning;computer science;mathematics;
Resource service optimal-selection based on intuitionistic fuzzy set and non-functionality QoS in manufacturing grid system,Fei Tao (Beihang University);Dongming Zhao (University of Michigan);Lin Zhang (Beihang University);,"2656051924,2207163866,2692847253","In manufacturing grid (MGrid) system, according to functional requirements of a task, there exist a lot of resource services which have similar functional characteristics. Multiple resource services with similar functional characteristics raise the concern over resource service optimal-selection (RSOS). It is important to select the optimal resource service according to their non-functionality characteristics or quality of service (QoS). However, QoS attributes are not easy to measure due to their complexity and involvement of ill-structured information. In this study, user’s feeling is taken into account in RSOS in an MGrid system. The non-functionality QoS evaluation of resource services is based on users’ feeling and transaction experiences using intuitionistic fuzzy set (IFS). Furthermore, the dynamics of non-functionality QoS is considered, and a time-decay function is introduced into non-functionality QoS evaluation. A new method is proposed for RSOS based on IFS and non-functionality QoS, and the procedures are presented in detail. A practice case study is used to illustrate the proposed method and procedure. The performance and advantage of the proposed method are discussed.",2010,Knowledge and Information Systems volume 25 issue 1 pp 185-208,mobile qos;intuitionistic logic;transaction processing;production system;functional requirement;service quality;quality of service;fuzzy set;resource allocation;functional analysis;artificial intelligence;simulation;computer science;
Attribute reduction in ordered information systems based on evidence theory,Wei-hua Xu (Chongqing University of Technology);Xiao-yan Zhang (Chongqing University of Technology);Jian-min Zhong (Chongqing University of Technology);Wen-xiu Zhang (Xi'an Jiaotong University);,"2722034276,2352812659,2685547615,2714299784","Attribute reduction is one of the most important problems in rough set theory. However, in real-world lots of information systems are based on dominance relation in stead of the classical equivalence relation because of various factors. The ordering properties of attributes play a crucial role in those systems. To acquire brief decision rules from the systems, attribute reductions are needed. This paper deals with attribute reduction in ordered information systems based on evidence theory. The concepts of plausibility and belief consistent sets as well as plausibility and belief reducts in ordered information systems are introduced. It is proved that a plausibility consistent set must be a consistent set and an attribute set is a belief reduct if and only if it is a classical reduction in ordered information system.",2010,Knowledge and Information Systems volume 25 issue 1 pp 169-184,variable and attribute;rough set;expert system;information theory;artificial intelligence;machine learning;statistics;algorithm;computer science;mathematics;
Expressive reasoning with horn rules and fuzzy description logics,Theofilos P. Mailis (National Technical University of Athens);Giorgos Stoilos (University of Oxford);Giorgos B. Stamou (National Technical University of Athens);,"377466301,170620773,2047892088","In this paper, we describe fuzzy CARIN, a knowledge representation language combining fuzzy Description Logics with Horn rules. Fuzzy CARIN integrates the management of fuzzy logic into the non-recursive CARIN language. We introduce the decision problems of answering to conjunctive queries, unions of conjunctive queries and the existential entailment problem and provide a sound and complete algorithm that permits reasoning with the DL fuzzy $${\mathcal{ALCNR}}$$extended with non-recursive Horn rules. This extension is most useful in realistic applications that handle uncertain or imprecise data such as multimedia processing and medical applications.",2010,Knowledge and Information Systems volume 25 issue 1 pp 105-136,t norm fuzzy logics;fuzzy set operations;conjunctive query;fuzzy classification;fuzzy number;neuro fuzzy;data processing;natural language processing;artificial intelligence;algorithm;computer science;mathematics;
Action rule discovery from incomplete data,Seunghyun Im (University of Pittsburgh);Zbigniew Raś (University of North Carolina at Chapel Hill);Hanna Wasyluk;,"2151301588,2043186548,2695262409","Action rule is an implication rule that shows the expected change in a decision value of an object as a result of changes made to some of its conditional values. An example of an action rule is ‘credit card holders of young age are expected to keep their cards for an extended period of time if they receive a movie ticket once a year’. In this case, the decision value is the account status, and the condition value is whether the movie ticket is sent to the customer. The type of action that can be taken by the company is to send out movie tickets to young customers. The conventional action rule discovery algorithms build action rules from existing classification rules. This paper discusses an agglomerative strategy that generates the shortest action rules directly from a decision system. In particular, the algorithm can be used to discover rules from an incomplete decision system where attribute values are partially incomplete. As one of the testing domains for our research we take HEPAR system that was built through a collaboration between the Institute of Biocybernetics and Biomedical Engineering of the Polish Academy of Sciences and physicians at the Medical Center of Postgraduate Education in Warsaw, Poland. HEPAR was designed for gathering and processing clinical data on patients with liver disorders. Action rules will be used to construct the decision-support module for HEPAR.",2010,Knowledge and Information Systems volume 25 issue 1 pp 21-33,decision model;complete information;missing data;decision rule;information system;biological classification;decision support system;operations research;data mining;artificial intelligence;computer science;
A semantical framework for hybrid knowledge bases,Jos de Bruijn (Vienna University of Technology);David Andrew Pearce (Technical University of Madrid);Axel Polleres (National University of Ireland);Agustin Valverde (University of Málaga);,"2720425893,2303473132,2167869104,2153489724","In the ongoing discussion about combining rules and ontologies on the Semantic Web a recurring issue is how to combine first-order classical logic with nonmonotonic rule languages. Whereas several modular approaches to define a combined semantics for such hybrid knowledge bases focus mainly on decidability issues, we tackle the matter from a more general point of view. In this paper, we show how Quantified Equilibrium Logic (QEL) can function as a unified framework which embraces classical logic as well as disjunctive logic programs under the (open) answer set semantics. In the proposed variant of QEL, we relax the unique names assumption, which was present in earlier versions of QEL. Moreover, we show that this framework elegantly captures the existing modular approaches for hybrid knowledge bases in a unified way.",2010,Knowledge and Information Systems volume 25 issue 1 pp 81-104,first order logic;ontology;knowledge base;semantics;theoretical computer science;artificial intelligence;algorithm;computer science;mathematics;
Hybrid rules with well-founded semantics,Włodzimierz Drabent (Polish Academy of Sciences);Jan Małuszyński (Linköping University);,"18067265,150280719",A general framework is proposed for integration of rules and external first-order theories. It is based on the well-founded semantics of normal logic programs and inspired by ideas of Constraint Logic Programming (CLP) and constructive negation for logic programs. Hybrid rules are normal clauses extended with constraints in the bodies; constraints are certain formulae in the language of the external theory. A hybrid program consists of a set of hybrid rules and an external theory. Instances of the framework are obtained by specifying the class of external theories and the class of constraints. An example instance is integration of (non-disjunctive) Datalog with ontologies formalized in description logics. The paper defines a declarative semantics of hybrid programs and a goal-driven formal operational semantics. The latter can be seen as a generalization of SLS-resolution. It provides a basis for hybrid implementations combining Prolog with constraint solvers (such as ontology reasoners). Soundness of the operational semantics is proven. Sufficient conditions for decidability of the declarative semantics and for completeness of the operational semantics are given.,2010,Knowledge and Information Systems volume 25 issue 1 pp 137-168,formal semantics;well founded semantics;action semantics;datalog;constraint logic programming;denotational semantics;stable model semantics;operational semantics;prolog;description logic;formal semantics;constraint satisfaction;decidability;semantic web;technology;ontology;theoretical computer science;database;artificial intelligence;programming language;algorithm;computer science;mathematics;
Forecasting PGR of the financial industry using a rough sets classifier based on attribute-granularity,You-Shyang Chen;Ching-Hsue Cheng (National Yunlin University of Science and Technology);,"2699631163,2161354717","In the financial industry, continually changing economic conditions and characteristics involving uncertainty and risk have made financial forecasts even more difficult, increasing the need for more reliable ways to forecast a bank’s operating performance. However, early related studies of performance analysis for using statistical methods usually become more complex when relationships in input/output data are nonlinear. Furthermore, strict data assumptions, such as linearity, normality, and independence, limit real-world applications often. Additionally, a drawback of traditional rough sets is that data must be discretized first for improving classification accuracy. To remedy the existing shortcomings above, the study proposes a hybrid procedure, which mixes professional knowledge, an attribute granularity, and a rough sets classifier, for automatically classifying profit growth rate (PGR) to solve real problems faced by investors. The proposed procedure is illustrated by examining a practical dataset for publicly traded financial holding stocks in Taiwan‘s stock markets. The experimental results reveal that the proposed procedure outperforms listing methods in terms of accuracy, and they provide useful insights in responsiveness to rapidly changing stock market conditions. Importantly, the output created by the rough sets LEM2 (Learning from Examples Module, version 2) algorithm is a set of comprehensible rules applied in a knowledge-based investment system for investors.",2010,Knowledge and Information Systems volume 25 issue 1 pp 57-79,rough set;decision tree;supervised learning;knowledge base;operations research;data mining;artificial intelligence;machine learning;statistics;computer science;
Semi-supervised learning by disagreement,Zhi-Hua Zhou (Nanjing University);Ming Li (Nanjing University);,"2286237009,2252453783","In many real-world tasks, there are abundant unlabeled examples but the number of labeled training examples is limited, because labeling the examples requires human efforts and expertise. So, semi-supervised learning which tries to exploit unlabeled examples to improve learning performance has become a hot topic. Disagreement-based semi-supervised learning is an interesting paradigm, where multiple learners are trained for the task and the disagreements among the learners are exploited during the semi-supervised learning process. This survey article provides an introduction to research advances in this paradigm.",2010,Knowledge and Information Systems volume 24 issue 3 pp 415-439,news;data analysis;supervised learning;pattern recognition;artificial intelligence;machine learning;computer science;
Periodic subgraph mining in dynamic networks,Mayank Lahiri (University of Illinois at Chicago);Tanya Y. Berger-Wolf (University of Illinois at Chicago);,"1972593125,107058130","In systems of interacting entities such as social networks, interactions that occur regularly typically correspond to significant, yet often infrequent and hard to detect, interaction patterns. To identify such regular behavior in streams of dynamic interaction data, we propose a new mining problem of finding a minimal set of periodically recurring subgraphs to capture all periodic behavior in a dynamic network. We analyze the computational complexity of the problem and show that it is polynomial, unlike many related subgraph or itemset mining problems. We propose an efficient and scalable algorithm to mine all periodic subgraphs in a dynamic network. The algorithm makes a single pass over the data and is also capable of accommodating imperfect periodicity. We demonstrate the applicability of our approach on several real-world networks and extract interesting and insightful periodic interaction patterns. We also show that periodic subgraphs can be an effective way to uncover and characterize the natural periodicities in a system.",2010,Knowledge and Information Systems volume 24 issue 3 pp 467-497,occam s razor;social network;computational complexity theory;social relation;complex systems;artificial intelligence;machine learning;algorithm;computer science;mathematics;
Multiple criteria optimization-based data mining methods and applications: a systematic survey,Yong Shi (University of Nebraska Omaha);,1986476467,"Support Vector Machine, an optimization technique, is well known in the data mining community. In fact, many other optimization techniques have been effectively used in dealing with data separation and analysis. For the last 10 years, the author and his colleagues have proposed and extended a series of optimization-based classification models via Multiple Criteria Linear Programming (MCLP) and Multiple Criteria Quadratic Programming (MCQP). These methods are different from statistics, decision tree induction, and neural networks. The purpose of this paper is to review the basic concepts and frameworks of these methods and promote the research interests in the data mining community. According to the evolution of multiple criteria programming, the paper starts with the bases of MCLP. Then, it further discusses penalized MCLP, MCQP, Multiple Criteria Fuzzy Linear Programming (MCFLP), Multi-Class Multiple Criteria Programming (MCMCP), and the kernel-based Multiple Criteria Linear Program, as well as MCLP-based regression. This paper also outlines several applications of Multiple Criteria optimization-based data mining methods, such as Credit Card Risk Analysis, Classification of HIV-1 Mediated Neuronal Dendritic and Synaptic Damage, Network Intrusion Detection, Firm Bankruptcy Prediction, and VIP E-Mail Behavior Analysis.",2010,Knowledge and Information Systems volume 24 issue 3 pp 369-391,synapse;intrusion detection system;kernel method;risk analysis;fuzzy logic;support vector machine;regression;decision tree;biological classification;data analysis;quadratic programming;linear programming;data mining;artificial intelligence;machine learning;algorithm;computer science;
A general framework for relation graph clustering,Bo Long (Yahoo!);Zhongfei Zhang (Binghamton University);Philip S. Yu (University of Illinois at Chicago);,"2512786853,2177285464,2125104194","Relation graphs, in which multi-type (or single type) nodes are related to each other, frequently arise in many important applications, such as Web mining, information retrieval, bioinformatics, and epidemiology. In this study, We propose a general framework for clustering on relation graphs. Under this framework, we derive a family of clustering algorithms including both hard and soft versions, which are capable of learning cluster patterns from relation graphs with various structures and statistical properties. A number of classic approaches on special cases of relation graphs, such as traditional graphs with singly-type nodes and bi-type relation graphs with two types of nodes, can be viewed as special cases of the proposed framework. The theoretic analysis and experiments demonstrate the great potential and effectiveness of the proposed framework and algorithm.",2010,Knowledge and Information Systems volume 24 issue 3 pp 393-413,modular decomposition;bregman divergence;clustering coefficient;association;cluster analysis;web mining;epidemiology;graph theory;combinatorics;data mining;statistics;computer science;mathematics;
A local asynchronous distributed privacy preserving feature selection algorithm for large peer-to-peer networks,"Kamalika Das (Ames Research Center);Kanishka Bhaduri (Ames Research Center);Hillol Kargupta (University of Maryland, Baltimore County);","2130991957,78114061,539587773","In this paper we develop a local distributed privacy preserving algorithm for feature selection in a large peer-to-peer environment. Feature selection is often used in machine learning for data compaction and efficient learning by eliminating the curse of dimensionality. There exist many solutions for feature selection when the data are located at a central location. However, it becomes extremely challenging to perform the same when the data are distributed across a large number of peers or machines. Centralizing the entire dataset or portions of it can be very costly and impractical because of the large number of data sources, the asynchronous nature of the peer-to-peer networks, dynamic nature of the data/network, and privacy concerns. The solution proposed in this paper allows us to perform feature selection in an asynchronous fashion with a low communication overhead where each peer can specify its own privacy constraints. The algorithm works based on local interactions among participating nodes. We present results on real-world dataset in order to test the performance of the proposed algorithm.",2010,Knowledge and Information Systems volume 24 issue 3 pp 341-367,curse of dimensionality;feature selection;distributed algorithm;world wide web;data mining;database;artificial intelligence;machine learning;computer science;
Fuzzy clustering-based discretization for gene expression classification,Keivan Kianmehr (University of Calgary);Mohammed Alshalalfa (University of Calgary);Reda Alhajj (University of Calgary);,"214783148,2009702304,497422635","This paper presents a novel classification approach that integrates fuzzy class association rules and support vector machines. A fuzzy discretization technique based on fuzzy c-means clustering algorithm is employed to transform the training set, particularly quantitative attributes, to a format appropriate for association rule mining. A hill-climbing procedure is adapted for automatic thresholds adjustment and fuzzy class association rules are mined accordingly. The compatibility between the generated rules and fuzzy patterns is considered to construct a set of feature vectors, which are used to generate a classifier. The reported test results show that compatibility rule-based feature vectors present a highly- qualified source of discrimination knowledge that can substantially impact the prediction power of the final classifier. In order to evaluate the applicability of the proposed method to a variety of domains, it is also utilized for the popular task of gene expression classification. Further, we show how this method provide biologists with an accurate and more understandable classifier model compared to other machine learning techniques.",2010,Knowledge and Information Systems volume 24 issue 3 pp 441-465,fuzzy associative matrix;fuzzy set operations;defuzzification;fuzzy classification;fuzzy number;fuzzy clustering;discretization;association rule learning;neuro fuzzy;support vector machine;cluster analysis;expert system;data mining;artificial intelligence;machine learning;computer science;mathematics;
Special issue on selected papers from IEEE DMF 2008,Keith C. C. Chan (Hong Kong Polytechnic University);Xindong Wu (University of Vermont);,"2110316897,2123651450",-,2010,Knowledge and Information Systems volume 24 issue 3 pp 339-340,-
On clustering massive text and categorical data streams,Charu C. Aggarwal (IBM);Philip S. Yu (University of Illinois at Chicago);,"2146335907,2125104194","In this paper, we will study the data stream clustering problem in the context of text and categorical data domains. While the clustering problem has been studied recently for numeric data streams, the problems of text and categorical data present different challenges because of the large and un-ordered nature of the corresponding attributes. Therefore, we will propose algorithms for text and categorical data stream clustering. We will propose a condensation based approach for stream clustering which summarizes the stream into a number of fine grained cluster droplets. These summarized droplets can be used in conjunction with a variety of user queries to construct the clusters for different input parameters. Thus, this provides an online analytical processing approach to stream clustering. We also study the problem of detecting noisy and outlier records in real time. We will test the approach for a number of real and synthetic data sets, and show the effectiveness of the method over the baseline OSKM algorithm for stream clustering.",2010,Knowledge and Information Systems volume 24 issue 2 pp 171-196,flame clustering;brown clustering;canopy clustering algorithm;correlation clustering;constrained clustering;data stream clustering;cure data clustering algorithm;fuzzy clustering;outlier;clustering high dimensional data;synthetic data;categorical variable;cluster analysis;data stream mining;document clustering;world wide web;data mining;database;machine learning;statistics;computer science;
A binary decision diagram based approach for mining frequent subsequences,Elsa Loekito (University of Melbourne);James Bailey (University of Melbourne);Jian Pei (Simon Fraser University);,"205954007,2131557737,2126330539","Sequential pattern mining is an important problem in data mining. State of the art techniques for mining sequential patterns, such as frequent subsequences, are often based on the pattern-growth approach, which recursively projects conditional databases. Explicitly creating database projections is thought to be a major computational bottleneck, but we will show in this paper that it can be beneficial when the appropriate data structure is used. Our technique uses a canonical directed acyclic graph as the sequence database representation, which can be represented as a binary decision diagram (BDD). In this paper, we introduce a new type of BDD, namely a sequence BDD (SeqBDD), and show how it can be used for efficiently mining frequent subsequences. A novel feature of the SeqBDD is its ability to share results between similar intermediate computations and avoid redundant computation. We perform an experimental study to compare the SeqBDD technique with existing pattern growth techniques, that are based on other data structures such as prefix trees. Our results show that a SeqBDD can be half as large as a prefix tree, especially when many similar sequences exist. In terms of mining time, it can be substantially more efficient when the support is low, the number of patterns is large, or the input sequences are long and highly similar.",2010,Knowledge and Information Systems volume 24 issue 2 pp 235-268,directed acyclic graph;directed graph;data structure;data mining;database;machine learning;algorithm;computer science;
Explanation and reliability of prediction models: the case of breast cancer recurrence,Erik Štrumbelj (University of Ljubljana);Zoran Bosnić (University of Ljubljana);Igor Kononenko (University of Ljubljana);Branko Zakotnik;Cvetka Grašič Kuhar;,"1153133090,277046765,1409703486,2569905956,1853318464","In this paper, we describe the first practical application of two methods, which bridge the gap between the non-expert user and machine learning models. The first is a method for explaining classifiers’ predictions, which provides the user with additional information about the decision-making process of a classifier. The second is a reliability estimation methodology for regression predictions, which helps the users to decide to what extent to trust a particular prediction. Both methods are successfully applied to a novel breast cancer recurrence prediction data set and the results are evaluated by expert oncologists.",2010,Knowledge and Information Systems volume 24 issue 2 pp 305-324,cancer;operations research;data mining;artificial intelligence;machine learning;computer science;
A heuristic method for learning Bayesian networks using discrete particle swarm optimization,Tong Wang (Shanghai Jiao Tong University);Jie Yang (Shanghai Jiao Tong University);,"2639347120,2645106908","Bayesian networks are a powerful approach for representing and reasoning under conditions of uncertainty. Many researchers aim to find good algorithms for learning Bayesian networks from data. And the heuristic search algorithm is one of the most effective algorithms. Because the number of possible structures grows exponentially with the number of variables, learning the model structure from data by considering all possible structures exhaustively is infeasible. PSO (particle swarm optimization), a powerful optimal heuristic search algorithm, has been applied in various fields. Unfortunately, the classical PSO algorithm only operates in continuous and real-valued space, and the problem of Bayesian networks learning is in discrete space. In this paper, two modifications of updating rules for velocity and position are introduced and a Bayesian networks learning based on binary PSO is proposed. Experimental results show that it is more efficient because only fewer generations are needed to obtain optimal Bayesian networks structures. In the comparison, this method outperforms other heuristic methods such as GA (genetic algorithm) and classical binary PSO.",2010,Knowledge and Information Systems volume 24 issue 2 pp 269-281,wake sleep algorithm;bayesian network;evolutionary algorithm;swarm intelligence;data structure;artificial intelligence;machine learning;algorithm;computer science;mathematics;
A general measure of similarity for categorical sequences,Abdellali Kelil (Université de Sherbrooke);Shengrui Wang (Université de Sherbrooke);Qingshan Jiang (Xiamen University);Ryszard Brzezinski (Université de Sherbrooke);,"1994722248,2106818440,2141296277,2196602626","Measuring the similarity between categorical sequences is a fundamental process in many data mining applications. A key issue is extracting and making use of significant features hidden behind the chronological and structural dependencies found in these sequences. Almost all existing algorithms designed to perform this task are based on the matching of patterns in chronological order, but such sequences often have similar structural features in chronologically different order. In this paper we propose SCS, a novel, effective and domain-independent method for measuring the similarity between categorical sequences, based on an original pattern matching scheme that makes it possible to capture chronological and non-chronological dependencies. SCS captures significant patterns that represent the natural structure of sequences, and reduces the influence of those which are merely noise. It constitutes an effective approach to measuring the similarity between data in the form of categorical sequences, such as biological sequences, natural language texts, speech recognition data, certain types of network transactions, and retail transactions. To show its effectiveness, we have tested SCS extensively on a range of data sets from different application fields, and compared the results with those obtained by various mainstream algorithms. The results obtained show that SCS produces results that are often competitive with domain-specific similarity approaches.",2010,Knowledge and Information Systems volume 24 issue 2 pp 197-220,transaction processing;case based reasoning;data type;similitude;similarity;pattern matching;natural language;matching;metric;algorithm design;data analysis;chronology;speech recognition;data mining;database;artificial intelligence;programming language;computer science;mathematics;
Mining incomplete survey data through classification,Hai Wang (Saint Mary's University);Shouhong Wang (University of Massachusetts Dartmouth);,"2723335329,2673553516","Data mining with incomplete survey data is an immature subject area. Mining a database with incomplete data, the patterns of missing data as well as the potential implication of these missing data constitute valuable knowledge. This paper presents the conceptual foundations of data mining with incomplete data through classification which is relevant to a specific decision making problem. The proposed technique generally supposes that incomplete data and complete data may come from different sub-populations. The major objective of the proposed technique is to detect the interesting patterns of data missing behavior that are relevant to a specific decision making, instead of estimation of individual missing value. Using this technique, a set of complete data is used to acquire a near-optimal classifier. This classifier provides the prediction reference information for analyzing the incomplete data. The data missing behavior concealed in the missing data is then revealed. Using a real-world survey data set, the paper demonstrates the usefulness of this technique.",2010,Knowledge and Information Systems volume 24 issue 2 pp 221-233,data pre processing;missing data;information extraction;data science;data mining;pattern recognition;statistics;computer science;
Image annotation techniques based on feature selection for class-pairs,"Jianjiang Lu (University of Science and Technology, Sana'a);Ran Li (University of Science and Technology, Sana'a);Yafei Zhang (University of Science and Technology, Sana'a);Tianzhong Zhao (University of Science and Technology, Sana'a);Zining Lu (University of Science and Technology, Sana'a);","2424934358,2438244148,2117411073,2120713489,2101893563","Image annotation technique can be formulated as a multi-class classification problem, which can be solved by the ensemble of multiple class-pair classifiers. Support vector machine (SVM) classifiers based on optimal class-pair feature subsets from the multimedia content description interface (MPEG-7) standard are used as the class-pair classifiers. We use a binary-coded chromosome genetic algorithm (GA) to select optimal class-pair feature subsets, and a bi-coded chromosome GA to simultaneously select optimal class-pair feature subsets and corresponding optimal weight subsets, i.e. optimal class-pair weighted feature subsets. We consider two kinds of methods for class-pair feature selection: a common optimal (or weighted) feature subset is selected for all the class-pairs, and an individual optimal (or weighted) feature subset is selected for each class-pair respectively. Majority voting scheme is used to combine the class-pair SVM classifiers. The experiments are performed on two different image sets to validate the performance of our image annotation techniques.",2010,Knowledge and Information Systems volume 24 issue 2 pp 325-337,content management;multiclass classification;binary code;majority rule;support vector machine;feature;genetic algorithm;feature selection;data mining;pattern recognition;machine learning;computer science;mathematics;
Generational analysis of variety in data structures: impact on automatic data integration and on the semantic web,Eli Rohn (New Jersey Institute of Technology);,2197313197,"We examine data definition languages (DDLs) from various computing era spanning almost 50 years to date. We prove that contemporary DDLs are indistinguishable from older ones using Zipf distribution of words, Zipf distributions of meanings, and information theory. None addresses the Law of Requisite Variety, which is necessary for enabling automatic data integration from autonomous heterogeneous data sources and for the realization of the Semantic Web. The growth of the entire computing industry is hampered by the lack of progress in the development of DDLs suitable for these two goals. Our findings set the stage for the future development of a mathematically sound DDL better suited for the aforementioned purposes.",2010,Knowledge and Information Systems volume 24 issue 2 pp 283-304,data definition language;distributive property;variety;zipf s law;data integration;semantic web;information integration;data integrity;information system;systems modeling;data structure;information theory;world wide web;data mining;database;artificial intelligence;computer science;
Evaluation of contextual information retrieval effectiveness: overview of issues and research,Lynda Tamine-Lechani (Susquehanna International Group);Mohand Boughanem (Susquehanna International Group);Mariam Daoud (Susquehanna International Group);,"70301992,117014290,2019012303","The increasing prominence of information arising from a wide range of sources delivered over electronic media has made traditional information retrieval systems less effective. Indeed, users are overwhelmed by the information delivered by such systems in response to their queries, particularly when the latter are ambiguous. In order to tackle this problem, the state-of-the-art reveals that there is a growing interest towards contextual information retrieval which relies on various sources of evidence issued from the user’s search background and environment like interests, preferences, time and location, in order to improve the retrieval accuracy. Contextual information retrieval systems are based on different definitions of the core concept of user’s context, various user’s context modeling approaches and several techniques of document relevance measurement, but all share the goal of providing the most useful information to the users in accordance with their context. However, the evaluation methodologies conceived in the past several years for traditional information retrieval and widely used in the evaluation campaigns have been challenged by the consideration of user’s context in the information retrieval process. Thus, we recognize that a critical review of existing evaluation methodologies in contextual information retrieval area is needed in order to design and develop standard evaluation frameworks. We present in this paper a comprehensive survey of contextual information retrieval evaluation methodologies and provide insights into how and why they are appropriate to measure the retrieval effectiveness. We also highlight some of the research challenges ahead that would constitute substantive research area for future research.",2010,Knowledge and Information Systems volume 24 issue 1 pp 1-34,cognitive models of information retrieval;human computer information retrieval;adversarial information retrieval;electronic media;context model;relevance;relevance;information system;internationalization and localization;world wide web;information retrieval;data mining;computer science;
Fuzzy entropy based optimization of clusters for the segmentation of lungs in CT scanned images,M. Arfan Jaffar (National University of Computer and Emerging Sciences);Ayyaz Hussain (National University of Computer and Emerging Sciences);Anwar Majid Mirza (National University of Computer and Emerging Sciences);,"1644977465,2172971535,2106211688","In this paper, we have proposed a method for segmentation of lungs from Computed Tomography (CT)-scanned images using spatial Fuzzy C-Mean and morphological techniques known as Fuzzy Entropy and Morphology based Segmentation. To determine dynamic and adaptive optimal threshold, we have incorporated Fuzzy Entropy. We have proposed a novel histogram-based background removal operator. The proposed system is capable to perform fully automatic segmentation of CT Scan Lung images, based solely on information contained by the image itself. We have used different cluster validity functions to find out optimal number of clusters. The proposed system can be used as a basic building block for Computer-Aided Diagnosis. The technique was tested against the 25 datasets of different patients received from Aga Khan Medical University, Pakistan. The results confirm the validity of technique as well as enhanced performance.",2010,Knowledge and Information Systems volume 24 issue 1 pp 91-111,scale space segmentation;adaptive optimization;thresholding;mathematical morphology;fuzzy logic;histogram;segmentation;image segmentation;respiratory system;computed tomography;computer vision;artificial intelligence;computer science;
A hybrid image restoration approach: fuzzy logic and directional weighted median based uniform impulse noise removal,Ayyaz Hussain (National University of Computer and Emerging Sciences);M. Arfan Jaffar (National University of Computer and Emerging Sciences);Anwar M. Mirza (National University of Computer and Emerging Sciences);,"2172971535,1644977465,2106211688","In this paper, a hybrid image restoration technique based on fuzzy logic and directional weighted median is presented. The proposed technique consists of noise detection and fuzzy filtering processes to detect and remove uniform (random-valued) impulse noise while preserving the image details efficiently. In order to preserve image details such as edges and texture information, a two-stage robust noise detection is presented in this paper. Pixels detected as noisy by both the noise detection stages are considered for noise removal by the fuzzy filtering process, which utilizes the direction based weighted median to construct fuzzy membership function, which is the main contributing factor in noise removal and detail preservation. Extensive experimentation shows that the proposed technique performs significantly better than state-of-the-art filters based on peak signal-to-noise ratio, structural similarity index measure and subjective evaluation criteria.",2010,Knowledge and Information Systems volume 24 issue 1 pp 77-90,value noise;gradient noise;weighted median;salt and pepper noise;impulse noise;median filter;membership function;noise figure;peak signal to noise ratio;similitude;similarity;noise measurement;fuzzy logic;structural similarity;image restoration;metric;filter;gaussian noise;texture;signal to noise ratio;image processing;speech recognition;artificial intelligence;computer science;mathematics;
Interpreting PET scans by structured patient data: a data mining case study in dementia research,Jana Schmidt (Technische Universität München);Andreas Hapfelmeier (Technische Universität München);Marianne Mueller (Technische Universität München);Robert Perneczky (Technische Universität München);Alexander Kurz (Technische Universität München);Alexander Drzezga (Technische Universität München);Stefan Kramer (Technische Universität München);,"2115170379,2174733847,2099970734,657931645,2140686133,1990821640,2283720790","One of the goals of medical research in the area of dementia is to correlate images of the brain with clinical tests. Our approach is to start with the images and explain the differences and commonalities in terms of the other variables. First, we cluster Positron emission tomography (PET) scans of patients to form groups sharing similar features in brain metabolism. To the best of our knowledge, it is the first time ever that clustering is applied to whole PET scans. Second, we explain the clusters by relating them to non-image variables. To do so, we employ RSD, an algorithm for relational subgroup discovery, with the cluster membership of patients as target variable. Our results enable interesting interpretations of differences in brain metabolism in terms of demographic and clinical variables. The approach was implemented and tested on an exceptionally large data collection of patients with different types of dementia. It comprises 10 GB of image data from 454 PET scans, and 42 variables from psychological and demographical data organized in 11 relations of a relational database. We believe that explaining medical images in terms of other variables (patient records, demographic information, etc.) is a challenging new and rewarding area for data mining research.",2010,Knowledge and Information Systems volume 24 issue 1 pp 149-170,cerebro;pet;relational database;central nervous system;metabolism;data analysis;information extraction;data processing;data mining;database;artificial intelligence;computer science;
A non-parametric semi-supervised discretization method,Alexis Bondu (Environmental Defense Fund);Marc Boullé (Orange S.A.);Vincent Lemaire (Orange S.A.);,"60721547,743486302,2096732835","Semi-supervised classification methods aim to exploit labeled and unlabeled examples to train a predictive model. Most of these approaches make assumptions on the distribution of classes. This article first proposes a new semi-supervised discretization method, which adopts very low informative prior on data. This method discretizes the numerical domain of a continuous input variable, while keeping the information relative to the prediction of classes. Then, an in-depth comparison of this semi-supervised method with the original supervised MODL approach is presented. We demonstrate that the semi-supervised approach is asymptotically equivalent to the supervised approach, improved with a post-optimization of the intervals bounds location.",2010,Knowledge and Information Systems volume 24 issue 1 pp 35-57,discretization;econometrics;machine learning;statistics;computer science;mathematics;
A fuzzy approach for solving a critical benchmarking problem,Javier Andrade (University of A Coruña);Juan Ares (University of A Coruña);María A. Martínez (Open University);Juan Pazos (Technical University of Madrid);Santiago Rodríguez (University of A Coruña);Sonia M. Suárez (University of A Coruña);,"2295419688,2141078053,2288684836,2163080748,2287404797,2113354472","Nowadays, benchmarking is a widespread technique for evaluating an aspect—process, product, service, etc.—by comparing it against the best in class with the aim of improving this aspect or identifying the best alternative. There have been numerous attempts at defining a rigorous benchmarking process by specifying steps that should be taken to put benchmarking into practice. All these proposals use a method of calculation that treats the weights and ratings of each criterion as numerical variables, even if they are not. This means that the binary and linguistic variables have to be artificially translated to numerical variables, misleading us into thinking that the concepts we are dealing with are quantitative when they really are not. In this paper, we propose a new method of calculation based on fuzzy logic to rectify this key methodological error. Its definition is based on: (i) a new division operator for fuzzy numbers representing conjugated variables, as in the case outlined here; (ii) a new aggregation operator that can integrate binary, numerical and/or linguistic variables; and, finally, (iii) an operator that can translate the final fuzzy rating into the linguistic variable that best represents it. Therefore, the resulting method is: (i) closer to the user since it manages more human-understandable values and (ii) not dependent on the above artificial translation process, which could lead to sizeable variations in the benchmarking result.",2010,Knowledge and Information Systems volume 24 issue 1 pp 59-75,fuzzy number;benchmarking;fuzzy logic;multiple comparisons problem;data analysis;information technology management;data mining;artificial intelligence;algorithm;computer science;mathematics;
A conceptual schema-based temporal meta database schemas generation technique for 3D objects,Vo Thi Ngoc Chau (Ho Chi Minh City University of Technology);Suphamit Chittayasothorn (King Mongkut's Institute of Technology Ladkrabang);,"1211284406,1249716625","In this paper, a temporal meta database for three-dimensional (3D) objects whose properties and relationships are supported by valid time is introduced. Based on our proposed temporal object-oriented conceptual schema model, a conceptual schema of the temporal meta database can be generated from a 3D graphical data source and other particular application requirements. Based on our proposed temporal object relational data model with attribute timestamping, logical schemas of the temporal meta database can be systematically and automatically generated from the conceptual schema. From the temporal meta database, non-temporal/temporal metadata about temporal 3D objects are available for temporal information system users. Convenient access using database languages such as SQL can be performed. Queries over 3D objects using a temporal object relational SQL are demonstrated.",2010,Knowledge and Information Systems volume 24 issue 1 pp 113-147,semi structured model;schema migration;database model;database schema;conceptual schema;temporal database;database design;3d computer graphics;information retrieval;data mining;database;computer science;
Sentiment-oriented contextual advertising,Teng-Kai Fan (National Central University);Chia-Hui Chang (National Central University);,"2689821071,2668643951","Web advertising (Online advertising), a form of advertising that uses the World Wide Web to attract customers, has become one of the world’s most important marketing channels. This paper addresses the mechanism of Content-based advertising (Contextual advertising), which refers to the assignment of relevant ads to a generic web page, e.g., a blog post. As blogs become a platform for expressing personal opinion, they naturally contain various kinds of expressions, including both facts and comments of both a positive and negative nature. Besides, in line with the major tenet of Web 2.0 (i.e., user-centric), we believe that the web-site owners would be willing to be in charge of the ads which are positively related to their contents. Hence, in this paper, we propose the utilization of sentiment detection to improve Web-based contextual advertising. The proposed sentiment-oriented contextual advertising (SOCA) framework aims to combine contextual advertising matching with sentiment analysis to select ads that are related to the positive (and neutral) aspects of a blog and rank them according to their relevance. We experimentally validate our approach using a set of data that includes both real ads and actual blog pages. The results indicate that our proposed method can effectively identify those ads that are positively correlated with the given blog pages.",2010,Knowledge and Information Systems volume 23 issue 3 pp 321-344,native advertising;informative advertising;keyword advertising;advertising campaign;relevance;web page;the internet;contextual advertising;online advertising;sentiment analysis;content analysis;world wide web;data mining;database;computer science;
Discovery of interactive graphs for understanding and searching time-indexed corpora,Ilija Subašić (Katholieke Universiteit Leuven);Bettina Berendt (Katholieke Universiteit Leuven);,"2192779830,2683720132","Rich information spaces (like the Web or scientific publications) are full of “stories”: sets of statements that evolve over time, manifested as, for example, collections of news articles reporting events that relate to an evolving crime investigation, sets of news articles and blog posts accompanying the development of a political election campaign, or sequences of scientific papers on a topic. In this paper, we formulate the problem of discovering such stories as Evolutionary Theme Pattern Discovery, Summary and Exploration (ETP3). We propose a method and a visualisation tool for solving ETP3 by understanding, searching and interacting with such stories and their underlying documents. In contrast to existing approaches, our method concentrates on relational information and on local patterns rather than on the occurrence of individual concepts and global models. In addition, it relies on interactive graphs rather than natural language as the abstracted story representations. Furthermore, we present an evaluation framework. Two real-life case studies are used to illustrate and evaluate the method and tool.",2010,Knowledge and Information Systems volume 23 issue 3 pp 293-319,graphical user interface;web mining;text mining;data science;world wide web;data mining;database;artificial intelligence;computer science;
StatApriori: an efficient algorithm for searching statistically significant association rules,Wilhelmiina Hämäläinen (University of Helsinki);,2709135832,"Searching statistically significant association rules is an important but neglected problem. Traditional association rules do not capture the idea of statistical dependence and the resulting rules can be spurious, while the most significant rules may be missing. This leads to erroneous models and predictions which often become expensive. The problem is computationally very difficult, because the significance is not a monotonic property. However, in this paper, we prove several other properties, which can be used for pruning the search space. The properties are implemented in the StatApriori algorithm, which searches statistically significant, non-redundant association rules. Empirical experiments have shown that StatApriori is very efficient, but in the same time it finds good quality rules.",2010,Knowledge and Information Systems volume 23 issue 3 pp 373-399,association rule learning;search algorithm;association;redundancy;systems modeling;statistical significance;data mining;machine learning;statistics;computer science;mathematics;
A distributed EM algorithm to estimate the parameters of a finite mixture of components,Behrooz Safarinejadian (Amirkabir University of Technology);Mohammad B. Menhaj (Amirkabir University of Technology);Mehdi Karrari (Amirkabir University of Technology);,"2422899029,1991060659,2573950972","In this paper, a distributed expectation maximization (DEM) algorithm is first introduced in a general form for estimating the parameters of a finite mixture of components. This algorithm is used for density estimation and clustering of data distributed over nodes of a network. Then, a distributed incremental EM algorithm (DIEM) with a higher convergence rate is proposed. After a full derivation of distributed EM algorithms, convergence of these algorithms is analyzed based on the negative free energy concept used in statistical physics. An analytical approach is also developed for evaluating the convergence rate of both incremental and distributed incremental EM algorithms. It is analytically shown that the convergence rate of DIEM is much faster than that of the DEM algorithm. Finally, simulation results approve that DIEM remarkably outperforms DEM for both synthetic and real data sets.",2010,Knowledge and Information Systems volume 23 issue 3 pp 267-292,expectation maximization algorithm;distributed algorithm;calculus;econometrics;machine learning;statistics;computer science;mathematics;
Ternary reversible extreme learning machines: the incremental tri-training method for semi-supervised classification,Xiao-Liang Tang (Dalian University of Technology);Min Han (Dalian University of Technology);,"2118274648,2165361383","Tri-training method proposed by Zhou et al., is an excellent method for semi-supervised classification; nevertheless, the heavy computational burden caused by the retraining strategy prevents the further application of tri-training method. To address this problem, this paper proposes the ternary reversible extreme learning machines (TRELM) which is an incremental tri-training method without relying on the retraining strategy. TRELM employs three reversible extreme learning machines (RELM) as its base learners and trains the RELM with extended (or detected) samples in each learning round. RELM is an incremental learning method with reversible derivation capability. RELM can overcome the difficulty for most incremental learning methods in removing the influence of previously learned mistaken samples. Experimental results indicate that TRELM significantly improves the learning speed of tri-training method. In addition, TRELM achieves comparable (or even better) classification performance to other effective semi-supervised learning methods. TRELM is an appropriate choice for semi-supervised classification tasks with large amounts of data sets or with strict demands for learning speed and classification accuracy.",2010,Knowledge and Information Systems volume 23 issue 3 pp 345-372,process capability index;data analysis;semi supervised learning;supervised learning;artificial intelligence;machine learning;algorithm;computer science;
Mining fuzzy association rules from uncertain data,Cheng-Hsiung Weng (Central Taiwan University of Science and Technology);Yen-Liang Chen (National Central University);,"2131819944,2162700644","Association rule mining is an important data analysis method that can discover associations within data. There are numerous previous studies that focus on finding fuzzy association rules from precise and certain data. Unfortunately, real-world data tends to be uncertain due to human errors, instrument errors, recording errors, and so on. Therefore, a question arising immediately is how we can mine fuzzy association rules from uncertain data. To this end, this paper proposes a representation scheme to represent uncertain data. This representation is based on possibility distributions because the possibility theory establishes a close connection between the concepts of similarity and uncertainty, providing an excellent framework for handling uncertain data. Then, we develop an algorithm to mine fuzzy association rules from uncertain data represented by possibility distributions. Experimental results from the survey data show that the proposed approach can discover interesting and valuable patterns with high certainty.",2010,Knowledge and Information Systems volume 23 issue 2 pp 129-152,data analysis;data science;data mining;machine learning;computer science;mathematics;
BusSEngine: a business search engine,Kamal Taha (University of Texas at Arlington);Ramez Elmasri (University of Texas at Arlington);,"2129123641,2079152302","With the emergence of World Wide Web, business' databases are increasingly being queried directly by customers. The customers may not be aware of the underlying data and its structure, and might have never learned a query language that enables them to issue structured queries. Some of the business' employees who query the databases may also not be aware of the structure of the data, but they are likely to be aware of some labels of elements containing data. We propose in this article: (1) an XML Keyword-Based search engine for answering business' customers called BusSEngine-K, and (2) an XML loosely Structured-Based search engine for answering business' employees called BusSEngine-L. The two engines employ novel context-driven search techniques and are built on top of XQuery search engine. The two engines were evaluated experimentally and compared with three recently proposed XML search engines. The results showed marked improvement.",2010,Knowledge and Information Systems volume 23 issue 2 pp 153-197,sargable;search analytics;phrase search;organic search;search oriented architecture;web search query;web query classification;search engine optimization;xml database;query expansion;query language;xml;search engine indexing;search engine;database search engine;semantic search;metasearch engine;web search engine;world wide web;information retrieval;database;computer science;
TOPSIL-Miner: an efficient algorithm for mining top-K significant itemsets over data streams,Bei Yang (Zhengzhou University);Houkuan Huang (Beijing Jiaotong University);,"2479944797,2692737160","Frequent itemset mining over data streams becomes a hot topic in data mining and knowledge discovery in recent years, and has been applied to different areas. However, the setting of a minimum support threshold needs some domain knowledge. Itwill bring a lot of difficulties or much burden to users if the support threshold is not set reasonably. It is interesting for users to find top-K frequent itemsets over data streams. In this paper, a dynamical incremental approximate algorithm TOPSIL-Miner is presented to mine top-K significant itemsets in landmark windows. A new data structure, TOPSIL-Tree, is designed to store the potential significant itemsets and other data structures of maximum support list, ordered item list, TOPSET and minimum support list are devised to maintain information about mining results. Moreover, three optimal strategies are exploited to reduce time and space cost of the algorithm: (1) pruning trivial nodes in the current data stream, (2) promoting ining support threshold during mining process adaptively and heuristically, and (3) promoting pruning threshold dynamically. The accuracy of the algorithm is also analyzed. Extensive experiments are performed to evaluate the good effectiveness and the high efficiency and precision of the algorithm.",2010,Knowledge and Information Systems volume 23 issue 2 pp 225-242,pruning;data stream mining;data mining;database;machine learning;computer science;
Enhancing the stability and efficiency of spectral ordering with partial supervision and feature selection,Dimitrios Mavroeidis (Athens University of Economics and Business);Ella Bingham (Helsinki Institute for Information Technology);,"2485292246,2121302356","Several studies have demonstrated the prospects of spectral ordering for data mining. One successful application is seriation of paleontological findings, i.e. ordering the sites of excavation, using data on mammal co-occurrences only. However, spectral ordering ignores the background knowledge that is naturally present in the domain: paleontologists can derive the ages of the sites within some accuracy. On the other hand, the age information is uncertain, so the best approach would be to combine the background knowledge with the information on mammal co-occurrences. Motivated by this kind of partial supervision we propose a novel semi-supervised spectral ordering algorithm that modifies the Laplacian matrix such that domain knowledge is taken into account. Also, it performs feature selection by discarding features that contribute most to the unwanted variability of the data in bootstrap sampling. Moreover, we demonstrate the effectiveness of the proposed framework on the seriation of Usenet newsgroup messages, where the task is to find out the underlying flow of discussion. The theoretical properties of our algorithm are thoroughly analyzed and it is demonstrated that the proposed framework enhances the stability of the spectral ordering output and induces computational gains.",2010,Knowledge and Information Systems volume 23 issue 2 pp 243-265,seriation;laplacian matrix;laplace operator;domain knowledge;perturbation theory;feature selection;data mining;pattern recognition;artificial intelligence;machine learning;statistics;computer science;mathematics;
"POTMiner: mining ordered, unordered, and partially-ordered trees",Aída Jiménez (University of Granada);Fernando Berzal (University of Granada);Juan-Carlos Cubero (University of Granada);,"2129692324,1562381855,2131691236","Non-linear data structures are becoming more and more common in data mining problems. Trees, in particular, are amenable to efficient mining techniques. In this paper, we introduce a scalable and parallelizable algorithm to mine partially-ordered trees. Our algorithm, POTMiner, is able to identify both induced and embedded subtrees in such trees. As special cases, it can also handle both completely ordered and completely unordered trees.",2010,Knowledge and Information Systems volume 23 issue 2 pp 199-224,weight balanced tree;partially ordered set;data structure;discrete mathematics;combinatorics;data mining;programming language;computer science;mathematics;
Integrating multiple document features in language models for expert finding,Jianhan Zhu (University College London);Xiangji Huang (York University);Dawei Song (Robert Gordon University);Stefan M. Rüger (Open University);,"2145486142,2100624012,2113829419,1385431002","We argue that expert finding is sensitive to multiple document features in an organizational intranet. These document features include multiple levels of associations between experts and a query topic from sentence, paragraph, up to document levels, document authority information such as the PageRank, indegree, and URL length of documents, and internal document structures that indicate the experts’ relationship with the content of documents. Our assumption is that expert finding can largely benefit from the incorporation of these document features. However, existing language modeling approaches for expert finding have not sufficiently taken into account these document features. We propose a novel language modeling approach, which integrates multiple document features, for expert finding. Our experiments on two large scale TREC Enterprise Track datasets, i.e., the W3C and CSIRO datasets, demonstrate that the natures of the two organizational intranets and two types of expert finding tasks, i.e., key contact finding for CSIRO and knowledgeable person finding for W3C, influence the effectiveness of different document features. Our work provides insights into which document features work for certain types of expert finding tasks, and helps design expert finding strategies that are effective for different scenarios. Our main contribution is to develop an effective formal method for modeling multiple document features in expert finding, and conduct a systematic investigation of their effects. It is worth noting that our novel approach achieves better results in terms of MAP than previous language model based approaches and the best automatic runs in both the TREC2006 and TREC2007 expert search tasks, respectively.",2010,Knowledge and Information Systems volume 23 issue 1 pp 29-54,design document listing;subject matter expert;document structure description;systems modeling;language model;world wide web;data mining;database;artificial intelligence;computer science;
A new multiobjective clustering technique based on the concepts of stability and symmetry,Sriparna Saha (Indian Statistical Institute);Sanghamitra Bandyopadhyay (Indian Statistical Institute);,"2664062897,2148373778","Most clustering algorithms operate by optimizing (either implicitly or explicitly) a single measure of cluster solution quality. Such methods may perform well on some data sets but lack robustness with respect to variations in cluster shape, proximity, evenness and so forth. In this paper, we have proposed a multiobjective clustering technique which optimizes simultaneously two objectives, one reflecting the total cluster symmetry and the other reflecting the stability of the obtained partitions over different bootstrap samples of the data set. The proposed algorithm uses a recently developed simulated annealing-based multiobjective optimization technique, named AMOSA, as the underlying optimization strategy. Here, points are assigned to different clusters based on a newly defined point symmetry-based distance rather than the Euclidean distance. Results on several artificial and real-life data sets in comparison with another multiobjective clustering technique, MOCK, three single objective genetic algorithm-based automatic clustering techniques, VGAPS clustering, GCUK clustering and HNGA clustering, and several hybrid methods of determining the appropriate number of clusters from data sets show that the proposed technique is well suited to detect automatically the appropriate number of clusters as well as the appropriate partitioning from data sets having point symmetric clusters. The performance of AMOSA as the underlying optimization technique in the proposed clustering algorithm is also compared with PESA-II, another evolutionary multiobjective optimization technique.",2010,Knowledge and Information Systems volume 23 issue 1 pp 1-27,flame clustering;k medians clustering;brown clustering;canopy clustering algorithm;determining the number of clusters in a data set;dbscan;correlation clustering;constrained clustering;data stream clustering;cure data clustering algorithm;single linkage clustering;affinity propagation;fuzzy clustering;clustering high dimensional data;hierarchical clustering;cluster analysis;consensus clustering;euclidean geometry;evolutionary algorithm;combinatorics;machine learning;mathematical optimization;algorithm;computer science;mathematics;
Performance study of distributed Apriori-like frequent itemsets mining,Lamine M. Aouad (University College Dublin);Nhien-An Le-Khac (University College Dublin);Tahar M. Kechadi (University College Dublin);,"394646440,2068831938,2013147527","In this article, we focus on distributed Apriori-based frequent itemsets mining. We present a new distributed approach which takes into account inherent characteristics of this algorithm. We study the distribution aspect of this algorithm and give a comparison of the proposed approach with a classical Apriori-like distributed algorithm, using both analytical and experimental studies. We find that under a wide range of conditions and datasets, the performance of a distributed Apriori-like algorithm is not related to global strategies of pruning since the performance of the local Apriori generation is usually characterized by relatively high success rates of candidate sets frequency at low levels which switch to very low rates at some stage, and often drops to zero. This means that the intermediate communication steps and remote support counts computation and collection in classical distributed schemes are computationally inefficient locally, and then constrains the global performance. Our performance evaluation is done on a large cluster of workstations using the Condor system and its workflow manager DAGMan. The results show that the presented approach greatly enhances the performance and achieves good scalability compared to a typical distributed Apriori founded algorithm.",2010,Knowledge and Information Systems volume 23 issue 1 pp 55-72,fsa red algorithm;workflow;distributed algorithm;data mining;database;real time computing;computer science;
Mining dynamic association rules with comments,Bin Shen (Zhejiang University);Min Yao (Zhejiang University);Zhaohui Wu (Zhejiang University);Yunjun Gao (Zhejiang University);,"2109407504,2238995329,2151930384,2637039255","In this paper, we study a new problem of mining dynamic association rules with comments (DAR-C for short). A DAR-C contains not only rule itself, but also its comments that specify when to apply the rule. In order to formalize this problem, we first present the expression method of candidate effective time slots, and then propose several definitions concerning DAR-C. Subsequently, two algorithms, namely ITS2 and EFP-Growth2, are developed for handling the problem of mining DAR-C. In particular, ITS2 is an improved two-stage dynamic association rule mining algorithm, while EFP-Growth2 is based on the EFP-tree structure and is suitable for mining high-density mass data. Extensive experimental results demonstrate that the efficiency and scalability of our proposed two algorithms (i.e., ITS2 and EFP-Growth2) on DAR-C mining tasks, and their practicability on real retail dataset.",2010,Knowledge and Information Systems volume 23 issue 1 pp 73-98,association rule learning;concept mining;data mining;artificial intelligence;machine learning;computer science;
Semi-automated schema integration with SASMINT,Ozgul Unal (University of Amsterdam);Hamideh Afsarmanesh (University of Amsterdam);,"2049912029,139710438","The emergence of increasing number of collaborating organizations has made clear the need for supporting interoperability infrastructures, enabling sharing and exchange of data among organizations. Schema matching and schema integration are the crucial components of the interoperability infrastructures, and their semi-automation to interrelate or integrate heterogeneous and autonomous databases in collaborative networks is desired. The Semi-Automatic Schema Matching and INTegration (SASMINT) System introduced in this paper identifies and resolves several important syntactic, semantic, and structural conflicts among schemas of relational databases to find their likely matches automatically. Furthermore, after getting the user validation on the matched results, it proposes an integrated schema. SASMINT uses a combination of a variety of metrics and algorithms from the Natural Language Processing and Graph Theory domains for its schema matching. For the schema integration, it utilizes a number of derivation rules defined in the scope of the research work explained in this paper. Furthermore, a derivation language called SASMINT Derivation Markup Language (SDML) is defined for capturing and formulating both the results of matching and the integration that can be further used, for example for federated query processing from independent databases. In summary, the paper focuses on addressing: (1) conflicts among schemas that make automatic schema matching and integration difficult, (2) the main components of the SASMINT approach and system, (3) in-depth exploration of SDML, (4) heuristic rules designed and implemented as part of the schema integration component of the SASMINT system, and (5) experimental evaluation of SASMINT.",2010,Knowledge and Information Systems volume 23 issue 1 pp 99-128,three schema approach;schema migration;information schema;star schema;document structure description;database schema;conceptual schema;interoperability;parsing;shared memory;markup language;collaboration;natural language;heterogeneity;relational database;metric;automation;distributed database;semantics;graph theory;data mining;database;algorithm;computer science;
Tracking recurring contexts using ensemble classifiers: an application to email filtering,Ioannis Katakis (Aristotle University of Thessaloniki);Grigorios Tsoumakas (Aristotle University of Thessaloniki);Ioannis P. Vlahavas (Aristotle University of Thessaloniki);,"1921192703,2012164496,695239088","Concept drift constitutes a challenging problem for the machine learning and data mining community that frequently appears in real world stream classification problems. It is usually defined as the unforeseeable concept change of the target variable in a prediction task. In this paper, we focus on the problem of recurring contexts, a special sub-type of concept drift, that has not yet met the proper attention from the research community. In the case of recurring contexts, concepts may re-appear in future and thus older classification models might be beneficial for future classifications. We propose a general framework for classifying data streams by exploiting stream clustering in order to dynamically build and update an ensemble of incremental classifiers. To achieve this, a transformation function that maps batches of examples into a new conceptual representation model is proposed. The clustering algorithm is then applied in order to group batches of examples into concepts and identify recurring contexts. The ensemble is produced by creating and maintaining an incremental classifier for every concept discovered in the data stream. An experimental study is performed using (a) two new real-world concept drifting datasets from the email domain, (b) an instantiation of the proposed framework and (c) five methods for dealing with drifting concepts. Results indicate the effectiveness of the proposed representation and the suitability of the concept-specific classifiers for problems with recurring contexts.",2010,Knowledge and Information Systems volume 22 issue 3 pp 371-391,concept drift;data stream mining;data mining;database;pattern recognition;artificial intelligence;machine learning;computer science;
Integration of decision support systems to improve decision support performance,Shaofeng Liu (University of Strathclyde);Alex H. B. Duffy (University of Strathclyde);Robert Ian Whitfield (University of Strathclyde);Iain M. Boyle (University of Strathclyde);,"2098061014,2149378444,2433143756,2113485764","Decision support system (DSS) is a well-established research and development area. Traditional isolated, stand-alone DSS has been recently facing new challenges. In order to improve the performance of DSS to meet the challenges, research has been actively carried out to develop integrated decision support systems (IDSS). This paper reviews the current research efforts with regard to the development of IDSS. The focus of the paper is on the integration aspect for IDSS through multiple perspectives, and the technologies that support this integration. More than 100 papers and software systems are discussed. Current research efforts and the development status of IDSS are explained, compared and classified. In addition, future trends and challenges in integration are outlined. The paper concludes that by addressing integration, better support will be provided to decision makers, with the expectation of both better decisions and improved decision making processes.",2010,Knowledge and Information Systems volume 22 issue 3 pp 261-286,decision making;intelligent decision support system;decision support system;management science;knowledge management;data mining;artificial intelligence;computer science;
Latent semantic analysis for vector space expansion and fuzzy logic-based genetic clustering,Wei Song (Chonbuk National University);Soon Cheol Park (Chonbuk National University);,"2575472768,2142070512","This paper proposes an improved latent semantic analysis (LSA) model to represent textual document and takes advantage of a fuzzy logic based genetic algorithm (FLGA) for clustering. The standard genetic algorithm (GA) in conventional vector space model is rather difficult to deal with because the high dimensional encoding of GA makes it explore the optimal solution in a complicated space which is prone to cause an overflow problem. The LSA-based corpus model not only reduces the dimensions drastically, but also creates an underlying semantic structure which enhances its ability of distinguishing documents in terms of concepts and indirectly improves the ability of GA for clustering (genetic clustering). A novel FLGA is proposed in conjunction with this semantic model in this study. According to the nature of biological evolution, several fuzzy controllers are given to adaptively adjust and optimize the behaviors of the GA which can effectively prevent the premature convergence to a suboptimum solution. The experiment results show that the fuzzy logic controllers enhance the ability of the GA to explore the global optimum solution, and the utilization of the LSA-based text representation method to FLGA further improves its clustering performance.",2010,Knowledge and Information Systems volume 22 issue 3 pp 347-369,semantic data model;fuzzy clustering;vector space model;premature convergence;latent semantic analysis;multidimensional scaling;fuzzy logic;dimensionality reduction;cluster analysis;vector space;genetic algorithm;data mining;database;artificial intelligence;machine learning;computer science;mathematics;
Linear response time for implicate and implicant queries,Neil V. Murray (State University of New York System);Erik Rosenthal (University of New Haven);,"2096490972,2104828423","Knowledge bases can be represented as propositional Formulas. A query of such a theory typically has the form, Is a clause an implicate of the theory? Answering such queries can require exponential time. In Kautz and Selman (Proceedings of the international workshop on processing declarative knowledge (PDK), Kaiserslautern, Germany, 1991), knowledge compilation was proposed as a solution to this problem: Pay the exponential penalty once by compiling the knowledge base into a target language that would guarantee fast response to queries. The reduced implicate trie (ri-trie), introduced in Murray and Rosenthal (Proceedings of the international conference TABLEAUX 2005—analytic tableaux and related methods, Koblenz, Germany. Lecture notes in artificial intelligence, vol 3702. Springer, Berlin, pp 231–244, 2005), may be used as a target language for knowledge compilation. It has the property that a query is processed in time linear in the size of the query, regardless of the size of the compiled knowledge base. In this paper, structures dual to ri-tries, the reduced implicant tries are investigated, and the dual problem—determining the implicants of a formula—is considered. The main result is that, for a given formula, the two structures can be merged into a single reduced implicate/implicant trie that can serve dual roles, representing both implicates and implicants. Furthermore, rii-tries can be computed directly, without separately computing and then merging the dual structures.",2010,Knowledge and Information Systems volume 22 issue 3 pp 287-317,propositional calculus;duality;knowledge base;theoretical computer science;data mining;database;artificial intelligence;machine learning;algorithm;computer science;mathematics;
OntoMap: an ontology-based architecture to perform the semantic mapping between an interlingua and software components,Flávia Linhalis (University of São Paulo);Renata Pontin de Mattos Fortes (University of São Paulo);Dilvan de Abreu Moreira (University of São Paulo);,"178889890,681198521,2154485114","This paper is about the use of natural language to communicate with computers. Most researches that have pursued this goal consider only requests expressed in English. A way to facilitate the use of several languages in natural language systems is by using an interlingua. An interlingua is an intermediary representation for natural language information that can be processed by machines. We propose to convert natural language requests into an interlingua [universal networking language (UNL)] and to execute these requests using software components. In order to achieve this goal, we propose OntoMap, an ontology-based architecture to perform the semantic mapping between UNL sentences and software components. OntoMap also performs component search and retrieval based on semantic information formalized in ontologies and rules.",2010,Knowledge and Information Systems volume 22 issue 3 pp 319-345,semantic computing;universal networking language;language identification;component based software engineering;natural language processing;database;programming language;computer science;
A document-sensitive graph model for multi-document summarization,Furu Wei (Hong Kong Polytechnic University);Wenjie Li (Hong Kong Polytechnic University);Qin Lu (Hong Kong Polytechnic University);Yanxiang He (Wuhan University);,"2631802616,2139868060,2112113948,2163664958","In recent years, graph-based models and ranking algorithms have drawn considerable attention from the extractive document summarization community. Most existing approaches take into account sentence-level relations (e.g. sentence similarity) but neglect the difference among documents and the influence of documents on sentences. In this paper, we present a novel document-sensitive graph model that emphasizes the influence of global document set information on local sentence evaluation. By exploiting document–document and document–sentence relations, we distinguish intra-document sentence relations from inter-document sentence relations. In such a way, we move towards the goal of truly summarizing multiple documents rather than a single combined document. Based on this model, we develop an iterative sentence ranking algorithm, namely DsR (Document-Sensitive Ranking). Automatic ROUGE evaluations on the DUC data sets show that DsR outperforms previous graph-based models in both generic and query-oriented summarization tasks.",2010,Knowledge and Information Systems volume 22 issue 2 pp 245-259,document structure description;iterative method;systems modeling;multi document summarization;automatic summarization;graph theory;natural language processing;data mining;database;pattern recognition;computer science;
Efficient mining of skyline objects in subspaces over data streams,Zhenhua Huang (Tongji University);Shengli Sun (Peking University);Wei Wang (Fudan University);,"2646911244,2318646501,2296847996","Given a set of k-dimensional objects, the skyline query finds the objects that are not dominated by others. In practice, different users may be interested in different dimensions of the data, and issue queries on any subset of k dimensions in stream environments. This paper focuses on supporting concurrent and unpredictable subspace skyline queries over data streams. Simply to compute and store the skyline objects of every subspace in stream environments will incur expensive update cost. To balance the query cost and update cost, we only maintain the full space skyline in this paper. We first propose an efficient maintenance algorithm and several novel pruning techniques. Then, an efficient and scalable two-phase algorithm is proposed to process the skyline queries in different subspaces based on the full space skyline. Furthermore, we present the theoretical analyses and extensive experiments that demonstrate our method is both efficient and effective.",2010,Knowledge and Information Systems volume 22 issue 2 pp 159-183,regular grid;query optimization;grid;sliding window protocol;pareto distribution;vector space;world wide web;data mining;database;computer science;
On integrating event definition and event detection,Nadim Obeid (University of Jordan);Raj B. K. N. Rao;,"2107019284,2706409932","We develop, in this paper, a representation of time and events that supports a range of reasoning tasks such as monitoring and detection of event patterns which may facilitate the explanation of root cause(s) of faults. We shall compare two approaches to event definition: the active database approach in which events are defined in terms of the conditions for their detection at an instant, and the knowledge representation approach in which events are defined in terms of the conditions for their occurrence over an interval. We shall show the shortcomings of the former definition and employ a three-valued temporal first order nonmonotonic logic, extended with events, in order to integrate both definitions.",2010,Knowledge and Information Systems volume 22 issue 2 pp 129-158,first order logic;knowledge representation and reasoning;data mining;artificial intelligence;algorithm;computer science;mathematics;
Bulk construction of dynamic clustered metric trees,Lior Aronovich (Tel Aviv University);Israel Spiegler (Tel Aviv University);,"2275526049,2590109954","Repositories of complex data types, such as images, audio, video and free text, are becoming increasingly frequent in various fields. A general searching approach for such data types is that of similarity search, where the search is for similar objects and similarity is modeled by a metric distance function. An important class of access methods for similarity search in metric data is that of dynamic clustered metric trees, where the index is structured as a paged and balanced tree and the space is partitioned hierarchically into compact regions. While access methods of this class allow dynamic insertions typically of single objects, the problem of efficiently inserting a given data set into the index in bulk is largely open. In this article we address this problem and propose novel algorithms corresponding to its two cases, where the index is initially empty (i.e. bulk loading), and where the index is initially non empty (i.e. bulk insertion). The proposed bulk loading algorithm builds the index bottom-up layer by layer, using a new sampling based clustering method, which improves clustering results by improving the quality of the selected sample sets. The proposed bulk insertion algorithm employs the bulk loading algorithm to load the given data into a new index structure, and then merges the new and the existing structures into a unified high quality index, using a novel decomposition method to reduce overlaps between the structures. Both algorithms yield significantly improved construction and search performance, and are applicable to all dynamic clustered metric trees. Results from an extensive experimental study show that the proposed algorithms outperform alternative methods, reducing construction costs by up to 47% for CPU costs and 99% for I/O costs, and search costs by up to 48% for CPU costs and 30% for I/O costs.",2010,Knowledge and Information Systems volume 22 issue 2 pp 211-244,metric space;metric;data warehouse;data mining;database;artificial intelligence;machine learning;algorithm;computer science;mathematics;
Consistent collective evaluation of multiple continuous queries for filtering heterogeneous data streams,Hyun-Ho Lee (Yonsei University);Won-Suk Lee (Yonsei University);,"2311539598,2169618082","Query processing for a data stream should also be continuous and rapid. This article proposes a novel approach for consistent collective evaluation of multiple continuous queries for filtering two different types of data streams: a relational stream and an XML stream. The proposed approach commonly provides region-based selection constructs: an attribute selection construct for relational queries and a path selection construct for XPath queries. Both collectively evaluate the selection predicates of the same attribute (path), based on the precomputed matching results of the queries in each of the disjoint regions divided by the selection predicates. The performance experiments show that the proposed approach is practically more efficient and stable than other approaches at run-time.",2010,Knowledge and Information Systems volume 22 issue 2 pp 185-210,xml;data type;relational database;filter;data processing;theoretical computer science;data mining;database;computer science;
MiSPOT: dynamic product placement for digital TV through MPEG-4 processing and semantic reasoning,Martín López-Nores (University of Vigo);José J. Pazos-Arias (University of Vigo);Jorge García-Duque (University of Vigo);Yolanda Blanco-Fernández (University of Vigo);Manuela I. Martín-Vicente (University of Vigo);Ana Fernández-Vilas (University of Vigo);Manuel Ramos-Cabrer (University of Vigo);Alberto Gil-Solla (University of Vigo);,"1991145387,271757994,70476713,237803386,1415777907,53292566,1931847364,224016252","In an increasingly competitive market, stakeholders of the television industry strive to exploit all the possibilities to get revenues from advertising, but their practices are usually at odds with the comfort of the TV viewers. This paper presents the proof of concept of MiSPOT, a system that brings a non-invasive and fully personalized form of advertising to Interactive Digital TV, targeting both domestic and mobile receivers. MiSPOT employs semantic reasoning techniques to select advertisements suited to the preferences, interests and needs of each individual viewer, and then relies on multimedia composition abilities to blend the advertising material with the TV program he/she is viewing at any time. The advertisements can be set to launch interactive commercials, thus enabling means for the provision of t-commerce services. Evaluation experiments are described to show the technical viability of the proposal, and also to gauge the opinions of end users. Questions about the potential impact and exploitation of this new form of advertising are addressed too.",2010,Knowledge and Information Systems volume 22 issue 1 pp 101-128,personalization;digital television;multimedia;database;computer science;
Effectiveness of NAQ-tree as index structure for similarity search in high-dimensional metric space,Ming Zhang (University of Calgary);Reda Alhajj (University of Calgary);,"2428428385,497422635","Similarity search (e.g., k-nearest neighbor search) in high-dimensional metric space is the key operation in many applications, such as multimedia databases, image retrieval and object recognition, among others. The high dimensionality and the huge size of the data set require an index structure to facilitate the search. State-of-the-art index structures are built by partitioning the data set based on distances to certain reference point(s). Using the index, search is confined to a small number of partitions. However, these methods either ignore the property of the data distribution (e.g., VP-tree and its variants) or produce non-disjoint partitions (e.g., M-tree and its variants, DBM-tree); these greatly affect the search efficiency. In this paper, we study the effectiveness of a new index structure, called Nested-Approximate-eQuivalence-class tree (NAQ-tree), which overcomes the above disadvantages. NAQ-tree is constructed by recursively dividing the data set into nested approximate equivalence classes. The conducted analysis and the reported comparative test results demonstrate the effectiveness of NAQ-tree in significantly improving the search efficiency.",2010,Knowledge and Information Systems volume 22 issue 1 pp 1-26,ball tree;interpolation search;jump search;beam search;nearest neighbor search;dimensionality reduction;image retrieval;discrete mathematics;combinatorics;machine learning;computer science;mathematics;
Protecting buying agents in e-marketplaces by direct experience trust modelling,Thomas T. Tran (University of Ottawa);,2108359352,"In this paper, we describe a framework for modelling the trustworthiness of sellers in the context of an electronic marketplace where multiple selling agents may offer the same good with different qualities and selling agents may alter the quality of their goods. We consider that there may be dishonest sellers in the market (for example, agents who offer goods with high quality and later offer the same goods with very low quality). In our approach, buying agents use a combination of reinforcement learning and trust modelling to enhance their knowledge about selling agents and hence their opportunities to purchase high value goods in the marketplace. This paper focuses on presenting the theoretical results demonstrating how the modelling of trust can protect buying agents from dishonest selling agents. The results show that our proposed buying agents will not be harmed infinitely by dishonest selling agents and therefore will not incur infinite loss, if they are cautious in setting their penalty factor. We also discuss the value of our particular model for trust, in contrast with related work and conclude with directions for future research.",2010,Knowledge and Information Systems volume 22 issue 1 pp 65-100,confidence;systems modeling;intelligent agent;reinforcement learning;artificial intelligence;simulation;computer science;
The analysis and management of non-canonical requirement specifications through a belief integration game,Ebrahim Bagheri (University of New Brunswick);Ali A. Ghorbani (University of New Brunswick);,"756122464,2134659517","Non-canonical requirement specifications refer to a set of software requirements that is either inconsistent, vague or incomplete. In this paper, we provide a correspondence between requirement specifications and annotated propositional belief bases. Through this analogy, we are able to analyze the contents of a given set of requirement collections known as viewpoints and specify whether they are incomplete, incoherent, or inconsistent under a closed-world reasoning assumption. Based on the requirement collections’ properties introduced in this paper, we define a viewpoint integration game through which the inconsistencies of non-canonical requirement specifications are resolved. The game consists of several rounds of negotiation and is performed by two main functions, namely choice and enhancement functions. The outcome of this game is a set of inconsistency-free requirement collections that can be integrated to form a unique fair representative of the given requirement collections.",2010,Knowledge and Information Systems volume 22 issue 1 pp 27-64,subjective logic;non functional requirement;software requirements;requirement;negotiation;belief;software development;data mining;database;artificial intelligence;computer science;
Evolving trees for the retrieval of mass spectrometry-based bacteria fingerprints,Stephan Simmuteit;Frank-Michael Schleif (University of Birmingham);Thomas Villmann (Leipzig University);Barbara Hammer (Citec);,"1969481714,2004635363,2173309013,2134858009","In this paper, we investigate the application of Evolving Trees (ET) for the analysis of mass spectrometric data of bacteria. Evolving Trees are extensions of self-organizing maps (SOMs) developed for hierarchical classification systems. Therefore, they are well suited for taxonomic problems such as the identification of bacteria. Here, we focus on three topics, an appropriate pre-processing and encoding of the spectra, an adequate data model by means of a hierarchical Evolving Tree and an interpretable visualization. First, the high dimensionality of the data is reduced by a compact representation. Here, we employ sparse coding, specifically tailored for the processing of mass spectra. In the second step, the topographic information which is expected in the fingerprints is used for advanced tree evaluation and analysis. We adapted the original topographic product for SOMs for ET to achieve a judgment of topography. Additionally we transferred the concept of U-matrix for evaluation of the separability of SOMs to their analog in ET. We demonstrate these extensions for two mass spectrometric data sets of bacteria fingerprints and show their classification and evaluation capabilities in comparison to state of the art techniques.",2010,Knowledge and Information Systems volume 25 issue 2 pp 327-343,news;visualization;mass spectrometry;artificial intelligence;machine learning;computer science;
TOPSIL-Miner: an efficient algorithm for mining top-,Bei Yang (Zhengzhou University);Hou-Kuan Huang (Beijing Jiaotong University);,"2479944797,2692737160",-,2010,Knowledge and Information Systems,fsa red algorithm;gsp algorithm;domain knowledge;data stream mining;data structure;data mining;database;pattern recognition;artificial intelligence;programming language;computer science;
The use of modality in in-vehicle information presentation: a brief overview,Yujia Cao (University of Twente);Mariet Theune (University of Twente);,"2155478900,1998152884",-,2010,Knowledge and Information Systems,management science;knowledge management;multimedia;computer science;
Subspace and projected clustering: experimental evaluation and analysis,Gabriela Moise (University of Alberta);Arthur Zimek (Ludwig Maximilian University of Munich);Peer Kröger (Ludwig Maximilian University of Munich);Hans-Peter Kriegel (Ludwig Maximilian University of Munich);Jörg Sander (University of Alberta);,"2059192723,242745652,2100673337,1919135125,2118842476","Subspace and projected clustering have emerged as a possible solution to the challenges associated with clustering in high-dimensional data. Numerous subspace and projected clustering techniques have been proposed in the literature. A comprehensive evaluation of their advantages and disadvantages is urgently needed. In this paper, we evaluate systematically state-of-the-art subspace and projected clustering techniques under a wide range of experimental settings. We discuss the observed performance of the compared techniques, and we make recommendations regarding what type of techniques are suitable for what kind of problems.",2009,Knowledge and Information Systems volume 21 issue 3 pp 299-326,clustering high dimensional data;program evaluation;cluster analysis;vector space;biological classification;data mining;machine learning;simulation;computer science;mathematics;
Redesigning business processes: a methodology based on simulation and process mining techniques,Laura Măruşter (University of Groningen);Nick R. T. P. van Beest (University of Groningen);,"130718764,2502610808","Nowadays, organizations have to adjust their business processes along with the changing environment in order to maintain a competitive advantage. Changing a part of the system to support the business process implies changing the entire system, which leads to complex redesign activities. In this paper, a bottom-up process mining and simulation-based methodology is proposed to be employed in redesign activities. The methodology starts with identifying relevant performance issues, which are used as basis for redesign. A process model is “mined” and simulated as a representation of the existing situation, followed by the simulation of the redesigned process model as prediction of the future scenario. Finally, the performance criteria of the current business process model and the redesigned business process model are compared such that the potential performance gains of the redesign can be predicted. We illustrate the methodology with three case studies from three different domains: gas industry, government institution and agriculture.",2009,Knowledge and Information Systems volume 21 issue 3 pp 267-297,artifact centric business process model;business process model and notation;business rule;process mining;business process discovery;business process modeling;business process management;business process;scenario;business model;agriculture;support;process modeling;competitive advantage;process management;operations research;data mining;computer science;
Towards a graph-based user profile modeling for a session-based personalized search,Mariam Daoud (Paul Sabatier University);Lynda-Tamine Lechani (Paul Sabatier University);Mohand Boughanem (Paul Sabatier University);,"2019012303,2285551354,117014290","Most Web search engines use the content of the Web documents and their link structures to assess the relevance of the document to the user’s query. With the growth of the information available on the web, it becomes difficult for such Web search engines to satisfy the user information need expressed by few keywords. First, personalized information retrieval is a promising way to resolve this problem by modeling the user profile by his general interests and then integrating it in a personalized document ranking model. In this paper, we present a personalized search approach that involves a graph-based representation of the user profile. The user profile refers to the user interest in a specific search session defined as a sequence of related queries. It is built by means of score propagation that allows activating a set of semantically related concepts of reference ontology, namely the ODP. The user profile is maintained across related search activities using a graph-based merging strategy. For the purpose of detecting related search activities, we define a session boundary recognition mechanism based on the Kendall rank correlation measure that tracks changes in the dominant concepts held by the user profile relatively to a new submitted query. Personalization is performed by re-ranking the search results of related queries using the user profile. Our experimental evaluation is carried out using the HARD 2003 TREC collection and showed that our session boundary recognition mechanism based on the Kendall measure provides a significant precision comparatively to other non-ranking based measures like the cosine and the WebJaccard similarity measures. Moreover, results proved that the graph-based search personalization is effective for improving the search accuracy.",2009,Knowledge and Information Systems volume 21 issue 3 pp 365-398,ranking;beam search;web search query;personalization;user modeling;search engine;ontology;web search engine;graph theory;world wide web;information retrieval;data mining;database;computer science;
Practical protocol for Yao’s millionaires problem enables secure multi-party computation of metrics and efficient privacy-preserving k -NN for large data sets,Artak Amirbekyan (University of Queensland);Vladimir Estivill-Castro (Griffith University);,"1904545193,273506593","Finding the nearest k objects to a query object is a fundamental operation for many data mining algorithms. With the recent interest in privacy, it is not surprising that there is strong interest in k-NN queries to enable clustering, classification and outlier-detection tasks. However, previous approaches to privacy-preserving k-NN have been costly and can only be realistically applied to small data sets. In this paper, we provide efficient solutions for k-NN queries for vertically partitioned data. We provide the first solution for the L ∞ (or Chessboard) metric as well as detailed privacy-preserving computation of all other Minkowski metrics. We enable privacy-preserving L ∞ by providing a practical approach to the Yao’s millionaires problem with more than two parties. This is based on a pragmatic and implementable solution to Yao’s millionaires problem with shares. We also provide privacy-preserving algorithms for combinations of local metrics into a global metric that handles the large dimensionality and diversity of attributes common in vertically partitioned data. To manage very large data sets, we provide a privacy-preserving SASH (a very successful data structure for associative queries in high dimensions). Besides providing a theoretical analysis, we illustrate the efficiency of our approach with an empirical evaluation.",2009,Knowledge and Information Systems volume 21 issue 3 pp 327-363,yao s millionaires problem;minkowski space;interest;secure multi party computation;outlier;curse of dimensionality;cluster analysis;biological classification;data analysis;pragmatics;information extraction;data structure;theoretical computer science;data mining;database;artificial intelligence;machine learning;statistics;computer science;mathematics;
Mining frequent arrangements of temporal intervals,Panagiotis Papapetrou (Boston University);George Kollios (Boston University);Stan Sclaroff (Boston University);Dimitrios Gunopulos (National and Kapodistrian University of Athens);,"2000108749,2217777545,689201655,2712250546","The problem of discovering frequent arrangements of temporal intervals is studied. It is assumed that the database consists of sequences of events, where an event occurs during a time-interval. The goal is to mine temporal arrangements of event intervals that appear frequently in the database. The motivation of this work is the observation that in practice most events are not instantaneous but occur over a period of time and different events may occur concurrently. Thus, there are many practical applications that require mining such temporal correlations between intervals including the linguistic analysis of annotated data from American Sign Language as well as network and biological data. Three efficient methods to find frequent arrangements of temporal intervals are described; the first two are tree-based and use breadth and depth first search to mine the set of frequent arrangements, whereas the third one is prefix-based. The above methods apply efficient pruning techniques that include a set of constraints that add user-controlled focus into the mining process. Moreover, based on the extracted patterns a standard method for mining association rules is employed that applies different interestingness measures to evaluate the significance of the discovered patterns and rules. The performance of the proposed algorithms is evaluated and compared with other approaches on real (American Sign Language annotations and network data) and large synthetic datasets.",2009,Knowledge and Information Systems volume 21 issue 2 pp 133-171,interval arithmetic;information extraction;data mining;artificial intelligence;algorithm;computer science;
SPARCL: an effective and efficient algorithm for mining arbitrary shape-based clusters,Vineet Chaoji (Rensselaer Polytechnic Institute);Mohammad Al Hasan (Rensselaer Polytechnic Institute);Saeed Salem (Rensselaer Polytechnic Institute);Mohammed J. Zaki (Rensselaer Polytechnic Institute);,"2344125319,2634411756,2164451553,2165917828","Clustering is one of the fundamental data mining tasks. Many different clustering paradigms have been developed over the years, which include partitional, hierarchical, mixture model based, density-based, spectral, subspace, and so on. The focus of this paper is on full-dimensional, arbitrary shaped clusters. Existing methods for this problem suffer either in terms of the memory or time complexity (quadratic or even cubic). This shortcoming has restricted these algorithms to datasets of moderate sizes. In this paper we propose SPARCL, a simple and scalable algorithm for finding clusters with arbitrary shapes and sizes, and it has linear space and time complexity. SPARCL consists of two stages—the first stage runs a carefully initialized version of the Kmeans algorithm to generate many small seed clusters. The second stage iteratively merges the generated clusters to obtain the final shape-based clusters. Experiments were conducted on a variety of datasets to highlight the effectiveness, efficiency, and scalability of our approach. On the large datasets SPARCL is an order of magnitude faster than the best existing approaches.",2009,Knowledge and Information Systems volume 21 issue 2 pp 201-229,time complexity;cluster analysis;information extraction;combinatorics;machine learning;statistics;algorithm;computer science;mathematics;
Parameter determination and feature selection for back-propagation network by particle swarm optimization,Shih-Wei Lin (Chang Gung University);Shih-Chieh Chen (National Taiwan University of Science and Technology);Wen-Jie Wu (Chang Gung University);Chih-Hsien Chen (Fo Guang University);,"2102152005,2715613181,2681176793,2690434051","The back-propagation network (BPN) is a popular tool with applications in a variety of fields. Nevertheless, different problems may require different parameter settings for a given network architecture. A dataset may contain many features, but not all features are beneficial for classification by the BPN. Therefore, a particle-swarm-optimization-based approach, denoted as PSOBPN, is proposed to obtain the suitable parameter settings for BPN and to select the beneficial subset of features which result in a better classification accuracy rate. A set of 23 problems with a range of examples and features drawn from the UCI (University of California, Irvine) machine learning repository is adopted to test the performance of the proposed algorithm. The results are compared with several well-known published algorithms. The comparative study shows that the proposed approach improves the classification accuracy rate in most test problems. Furthermore, when the feature selection is taken into consideration, the classification accuracy rates of most datasets are increased. The proposed algorithm should thus be useful to both practitioners and researchers.",2009,Knowledge and Information Systems volume 21 issue 2 pp 249-266,network architecture;particle swarm optimization;backpropagation;biological classification;comparative research;feature selection;swarm intelligence;data mining;artificial intelligence;machine learning;computer science;
Data mining of vector–item patterns using neighborhood histograms,Anne M. Denton (North Dakota State University);Jianfei Wu (North Dakota State University);,"2126706684,2155209623","The representation of multiple continuous attributes as dimensions in a vector space has been among the most influential concepts in machine learning and data mining. We consider sets of related continuous attributes as vector data and search for patterns that relate a vector attribute to one or more items. The presence of an item set defines a subset of vectors that may or may not show unexpected density fluctuations. We test for fluctuations by studying density histograms. A vector–item pattern is considered significant if its density histogram significantly differs from what is expected for a random subset of transactions. Using two different density measures, we evaluate the algorithm on two real data sets and one that was artificially constructed from time series data.",2009,Knowledge and Information Systems volume 21 issue 2 pp 173-199,time series;gene expression;concept learning;information extraction;data mining;artificial intelligence;statistics;algorithm;computer science;mathematics;
Context inclusive function evaluation: a case study with EM-based multi-scale multi-granular image classification,Vijay Gandhi (University of Minnesota);James M. Kang (University of Minnesota);Shashi Shekhar (University of Minnesota);Junchang Ju (Boston University);Eric D. Kolaczyk (Boston University);Sucharita Gopal (Boston University);,"2495960468,2149178846,2134885186,2306350440,274731230,2102024760","Many statistical queries such as maximum likelihood estimation involve finding the best candidate model given a set of candidate models and a quality estimation function. This problem is common in important applications like land-use classification at multiple spatial resolutions from remote sensing raster data. Such a problem is computationally challenging due to the significant computation cost to evaluate the quality estimation function for each candidate model. For example, a recently proposed method of multi-scale, multi-granular classification has high computational overhead of function evaluation for various candidate models independently before comparison. In contrast, we propose an upper bound based context-inclusive approach that reduces computational overhead based on the context, i.e. the value of the quality estimation function for the best candidate model so far. We also prove that an upper bound exists for each candidate model and the proposed algorithm is correct. Experimental results using land-use classification at multiple spatial resolutions from satellite imagery show that the proposed approach reduces the computational cost significantly.",2009,Knowledge and Information Systems volume 21 issue 2 pp 231-247,maximum likelihood;image processing;artificial intelligence;statistics;computer science;
Graph OLAP: a multi-dimensional framework for graph data analysis,"Chen Chen (University of Illinois at Urbana–Champaign);Xifeng Yan (University of California, Santa Barbara);Feida Zhu (University of Illinois at Urbana–Champaign);Jiawei Han (University of Illinois at Urbana–Champaign);Philip S. Yu (University of Illinois at Chicago);","2703921967,2116657824,2160602068,2121939561,2125104194","Databases and data warehouse systems have been evolving from handling normalized spreadsheets stored in relational databases, to managing and analyzing diverse application-oriented data with complex interconnecting structures. Responding to this emerging trend, graphs have been growing rapidly and showing their critical importance in many applications, such as the analysis of XML, social networks, Web, biological data, multimedia data and spatiotemporal data. Can we extend useful functions of databases and data warehouse systems to handle graph structured data? In particular, OLAP (On-Line Analytical Processing) has been a popular tool for fast and user-friendly multi-dimensional analysis of data warehouses. Can we OLAP graphs? Unfortunately, to our best knowledge, there are no OLAP tools available that can interactively view and analyze graph data from different perspectives and with multiple granularities. In this paper, we argue that it is critically important to OLAP graph structured data and propose a novel Graph OLAP framework. According to this framework, given a graph dataset with its nodes and edges associated with respective attributes, a multi-dimensional model can be built to enable efficient on-line analytical processing so that any portions of the graphs can be generalized/specialized dynamically, offering multiple, versatile views of the data. The contributions of this work are three-fold. First, starting from basic definitions, i.e., what are dimensions and measures in the Graph OLAP scenario, we develop a conceptual framework for data cubes on graphs. We also look into different semantics of OLAP operations, and classify the framework into two major subcases: informational OLAP and topological OLAP. Second, we show how a graph cube can be materialized by calculating a special kind of measure called aggregated graph and how to implement it efficiently. This includes both full materialization and partial materialization where constraints are enforced to obtain an iceberg cube. As we can see, due to the increased structural complexity of data, aggregated graphs that depend on the underlying “network” properties of the graph dataset are much harder to compute than their traditional OLAP counterparts. Third, to provide more flexible, interesting and informative OLAP of graphs, we further propose a discovery-driven multi-dimensional analysis model to ensure that OLAP is performed in an intelligent manner, guided by expert rules and knowledge discovery processes. We outline such a framework and discuss some challenging research issues for discovery-driven Graph OLAP.",2009,Knowledge and Information Systems volume 21 issue 1 pp 41-63,multigraph;online analytical processing;cube;data cube;multidimensional analysis;structural complexity;biological data;spatial database;dimensional analysis;xml;data aggregator;grain size;relational database;data model;conceptual framework;social network;information system;systems modeling;data analysis;data warehouse;data structure;data processing;graph theory;theoretical computer science;data mining;database;artificial intelligence;computer science;
Asymmetric information distances for automated taxonomy construction,Wei Lee Woon (Masdar Institute of Science and Technology);Stuart E. Madnick (Massachusetts Institute of Technology);,"2083129847,300074472","A novel method for automatically constructing taxonomies for specific research domains is presented. The proposed methodology uses term co-occurrence frequencies as an indicator of the semantic closeness between terms. To support the automated creation of taxonomies or subject classifications we present a simple modification to the basic distance measure, and describe a set of procedures by which these measures may be converted into estimates of the desired taxonomy. To demonstrate the viability of this approach, a pilot study on renewable energy technologies is conducted, where the proposed method is used to construct a hierarchy of terms related to alternative energy. These techniques have many potential applications, but one activity in which we are particularly interested is the mapping and subsequent prediction of future developments in the technology and research.",2009,Knowledge and Information Systems volume 21 issue 1 pp 91-111,alternative energy;interest;renewable energy;information asymmetry;asymmetry;biological classification;data analysis;information extraction;taxonomy;semantics;data mining;artificial intelligence;computer science;
Improving keyword based web image search with visual feature distribution and term expansion,Zhiguo Gong (University of Macau);Qian Liu (University of Macau);,"2159588081,2236787533","This paper discusses techniques for improving the performance of keyword-based web image queries. Firstly, a web page is segmented into several text blocks based on semantic cohesion. The text blocks which contain web images are taken as the associated texts of corresponding images and TF*IDF model is initially used to index those web images. Then, for each keyword, both relevant web image set and irrelevant web image set are selected according to their TF*IDF values. And visual feature distributions of both positive image and negative image are modeled using Gaussian Mixture Model. An image’s relevance to the keyword with respect to visual feature is thus defined as the ratio of positive distribution density over negative distribution density. We combine the text-based relevance model with visual feature relevance model to improve the performance. Thirdly, a query expansion model is used to improve the performance further. Expansion terms are selected according to their cooccurrences with the query terms in the top-relevant set of the original query. Our experiments show that our approach yield significant improvement over the traditional keyword based query model.",2009,Knowledge and Information Systems volume 21 issue 1 pp 113-132,keyword density;web query classification;query expansion;cohesion;visual search;relevance;mixture model;web page;density estimation;the internet;systems modeling;image retrieval;semantics;image analysis;world wide web;information retrieval;data mining;computer science;
Approximating the number of frequent sets in dense data,Mario Boley (Fraunhofer Society);Henrik Grosskreutz (Fraunhofer Society);,"2074964178,290692522","We investigate the problem of counting the number of frequent (item)sets—a problem known to be intractable in terms of an exact polynomial time computation. In this paper, we show that it is in general also hard to approximate. Subsequently, a randomized counting algorithm is developed using the Markov chain Monte Carlo method. While for general inputs an exponential running time is needed in order to guarantee a certain approximation bound, we show that the algorithm still has the desired accuracy on several real-world datasets when its running time is capped polynomially.",2009,Knowledge and Information Systems volume 21 issue 1 pp 65-89,counting;markov chain monte carlo;randomized algorithm;time complexity;markov chain;systems modeling;data analysis;monte carlo method;information extraction;econometrics;statistics;algorithm;computer science;mathematics;
A survey and taxonomy of performance improvement of canonical genetic programming,Peyman Kouchakpour (University of Western Australia);Anthony Zaknich (University of Western Australia);Thomas Bräunl (University of Western Australia);,"349250449,126254130,345905177","The genetic programming (GP) paradigm, which applies the Darwinian principle of evolution to hierarchical computer programs, has been applied with breakthrough success in various scientific and engineering applications. However, one of the main drawbacks of GP has been the often large amount of computational effort required to solve complex problems. Much disparate research has been conducted over the past 25 years to devise innovative methods to improve the efficiency and performance of GP. This paper attempts to provide a comprehensive overview of this work related to Canonical Genetic Programming based on parse trees and originally championed by Koza (Genetic programming: on the programming of computers by means of natural selection. MIT, Cambridge, 1992). Existing approaches that address various techniques for performance improvement are identified and discussed with the aim to classify them into logical categories that may assist with advancing further research in this area. Finally, possible future trends in this discipline and some of the open areas of research are also addressed.",2009,Knowledge and Information Systems volume 21 issue 1 pp 1-39,genetic programming;software development;natural selection;efficiency;genetic algorithm;evolutionary algorithm;taxonomy;operations research;artificial intelligence;machine learning;algorithm;computer science;
A new ant colony optimization based algorithm for data allocation problem in distributed databases,Rosa Karimi Adl (Shahid Beheshti University);Seyed Mohammad Taghi Rouhani Rankoohi (Shahid Beheshti University);,"2414262974,349883525","The Performance and the efficiency of a distributed database system depend highly on the way data are allocated to the sites. The NP-completeness of the data allocation problem and the large size of its real occurrence, call for employing a fast and scalable heuristic algorithm. In this paper, we address the data allocation problem in terms of minimizing two different types of data transmission across the network, i.e., data transmissions due to site-fragment dependencies and those caused by inter-fragment dependencies. We propose a new heuristic algorithm which is based on the ant colony optimization meta-heuristic, with regards to the applied strategies for query optimization and integrity enforcement. The goal is to design an efficient data allocation scheme to minimize the total transaction response time under memory capacity constraints of the sites. Experimental tests indicate that our algorithm is capable of producing near- optimal solutions within a reasonable time. The results also reveal the flexibility and scalability of the proposed algorithm.",2009,Knowledge and Information Systems volume 20 issue 3 pp 349-373,query optimization;transaction processing;np complete;ant colony optimization algorithms;response time;heuristic;scalability;input output;data transmission;distributed database;swarm intelligence;operations research;database;artificial intelligence;algorithm;computer science;
Hiding sensitive knowledge without side effects,Aris Gkoulalas-Divanis (University of Thessaly);Vassilios S. Verykios (University of Thessaly);,"57288862,77221159","Sensitive knowledge hiding in large transactional databases is one of the major goals of privacy preserving data mining. However, it is only recently that researchers were able to identify exact solutions for the hiding of knowledge, depicted in the form of sensitive frequent itemsets and their related association rules. Exact solutions allow for the hiding of vulnerable knowledge without any critical compromises, such as the hiding of nonsensitive patterns or the accidental uncovering of infrequent itemsets, amongst the frequent ones, in the sanitized outcome. In this paper, we highlight the process of border revision, which plays a significant role towards the identification of exact hiding solutions, and we provide efficient algorithms for the computation of the revised borders. Furthermore, we review two algorithms that identify exact hiding solutions, and we extend the functionality of one of them to effectively identify exact solutions for a wider range of problems (than its original counterpart). Following that, we introduce a novel framework for decomposition and parallel solving of hiding problems, which are handled by each of these approaches. This framework improves to a substantial degree the size of the problems that both algorithms can handle and significantly decreases their runtime. Through experimentation, we demonstrate the effectiveness of these approaches toward providing high quality knowledge hiding solutions.",2009,Knowledge and Information Systems volume 20 issue 3 pp 263-299,transaction processing;association rule learning;side effect;association;systems modeling;data analysis;information extraction;world wide web;data mining;database;computer science;
PADS: a simple yet effective pattern-aware dynamic search method for fast maximal frequent pattern mining,Xinghuo Zeng (Simon Fraser University);Jian Pei (Simon Fraser University);Ke Wang (Simon Fraser University);Jinyan Li (Nanyang Technological University);,"2161745271,2126330539,2626264286,2190567876","While frequent pattern mining is fundamental for many data mining tasks, mining maximal frequent patterns efficiently is important in both theory and applications of frequent pattern mining. The fundamental challenge is how to search a large space of item combinations. Most of the existing methods search an enumeration tree of item combinations in a depth-first manner. In this paper, we develop a new technique for more efficient max-pattern mining. Our method is pattern-aware: it uses the patterns already found to schedule its future search so that many search subspaces can be pruned. We present efficient techniques to implement the new approach. As indicated by a systematic empirical study using the benchmark data sets, our new approach outperforms the currently fastest max-pattern mining algorithms FPMax* and LCM2 clearly. The source code and the executable code (on both Windows and Linux platforms) are publicly available at http://www.cs.sfu.ca/~jpei/Software/PADS.zip.",2009,Knowledge and Information Systems volume 20 issue 3 pp 375-391,empirical research;information extraction;data mining;database;algorithm;computer science;
Accelerating sequence searching: dimensionality reduction method,Guojie Song (Peking University);Bin Cui (Peking University);Baihua Zheng (Singapore Management University);Kunqing Xie (Peking University);Dongqing Yang (Peking University);,"2101713283,2070862325,2200653933,2144426229,2113473341","Similarity search over long sequence dataset becomes increasingly popular in many emerging applications, such as text retrieval, genetic sequences exploring, etc. In this paper, a novel index structure, namely Sequence Embedding Multiset tree (SEM − tree), has been proposed to speed up the searching process over long sequences. The SEM-tree is a multi-level structure where each level represents the sequence data with different compression level of multiset, and the length of multiset increases towards the leaf level which contains original sequences. The multisets, obtained using sequence embedding algorithms, have the desirable property that they do not need to keep the character order in the sequence, i.e. shorter representation, but can reserve the majority of distance information of sequences. Each level of the tree serves to prune the search space more efficiently as the multisets utilize the predicability to finish the searching process beforehand and reduce the computational cost greatly. A set of comprehensive experiments are conducted to evaluate the performance of the SEM-tree, and the experimental results show that the proposed method is much more efficient than existing representative methods.",2009,Knowledge and Information Systems volume 20 issue 3 pp 301-322,multiplicity;dimensionality reduction;data compression;discrete mathematics;combinatorics;data mining;database;artificial intelligence;machine learning;algorithm;computer science;mathematics;
Taking advantage of metadata semantics: the case of learning-object-based lesson graphs,Olivier Motelet (University of Chile);Nelson Baloian (University of Chile);José A. Pino (University of Chile);,"2311770367,402691554,2136833878","Learning objects (LOs) are pieces of educational material characterized with a valuable amount of information about their content and usage. This additional information is defined as a set of metadata generally following the IEEE LOM specification. This specification also serves to characterize the relations existing between LOs. LOs whose relations are explicit are regarded as the nodes of a lesson graph. Link types and LO metadata constitute the lesson graph semantics. This article proposes to take advantage of lesson graph semantics using a context diffusion approach. It consists in diffusing the metadata-based processes along the edges of the lesson graph. This technique aims at coping with the metadata processing issues arising when some graph metadata are missing, incorrect, or incomplete. This article also presents a three-layer extensible framework for easing the use of context diffusion in a graph. As part of the framework, two original types of metadata processes are introduced. The first one takes advantage of the metadata attribute similarities between related LOs. The second one focuses on the lesson graph consistency. The framework and the application examples were implemented as an open-source Java library used in the lesson graph authoring tool LessonMapper2. During the lesson authoring process, we show that the framework can bring support not only for generating and validating metadata, but also for retrieving LOs.",2009,Knowledge and Information Systems volume 20 issue 3 pp 323-348,metadata repository;metadata;case based reasoning;cognitive neuroscience of visual object recognition;concept learning;semantics;theoretical computer science;world wide web;data mining;database;artificial intelligence;machine learning;computer science;
On classification and segmentation of massive audio data streams,Charu C. Aggarwal (IBM);,2146335907,"In recent years, the proliferation of VOIP data has created a number of applications in which it is desirable to perform quick online classification and recognition of massive voice streams. Typically such applications are encountered in real time intelligence and surveillance. In many cases, the data streams can be in compressed format, and the rate of data processing can often run at the rate of Gigabits per second. All known techniques for speaker voice analysis require the use of an offline training phase in which the system is trained with known segments of speech. The state-of-the-art method for text-independent speaker recognition is known as Gaussian mixture modeling (GMM), and it requires an iterative expectation maximization procedure for training, which cannot be implemented in real time. In many real applications (such as surveillance) it is desirable to perform the recognition process in online time, so that the system can be quickly adapted to new segments of the data. In many cases, it may also be desirable to quickly create databases of training profiles for speakers of interest. In this paper, we discuss the details of such an online voice recognition system. For this purpose, we use our micro-clustering algorithms to design concise signatures of the target speakers. One of the surprising and insightful observations from our experiences with such a system is that while it was originally designed only for efficiency, we later discovered that it was also more accurate than the widely used GMM. This was because of the conciseness of the micro-cluster model, which made it less prone to over training. This is evidence of the fact that it is often possible to get the best of both worlds and do better than complex models both from an efficiency and accuracy perspective. We present experimental results illustrating the effectiveness and efficiency of the method.",2009,Knowledge and Information Systems volume 20 issue 2 pp 137-156,expectation maximization algorithm;speaker recognition;data processing;telecommunications;speech recognition;artificial intelligence;machine learning;computer science;
SPARQL graph pattern rewriting for OWL-DL inference queries,Yixin Jing (Korea University);Dongwon Jeong (Kunsan National University);Doo Kwon Baik (Korea University);,"2152080295,2047072712,2153360310","This paper focuses on the issue of OWL-DL ontology queries implemented in SPARQL. Currently, ontology repositories construct inference ontology models, and match SPARQL queries to the models, to derive inference results. Because an inference model uses much more storage space than the original model, and cannot be reused as inference requirements vary, this method is not suitable for large-scale deployment. To solve this problem, this paper proposes a novel method that passes rewritten SPARQL queries to the original ontology model, to retrieve inference results. We define OWL-DL inference rules and apply them to rewriting Graph Patterns in queries. The paper classifies the inference rules and discusses how these rules affect query rewriting. To illustrate the advantages of our proposal, we present a prototype system based on Jena, and address query optimization, to eliminate the disadvantages of augmented query sentences. We perform a set of query tests and compare the results with related works. The results show that the proposed method results in significantly improved query efficiency, without compromising completeness or soundness.",2009,Knowledge and Information Systems volume 20 issue 2 pp 243-262,sparql;rewriting;query optimization;semantic web;rule of inference;web service;systems modeling;ontology;information retrieval;data mining;database;computer science;
A distributed approach to enabling privacy-preserving model-based classifier training,Hangzai Luo (East China Normal University);Jianping Fan (University of North Carolina at Chapel Hill);Xiaodong Lin (University of Cincinnati);Aoying Zhou (East China Normal University);Elisa Bertino (Purdue University);,"2720076905,2103155697,2117834427,2652990193,717547165","This paper proposes a novel approach for privacy-preserving distributed model-based classifier training. Our approach is an important step towards supporting customizable privacy modeling and protection. It consists of three major steps. First, each data site independently learns a weak concept model (i.e., local classifier) for a given data pattern or concept by using its own training samples. An adaptive EM algorithm is proposed to select the model structure and estimate the model parameters simultaneously. The second step deals with combined classifier training by integrating the weak concept models that are shared from multiple data sites. To reduce the data transmission costs and the potential privacy breaches, only the weak concept models are sent to the central site and synthetic samples are directly generated from these shared weak concept models at the central site. Both the shared weak concept models and the synthetic samples are then incorporated to learn a reliable and complete global concept model. A computational approach is developed to automatically achieve a good trade off between the privacy disclosure risk, the sharing benefit and the data utility. The third step deals with validating the combined classifier by distributing the global concept model to all these data sites in the collaboration network while at the same time limiting the potential privacy breaches. Our approach has been validated through extensive experiments carried out on four UCI machine learning data sets and two image data sets.",2009,Knowledge and Information Systems volume 20 issue 2 pp 157-185,shared memory;data modeling;data transmission;expectation maximization algorithm;systems modeling;biological classification;information privacy;data mining;database;artificial intelligence;machine learning;simulation;computer science;
Content based similarity of geographic classes organized as partition hierarchies,Anna Formica (National Research Council);Elaheh Pourabbas (National Research Council);,"2708735633,3256131","In this paper we propose a method to measure the semantic similarity of geographic classes organized as partition hierarchies within Naive Geography. The contribution of this work consists in extending and integrating the information content approach, and the method for comparing concept attributes in the ontology management system SymOntos developed at IASI. As a result, this proposal allows us to address both the concept similarity within the partition hierarchy, and the attribute similarity of geographic classes and, therefore, to reduce the gap among the different similarity approaches defined in the literature.",2009,Knowledge and Information Systems volume 20 issue 2 pp 221-241,semantic similarity;case based reasoning;management system;similitude;similarity;metric;self information;ontology;semantics;discrete mathematics;data mining;database;computer science;mathematics;
Exploiting maximal redundancy to optimize SQL queries,Bin Cao (Indiana University);Antonio Badia (University of Louisville);,"2424382266,2100467944","Detecting and dealing with redundancy is an ubiquitous problem in query optimization, which manifests itself in many areas of research such as materialized views, multi-query optimization, and query-containment algorithms. In this paper, we focus on the issue of intra-query redundancy, redundancy present within a query. We present a method to detect the maximal redundancy present between a main (outer) query block and a subquery block. We then use the method for query optimization, introducing query plans and a new operator that take full advantage of the redundancy discovered. Our approach can deal with redundancy in a wider spectrum of queries than existing techniques. We show experimental evidence that our approach works under certain conditions, and compares favorably to existing optimization techniques when applicable.",2009,Knowledge and Information Systems volume 20 issue 2 pp 187-220,sargable;redundancy;dual modular redundancy;materialized view;query optimization;theoretical computer science;data mining;database;computer science;
Learning decision tree for ranking,Liangxiao Jiang (China University of Geosciences);Chaoqun Li (China University of Geosciences);Zhihua Cai (China University of Geosciences);,"2711203041,2129023193,2136429757","Decision tree is one of the most effective and widely used methods for classification. However, many real-world applications require instances to be ranked by the probability of class membership. The area under the receiver operating characteristics curve, simply AUC, has been recently used as a measure for ranking performance of learning algorithms. In this paper, we present two novel class probability estimation algorithms to improve the ranking performance of decision tree. Instead of estimating the probability of class membership using simple voting at the leaf where the test instance falls into, our algorithms use similarity-weighted voting and naive Bayes. We design empirical experiments to verify that our new algorithms significantly outperform the recent decision tree ranking algorithm C4.4 in terms of AUC.",2009,Knowledge and Information Systems volume 20 issue 1 pp 123-135,incremental decision tree;id3 algorithm;ranking svm;decision tree learning;decision tree;pattern recognition;machine learning;statistics;computer science;mathematics;
Location determination of mobile devices for an indoor WLAN application using a neural network,Chih-Yung Tsai (National Penghu University of Science and Technology);Shuo-Yan Chou (National Taiwan University of Science and Technology);Shih-Wei Lin (Chang Gung University);Wei-Hao Wang (National Taiwan University of Science and Technology);,"2104276922,2155332227,2102152005,2442237185","Due to the popularity of location-based services, determining the location of a device at all times has become a subject of great interests. Although many GPS-based applications have been developed and successfully deployed in various fields, their applicabilities are hindered by the obstruction of the objects in the environment. Essentially, as satellite signals cannot penetrate the walls of buildings, the coverage of GPS systems is limited to outdoor environments. To fully exploit the benefit of location-based services, approaches that determine the location of a device in indoor environments need to be established. This study presents a novel location determination mechanism that uses an indoor WLAN and back-propagation neural network (BPN). A museum is taken as the context of the example indoor environment. Location determination is achieved using the combined strengths of 802.11b wireless access signals. With a significant number of access points (APs) installed in the museum, hand-held devices can sense the strengths of the signals from all APs to which the devices can connect. Using a back-propagation network, device locations can be estimated with sufficient accuracy. A novel adaptive algorithm is implemented for enhancing the accuracy of the estimation.",2009,Knowledge and Information Systems volume 20 issue 1 pp 81-93,hybrid positioning system;mobile device;satellite;mobile computing;embedded system;telecommunications;computer science;
“Best K”: critical clustering structures in categorical datasets,Keke Chen (Wright State University);Ling Liu (Georgia Institute of Technology College of Computing);,"2125330238,2125988131","The demand on cluster analysis for categorical data continues to grow over the last decade. A well-known problem in categorical clustering is to determine the best K number of clusters. Although several categorical clustering algorithms have been developed, surprisingly, none has satisfactorily addressed the problem of best K for categorical clustering. Since categorical data does not have an inherent distance function as the similarity measure, traditional cluster validation techniques based on geometric shapes and density distributions are not appropriate for categorical data. In this paper, we study the entropy property between the clustering results of categorical data with different K number of clusters, and propose the BKPlot method to address the three important cluster validation problems: (1) How can we determine whether there is significant clustering structure in a categorical dataset? (2) If there is significant clustering structure, what is the set of candidate “best Ks”? (3) If the dataset is large, how can we efficiently and reliably determine the best Ks?",2009,Knowledge and Information Systems volume 20 issue 1 pp 1-33,k medians clustering;correlation clustering;constrained clustering;single linkage clustering;fuzzy clustering;clustering high dimensional data;metric;cluster analysis;data structure;data mining;pattern recognition;machine learning;statistics;computer science;mathematics;
Answering linear optimization queries with an approximate stream index,Gang Luo (IBM);Kun Lung Wu (IBM);Philip S. Yu (IBM);,"2104365585,2095612124,2125104194","We propose a SAO index to approximately answer arbitrary linear optimization queries in a sliding window of a data stream. It uses limited memory to maintain the most “important” tuples. At any time, for any linear optimization query, we can retrieve the approximate top-K tuples in the sliding window almost instantly. The larger the amount of available memory, the better the quality of the answers is. More importantly, for a given amount of memory, the quality of the answers can be further improved by dynamically allocating a larger portion of the memory to the outer layers of the SAO index.",2009,Knowledge and Information Systems volume 20 issue 1 pp 95-121,stream processing;sliding window protocol;search engine indexing;relational database;time complexity;linear programming;theoretical computer science;database;algorithm;computer science;
Bayesian network based business information retrieval model,Zheng Wang (Northeastern University);Qing Wang (Northeastern University);Ding-Wei Wang (Northeastern University);,"2306861534,2699019699,2660683924","The quality of business information can significantly affect the operation level of enterprise. This paper analyses the problem of business information retrieval (BIR). A Bayesian Network Based business information retrieval model (BN-BIRM) is proposed by means of Bayesian network (BN) and information retrieval (IR) theory and a method for query adaptation is presented. In this model the customized query requirement of enterprise (CQR) is expressed in terms of the predefined illustrative documents related to business domain. The similarities between the documents and the query are evaluated with the conditional probabilities among the nodes in the BN. In the experiments, BN-BIRM is compared with the Belief Network model based on vector space model (VSM) ranking strategy and the Inference Network model based on TF-IDF ranking strategy. The experimental results show that BN-BIRM is effective for collecting business information on a large scale.",2009,Knowledge and Information Systems volume 20 issue 1 pp 63-79,divergence from randomness model;ranking;business domain;query expansion;vector space model;bayesian network;quality assurance;information theory;data mining;database;artificial intelligence;machine learning;computer science;
Subspace sums for extracting non-random data from massive noise,Anne M. Denton (North Dakota State University);,2126706684,"An algorithm is introduced that distinguishes relevant data points from randomly distributed noise. The algorithm is related to subspace clustering based on axis-parallel projections, but considers membership in any projected cluster of a given side length, as opposed to a particular cluster. An aggregate measure is introduced that is based on the total number of points that are close to the given point in all possible 2 d projections of a d-dimensional hypercube. No explicit summation over subspaces is required for evaluating this measure. Attribute values are normalized based on rank order to avoid making assumptions on the distribution of random data. Effectiveness of the algorithm is demonstrated through comparison with conventional outlier detection on a real microarray data set as well as on time series subsequence data.",2009,Knowledge and Information Systems volume 20 issue 1 pp 35-62,gene expression;information extraction;data mining;statistics;algorithm;computer science;mathematics;
Using Wikipedia knowledge to improve text classification,Pu Wang (George Mason University);Jian Hu (Microsoft);Hua-Jun Zeng (Microsoft);Zheng Chen (Microsoft);,"2296294083,2172181550,2157931964,2425877144","Text classification has been widely used to assist users with the discovery of useful information from the Internet. However, traditional classification methods are based on the “Bag of Words” (BOW) representation, which only accounts for term frequency in the documents, and ignores important semantic relationships between key terms. To overcome this problem, previous work attempted to enrich text representation by means of manual intervention or automatic document expansion. The achieved improvement is unfortunately very limited, due to the poor coverage capability of the dictionary, and to the ineffectiveness of term expansion. In this paper, we automatically construct a thesaurus of concepts from Wikipedia. We then introduce a unified framework to expand the BOW representation with semantic relations (synonymy, hyponymy, and associative relations), and demonstrate its efficacy in enhancing previous approaches for text classification. Experimental results on several data sets show that the proposed approach, integrated with the thesaurus built from Wikipedia, can achieve significant improvements with respect to the baseline algorithm.",2009,Knowledge and Information Systems volume 19 issue 3 pp 265-281,bag of words model;brand;tf idf;natural language;the internet;social network;biological classification;content analysis;natural language processing;information retrieval;data mining;artificial intelligence;computer science;
Characterization and evaluation of similarity measures for pairs of clusterings,Darius Pfitzner (Flinders University);Richard Leibbrandt (Flinders University);David M. W. Powers (Flinders University);,"2400476527,699495520,2140899347","In evaluating the results of cluster analysis, it is common practice to make use of a number of fixed heuristics rather than to compare a data clustering directly against an empirically derived standard, such as a clustering empirically obtained from human informants. Given the dearth of research into techniques to express the similarity between clusterings, there is broad scope for fundamental research in this area. In defining the comparative problem, we identify two types of worst-case matches between pairs of clusterings, characterised as independently codistributed clustering pairs and conjugate partition pairs. Desirable behaviour for a similarity measure in either of the two worst cases is discussed, giving rise to five test scenarios in which characteristics of one of a pair of clusterings was manipulated in order to compare and contrast the behaviour of different clustering similarity measures. This comparison is carried out for previously-proposed clustering similarity measures, as well as a number of established similarity measures that have not previously been applied to clustering comparison. We introduce a paradigm apparatus for the evaluation of clustering comparison techniques and distinguish between the goodness of clusterings and the similarity of clusterings by clarifying the degree to which different measures confuse the two. Accompanying this is the proposal of a novel clustering similarity measure, the Measure of Concordance (MoC). We show that only MoC, Powers’s measure, Lopez and Rajski’s measure and various forms of Normalised Mutual Information exhibit the desired behaviour under each of the test scenarios.",2009,Knowledge and Information Systems volume 19 issue 3 pp 361-394,correlation clustering;single linkage clustering;fuzzy clustering;cluster analysis;data mining;machine learning;statistics;computer science;mathematics;
Evolving rule induction algorithms with multi-objective grammar-based genetic programming,Gisele L. Pappa (Universidade Federal de Minas Gerais);Alex Alves Freitas (University of Kent);,"2089254527,2131502281","Multi-objective optimization has played a major role in solving problems where two or more conflicting objectives need to be simultaneously optimized. This paper presents a Multi-Objective grammar-based genetic programming (MOGGP) system that automatically evolves complete rule induction algorithms, which in turn produce both accurate and compact rule models. The system was compared with a single objective GGP and three other rule induction algorithms. In total, 20 UCI data sets were used to generate and test generic rule induction algorithms, which can be now applied to any classification data set. Experiments showed that, in general, the proposed MOGGP finds rule induction algorithms with competitive predictive accuracies and more compact models than the algorithms it was compared with.",2009,Knowledge and Information Systems volume 19 issue 3 pp 283-309,grammar;multi objective optimization;computer programming;genetic algorithm;data analysis;information extraction;data mining;artificial intelligence;machine learning;algorithm;computer science;
Characterizing pattern preserving clustering,Hui Xiong (Rutgers University);Michael Steinbach (University of Minnesota);Arifin Ruslim (University of Minnesota);Vipin Kumar (University of Minnesota);,"2153710278,2102135651,2545779085,2161062602","This paper describes a new approach for clustering—pattern preserving clustering—which produces more easily interpretable and usable clusters. This approach is motivated by the following observation: while there are usually strong patterns in the data—patterns that may be key for the analysis and description of the data—these patterns are often split among different clusters by current clustering approaches. This is, perhaps, not surprising, since clustering algorithms have no built-in knowledge of these patterns and may often have goals that are in conflict with preserving patterns, e.g., minimize the distance of points to their nearest cluster centroids. In this paper, our focus is to characterize (1) the benefits of pattern preserving clustering and (2) the most effective way of performing pattern preserving clustering. To that end, we propose and evaluate two clustering algorithms, HIerarchical Clustering with pAttern Preservation (HICAP) and bisecting K-means Clustering with pAttern Preservation (K-CAP). Experimental results on document data show that HICAP can produce overlapping clusters that preserve useful patterns, but has relatively worse clustering performance than bisecting K-means with respect to the clustering evaluation criterion of entropy. By contrast, in terms of entropy, K-CAP can perform substantially better than the bisecting K-means algorithm when data sets contain clusters of widely different sizes—a common situation in the real-world. Most importantly, we also illustrate how patterns, if preserved, can aid cluster interpretation.",2009,Knowledge and Information Systems volume 19 issue 3 pp 311-336,flame clustering;brown clustering;determining the number of clusters in a data set;dbscan;correlation clustering;constrained clustering;cure data clustering algorithm;affinity propagation;fuzzy clustering;k means clustering;clustering high dimensional data;cluster analysis;consensus clustering;biclustering;data mining;pattern recognition;machine learning;computer science;mathematics;
Dynamic micro-targeting: fitness-based approach to predicting individual preferences,Tianyi Jiang (New York University);Alexander Tuzhilin (New York University);,"2300260723,2057138063","It is crucial to segment customers intelligently in order to offer more targeted and personalized products and services. Traditionally, customer segmentation is achieved using statistics-based methods that compute a set of statistics from the customer data and group customers into segments by applying clustering algorithms. Recent research proposed a direct grouping-based approach that combines customers into segments by optimally combining transactional data of several customers and building a data mining model of customer behavior for each group. This paper proposes a new micro-targeting method that builds predictive models of customer behavior not on the segments of customers but rather on the customer-product groups. This micro-targeting method is more general than the previously considered direct grouping method. We empirically show that it outperforms the direct grouping and statistics-based segmentation methods across multiple experimental conditions and that it generates predominately small-sized segments, thus providing additional support for the micro-targeting approach to personalization.",2009,Knowledge and Information Systems volume 19 issue 3 pp 337-360,personalization;information extraction;data mining;computer science;
Embedding new data points for manifold learning via coordinate propagation,Shiming Xiang (Tsinghua University);Feiping Nie (Tsinghua University);Yangqiu Song (Tsinghua University);Changshui Zhang (Tsinghua University);Chunxia Zhang (Beijing Institute of Technology);,"1995762524,2245267964,2645747494,2105451568,2103255508","In recent years, a series of manifold learning algorithms have been proposed for nonlinear dimensionality reduction. Most of them can run in a batch mode for a set of given data points, but lack a mechanism to deal with new data points. Here we propose an extension approach, i.e., mapping new data points into the previously learned manifold. The core idea of our approach is to propagate the known coordinates to each of the new data points. We first formulate this task as a quadratic programming, and then develop an iterative algorithm for coordinate propagation. Tangent space projection and smooth splines are used to yield an initial coordinate for each new data point, according to their local geometrical relations. Experimental results and applications to camera direction estimation and face pose estimation illustrate the validity of our approach.",2009,Knowledge and Information Systems volume 19 issue 2 pp 159-184,local tangent space alignment;nonlinear dimensionality reduction;iterative method;quadratic programming;geometry;artificial intelligence;machine learning;mathematical optimization;statistics;computer science;mathematics;
Object discovery in high-resolution remote sensing images: a semantic perspective,Dihua Guo (Rutgers University);Hui Xiong (Rutgers University);Vijayalakshmi Atluri (Rutgers University);Nabil R. Adam (Rutgers University);,"2101251879,2153710278,2100792061,2300106300","Given its importance, the problem of object discovery in high-resolution remote-sensing (HRRS) imagery has received a lot of attention in the literature. Despite the vast amount of expert endeavor spent on this problem, more efforts have been expected to discover and utilize hidden semantics of images for object detection. To that end, in this paper, we address this problem from two semantic perspectives. First, we propose a semantic-aware two-stage image segmentation approach, which preserves the semantics of real-world objects during the segmentation process. Second, to better capture semantic features for object discovery, we exploit a hyperclique pattern discovery method to find complex objects that consist of several co-existing individual objects that usually form a unique semantic concept. We consider the identified groups of co-existing objects as new feature sets and feed them into the learning model for better performance of image retrieval. Experiments with real-world datasets show that, with reliable segmentation and new semantic features as starting points, we can improve the performance of object discovery in terms of various external criteria.",2009,Knowledge and Information Systems volume 19 issue 2 pp 211-233,image resolution;image retrieval;semantics;image processing;computer vision;artificial intelligence;computer science;
A hybrid multi-group approach for privacy-preserving data mining,Zhouxuan Teng (Syracuse University);Wenliang Du (Syracuse University);,"2665243578,2657458691","In this paper, we propose a hybrid multi-group approach for privacy preserving data mining. We make two contributions in this paper. First, we propose a hybrid approach. Previous work has used either the randomization approach or the secure multi-party computation (SMC) approach. However, these two approaches have complementary features: the randomization approach is much more efficient but less accurate, while the SMC approach is less efficient but more accurate. We propose a novel hybrid approach, which takes advantage of the strength of both approaches to balance the accuracy and efficiency constraints. Compared to the two existing approaches, our proposed approach can achieve much better accuracy than randomization approach and much reduced computation cost than SMC approach. We also propose a multi-group scheme that makes it flexible for the data miner to control the balance between data mining accuracy and privacy. This scheme is motivated by the fact that existing randomization schemes that randomize data at individual attribute level can produce insufficient accuracy when the number of dimensions is high. We partition attributes into groups, and develop a scheme to conduct group-based randomization to achieve better data mining accuracy. To demonstrate the effectiveness of the proposed general schemes, we have implemented them for the ID3 decision tree algorithm and association rule mining problem and we also present experimental results.",2009,Knowledge and Information Systems volume 19 issue 2 pp 133-157,secure multi party computation;hybrid;association rule learning;randomization;association;decision tree;privacy;data analysis;information extraction;computer security;data mining;database;statistics;computer science;
An optimized sequential pattern matching methodology for sequence classification,Themis P. Exarchos (University of Ioannina);Markos G. Tsipouras (University of Ioannina);Costas Papaloukas (University of Ioannina);Dimitrios I. Fotiadis (University of Ioannina);,"1988519054,91269169,268892670,2238025108","In this paper we present a novel methodology for sequence classification, based on sequential pattern mining and optimization algorithms. The proposed methodology automatically generates a sequence classification model, based on a two stage process. In the first stage, a sequential pattern mining algorithm is applied to a set of sequences and the sequential patterns are extracted. Then, the score of every pattern with respect to each sequence is calculated using a scoring function and the score of each class under consideration is estimated by summing the specific pattern scores. Each score is updated, multiplied by a weight and the output of the first stage is the classification confusion matrix of the sequences. In the second stage an optimization technique, aims to finding a set of weights which minimize an objective function, defined using the classification confusion matrix. The set of the extracted sequential patterns and the optimal weights of the classes comprise the sequence classification model. Extensive evaluation of the methodology was carried out in the protein classification domain, by varying the number of training and test sequences, the number of patterns and the number of classes. The methodology is compared with other similar sequence classification approaches. The proposed methodology exhibits several advantages, such as automated weight assignment to classes using optimization techniques and knowledge discovery in the domain of application.",2009,Knowledge and Information Systems volume 19 issue 2 pp 249-264,classification rule;confusion matrix;sequential pattern mining;score;pattern matching;systems modeling;biological classification;knowledge extraction;data mining;pattern recognition;algorithm;computer science;mathematics;
Multiresolution-based bilinear recurrent neural network,Dong-Chul Park;,2430681465,A multiresolution-based bilinear recurrent neural network (MBLRNN) is proposed in this paper. The proposed MBLRNN is based on the BLRNN that has robust abilities in modeling and predicting time series. The learning process is further improved by using a multiresolution-based learning algorithm for training the BLRNN so as to make it more robust for the prediction of time series data. The proposed MBLRNN is applied to the problems of network traffic prediction and electric load forecasting. Experiments and results on both practical problems show that the proposed MBLRNN outperforms both the traditional multilayer perceptron type neural network (MLPNN) and the BLRNN in the prediction accuracy.,2009,Knowledge and Information Systems volume 19 issue 2 pp 235-248,time series;artificial neural network;artificial intelligence;machine learning;statistics;algorithm;computer science;
Direct estimation of class membership probabilities for multiclass classification using multiple scores,Kazuko Takahashi (Keiai University);Hiroya Takamura (Tokyo Institute of Technology);Manabu Okumura (Tokyo Institute of Technology);,"2141205835,2170796186,2165400638","Accurate estimation of class membership probability is needed for many applications in data mining and decision-making, to which multiclass classification is often applied. Since existing methods for estimation of class membership probability are designed for binary classification, in which only a single score outputted from a classifier can be used, an approach for multiclass classification requires both a decomposition of a multiclass classifier into binary classifiers and a combination of estimates obtained from each binary classifier to a target estimate. We propose a simple and general method for directly estimating class membership probability for any class in multiclass classification without decomposition and combination, using multiple scores not only for a predicted class but also for other proper classes. To make it possible to use multiple scores, we propose to modify or extend representative existing methods. As a non-parametric method, which refers to the idea of a binning method as proposed by Zadrozny et al., we create an “accuracy table” by a different method. Moreover we smooth accuracies on the table with methods such as the moving average to yield reliable probabilities (accuracies). As a parametric method, we extend Platt’s method to apply a multiple logistic regression. On two different datasets (open-ended data from Japanese social surveys and the 20 Newsgroups) both with Support Vector Machines and naive Bayes classifiers, we empirically show that the use of multiple scores is effective in the estimation of class membership probabilities in multiclass classification in terms of cross entropy, the reliability diagram, the ROC curve and AUC (area under the ROC curve), and that the proposed smoothing method for the accuracy table works quite well. Finally, we show empirically that in terms of MSE (mean squared error), our best proposed method is superior to an expansion for multiclass classification of a PAV method proposed by Zadrozny et al., in both the 20 Newsgroups dataset and the Pendigits dataset, but is slightly worse than the state-of-the-art method, which is an expansion for multiclass classification of a combination of boosting and a PAV method, on the Pendigits dataset.",2009,Knowledge and Information Systems volume 19 issue 2 pp 185-210,classification rule;japanese;binary classification;cross entropy;naive bayes classifier;multiclass classification;moving average;support vector machine;logistic regression;reliability;data analysis;probabilistic classification;information extraction;econometrics;data mining;pattern recognition;machine learning;statistics;computer science;mathematics;
Editorial: special issue on selected papers of PAKDD 2007,Zhi-Hua Zhou (Nanjing University);Hang Li (Microsoft);Qiang Yang (Hong Kong University of Science and Technology);,"2286237009,2128739099,2109031554",-,2009,Knowledge and Information Systems volume 19 issue 2 pp 131-132,-
Computing compatibility in dynamic service composition,Zhaohui Wu (Zhejiang University);Shuiguang Deng (Zhejiang University);Ying Li (Zhejiang University);Jian Wu (Zhejiang University);,"2151930384,2012690264,2424189881,2525822525","Dynamically composing services requires mechanisms to ensure component services compatible with each other both at all of the syntax, semantic and behavioral level. This paper focuses on the issue of behavioral compatibility in a service composition. It adopts the π-calculus to model service behaviors and interactions in a formal way. Based on the formalization, it proposes a method to automatically check the behavioral compatibility in a qualitative way. Furthermore, it presents an algorithm to compute the compatibility degree in a quantitative way. The algorithm is implemented in a prototype and its performance analysis is also carried out to show that it can help composing services on the fly and ensure the services compatible with each other to provide functions with newly-added values.",2009,Knowledge and Information Systems volume 19 issue 1 pp 107-129,π calculus;semantics;artificial intelligence;simulation;programming language;computer science;
Data discretization unification,Ruoming Jin (Kent State University);Yuri Breitbart (Kent State University);Chibuike Muoh (Kent State University);,"2119237514,1985207418,281210348","Data discretization is defined as a process of converting continuous data attribute values into a finite set of intervals with minimal loss of information. In this paper, we prove that discretization methods based on informational theoretical complexity and the methods based on statistical measures of data dependency are asymptotically equivalent. Furthermore, we define a notion of generalized entropy and prove that discretization methods based on Minimal description length principle, Gini index, AIC, BIC, and Pearson’s X 2 and G 2 statistics are all derivable from the generalized entropy function. We design a dynamic programming algorithm that guarantees the best discretization based on the generalized entropy notion. Furthermore, we conducted an extensive performance evaluation of our method for several publicly available data sets. Our results show that our method delivers on the average 31% less classification errors than many previously known discretization methods.",2009,Knowledge and Information Systems volume 19 issue 1 pp 1-29,discretization of continuous features;discretization error;discretization;dynamic programming;entropy;calculus;statistics;algorithm;computer science;mathematics;
Resolution-based outlier factor: detecting the top- n most outlying data points in engineering data,Hongqin Fan (Missouri Western State University);Osmar R. Zaïane (University of Alberta);Andrew Foss (University of Alberta);Junfeng Wu (University of Alberta);,"2504058715,2308328903,2011760006,2683939492","One of the common endeavours in engineering applications is outlier detection, which aims to identify inconsistent records from large amounts of data. Although outlier detection schemes in data mining discipline are acknowledged as a more viable solution to efficient identification of anomalies from these data repository, current outlier mining algorithms require the input of domain parameters. These parameters are often unknown, difficult to determine and vary across different datasets containing different cluster features. This paper presents a novel resolution-based outlier notion and a nonparametric outlier-mining algorithm, which can efficiently identify and rank top listed outliers from a wide variety of datasets. The algorithm generates reasonable outlier results by taking both local and global features of a dataset into account. Experiments are conducted using both synthetic datasets and a real life construction equipment dataset from a large road building contractor. Comparison with the current outlier mining algorithms indicates that the proposed algorithm is more effective and can be integrated into a decision support system to serve as a universal detector of potentially inconsistent records.",2009,Knowledge and Information Systems volume 19 issue 1 pp 31-51,outlier;anomaly;data analysis;data warehouse;decision support system;information extraction;data science;data mining;statistics;computer science;
Encoding and decoding the knowledge of association rules over SVM classification trees,Shaoning Pang (Auckland University of Technology);Nikola K. Kasabov (Auckland University of Technology);,"2107624962,720605457","This paper presents a constructive method for association rule extraction, where the knowledge of data is encoded into an SVM classification tree (SVMT), and linguistic association rule is extracted by decoding of the trained SVMT. The method of rule extraction over the SVMT (SVMT-rule), in the spirit of decision-tree rule extraction, achieves rule extraction not only from SVM, but also over the decision-tree structure of SVMT. Thus, the obtained rules from SVMT-rule have the better comprehensibility of decision-tree rule, meanwhile retains the good classification accuracy of SVM. Moreover, profiting from the super generalization ability of SVMT owing to the aggregation of a group of SVMs, the SVMT-rule is capable of performing a very robust classification on such datasets that have seriously, even overwhelmingly, class-imbalanced data distribution. Experiments with a Gaussian synthetic data, seven benchmark cancers diagnosis, and one application of cell-phone fraud detection have highlighted the utility of SVMT and SVMT-rule on comprehensible and effective knowledge discovery, as well as the superior properties of SVMT-rule as compared to a purely support-vector based rule extraction. (A version of SVMT Matlab software is available online at http://kcir.kedri.info)",2009,Knowledge and Information Systems volume 19 issue 1 pp 79-105,support vector machine;cancer;knowledge extraction;data mining;pattern recognition;artificial intelligence;machine learning;statistics;computer science;
Learning element similarity matrix for semi-structured document analysis,Jianwu Yang (Peking University);William Kwok-Wai Cheung (Hong Kong Baptist University);Xiaoou Chen (Peking University);,"2162807091,2099585548,2691320526","Capturing latent structural and semantic properties in semi-structured documents (e.g., XML documents) is crucial for improving the performance of related document analysis tasks. Structured Link Vector Mode (SLVM) is a representation recently proposed for modeling semi-structured documents. It uses an element similarity matrix to capture the latent relationships between XML elements—the constructing components of an XML document. In this paper, instead of applying heuristics to define the element similarity matrix, we propose to compute the matrix using the machine learning approach. In addition, we incorporate term semantics into SLVM using latent semantic indexing to enhance the model accuracy, with the element similarity learnability property preserved. For performance evaluation, we applied the similarity learning to k-nearest neighbors search and similarity-based clustering, and tested the performance using two different XML document collections. The SLVM obtained via learning was found to outperform significantly the conventional Vector Space Model and the edit-distance-based methods. Also, the similarity matrix, obtained as a by-product, can provide higher-level knowledge on the semantic relationships between the XML elements.",2009,Knowledge and Information Systems volume 19 issue 1 pp 53-78,document term matrix;semantic similarity;xml;semantics;natural language processing;data mining;database;artificial intelligence;machine learning;computer science;
On weighted lower and upper possibilistic means and variances of fuzzy numbers and its application in decision,Wei-Guo Zhang (South China University of Technology);Wei-Lin Xiao (South China University of Technology);,"2631063324,2674073939","In this paper, we define the weighted lower and upper possibilistic variances and covariances of fuzzy numbers. We also obtain their many properties similar to variance and covariance in probability theory. On the basis of the weighted lower and upper possibilistic means and variances, we present two new possibilistic portfolio selection models with tolerated risk level and holdings of assets constraints. The conventional probabilistic mean–variance model can be transformed to a linear programming problem under possibility distributions. Finally, an estimation method of possibility distribution is offered and a real example for portfolio selection problem is given to illustrate the usability of the approach and the effectiveness of our methods.",2009,Knowledge and Information Systems volume 18 issue 3 pp 311-330,fuzzy number;possibility theory;usability;covariance;fuzzy logic;probability theory;variance;systems modeling;linear programming;econometrics;artificial intelligence;mathematical optimization;statistics;computer science;mathematics;
Optimizing the execution of XSLT stylesheets for querying transformed XML data,Sven Groppe (University of Lübeck);Jinghua Groppe (University of Lübeck);Stefan Böttcher (University of Paderborn);Thomas Wycisk (University of Paderborn);Le Gruenwald (University of Oklahoma);,"1246451812,2007981037,2100168392,2049914593,2188709943","We have to deal with different data formats whenever data formats evolve or data must be integrated from heterogeneous systems. These data when implemented in XML for data exchange cannot be shared freely among applications without data transformation. A common approach to solve this problem is to convert the entire XML data from their source format to the applications’ target formats using the transformations rules specified in XSLT stylesheets. However, in many cases, not all XML data are required to be transformed except for a smaller part described by a user’s query (application). In this paper, we present an approach that optimizes the execution time of an XSLT stylesheet for answering a given XPath query by modifying the XSLT stylesheet in such a way that it would (a) capture only the parts in the XML data that are relevant to the query and (b) process only those XSLT instructions that are relevant to the query. We prove the correctness of our optimization approach, analyze its complexity and present experimental results. The experimental results show that our approach performs the best in terms of execution time, especially when many cost-intensive XSLT instructions can be excluded in the XSLT stylesheet.",2009,Knowledge and Information Systems volume 18 issue 3 pp 331-391,xs3p;schematron;processing instruction;streaming xml;xslt;xml database;xml;world wide web;data mining;database;programming language;computer science;
String alignment for automated document versioning,"Wei Lee Woon (Masdar Institute of Science and Technology);Kuok-Shoong Daniel Wong (University of Science and Technology, Sana'a);","2083129847,2306845509","The automated analysis of documents is an important task given the rapid increase in availability of digital texts. Automatic text processing systems often encode documents as vectors of term occurrence frequencies, a representation which facilitates the classification and clustering of documents. Historically, this approach derives from the related field of data mining, where database entries are commonly represented as points in a vector space. While this lineage has certainly contributed to the development of text processing, there are situations where document collections do not conform to this clustered structure, and where the vector representation may be unsuitable for text analysis. As a proof-of-concept, we had previously presented a framework where the optimal alignments of documents could be used for visualising the relationships within small sets of documents. In this paper we develop this approach further by using it to automatically generate the version histories of various document collections. For comparison, version histories generated using conventional methods of document representation are also produced. To facilitate this comparison, a simple procedure for evaluating the accuracy of the version histories thus generated is proposed.",2009,Knowledge and Information Systems volume 18 issue 3 pp 293-309,software versioning;data field;string;string searching algorithm;proof of concept;pattern matching;availability;cluster analysis;vector space;biological classification;data analysis;information extraction;image analysis;data mining;programming language;algorithm;computer science;
A dynamic attribute-based data filtering and recovery scheme for web information processing,Amit Ahuja (Brigham Young University);Yiu-Kai D. Ng (Brigham Young University);,"2489183555,2707838932","Web data being transmitted over a network channel on the Internet with excessive amount of data causes data processing problems, which include selectively choosing useful information to be retained for various data applications. In this paper, we present an approach for filtering less-informative attribute data from a source Website. A scheme for filtering attributes, instead of tuples (records), from a Website becomes imperative, since filtering a complete tuple would lead to filtering some informative, as well as less-informative, attribute data in the tuple. Since filtered data at the source Website may be of interest to the user at the destination Website, we design a data recovery approach that maintains the minimal amount of information for data recovery purpose while imposing minimal overhead for data recovery at the source Website. Our data filtering and recovery approach (1) handles a wide range of Web data in different application domains (such as weather, stock exchanges, Internet traffic, etc.), (2) is dynamic in nature, since each filtering scheme adjusts the amount of data to be filtered as needed, and (3) is adaptive, which is appealing in an ever-changing Internet environment.",2009,Knowledge and Information Systems volume 18 issue 3 pp 263-291,data recovery;stock exchange;internet traffic;collaborative filtering;software development;filter;the internet;information processing;data processing;world wide web;data mining;database;computer science;
vEye: behavioral footprinting for self-propagating worm detection and profiling,Xuxian Jiang (George Mason University);Xingquan Zhu (Florida Atlantic University);,"2101920419,2618356905","With unprecedented speed, virulence, and sophistication, self-propagating worms remain as one of the most severe threats to information systems and Internet in general. In order to mitigate the threat, efficient mechanisms are needed to accurately profile and detect the worms before or during their outbreaks. Particularly, deriving a worm’s unique signatures, or fingerprints, is of the first priority to achieve this goal. One of the most popular approaches is to use content-based signatures, which characterize a worm by extracting its unique information payload. In practice, such content-based signatures, unfortunately, suffer from numerous disadvantages, such as vulnerable to content mutation attacks or not applicable for polymorphic worms. In this paper, we propose a new behavioral footprinting (BF) approach that nicely complements the state-of-the-art content-based signature approaches and allows users to detect and profile self-propagating worms from the unique worm behavioral perspective. More specifically, our behavioral footprinting method uniquely captures a worm’s dynamic infection sequences (e.g., probing, exploitation, and replication) by modeling each interaction step as a behavior phenotype and denoting a complete infection process as a chained sequence. We argue that a self-propagating worm’s inherent behaviors or infection patterns can be detected and characterized by using sequence alignment tools, where patterns shared by the infection sequences will imply the behavioral footprints of the worm. A systematic platform called vEye has been built to validate the proposed design with either “live” or historical worms, where a number of real-world infection sequences are used to build worm behavioral footprints. Experimental comparisons with existing content-based fingerprints will demonstrate the uniqueness and effectiveness of the proposed behavior footprints in self-propagating worm detection and profiling.",2009,Knowledge and Information Systems volume 18 issue 2 pp 231-262,information system;network security;bioinformatics;computer security;computer science;
Agents’ model of uncertainty,Germano Resconi (The Catholic University of America);Boris Kovalerchuk (Central Washington University);,"1522889589,2053495855","Multi-agent systems play an increasing role in sensor networks, software engineering, web design, e-commerce, robotics, and many others areas. Uncertainty is a fundamental property of these areas. Agent-based systems use probabilistic and other uncertainty models developed earlier without explicit consideration of agents. This paper explores the impact of agents on uncertainty models and theories. We compare two methods of introducing agents to uncertainty theories and propose a new theory called the agent-based uncertainty theory (AUT). We show advantages of AUT for advancing multi-agent systems and for solving an internal fundamental question of uncertainty theories, that is identifying coherent approaches to uncertainty. The advantages of AUT are that it provides a uniform agent-based representation and an operational empirical interpretation for several uncertainty theories such as rough set theory, fuzzy sets theory, evidence theory, and probability theory. We show also that the introduction of agents to intuitionist uncertainty formalisms can reduce their conceptual complexity. To build such uniformity the AUT exploits the fact that agents as independent entities can give conflicting evaluations of the same attribute. The AUT is based on complex aggregations of crisp (non-fuzzy) conflicting judgments of agents. The generality of AUT is derived from the logical classification of types (orders) of conflicts in the agent populations. At the first order of conflict, the two agent populations are disjoint and there is no interference of logic values assigned to any statement p and its negation by agents. The second order of conflict models superposition (interference) of logic values for overlapping agent populations where an agent assigns conflicting logic values (true, false) to the same attribute simultaneously.",2009,Knowledge and Information Systems volume 18 issue 2 pp 213-229,rough set;robotics;data mining;artificial intelligence;machine learning;algorithm;computer science;mathematics;
Consensus-based evaluation framework for distributed information retrieval systems,Jason J. Jung (Yeungnam University);,2107704472,"Multi-agent systems have been attacking the challenges of information retrieval tasks on distributed environment. In this paper, we propose a consensus choice selection method based framework to evaluate the performance of cooperative information retrieval tasks of the multiple agents. Thereby, two well-known measurements, precision and recall, are extended to handle consensual closeness (i.e., local and global consensus) between the sets of retrieved results. We show that in a motivating example the proposed criteria are prone to solve the rigidity problem of classical precision and recall. More importantly, the retrieved results can be ranked with respect to the consensual score, and the ranking mechanism has been verified to be more reasonable.",2009,Knowledge and Information Systems volume 18 issue 2 pp 199-211,distributed computing environment;consensus;cooperation;evaluation;multi agent system;data mining;artificial intelligence;simulation;computer science;
Approximate solutions of moral dilemmas in multiple agent system,Matteo Cristani (University of Verona);Elisa Burato (University of Verona);,"2025597422,84106262","Moral dilemmas are one of the major issues of current research in ethical reasoning. In particular, it is well known that admitting moral dilemmas in Standard Deontic Logic generates a family of inconsistencies that are intrinsically unsolvable. Since managing dilemmas means performing preferential reasoning, we argue that one simple approach to both types of problems is by ordering actions. We notice that in general, more than local orderings between two actions, agents have intrinsic preferences based on classification issues, like the action type, and that, once we have discharged the dilemma as it is intrinsically, preferential reasoning is performed by using a second-level choice approach. Decision theory has dealt with the problem of making decisions in presence of conflicting decision criteria, and some researcher has pointed out that this is the case of moral dilemmas as well. In practice, the choice of preferences in presence of conflicting criteria can be seen as a form of preferential-ethical reasoning. Although this is certainly an important topic in multiple agent investigations, it is definitely neglected in the current investigations. It is well known that humans are quite clever in solving moral dilemmas, and the usage they make of preferential reasoning is very complex. In this paper we address the problems of preferential-ethical reasoning in a combinatorial fashion and provide an algorithm for making decisions on moral dilemmas in presence of conflicting decision criteria. We then evaluate the complexity of the algorithm and prove that this approach can be applied in practice.",2009,Knowledge and Information Systems volume 18 issue 2 pp 157-181,deontic logic;ring;order theory;ethics;decision theory;biological classification;artificial intelligence;statistics;algorithm;mathematics;
Developing actionable trading agents,"Longbing Cao (University of Technology, Sydney);Tony He (University of Technology, Sydney);","2115085568,2272861581","Trading agents are useful for developing and back-testing quality trading strategies to support smart trading actions in the market. However, most of the existing trading agent research oversimplifies trading strategies, and focuses on simulated ones. As a result, there exists a big gap between the deliverables and business needs when the developed strategies are deployed into the real life. Therefore, the actionable capability of developed trading agents is often very limited. This paper for the first time introduces effective approaches for optimizing and integrating multiple classes of strategies through trading agent collaboration. An integration and optimization approach is proposed to identify optimal trading strategy in each category, and further integrate optimal strategies crossing classes. Positions associated with these optimal strategies are recommended for trading agents to take actions in the market. Extensive experiments on a large quantity of real-life market data show that trading agents following the recommended strategies have great potential to obtain high benefits while low costs. This verifies that it is promising to develop trading agents toward workable and satisfying business needs.",2009,Knowledge and Information Systems volume 18 issue 2 pp 183-198,alternative trading system;trading strategy;cooperation;satisfiability;intelligent agent;artificial intelligence;computer science;
Norm negotiation in online multi-player games,Guido Boella (University of Turin);Patrice Caire (University of Luxembourg);Leendert W. N. van der Torre (University of Luxembourg);,"2002208782,2117854920,2124170260","In this paper, we introduce an agent communication protocol and speech acts for norm negotiation. The protocol creates individual or contractual obligations to fulfill goals of the agents based on the so-called social delegation cycle. First, agents communicate their individual goals and powers. Second, they propose social goals which can be accepted or rejected by other agents. Third, they propose obligations and sanctions to achieve the social goal, which can again be accepted or rejected. Finally, the agents accept the new norm by indicating which of their communicated individual goals the norm achieves. The semantics of the speech acts is based on a commitment to public mental attitudes. The norm negotiation model is illustrated by an example of norm negotiation in multi-player online gaming.",2009,Knowledge and Information Systems volume 18 issue 2 pp 137-156,attitude;semantics;artificial intelligence;simulation;computer science;
Actions and social interactions in multi-agent systems,Ngoc Thanh Nguyen (Wrocław University of Technology);Radosław P. Katarzyniak (Wrocław University of Technology);,"2164690633,2037635821",-,2009,Knowledge and Information Systems volume 18 issue 2 pp 133-136,artificial intelligence;computer science;
A framework for monitoring classifiers’ performance: when and why failure occurs?,David A. Cieslak (University of Notre Dame);Nitesh V. Chawla (University of Notre Dame);,"2004286868,1979796846","Classifier error is the product of model bias and data variance. While understanding the bias involved when selecting a given learning algorithm, it is similarly important to understand the variability in data over time, since even the One True Model might perform poorly when training and evaluation samples diverge. Thus, it becomes the ability to identify distributional divergence is critical towards pinpointing when fracture points in classifier performance will occur, particularly since contemporary methods such as tenfolds and hold-out are poor predictors in divergent circumstances. This article implement a comprehensive evaluation framework to proactively detect breakpoints in classifiers’ predictions and shifts in data distributions through a series of statistical tests. We outline and utilize three scenarios under which data changes: sample selection bias, covariate shift, and shifting class priors. We evaluate the framework with a variety of classifiers and datasets.",2009,Knowledge and Information Systems volume 18 issue 1 pp 83-108,selection bias;statistical hypothesis testing;data mining;pattern recognition;machine learning;statistics;computer science;
One in a million: picking the right patterns,Björn Bringmann (Katholieke Universiteit Leuven);Albrecht Zimmermann (Katholieke Universiteit Leuven);,"2119732911,2100256487","Constrained pattern mining extracts patterns based on their individual merit. Usually this results in far more patterns than a human expert or a machine leaning technique could make use of. Often different patterns or combinations of patterns cover a similar subset of the examples, thus being redundant and not carrying any new information. To remove the redundant information contained in such pattern sets, we propose two general heuristic algorithms—Bouncer and Picker—for selecting a small subset of patterns. We identify several selection techniques for use in this general algorithm and evaluate those on several data sets. The results show that both techniques succeed in severely reducing the number of patterns, while at the same time apparently retaining much of the original information. Additionally, the experiments show that reducing the pattern set indeed improves the quality of classification results. Both results show that the developed solutions are very well suited for the goals we aim at.",2009,Knowledge and Information Systems volume 18 issue 1 pp 61-81,video post processing;heuristic;genetic algorithm;data mining;machine learning;algorithm;computer science;
Pattern-based time-series subsequence clustering using radial distribution functions,Anne M. Denton (North Dakota State University);Christopher A. Besemann (North Dakota State University);Dietmar H. Dorr (North Dakota State University);,"2126706684,1182558981,2155114978","Clustering of time series subsequence data commonly produces results that are unspecific to the data set. This paper introduces a clustering algorithm, that creates clusters exclusively from those subsequences that occur more frequently in a data set than would be expected by random chance. As such, it partially adopts a pattern mining perspective into clustering. When subsequences are being labeled based on such clusters, they may remain without label. In fact, if the clustering was done on an unrelated time series it is expected that the subsequences should not receive a label. We show that pattern-based clusters are indeed specific to the data set for 7 out of 10 real-world sets we tested, and for window-lengths up to 128 time points. While kernel-density-based clustering can be used to find clusters with similar properties for window sizes of 8–16 time points, its performance degrades fast for increasing window sizes.",2009,Knowledge and Information Systems volume 18 issue 1 pp 1-27,flame clustering;k medians clustering;brown clustering;canopy clustering algorithm;complete linkage clustering;determining the number of clusters in a data set;dbscan;correlation clustering;constrained clustering;data stream clustering;cure data clustering algorithm;single linkage clustering;affinity propagation;fuzzy clustering;kernel density estimation;radial distribution function;clustering high dimensional data;hierarchical clustering;cluster analysis;time series;consensus clustering;biclustering;data mining;pattern recognition;machine learning;statistics;mathematics;
Defining the notion of ‘Information Content’ and reasoning about it in a database,Kaibo Xu (Beijing Union University);Junkang Feng (Beijing Union University);Malcolm Crowe (University of the West of Scotland);,"2146182649,2664111902,2115425397","The problem of ‘information content’ of an information system appears elusive. In the field of databases, the information content of a database has been taken as the instance of a database. We argue that this view misses two fundamental points. One is a convincing conception of the phenomenon concerning information in databases, especially a properly defined notion of ‘information content’. The other is a framework for reasoning about information content. In this paper, we suggest a modification of the well known definition of ‘information content’ given by Dretske(Knowledge and the flow of information,1981). We then define what we call the ‘information content inclusion’ relation (IIR for short) between two random events. We present a set of inference rules for reasoning about information content, which we call the IIR Rules. Then we explore how these ideas and the rules may be used in a database setting to look at databases and to derive otherwise hidden information by deriving new relations from a given set of IIR. A prototype is presented, which shows how the idea of IIR-Reasoning might be exploited in a database setting including the relationship between real world events and database values.",2009,Knowledge and Information Systems volume 18 issue 1 pp 29-59,information algebra;variation of information;interaction information;information quality;rule of inference;information integration;information retrieval;data mining;database;artificial intelligence;machine learning;statistics;computer science;
Rule cubes for causal investigations,Axel Blumenstock (University of Ulm);Franz Schweiggert (University of Ulm);Markus Müller (University of Bamberg);Carsten Lanquillon (Heilbronn University);,"2012757302,747489575,2235230844,337921001","With the complexity of modern vehicles tremendously increasing, quality engineers play a key role within today’s automotive industry. Field data analysis supports corrective actions in development, production and after sales support. We decompose the requirements and show that association rules, being a popular approach to generating explanative models, still exhibit shortcomings. Interactive rule cubes, which have been proposed recently, are a promising alternative. We extend this work by introducing a way of intuitively visualizing and meaningfully ranking them. Moreover, we present methods to interactively factorize a problem and validate hypotheses by ranking patterns based on expectations, and by browsing a cube-based network of related influences. All this is currently in use as an interactive tool for warranty data analysis in the automotive industry. A real-world case study shows how engineers successfully use it in identifying root causes of quality issues.",2009,Knowledge and Information Systems volume 18 issue 1 pp 109-132,automotive industry;interactivity;association rule learning;data analysis;data mining;database;artificial intelligence;machine learning;computer science;
Non-negative matrix factorization for semi-supervised data clustering,Yanhua Chen (Wayne State University);Manjeet Rege (Wayne State University);Ming Dong (Wayne State University);Jing Hua (Wayne State University);,"2141575533,2492937077,2618285502,1979157086","Traditional clustering algorithms are inapplicable to many real-world problems where limited knowledge from domain experts is available. Incorporating the domain knowledge can guide a clustering algorithm, consequently improving the quality of clustering. In this paper, we propose SS-NMF: a semi-supervised non-negative matrix factorization framework for data clustering. In SS-NMF, users are able to provide supervision for clustering in terms of pairwise constraints on a few data objects specifying whether they “must” or “cannot” be clustered together. Through an iterative algorithm, we perform symmetric tri-factorization of the data similarity matrix to infer the clusters. Theoretically, we show the correctness and convergence of SS-NMF. Moveover, we show that SS-NMF provides a general framework for semi-supervised clustering. Existing approaches can be considered as special cases of it. Through extensive experiments conducted on publicly available datasets, we demonstrate the superior performance of SS-NMF for clustering.",2008,Knowledge and Information Systems volume 17 issue 3 pp 355-379,flame clustering;brown clustering;canopy clustering algorithm;determining the number of clusters in a data set;dbscan;correlation clustering;constrained clustering;data stream clustering;cure data clustering algorithm;affinity propagation;fuzzy clustering;clustering high dimensional data;iterative method;cluster analysis;consensus clustering;biclustering;conceptual clustering;supervised learning;expert system;data mining;pattern recognition;machine learning;computer science;mathematics;
Multirelational classification: a multiple view approach,Hongyu Guo (University of Ottawa);Herna L. Viktor (University of Ottawa);,"2131034304,2091794408","Multirelational classification aims at discovering useful patterns across multiple inter-connected tables (relations) in a relational database. Many traditional learning techniques, however, assume a single table or a flat file as input (the so-called propositional algorithms). Existing multirelational classification approaches either “upgrade” mature propositional learning methods to deal with relational presentation or extensively “flatten” multiple tables into a single flat file, which is then solved by propositional algorithms. This article reports a multiple view strategy—where neither “upgrading” nor “flattening” is required—for mining in relational databases. Our approach learns from multiple views (feature set) of a relational databases, and then integrates the information acquired by individual view learners to construct a final model. Our empirical studies show that the method compares well in comparison with the classifiers induced by the majority of multirelational mining systems, in terms of accuracy obtained and running time needed. The paper explores the implications of this finding for multirelational research and applications. In addition, the method has practical significance: it is appropriate for directly mining many real-world databases.",2008,Knowledge and Information Systems volume 17 issue 3 pp 287-312,empirical research;information extraction;data mining;database;artificial intelligence;machine learning;computer science;
Ranking with decision tree,Fen Xia (Chinese Academy of Sciences);Wensheng Zhang (Chinese Academy of Sciences);Fuxin Li (Chinese Academy of Sciences);Yanwu Yang (Chinese Academy of Sciences);,"2147779796,2710030623,2210731946,2643822983","Ranking problems have recently become an important research topic in the joint field of machine learning and information retrieval. This paper presented a new splitting rule that introduces a metric, i.e., an impurity measure, to construct decision trees for ranking tasks. We provided a theoretical basis and some intuitive explanations for the splitting rule. Our approach is also meaningful to collaborative filtering in the sense of dealing with categorical data and selecting relative features. Some experiments were made to illustrate our ranking approach, whose results showed that our algorithm outperforms both perceptron-based ranking and the classification tree algorithms in term of accuracy as well as speed.",2008,Knowledge and Information Systems volume 17 issue 3 pp 381-395,ranking;ranking svm;perceptron;ranking;collaborative filtering;decision tree learning;decision tree;categorical variable;metric;artificial neural network;data mining;pattern recognition;artificial intelligence;machine learning;computer science;
Modified algorithms for synthesizing high-frequency rules from different data sources,Thirunavukkarasu Ramkumar (A. V. C. College of Engineering);Rengaramanujam Srinivasan;,"2493876470,2649386898","Because of the rapid growth in information and communication technologies, a company’s data may be spread over several continents. For an effective decision-making process, knowledge workers need data, which may be geographically spread in different locations. In such circumstances, multi-database mining plays a major role in the process of extracting knowledge from different data sources. In this paper, we have proposed a new methodology for synthesizing high-frequency rules from different data sources, where data source weight has been calculated on the basis of their transaction population. We have also proposed a new method for calculating global confidence. Our goal in synthesizing local patterns to obtain global patterns is that, the support and confidence of synthesized patterns must be very nearly same if all the databases are integrated and mono-mining has been done. Experiments conducted clearly establish that the proposed method of synthesizing high-frequency rules fairly meets the stipulation.",2008,Knowledge and Information Systems volume 17 issue 3 pp 313-334,selection rule;transaction processing;confidence;decision making;information and communications technology;high frequency;internationalization and localization;data analysis;knowledge engineering;information extraction;information technology;operations research;data mining;artificial intelligence;computer science;
Finding cohesive clusters for analyzing knowledge communities,Vasileios Kandylas (University of Pennsylvania);S. Phineas Upham (University of Pennsylvania);Lyle H. Ungar (University of Pennsylvania);,"2116808351,1975399585,2147282416","Documents and authors can be clustered into “knowledge communities” based on the overlap in the papers they cite. We introduce a new clustering algorithm, Streemer, which finds cohesive foreground clusters embedded in a diffuse background, and use it to identify knowledge communities as foreground clusters of papers which share common citations. To analyze the evolution of these communities over time, we build predictive models with features based on the citation structure, the vocabulary of the papers, and the affiliations and prestige of the authors. Findings include that scientific knowledge communities tend to grow more rapidly if their publications build on diverse information and if they use a narrow vocabulary.",2008,Knowledge and Information Systems volume 17 issue 3 pp 335-354,sociology of scientific knowledge;overlay;predictive modelling;social network;cluster analysis;systems modeling;biological classification;knowledge base;world wide web;data mining;database;artificial intelligence;machine learning;computer science;
A hybrid aggregation and compression technique for road network databases,Ali Khoshgozaran (University of Southern California);Ali Khodaei (University of Southern California);Mehdi Sharifzadeh (University of Southern California);Cyrus Shahabi (University of Southern California);,"243340819,1988171951,1977081371,240820708","Vector data and in particular road networks are being queried, hosted and processed in many application domains such as in mobile computing. Many client systems such as PDAs would prefer to receive the query results in unrasterized format without introducing an overhead on overall system performance and result size. While several general vector data compression schemes have been studied by different communities, we propose a novel approach in vector data compression which is easily integrated within a geospatial query processing system. It uses line aggregation to reduce the number of relevant tuples and Huffman compression to achieve a multi-resolution compressed representation of a road network database. Our experiments performed on an end-to-end prototype verify that our approach exhibits fast query processing on both client and server sides as well as high compression ratio.",2008,Knowledge and Information Systems volume 17 issue 3 pp 265-286,client server model;spatial database;lossless compression;compression ratio;multiresolution analysis;data compression;mobile computing;telecommunications;database;simulation;computer science;
Disk aware discord discovery: finding unusual time series in terabyte sized datasets,"Dragomir Yankov (University of California, Riverside);Eamonn J. Keogh (University of California, Riverside);Umaa Rebbapragada (Tufts University);","2162388936,2170070822,2231969277","The problem of finding unusual time series has recently attracted much attention, and several promising methods are now in the literature. However, virtually all proposed methods assume that the data reside in main memory. For many real-world problems this is not be the case. For example, in astronomy, multi-terabyte time series datasets are the norm. Most current algorithms faced with data which cannot fit in main memory resort to multiple scans of the disk /tape and are thus intractable. In this work we show how one particular definition of unusual time series, the time series discord, can be discovered with a disk aware algorithm. The proposed algorithm is exact and requires only two linear scans of the disk with a tiny buffer of main memory. Furthermore, it is very simple to implement. We use the algorithm to provide further evidence of the effectiveness of the discord definition in areas as diverse as astronomy, web query mining, video surveillance, etc., and show the efficiency of our method on datasets which are many orders of magnitude larger than anything else attempted in the literature.",2008,Knowledge and Information Systems volume 17 issue 2 pp 241-262,outlier;time series;knowledge extraction;data mining;database;artificial intelligence;simulation;statistics;computer science;
A framework for simulating real-time multi-agent systems,Chris Micacchi (University of Waterloo);Robin Cohen (University of Waterloo);,"726731387,2286581330","In this paper, we describe an implementation of use in demonstrating the effectiveness of architectures for real-time multi-agent systems. The implementation provides a simulation of a simplified RoboCup Search and Rescue environment, with unexpected events, and includes a simulator for both a real-time operating system and a CPU. We present experimental evidence to demonstrate the benefit of the implementation in the context of a particular hybrid architecture for multi-agent systems that allows certain agents to remain fully autonomous, while others are fully controlled by a coordinating agent. In addition, we discuss the value of the implementation for testing any models for the construction of real-time multi-agent systems and include a comparison to related work.",2008,Knowledge and Information Systems volume 17 issue 2 pp 135-166,disaster;agent architecture;systems modeling;multi agent system;real time operating system;embedded system;artificial intelligence;simulation;computer science;
RRSi: indexing XML data for proximity twig queries,Patrick K. L. Ng (Hong Kong Polytechnic University);Vincent T. Y. Ng (Hong Kong Polytechnic University);,"2143992264,2338715729","Twig query pattern matching is a core operation in XML query processing. Indexing XML documents for twig query processing is of fundamental importance to supporting effective information retrieval. In practice, many XML documents on the web are heterogeneous and have their own formats; documents describing relevant information can possess different structures. Therefore some “user-interesting” documents having similar but non-exact structures against a user query are often missed out. In this paper, we propose the RRSi, a novel structural index designed for structure-based query lookup on heterogeneous sources of XML documents supporting proximate query answers. The index avoids the unnecessary processing of structurally irrelevant candidates that might show good content relevance. An optimized version of the index, oRRSi, is also developed to further reduce both space requirements and computational complexity. To our knowledge, these structural indexes are the first to support proximity twig queries on XML documents. The results of our preliminary experiments show that RRSi and oRRSi based query processing significantly outperform previously proposed techniques in XML repositories with structural heterogeneity.",2008,Knowledge and Information Systems volume 17 issue 2 pp 193-216,xml schema editor;ranking;xml catalog;xml validation;efficient xml interchange;xml framework;streaming xml;xml signature;web search query;web query classification;xml database;document structure description;xml schema;query expansion;query optimization;query language;xml;world wide web;information retrieval;data mining;database;programming language;computer science;
Determining error bounds for spectral filtering based reconstruction methods in privacy preserving data mining,Songtao Guo (University of North Carolina at Charlotte);Xintao Wu (University of North Carolina at Charlotte);Yingjiu Li (Singapore Management University);,"2130315782,2623789330,2263273413","Additive randomization has been a primary tool for hiding sensitive private information. Previous work empirically showed that individual data values can be approximately reconstructed from the perturbed values, using spectral filtering techniques. This poses a serious threat of privacy breaches. In this paper we conduct a theoretical study on how the reconstruction error varies, for different types of additive noise. In particular, we first derive an upper bound for the reconstruction error using matrix perturbation theory. Attackers who use spectral filtering techniques to estimate the true data values may leverage this bound to determine how close their estimates are to the original data. We then derive a lower bound for the reconstruction error, which can help data owners decide how much noise should be added to satisfy a given threshold of the tolerated privacy breach.",2008,Knowledge and Information Systems volume 17 issue 2 pp 217-240,upper and lower bounds;information privacy;data mining;statistics;algorithm;computer science;mathematics;
Knowledge-empowered automated negotiation system for e-Commerce,Yan Zhuang (University of Macau);Simon Fong (University of Macau);Meilin Shi (Tsinghua University);,"2642159826,2132140839,2676370485","This paper focuses on knowledge empowered automated negotiation systems for buyer-centric multi-bilateral multi-attribute e-Procurement. We propose two knowledge empowered models, namely KERM and KACM. KERM is used for the buyer to determine a list of suppliers which are the best qualified candidates to negotiate with. The use of knowledge features largely in the model, which incorporates both the buyer’s and supplier’s profiles in evaluating a quote. Historical trade records of a supplier contribute to the supplier’s profile credit and therefore the rank of the supplier’s quote. KERM also allows the flexibility to assign appropriate weights, based on buyer’s interests, to each knowledge factor affecting the overall evaluation result of a quote. The resulted list of quotes of high rank is believed to produce satisfactory negotiation result for the buyer. KACM enables an automated concession process, while at the same time facilitates a flexible negotiation via the use of concept switch and tagged rules. Different from other negotiation models, KACM emphasizes the utilization of knowledge originated from the historical negotiation data in estimating and fine-tuning the negotiation parameters, for improving the performance of automated negotiation. Graph results show that our software prototype system makes significant improvement in the satisfaction level of negotiation results.",2008,Knowledge and Information Systems volume 17 issue 2 pp 167-191,negotiation;systems modeling;knowledge engineering;data mining;database;artificial intelligence;computer science;
Self-supervised relation extraction from the Web,Benjamin Rozenfeld (Hebrew University of Jerusalem);Ronen Feldman (Hebrew University of Jerusalem);,"2140949427,2159469398","Web extraction systems attempt to use the immense amount of unlabeled text in the Web in order to create large lists of entities and relations. Unlike traditional Information Extraction methods, the Web extraction systems do not label every mention of the target entity or relation, instead focusing on extracting as many different instances as possible while keeping the precision of the resulting list reasonably high. SRES is a self-supervised Web relation extraction system that learns powerful extraction patterns from unlabeled text, using short descriptions of the target relations and their attributes. SRES automatically generates the training data needed for its pattern-learning component. The performance of SRES is further enhanced by classifying its output instances using the properties of the instances and the patterns. The features we use for classification and the trained classification model are independent from the target relation, which we demonstrate in a series of experiments. We also compare the performance of SRES to the performance of the state-of-the-art KnowItAll system, and to the performance of its pattern learning component, which learns simpler pattern language than SRES.",2008,Knowledge and Information Systems volume 17 issue 1 pp 17-33,entity relationship model;pattern language;the internet;systems modeling;biological classification;relationship extraction;world wide web;data mining;database;artificial intelligence;machine learning;computer science;
Maximum entropy based significance of itemsets,Nikolaj Tatti (Helsinki University of Technology);,1367500519,"We consider the problem of defining the significance of an itemset. We say that the itemset is significant if we are surprised by its frequency when compared to the frequencies of its sub-itemsets. In other words, we estimate the frequency of the itemset from the frequencies of its sub-itemsets and compute the deviation between the real value and the estimate. For the estimation we use Maximum Entropy and for measuring the deviation we use Kullback–Leibler divergence. A major advantage compared to the previous methods is that we are able to use richer models whereas the previous approaches only measure the deviation from the independence model. We show that our measure of significance goes to zero for derivable itemsets and that we can use the rank as a statistical test. Our empirical results demonstrate that for our real datasets the independence assumption is too strong but applying more flexible models leads to good results.",2008,Knowledge and Information Systems volume 17 issue 1 pp 57-77,divergence;kullback leibler divergence;principle of maximum entropy;statistical hypothesis testing;systems modeling;data analysis;information extraction;econometrics;data mining;statistics;mathematics;
DSM-FI: an efficient algorithm for mining frequent itemsets in data streams,Hua-Fu Li (Kainan University);Man-Kwan Shan (National Chengchi University);Suh-Yin Lee (National Chiao Tung University);,"2164056773,2007106882,2251835087","Online mining of data streams is an important data mining problem with broad applications. However, it is also a difficult problem since the streaming data possess some inherent characteristics. In this paper, we propose a new single-pass algorithm, called DSM-FI (data stream mining for frequent itemsets), for online incremental mining of frequent itemsets over a continuous stream of online transactions. According to the proposed algorithm, each transaction of the stream is projected into a set of sub-transactions, and these sub-transactions are inserted into a new in-memory summary data structure, called SFI-forest (summary frequent itemset forest) for maintaining the set of all frequent itemsets embedded in the transaction data stream generated so far. Finally, the set of all frequent itemsets is determined from the current SFI-forest. Theoretical analysis and experimental studies show that the proposed DSM-FI algorithm uses stable memory, makes only one pass over an online transactional data stream, and outperforms the existing algorithms of one-pass mining of frequent itemsets.",2008,Knowledge and Information Systems volume 17 issue 1 pp 79-97,transaction data;virtual memory;transaction processing;shared memory;data flow diagram;data analysis;data stream mining;world wide web;data mining;database;computer science;
Protecting business intelligence and customer privacy while outsourcing data mining tasks,Ling Qiu (James Cook University);Yingjiu Li (Singapore Management University);Xintao Wu (University of North Carolina at Charlotte);,"2305543546,2263273413,2623789330","Nowadays data mining plays an important role in decision making. Since many organizations do not possess the in-house expertise of data mining, it is beneficial to outsource data mining tasks to external service providers. However, most organizations hesitate to do so due to the concern of loss of business intelligence and customer privacy. In this paper, we present a Bloom filter based solution to enable organizations to outsource their tasks of mining association rules, at the same time, protect their business intelligence and customer privacy. Our approach can achieve high precision in data mining by trading-off the storage requirement.",2008,Knowledge and Information Systems volume 17 issue 1 pp 99-120,customer intelligence;customer to customer;bloom filter;outsourcing;service provider;association rule learning;information system;numerical analysis;data analysis;competitive intelligence;information extraction;business intelligence;knowledge management;computer security;data mining;database;artificial intelligence;computer science;
Clustering based on matrix approximation: a unifying view,Tao Li (Florida International University);,2472069284,"Clustering is the problem of identifying the distribution of patterns and intrinsic correlations in large data sets by partitioning the data points into similarity classes. Recently, a number of methods have been proposed and demonstrated good performance based on matrix approximation. Despite significant research on these methods, few attempts have been made to establish the connections between them while highlighting their differences. In this paper, we present a unified view of these methods within a general clustering framework where the problem of clustering is formulated as matrix approximations and the clustering objective is minimizing the approximation error between the original data matrix and the reconstructed matrix based on the cluster structures. The general framework provides an elegant base to compare and understand various clustering methods. We provide characterizations of different clustering methods within the general framework including traditional one-side clustering, subspace clustering and two-side clustering. We also establish the connections between our general clustering framework with existing frameworks.",2008,Knowledge and Information Systems volume 17 issue 1 pp 1-15,flame clustering;brown clustering;canopy clustering algorithm;dbscan;correlation clustering;constrained clustering;cure data clustering algorithm;affinity propagation;spectral clustering;fuzzy clustering;clustering high dimensional data;hierarchical clustering;cluster analysis;consensus clustering;biclustering;conceptual clustering;discrete mathematics;combinatorics;data mining;machine learning;statistics;mathematics;
A new concise representation of frequent itemsets using generators and a positive border,Guimei Liu (National University of Singapore);Jinyan Li (Nanyang Technological University);Limsoon Wong (National University of Singapore);,"2016309378,2190567876,2263195242","A complete set of frequent itemsets can get undesirably large due to redundancy when the minimum support threshold is low or when the database is dense. Several concise representations have been previously proposed to eliminate the redundancy. Generator based representations rely on a negative border to make the representation lossless. However, the number of itemsets on a negative border sometimes even exceeds the total number of frequent itemsets. In this paper, we propose to use a positive border together with frequent generators to form a lossless representation. A positive border is usually orders of magnitude smaller than its corresponding negative border. A set of frequent generators plus its positive border is always no larger than the corresponding complete set of frequent itemsets, thus it is a true concise representation. The generalized form of this representation is also proposed. We develop an efficient algorithm, called GrGrowth, to mine generators and positive borders as well as their generalizations. The GrGrowth algorithm uses the depth-first-search strategy to explore the search space, which is much more efficient than the breadth-first-search strategy adopted by most of the existing generator mining algorithms. Our experiment results show that the GrGrowth algorithm is significantly faster than level-wise algorithms for mining generator based representations, and is comparable to the state-of-the-art algorithms for mining frequent closed itemsets.",2008,Knowledge and Information Systems volume 17 issue 1 pp 35-56,breadth first search;electric generator;redundancy;data analysis;information extraction;data mining;database;algorithm;computer science;mathematics;
Adaptive learning of dynamic Bayesian networks with changing structures by detecting geometric structures of time series,Kaijun Wang (Xidian University);Junying Zhang (Xidian University);Fengshan Shen (Xidian University);Lingfeng Shi (Xidian University);,"2591491999,2713678390,2719425399,2723009307","A dynamic Bayesian network (DBN) is one of popular approaches for relational knowledge discovery such as modeling relations or dependencies, which change over time, between variables of a dynamic system. In this paper, we propose an adaptive learning method (autoDBN) to learn DBNs with changing structures from multivariate time series. In autoDBN, segmentation of time series is achieved first through detecting geometric structures transformed from time series, and then model regions are found from the segmentation by designed finding strategies; in each found model region, a DBN model is established by existing structure learning methods; finally, model revisiting is developed to refine model regions and improve DBN models. These techniques provide a special mechanism to find accurate model regions and discover a sequence of DBNs with changing structures, which are adaptive to changing relations between multivariate time series. Experimental results on simulated and real time series show that autoDBN is very effective in finding accurate/reasonable model regions and gives lower error rates, outperforming the switching linear dynamic system method and moving window method.",2008,Knowledge and Information Systems volume 17 issue 1 pp 121-133,bayesian network;dynamical system;unsupervised learning;artificial intelligence;machine learning;algorithm;computer science;
A new and improved version of particle swarm optimization algorithm with global–local best parameters,M. Senthil Arumugam (Multimedia University);M. V. C. Rao (Multimedia University);Aarthi Chandramohan (Multimedia University);,"2105306997,2234478769,2011051172","This paper presents a new and improved version of particle swarm optimization algorithm (PSO) combining the global best and local best model, termed GLBest-PSO. The GLBest-PSO incorporates global–local best inertia weight (GLBest IW) with global–local best acceleration coefficient (GLBest Ac). The velocity equation of the GLBest-PSO is also simplified. The ability of the GLBest-PSO is tested with a set of bench mark problems and the results are compared with those obtained through conventional PSO (cPSO), which uses time varying inertia weight (TVIW) and acceleration coefficient (TVAC). Fine tuning variants such as mutation, cross-over and RMS variants are also included with both cPSO and GLBest-PSO to improve the performance. The simulation results clearly elucidate the advantage of the fine tuning variants, which sharpen the convergence and tune to the best solution for both cPSO and GLBest-PSO. To compare and verify the validity and effectiveness of the GLBest-PSO, a number of statistical analyses are carried out. It is also observed that the convergence speed of GLBest-PSO is considerably higher than cPSO. All the results clearly demonstrate the superiority of the GLBest-PSO.",2008,Knowledge and Information Systems volume 16 issue 3 pp 331-357,particle swarm optimization;systems modeling;evolutionary algorithm;swarm intelligence;calculus;artificial intelligence;machine learning;mathematical optimization;computer science;mathematics;
Voting techniques for expert search,Craig Macdonald (University of Glasgow);Iadh Ounis (University of Glasgow);,"2148910894,336997814","In an expert search task, the users’ need is to identify people who have relevant expertise to a topic of interest. An expert search system predicts and ranks the expertise of a set of candidate persons with respect to the users’ query. In this paper, we propose a novel approach for predicting and ranking candidate expertise with respect to a query, called the Voting Model for Expert Search. In the Voting Model, we see the problem of ranking experts as a voting problem. We model the voting problem using 12 various voting techniques, which are inspired from the data fusion field. We investigate the effectiveness of the Voting Model and the associated voting techniques across a range of document weighting models, in the context of the TREC 2005 and TREC 2006 Enterprise tracks. The evaluation results show that the voting paradigm is very effective, without using any query or collection-specific heuristics. Moreover, we show that improving the quality of the underlying document representation can significantly improve the retrieval performance of the voting techniques on an expert search task. In particular, we demonstrate that applying field-based weighting models improves the ranking of candidates. Finally, we demonstrate that the relative performance of the voting techniques for the proposed approach is stable on a given task regardless of the used weighting models, suggesting that some of the proposed voting techniques will always perform better than other voting techniques.",2008,Knowledge and Information Systems volume 16 issue 3 pp 259-280,data field;document structure description;ranking;voting;weighting;sensor fusion;systems modeling;expert system;information retrieval;data mining;database;artificial intelligence;machine learning;computer science;
SVM based adaptive learning method for text classification from positive and unlabeled documents,Tao Peng (Jilin University);Wanli Zuo (Jilin University);Fengling He (Jilin University);,"2069755387,2055049036,2160044922","Automatic text classification is one of the most important tools in Information Retrieval. This paper presents a novel text classifier using positive and unlabeled examples. The primary challenge of this problem as compared with the classical text classification problem is that no labeled negative documents are available in the training example set. Firstly, we identify many more reliable negative documents by an improved 1-DNF algorithm with a very low error rate. Secondly, we build a set of classifiers by iteratively applying the SVM algorithm on a training data set, which is augmented during iteration. Thirdly, different from previous PU-oriented text classification works, we adopt the weighted vote of all classifiers generated in the iteration steps to construct the final classifier instead of choosing one of the classifiers as the final classifier. Finally, we discuss an approach to evaluate the weighted vote of all classifiers generated in the iteration steps to construct the final classifier based on PSO (Particle Swarm Optimization), which can discover the best combination of the weights. In addition, we built a focused crawler based on link-contexts guided by different classifiers to evaluate our method. Several comprehensive experiments have been conducted using the Reuters data set and thousands of web pages. Experimental results show that our method increases the performance (F1-measure) compared with PEBL, and a focused web crawler guided by our PSO-based classifier outperforms other several classifiers both in harvest rate and target recall.",2008,Knowledge and Information Systems volume 16 issue 3 pp 281-301,random subspace method;web crawler;content analysis;evolutionary algorithm;swarm intelligence;world wide web;data mining;artificial intelligence;machine learning;computer science;
S2S: structural-to-syntactic matching similar documents,Ramazan Savas Aygün (University of Alabama in Huntsville);,2293304725,"Management of large collection of replicated data in centralized or distributed environments is important for many systems that provide data mining, mirroring, storage, and content distribution. In its simplest form, the documents are generated, duplicated and updated by emails and web pages. Although redundancy may increase the reliability at a level, uncontrolled redundancy aggravates the retrieval performance and might be useless if the returned documents are obsolete. Document similarity matching algorithms do not provide the information on the differences of documents, and file synchronization algorithms are usually inefficient and ignore the structural and syntactic organization of documents. In this paper, we propose the S2S matching approach. The S2S matching is composed of structural and syntactic phases to compare documents. Firstly, in the structural phase, documents are decomposed into components by its syntax and compared at the coarse level. The structural mapping processes the decomposed documents based on its syntax without actually mapping at the word level. The structural mapping can be applied in a hierarchical way based on the structural organization of a document. Secondly, the syntactic matching algorithm uses a heuristic look-ahead algorithm for matching consecutive tokens with a verification patch. Our two-phase S2S matching approach provides faster results than currently available string matching algorithms.",2008,Knowledge and Information Systems volume 16 issue 3 pp 303-329,synchronization;information extraction;world wide web;data mining;database;artificial intelligence;computer science;
Estimating average precision when judgments are incomplete,Emine Yilmaz (Northeastern University College of Computer and Information Science);Javed A. Aslam (Northeastern University College of Computer and Information Science);,"2342836604,1997938366","We consider the problem of evaluating retrieval systems with incomplete relevance judgments. Recently, Buckley and Voorhees showed that standard measures of retrieval performance are not robust to incomplete judgments, and they proposed a new measure, bpref, that is much more robust to incomplete judgments. Although bpref is highly correlated with average precision when the judgments are effectively complete, the value of bpref deviates from average precision and from its own value as the judgment set degrades, especially at very low levels of assessment. In this work, we propose three new evaluation measures induced AP, subcollection AP, and inferred AP that are equivalent to average precision when the relevance judgments are complete and that are statistical estimates of average precision when relevance judgments are a random subset of complete judgments. We consider natural scenarios which yield highly incomplete judgments such as random judgment sets or very shallow depth pools. We compare and contrast the robustness of the three measures proposed in this work with bpref for both of these scenarios. Through the use of TREC data, we demonstrate that these measures are more robust to incomplete relevance judgments than bpref, both in terms of how well the measures estimate average precision (as measured with complete relevance judgments) and how well they estimate themselves (as measured with complete relevance judgments). Finally, since inferred AP is the most accurate approximation to average precision and the most robust measure in the presence of incomplete judgments, we provide a detailed analysis of this measure, both in terms of its behavior in theory and its implementation in practice.",2008,Knowledge and Information Systems volume 16 issue 2 pp 173-211,relevance;robustness;evaluation;econometrics;data mining;statistics;mathematics;
Online mining of frequent sets in data streams with error guarantee,Xuan Hong Dang (Nanyang Technological University);Wee Keong Ng (Nanyang Technological University);Kok-Leong Ong (Deakin University);,"1964285635,2120738522,2124123604","For most data stream applications, the volume of data is too huge to be stored in permanent devices or to be thoroughly scanned more than once. It is hence recognized that approximate answers are usually sufficient, where a good approximation obtained in a timely manner is often better than the exact answer that is delayed beyond the window of opportunity. Unfortunately, this is not the case for mining frequent patterns over data streams where algorithms capable of online processing data streams do not conform strictly to a precise error guarantee. Since the quality of approximate answers is as important as their timely delivery, it is necessary to design algorithms to meet both criteria at the same time. In this paper, we propose an algorithm that allows online processing of streaming data and yet guaranteeing the support error of frequent patterns strictly within a user-specified threshold. Our theoretical and experimental studies show that our algorithm is an effective and reliable method for finding frequent sets in data stream environments when both constraints need to be satisfied.",2008,Knowledge and Information Systems volume 16 issue 2 pp 245-258,online algorithm;data flow diagram;satisfiability;data analysis;information extraction;approximation algorithm;combinatorial optimization;data processing;data mining;database;algorithm;computer science;
An information-theoretic approach to quantitative association rule mining,Yiping Ke (Hong Kong University of Science and Technology);James Cheng (Hong Kong University of Science and Technology);Wilfred Ng (Hong Kong University of Science and Technology);,"2164055299,2304873892,2170178419","Quantitative association rule (QAR) mining has been recognized an influential research problem over the last decade due to the popularity of quantitative databases and the usefulness of association rules in real life. Unlike boolean association rules (BARs), which only consider boolean attributes, QARs consist of quantitative attributes which contain much richer information than the boolean attributes. However, the combination of these quantitative attributes and their value intervals always gives rise to the generation of an explosively large number of itemsets, thereby severely degrading the mining efficiency. In this paper, we propose an information-theoretic approach to avoid unrewarding combinations of both the attributes and their value intervals being generated in the mining process. We study the mutual information between the attributes in a quantitative database and devise a normalization on the mutual information to make it applicable in the context of QAR mining. To indicate the strong informative relationships among the attributes, we construct a mutual information graph (MI graph), whose edges are attribute pairs that have normalized mutual information no less than a predefined information threshold. We find that the cliques in the MI graph represent a majority of the frequent itemsets. We also show that frequent itemsets that do not form a clique in the MI graph are those whose attributes are not informatively correlated to each other. By utilizing the cliques in the MI graph, we devise an efficient algorithm that significantly reduces the number of value intervals of the attribute sets to be joined during the mining process. Extensive experiments show that our algorithm speeds up the mining process by up to two orders of magnitude. Most importantly, we are able to obtain most of the high-confidence QARs, whereas the QARs that are not returned by MIC are shown to be less interesting.",2008,Knowledge and Information Systems volume 16 issue 2 pp 213-244,association rule learning;information theory;data mining;database;pattern recognition;machine learning;statistics;computer science;mathematics;
Effective and efficient classification on a search-engine model,Aris Anagnostopoulos (Yahoo!);Andrei Z. Broder (Yahoo!);Kunal Punera (University of Texas at Austin);,"2136686850,2637163715,2343758766","Traditional document classification frameworks, which apply the learned classifier to each document in a corpus one by one, are infeasible for extremely large document corpora, like the Web or large corporate intranets. We consider the classification problem on a corpus that has been processed primarily for the purpose of searching, and thus our access to documents is solely through the inverted index of a large scale search engine. Our main goal is to build the “best” short query that characterizes a document class using operators normally available within search engines. We show that surprisingly good classification accuracy can be achieved on average over multiple classes by queries with as few as 10 terms. As part of our study, we enhance some of the feature-selection techniques that are found in the literature by forcing the inclusion of terms that are negatively correlated with the target class and by making use of term correlations; we show that both of those techniques can offer significant advantages. Moreover, we show that optimizing the efficiency of query execution by careful selection of terms can further reduce the query costs. More precisely, we show that on our set-up the best 10-term query can achieve 93% of the accuracy of the best SVM classifier (14,000 terms), and if we are willing to tolerate a reduction to 89% of the best SVM, we can build a 10-term query that can be executed more than twice as fast as the best 10-term query.",2008,Knowledge and Information Systems volume 16 issue 2 pp 129-154,sargable;ranking;web query classification;inverted index;query expansion;query optimization;content analysis;feature selection;world wide web;information retrieval;data mining;database;artificial intelligence;computer science;
Measuring ranked list robustness for query performance prediction,Yun Zhou (University of Massachusetts Amherst);W. Bruce Croft (University of Massachusetts Amherst);,"2310124346,2127889770","We introduce the notion of ranking robustness, which refers to a property of a ranked list of documents that indicates how stable the ranking is in the presence of uncertainty in the ranked documents. We propose a statistical measure called the robustness score to quantify this notion. Our initial motivation for measuring ranking robustness is to predict topic difficulty for content-based queries in the ad-hoc retrieval task. Our results demonstrate that the robustness score is positively and consistently correlation with average precision of content-based queries across a variety of TREC test collections. Though our focus is on prediction under the ad-hoc retrieval task, we observe an interesting negative correlation with query performance when our technique is applied to named-page finding queries, which are a fundamentally different kind of queries. A side effect of this different behavior of the robustness score between the two types of queries is that the robustness score is also found to be a good feature for query classification.",2008,Knowledge and Information Systems volume 16 issue 2 pp 155-171,ranking;web query classification;side effect;correlation;robustness;theory;information retrieval;data mining;database;pattern recognition;programming language;computer science;
A survey on algorithms for mining frequent itemsets over data streams,James Cheng (Hong Kong University of Science and Technology);Yiping Ke (Hong Kong University of Science and Technology);Wilfred Ng (Hong Kong University of Science and Technology);,"2304873892,2164055299,2170178419","The increasing prominence of data streams arising in a wide range of advanced applications such as fraud detection and trend learning has led to the study of online mining of frequent itemsets (FIs). Unlike mining static databases, mining data streams poses many new challenges. In addition to the one-scan nature, the unbounded memory requirement and the high data arrival rate of data streams, the combinatorial explosion of itemsets exacerbates the mining task. The high complexity of the FI mining problem hinders the application of the stream mining techniques. We recognize that a critical review of existing techniques is needed in order to design and develop efficient mining algorithms and data structures that are able to match the processing rate of the mining with the high arrival rate of data streams. Within a unifying set of notations and terminologies, we describe in this paper the efforts and main techniques for mining data streams and present a comprehensive survey of a number of the state-of-the-art algorithms on mining frequent itemsets over data streams. We classify the stream-mining techniques into two categories based on the window model that they adopt in order to provide insights into how and why the techniques are useful. Then, we further analyze the algorithms according to whether they are exact or approximate and, for approximate approaches, whether they are false-positive or false-negative. We also discuss various interesting issues, including the merits and limitations in existing research and substantive areas for future research.",2008,Knowledge and Information Systems volume 16 issue 1 pp 1-27,type i and type ii errors;terminology;systems modeling;data analysis;concept mining;data stream mining;information extraction;data science;data mining;database;machine learning;computer science;
Discovering correlated spatio-temporal changes in evolving graphs,Jeffrey Chan (University of Melbourne);James Bailey (University of Melbourne);Christopher Leckie (University of Melbourne);,"2408228308,2131557737,2111831791","Graphs provide powerful abstractions of relational data, and are widely used in fields such as network management, web page analysis and sociology. While many graph representations of data describe dynamic and time evolving relationships, most graph mining work treats graphs as static entities. Our focus in this paper is to discover regions of a graph that are evolving in a similar manner. To discover regions of correlated spatio-temporal change in graphs, we propose an algorithm called cSTAG. Whereas most clustering techniques are designed to find clusters that optimise a single distance measure, cSTAG addresses the problem of finding clusters that optimise both temporal and spatial distance measures simultaneously. We show the effectiveness of cSTAG using a quantitative analysis of accuracy on synthetic data sets, as well as demonstrating its utility on two large, real-life data sets, where one is the routing topology of the Internet, and the other is the dynamic graph of files accessed together on the 1998 World Cup official website.",2008,Knowledge and Information Systems volume 16 issue 1 pp 53-96,intersection graph;graph;null model;relational database;cluster analysis;information extraction;graph theory;data science;data mining;database;artificial intelligence;machine learning;computer science;
Clustering multidimensional sequences in spatial and temporal databases,Ira Assent (RWTH Aachen University);Ralph Krieger (RWTH Aachen University);Boris Glavic (RWTH Aachen University);Thomas Seidl (RWTH Aachen University);,"145164693,2123847344,1263411180,2140301036","Many environmental, scientific, technical or medical database applications require effective and efficient mining of time series, sequences or trajectories of measurements taken at different time points and positions forming large temporal or spatial databases. Particularly the analysis of concurrent and multidimensional sequences poses new challenges in finding clusters of arbitrary length and varying number of attributes. We present a novel algorithm capable of finding parallel clusters in different subspaces and demonstrate our results for temporal and spatial applications. Our analysis of structural quality parameters in rivers is successfully used by hydrologists to develop measures for river quality improvements.",2008,Knowledge and Information Systems volume 16 issue 1 pp 29-51,temporal database;information extraction;data mining;database;computer science;
A relational model for XML structural joins and their size estimations,Cheng Luo (Southern Illinois University Carbondale);Zhewei Jiang (Southern Illinois University Carbondale);Wen Chi Hou (Southern Illinois University Carbondale);Feng Yan (Southern Illinois University Carbondale);Qiang Zhu (University of Michigan);,"2208147014,2162146863,2119586575,2267292753,2292350935","XML structural joins, which evaluate the containment (ancestor-descendant) relationships between XML elements, are important operations of XML query processing. Estimating structural join size accurately and quickly is crucial to the success of XML query plan selection and the query optimization. XML structural joins are essentially complex θ-joins, which render well-known estimation techniques for relational equijoins, such as discrete cosine transform, wavelet transform, and sketch, not applicable. In this paper, we model structural joins from a relational point of view and convert the complex θ-joins to equijoins so that those well-known estimation techniques become applicable to structural join size estimation. Theoretical analyses and extensive experiments have been performed on these estimation methods. It is shown that discrete cosine transform requires the least memory and yields the best estimates among the three techniques. Compared with state-of-the-art method IM-DA-Est, discrete cosine transform is much faster, requires less memory, and yields comparable estimates.",2008,Knowledge and Information Systems volume 16 issue 1 pp 97-127,xml database;theoretical computer science;data mining;database;computer science;
Instance weighting versus threshold adjusting for cost-sensitive classification,Huimin Zhao (University of Wisconsin–Milwaukee);,2663058229,"In real-world classification problems, different types of misclassification errors often have asymmetric costs, thus demanding cost-sensitive learning methods that attempt to minimize average misclassification cost rather than plain error rate. Instance weighting and post hoc threshold adjusting are two major approaches to cost-sensitive classifier learning. This paper compares the effects of these two approaches on several standard, off-the-shelf classification methods. The comparison indicates that the two approaches lead to similar results for some classification methods, such as Naive Bayes, logistic regression, and backpropagation neural network, but very different results for other methods, such as decision tree, decision table, and decision rule learners. The findings from this research have important implications on the selection of the cost-sensitive classifier learning approach as well as on the interpretation of a recently published finding about the relative performance of Naive Bayes and decision trees.",2008,Knowledge and Information Systems volume 15 issue 3 pp 321-334,bayes error rate;decision table;bayes classifier;weighting;receiver operating characteristic;decision rule;asymmetry;decision tree;word error rate;logistic regression;backpropagation;biological classification;probabilistic classification;artificial neural network;pattern recognition;machine learning;statistics;computer science;
Boosting text segmentation via progressive classification,Eugenio Cesario (Indian Council of Agricultural Research);Francesco Folino (Indian Council of Agricultural Research);Antonio Locane (Indian Council of Agricultural Research);Giuseppe Manco (Indian Council of Agricultural Research);Riccardo Ortale (Indian Council of Agricultural Research);,"2021815840,2016017692,326732009,2093732677,1974615881","A novel approach for reconciling tuples stored as free text into an existing attribute schema is proposed. The basic idea is to subject the available text to progressive classification, i.e., a multi-stage classification scheme where, at each intermediate stage, a classifier is learnt that analyzes the textual fragments not reconciled at the end of the previous steps. Classification is accomplished by an ad hoc exploitation of traditional association mining algorithms, and is supported by a data transformation scheme which takes advantage of domain-specific dictionaries/ontologies. A key feature is the capability of progressively enriching the available ontology with the results of the previous stages of classification, thus significantly improving the overall classification accuracy. An extensive experimental evaluation shows the effectiveness of our approach.",2008,Knowledge and Information Systems volume 15 issue 3 pp 285-320,classification scheme;text segmentation;data transformation;natural language;segmentation;biological classification;ontology;supervised learning;content analysis;data mining;artificial intelligence;machine learning;computer science;
Compressed hierarchical binary histograms for summarizing multi-dimensional data,Filippo Furfaro (University of Calabria);Giuseppe M. Mazzeo (University of Calabria);Domenico Saccà (Indian Council of Agricultural Research);Cristina Sirangelo (University of Calabria);,"1972243421,2306940198,2155445851,2568142080","Hierarchical binary partitions of multi-dimensional data are investigated as a basis for the construction of effective histograms. Specifically, the impact of adopting lossless compression techniques for representing the histogram on both the accuracy and the efficiency of query answering is investigated. Compression is obtained by exploiting the hierarchical partition scheme underlying the histogram, and then introducing further restrictions on the partitioning which enable a more compact representation of bucket boundaries. Basically, these restrictions consist of constraining the splits of the partition to be laid onto regular grids defined on the buckets. Several heuristics guiding the histogram construction are also proposed, and a thorough experimental analysis comparing the accuracy of histograms resulting from combining different heuristics with different representation models (both the new compression-based and the traditional ones) is provided. The best accuracy turns out from combining our grid-constrained partitioning scheme with one of the new heuristics. Histograms resulting from this combination are compared with state-of-the-art summarization techniques, showing that the proposed approach yields lower error rates and is much less sensitive to dimensionality, and that adopting our compression scheme results in improving the efficiency of query estimation.",2008,Knowledge and Information Systems volume 15 issue 3 pp 335-380,histogram;theoretical computer science;data mining;database;statistics;algorithm;computer science;mathematics;
Typed Abstract State Machines for data-intensive applications,Klaus-Dieter Schewe (Massey University);Jane Zhao (Massey University);,"2632317859,2156824418","This paper focuses on extensions of Abstract State Machines (ASMs) for the benefit of a refinement calculus in the area of data-intensive applications, in particular data warehouses and on-line analytical processing. The extensions lead to a typed version of ASMs and a more specific notion of strong data refinement, which incorporates the preservation of information content by means of schema dominance. Each typed ASM can be effectively translated into an equivalent ordinary one. Providing typed ASMs helps to exploit the existing logical formalisms used in data-intensive applications to define a ground model. Furthermore, strong data refinement helps to set up a refinement-based development method that uses provably correct standard refinement rules.",2008,Knowledge and Information Systems volume 15 issue 3 pp 381-391,refinement calculus;abstract state machines;data warehouse;data mining;database;programming language;algorithm;computer science;
Tracking clusters in evolving data streams over sliding windows,Aoying Zhou (Fudan University);Feng Cao (IBM);Weining Qian (Fudan University);Cheqing Jin (Fudan University);,"2620616608,2428634916,2344638167,2127683674","Mining data streams poses great challenges due to the limited memory availability and real-time query response requirement. Clustering an evolving data stream is especially interesting because it captures not only the changing distribution of clusters but also the evolving behaviors of individual clusters. In this paper, we present a novel method for tracking the evolution of clusters over sliding windows. In our SWClustering algorithm, we combine the exponential histogram with the temporal cluster features, propose a novel data structure, the Exponential Histogram of Cluster Features (EHCF). The exponential histogram is used to handle the in-cluster evolution, and the temporal cluster features represent the change of the cluster distribution. Our approach has several advantages over existing methods: (1) the quality of the clusters is improved because the EHCF captures the distribution of recent records precisely; (2) compared with previous methods, the mechanism employed to adaptively maintain the in-cluster synopsis can track the cluster evolution better, while consuming much less memory; (3) the EHCF provides a flexible framework for analyzing the cluster evolution and tracking a specific cluster efficiently without interfering with other clusters, thus reducing the consumption of computing resources for data stream clustering. Both the theoretical analysis and extensive experiments show the effectiveness and efficiency of the proposed method.",2008,Knowledge and Information Systems volume 15 issue 2 pp 181-214,sliding window protocol;response time;availability;tracking;histogram;cluster analysis;biological classification;data analysis;information extraction;data structure;data mining;real time computing;artificial intelligence;machine learning;computer science;
Computing the minimum-support for mining frequent patterns,"Shichao Zhang (Guangxi Normal University);Xindong Wu (University of Vermont);Chengqi Zhang (University of Technology, Sydney);Jingli Lu (Massey University);","2167944170,2123651450,2166080598,2692752609","Frequent pattern mining is based on the assumption that users can specify the minimum-support for mining their databases. It has been recognized that setting the minimum-support is a difficult task to users. This can hinder the widespread applications of these algorithms. In this paper we propose a computational strategy for identifying frequent itemsets, consisting of polynomial approximation and fuzzy estimation. More specifically, our algorithms (polynomial approximation and fuzzy estimation) automatically generate actual minimum-supports (appropriate to a database to be mined) according to users’ mining requirements. We experimentally examine the algorithms using different datasets, and demonstrate that our fuzzy estimation algorithm fittingly approximates actual minimum-supports from the commonly-used requirements.",2008,Knowledge and Information Systems volume 15 issue 2 pp 233-257,user requirements document;association rule learning;fuzzy logic;data analysis;information extraction;approximation algorithm;data mining;artificial intelligence;algorithm;computer science;mathematics;
A scalable algorithm for mining maximal frequent sequences using a sample,Congnan Luo (Wright State University);Soon M. Chung (Wright State University);,"2185760495,2145880949","In this paper, we propose an efficient scalable algorithm for mining Maximal Sequential Patterns using Sampling (MSPS). The MSPS algorithm reduces much more search space than other algorithms because both the subsequence infrequency-based pruning and the supersequence frequency-based pruning are applied. In MSPS, a sampling technique is used to identify long frequent sequences earlier, instead of enumerating all their subsequences. We propose how to adjust the user-specified minimum support level for mining a sample of the database to achieve better overall performance. This method makes sampling more efficient when the minimum support is small. A signature-based method and a hash-based method are developed for the subsequence infrequency-based pruning when the seed set of frequent sequences for the candidate generation is too big to be loaded into memory. A prefix tree structure is developed to count the candidate sequences of different sizes during the database scanning, and it also facilitates the customer sequence trimming. Our experiments showed MSPS has very good performance and better scalability than other algorithms.",2008,Knowledge and Information Systems volume 15 issue 2 pp 149-179,sampling;information extraction;data mining;pattern recognition;algorithm;computer science;mathematics;
A compact multi-resolution index for variable length queries in time series databases,Srividya Kadiyala (Concordia University);Nematollaah Shiri (Concordia University);,"2277261444,2207586587","We study the problem of searching similar patterns in time series data for variable length queries. Recently, a multi-resolution indexing technique (MRI) was proposed in (Kahveci and Singh, in proceedings of the international conference on data engineering, pp. 273–282, 2001; Kahveci and Singh, IEEE Trans Knowl Data Eng 16(4):418–433, 2004) to address this problem, which uses compression as an additional step to reduce the index size. In this paper, we propose an alternative technique, called compact MRI (CMRI), which uses adaptive piecewise constant approximation (APCA) representation as dimensionality reduction technique, and which occupies much less space without requiring compression. We implemented both MRI and CMRI, and conducted extensive experiments to evaluate and compare their performance on real stock data as well as synthetic. Our results indicate that CMRI provides a much better precision ranging from 0.75 to 0.89 on real data, and from 0.80 to 0.95 on synthetic data, while for MRI, these ranges are from 0.16 to 0.34, and from 0.03 to 0.65, respectively. Compared to sequential scan, we found that CMRI is 4–30 times faster and the number of disk I/Os it required is close to minimal. In terms of storage utilization, CMRI occupies 1% of the memory occupied by MRI. These results and analysis show CMRI to be an efficient and scalable indexing technique for large time series databases.",2008,Knowledge and Information Systems volume 15 issue 2 pp 131-147,r tree;dimensionality reduction;time series;theoretical computer science;data mining;database;machine learning;statistics;algorithm;computer science;mathematics;
"CaBMA: a case-based reasoning system for capturing, refining, and reusing project plans",Ke Xu (Lehigh University);Hèctor Muñoz-Avila (Lehigh University);,"2310449357,2133414589","In this paper, we present CaBMA, a prototype of a knowledge-based system designed to assist with project planning tasks using case-based reasoning. CaBMA introduces a novel approach to project planning in that, for the first time, a knowledge layer is added on top of traditional project management software. Project management software provides editing and bookkeeping capabilities. CaBMA enhances these capabilities by automatically capturing project plans in the form of cases, refining these cases over time to avoid potential inconsistency between them, reusing these cases to generate plans for new projects, and indicating possible repairs for project plans when they derive away from existing knowledge. We will give an overview of the system, provide a detailed explanation on each component, and present an empirical study based on synthetic data.",2008,Knowledge and Information Systems volume 15 issue 2 pp 215-232,project management triangle;project planning;software project management;estimation;empirical research;project management;knowledge base;artificial intelligence;simulation;computer science;
Beyond topical similarity: a structural similarity measure for retrieving highly similar documents,Xiaojun Wan (Peking University);,2653340806,"Accurately measuring document similarity is important for many text applications, e.g. document similarity search, document recommendation, etc. Most traditional similarity measures are based only on “bag of words” of documents and can well evaluate document topical similarity. In this paper, we propose the notion of document structural similarity, which is expected to further evaluate document similarity by comparing document subtopic structures. Three related factors (i.e. the optimal matching factor, the text order factor and the disturbing factor) are proposed and combined to evaluate document structural similarity, among which the optimal matching factor plays the key role and the other two factors rely on its results. The experimental results demonstrate the high performance of the optimal matching factor for evaluating document topical similarity, which is as well as or better than most popular measures. The user study shows the good ability of the proposed overall measure with all three factors to further find highly similar documents from those topically similar documents, which is much better than that of the popular measures and other baseline structural similarity measures.",2008,Knowledge and Information Systems volume 15 issue 1 pp 55-73,document structure description;optimal matching;bag of words model;semantic similarity;case based reasoning;similitude;similarity;structural similarity;metric;world wide web;information retrieval;data mining;computer science;mathematics;
Expertise-based peer selection in Peer-to-Peer networks,Peter Haase (Karlsruhe Institute of Technology);Ronny Siebes (VU University Amsterdam);Frank van Harmelen (VU University Amsterdam);,"2290566231,2622868404,333553561","Peer-to-Peer systems have proven to be an effective way of sharing data. Modern protocols are able to efficiently route a message to a given peer. However, determining the destination peer in the first place is not always trivial. We propose a model in which peers advertise their expertise in the Peer-to-Peer network. The knowledge about the expertise of other peers forms a semantic topology. Based on the semantic similarity between the subject of a query and the expertise of other peers, a peer can select appropriate peers to forward queries to, instead of broadcasting the query or sending it to a random set of peers. To calculate our semantic similarity measure, we make the simplifying assumption that the peers share the same ontology. We evaluate the model in a bibliographic scenario, where peers share bibliographic descriptions of publications among each other. In simulation experiments complemented with a real-world field experiment, we show how expertise-based peer selection improves the performance of a Peer-to-Peer system with respect to precision, recall and the number of messages.",2008,Knowledge and Information Systems volume 15 issue 1 pp 75-107,peer to peer;ontology;semantic similarity;shared memory;field experiment;similitude;similarity;routing;metric;systems modeling;ontology;semantics;computer simulation;world wide web;data mining;database;artificial intelligence;computer science;
Mining partial periodic correlations in time series,Zhen He (La Trobe University);X. Sean Wang (University of Vermont);Byung Suk Lee (University of Vermont);Alan C. H. Ling (University of Vermont);,"2143353724,2108008679,2117647038,2665191774","Recently, periodic pattern mining from time series data has been studied extensively. However, an interesting type of periodic pattern, called partial periodic (PP) correlation in this paper, has not been investigated. An example of PP correlation is that power consumption is high either on Monday or Tuesday but not on both days. In general, a PP correlation is a set of offsets within a particular period such that the data at these offsets are correlated with a certain user-desired strength. In the above example, the period is a week (7 days), and each day of the week is an offset of the period. PP correlations can provide insightful knowledge about the time series and can be used for predicting future values. This paper introduces an algorithm to mine time series for PP correlations based on the principal component analysis (PCA) method. Specifically, given a period, the algorithm maps the time series data to data points in a multidimensional space, where the dimensions correspond to the offsets within the period. A PP correlation is then equivalent to correlation of data when projected to a subset of the dimensions. The algorithm discovers, with one sequential scan of data, all those PP correlations (called minimum PP correlations) that are not unions of some other PP correlations. Experiments using both real and synthetic data sets show that the PCA-based algorithm is highly efficient and effective in finding the minimum PP correlations.",2008,Knowledge and Information Systems volume 15 issue 1 pp 31-54,time series;digital library;calculus;statistics;
Querying color images using user-specified wavelet features,Te-Wei Chiang (Chihlee Institute of Technology);Tienwei Tsai (Chihlee Institute of Technology);,"2664254921,2709733215","In this paper, an image retrieval method based on wavelet features is proposed. Due to the superiority in multiresolution analysis and spatial-frequency localization, the discrete wavelet transform (DWT) is used to extract wavelet features (i.e., approximations, horizontal details, vertical details, and diagonal details) at each resolution level. During the feature-extraction process, each image is first transformed from the standard RGB color space to the YUV space for the purpose of efficiency and ease of extracting the features based on color tones; then each component (i.e., Y, U, and V) of the image is further transformed to the wavelet domain. In the image database establishing phase, the wavelet coefficients of each image are stored; in the image retrieving phase, the system compares the wavelet coefficients of the Y, U, and V components of the query image with those of the images in the database, based on the weight factors adjusted by users, and find out good matches. To benefit from the user–machine interaction, a friendly graphic user interface (GUI) for fuzzy cognition is developed, allowing users to easily adjust weights for each feature according to their preferences. In our experiment, 1000 test images are used to demonstrate the effectiveness of our system.",2008,Knowledge and Information Systems volume 15 issue 1 pp 109-129,second generation wavelet transform;cascade algorithm;stationary wavelet transform;query by example;diagonal matrix;discrete wavelet transform;color space;wavelet packet decomposition;color image;wavelet;spatial analysis;spatial frequency;fuzzy logic;multiresolution analysis;computer graphics;graphical user interface;user interface;discrete time and continuous time;wavelet transform;feature extraction;cognition;image retrieval;image processing;computer graphics images;computer vision;computer science;
TeMAS–a multi-agent system for temporally rich domains,Slobodan Ribarić (University of Zagreb);Tomislav Hrkać (University of Zagreb);,"2088534606,2225155248","In this paper, we present the model and simulator of a multi-agent system (MAS) for temporally rich domains. The theoretical foundations of the model include a knowledge representation scheme based on an original modification of Petri nets, called Petri nets with time tokens (PNTTs), as well as temporal reasoning based on the extension of Allen's temporal logic. The proposed MAS, called TeMAS, has a hierarchical structure, consisting of different levels, where each level contains clusters of agents. A paradigm of hierarchically organized blackboards is used for the communication among agents, clusters, as well as levels. We describe an object-oriented implementation of a program simulator of TeMAS and give an example of the use of the simulator for interpretation of events in a dynamic scene.",2008,Knowledge and Information Systems volume 15 issue 1 pp 1-30,petri net;modeling and simulation;knowledge base;artificial intelligence;simulation;algorithm;computer science;
Random walk with restart: fast solutions and applications,Hanghang Tong (Carnegie Mellon University);Christos Faloutsos (Carnegie Mellon University);Jia Yu Pan (Carnegie Mellon University);,"2224718883,2198983026,2159823517","How closely related are two nodes in a graph? How to compute this score quickly, on huge, disk-resident, real graphs? Random walk with restart (RWR) provides a good relevance score between two nodes in a weighted graph, and it has been successfully used in numerous settings, like automatic captioning of images, generalizations to the “connection subgraphs”, personalized PageRank, and many more. However, the straightforward implementations of RWR do not scale for large graphs, requiring either quadratic space and cubic pre-computation time, or slow response time on queries. We propose fast solutions to this problem. The heart of our approach is to exploit two important properties shared by many real graphs: (a) linear correlations and (b) block-wise, community-like structure. We exploit the linearity by using low-rank matrix approximation, and the community structure by graph partitioning, followed by the Sherman–Morrison lemma for matrix inversion. Experimental results on the Corel image and the DBLP dabasets demonstrate that our proposed methods achieve significant savings over the straightforward implementations: they can save several orders of magnitude in pre-computation and storage cost, and they achieve up to 150 × speed up with 90%+ quality preservation.",2008,Knowledge and Information Systems volume 14 issue 3 pp 327-346,response time;graph partition;community structure;linearity;random graph;linear equation;random walk;correlation;systems modeling;graph theory;discrete mathematics;combinatorics;artificial intelligence;machine learning;statistics;algorithm;mathematics;
Robust projected clustering,Gabriela Moise (University of Alberta);Jörg Sander (University of Alberta);Martin Ester (Simon Fraser University);,"2059192723,2118842476,2067196623","Projected clustering partitions a data set into several disjoint clusters, plus outliers, so that each cluster exists in a subspace. Subspace clustering enumerates clusters of objects in all subspaces of a data set, and it tends to produce many overlapping clusters. Such algorithms have been extensively studied for numerical data, but only a few have been proposed for categorical data. Typical drawbacks of existing projected and subspace clustering algorithms for numerical or categorical data are that they rely on parameters whose appropriate values are difficult to set appropriately or that they are unable to identify projected clusters with few relevant attributes. We present P3C, a robust algorithm for projected clustering that can effectively discover projected clusters in the data while minimizing the number of required parameters. P3C does not need the number of projected clusters as input, and can discover, under very general conditions, the true number of projected clusters. P3C is effective in detecting very low-dimensional projected clusters embedded in high dimensional spaces. P3C positions itself between projected and subspace clustering in that it can compute both disjoint or overlapping clusters. P3C is the first projected clustering algorithm for both numerical and categorical data.",2008,Knowledge and Information Systems volume 14 issue 3 pp 273-298,complete linkage clustering;determining the number of clusters in a data set;correlation clustering;cure data clustering algorithm;single linkage clustering;multidimensional analysis;fuzzy clustering;outlier;categorical variable;cluster analysis;vector space;biological classification;data mining;machine learning;statistics;mathematics;
"Forecasting skewed biased stochastic ozone days: analyses, solutions and beyond",Kun Zhang (Xavier University);Wei Fan (IBM);,"2482265863,2491264568","Much work on skewed, stochastic, high dimensional, and biased datasets usually implicitly solve each problem separately. Recently, we have been approached by Texas Commission on Environmental Quality (TCEQ) to help them build highly accurate ozone level alarm forecasting models for the Houston area, where these technical difficulties come together in one single problem. Key characteristics of this problem that is challenging and interesting include: (1) the dataset is sparse (72 features, and 2 or 5% positives depending on the criteria of “ozone days”), (2) evolving over time from year to year, (3) limited in collected data size (7  years or around 2,500 data entries), (4) contains a large number of irrelevant features, (5) is biased in terms of “sample selection bias”, and (6) the true model is stochastic as a function of measurable factors. Besides solving a difficult application problem, this dataset offers a unique opportunity to explore new and existing data mining techniques, and to provide experience, guidance and solution for similar problems. Our main technical focus addresses on how to estimate reliable probability given both sample selection bias and a large number of irrelevant features, and how to choose the most reliable decision threshold to predict the unknown future with different distribution. On the application side, the prediction accuracy of our chosen approach (bagging probabilistic decision trees and random decision trees) is 20% higher in recall (correctly detects 1–3 more ozone days, depending on the year) and 10% higher in precision (15–30 fewer false alarm days per year) than state-of-the-art methods used by air quality control scientists, and these results are significant for TCEQ. On the technical side of data mining, extensive empirical results demonstrate that, at least for this problem, and probably other problems with similar characteristics, these two straight-forward non-parametric methods can provide significantly more accurate and reliable solutions than a number of sophisticated and well-known algorithms, such as SVM and AdaBoost among many others.",2008,Knowledge and Information Systems volume 14 issue 3 pp 299-326,measurable function;multidimensional analysis;constant false alarm rate;air quality index;skewness;ozone;selection bias;decision tree;quality control;bias;forecasting;systems modeling;biological classification;data analysis;information extraction;econometrics;data mining;statistics;
Ensembles of relational classifiers,Christine Preisach (University of Hildesheim);Lars Schmidt-Thieme (University of Hildesheim);,"73418403,78243962","Relational classification aims at including relations among entities into the classification process, for example taking relations among documents such as common authors or citations into account. However, considering more than one relation can further improve classification accuracy. Here we introduce a new approach to make use of several relations as well as both, relations and local attributes for classification using ensemble methods. To accomplish this, we present a generic relational ensemble model that can use different relational and local classifiers as components. Furthermore, we discuss solutions for several problems concerning relational data such as heterogeneity, sparsity, and multiple relations. Especially the sparsity problem will be discussed in more detail. We introduce a new method called PRNMultiHop that tries to handle this problem. Furthermore we categorize relational methods in a systematic way. Finally, we provide empirical evidence, that our relational ensemble methods outperform existing relational classification methods, even rather complex models such as relational probability trees (RPTs), relational dependency networks (RDNs) and relational Bayesian classifiers (RBCs).",2008,Knowledge and Information Systems volume 14 issue 3 pp 249-272,entity relationship model;relational model;relational database;autocorrelation;statistical relational learning;information extraction;data mining;pattern recognition;machine learning;statistics;computer science;mathematics;
The importance of generalizability for anomaly detection,Gilbert L. Peterson (Air Force Institute of Technology);Brent T. McBride (Air Force Institute of Technology);,"2157047087,2149303370","In security-related areas there is concern over novel “zero-day” attacks that penetrate system defenses and wreak havoc. The best methods for countering these threats are recognizing “nonself” as in an Artificial Immune System or recognizing “self” through clustering. For either case, the concern remains that something that appears similar to self could be missed. Given this situation, one could incorrectly assume that a preference for a tighter fit to self over generalizability is important for false positive reduction in this type of learning problem. This article confirms that in anomaly detection as in other forms of classification a tight fit, although important, does not supersede model generality. This is shown using three systems each with a different geometric bias in the decision space. The first two use spherical and ellipsoid clusters with a k-means algorithm modified to work on the one-class/blind classification problem. The third is based on wrapping the self points with a multidimensional convex hull (polytope) algorithm capable of learning disjunctive concepts via a thresholding constant. All three of these algorithms are tested using the Voting dataset from the UCI Machine Learning Repository, the MIT Lincoln Labs intrusion detection dataset, and the lossy-compressed steganalysis domain.",2008,Knowledge and Information Systems volume 14 issue 3 pp 377-392,cluster analysis;anomaly detection;concept learning;artificial intelligence;machine learning;algorithm;computer science;mathematics;
Privacy-preserving SVM classification,Jaideep Vaidya (Rutgers University);Hwanjo Yu (University of Iowa);Xiaoqian Jiang (University of Iowa);,"2164601541,2257366035,2132814769","Traditional Data Mining and Knowledge Discovery algorithms assume free access to data, either at a centralized location or in federated form. Increasingly, privacy and security concerns restrict this access, thus derailing data mining projects. What is required is distributed knowledge discovery that is sensitive to this problem. The key is to obtain valid results, while providing guarantees on the nondisclosure of data. Support vector machine classification is one of the most widely used classification methodologies in data mining and machine learning. It is based on solid theoretical foundations and has wide practical application. This paper proposes a privacy-preserving solution for support vector machine (SVM) classification, PP-SVM for short. Our solution constructs the global SVM classification model from data distributed at multiple parties, without disclosing the data of each party to others. Solutions are sketched out for data that is vertically, horizontally, or even arbitrarily partitioned. We quantify the security and efficiency of the proposed method, and highlight future challenges.",2008,Knowledge and Information Systems volume 14 issue 2 pp 161-178,data modeling;support vector machine;privacy;internationalization and localization;biological classification;data analysis;data stream mining;information extraction;data science;data mining;artificial intelligence;machine learning;computer science;
Fast and effective clustering of XML data using structural information,Richi Nayak (Queensland University of Technology);,2144864747,"This paper presents the incremental clustering algorithm, XML documents Clustering with Level Similarity (XCLS), that groups the XML documents according to structural similarity. A level structure format is introduced to represent the structure of XML documents for efficient processing. A global criterion function that measures the similarity between the new document and existing clusters is developed. It avoids the need to compute the pair-wise similarity between two individual documents and hence saves a huge amount of computing effort. XCLS is further modified to incorporate the semantic meanings of XML tags for investigating the trade-offs between accuracy and efficiency. The empirical analysis shows that the structural similarity overplays the semantic similarity in the clustering process of the structured data such as XML. The experimental analysis shows that the XCLS method is fast and accurate in clustering the heterogeneous documents by structures.",2008,Knowledge and Information Systems volume 14 issue 2 pp 197-215,xml validation;efficient xml interchange;correlation clustering;simple api for xml;xml schema;fuzzy clustering;semantic similarity;xml;semantics;data structure;world wide web;data mining;database;programming language;computer science;
Learning to extract and summarize hot item features from multiple auction web sites,Tak-Lam Wong (City University of Hong Kong);Wai Lam (The Chinese University of Hong Kong);,"2141897598,2119595446","It is difficult to digest the poorly organized and vast amount of information contained in auction Web sites which are fast changing and highly dynamic. We develop a unified framework which can automatically extract product features and summarize hot item features from multiple auction sites. To deal with the irregularity in the layout format of Web pages and harness the uncertainty involved, we formulate the tasks of product feature extraction and hot item feature summarization as a single graph labeling problem using conditional random fields. One characteristic of this graphical model is that it can model the inter-dependence between neighbouring tokens in a Web page, tokens in different Web pages, as well as various information such as hot item features across different auction sites. We have conducted extensive experiments on several real-world auction Web sites to demonstrate the effectiveness of our framework.",2008,Knowledge and Information Systems volume 14 issue 2 pp 143-160,conditional random field;web mining;information extraction;graph theory;world wide web;data mining;database;artificial intelligence;machine learning;computer science;
Mining interesting imperfectly sporadic rules,Yun Sing Koh (University of Otago);Nathan Rountree (University of Otago);Richard A. O’Keefe (University of Otago);,"1987110578,2043598640,2313055114","Detecting association rules with low support but high confidence is a difficult data mining problem. To find such rules using approaches like the Apriori algorithm, minimum support must be set very low, which results in a large number of redundant rules. We are interested in sporadic rules; i.e. those that fall below a maximum support level but above the level of support expected from random coincidence. There are two types of sporadic rules: perfectly sporadic and imperfectly sporadic. Here we are more concerned about finding imperfectly sporadic rules, where the support of the antecedent as a whole falls below maximum support, but where items may have quite high support individually. In this paper, we introduce an algorithm called Mining Interesting Imperfectly Sporadic Rules (MIISR) to find imperfectly sporadic rules efficiently, e.g. fever, headache, stiff neck → meningitis. Our proposed method uses item constraints and coincidence pruning to discover these rules in reasonable time. This paper is an expanded version of Koh et al. [Advances in knowledge discovery and data mining: 10th Pacific-Asia Conference (PAKDD 2006), Singapore. Lecture Notes in Computer Science 3918, Springer, Berlin, pp 473–482].",2008,Knowledge and Information Systems volume 14 issue 2 pp 179-196,confidence;association rule learning;association;redundancy;data analysis;information extraction;data mining;database;artificial intelligence;machine learning;computer science;mathematics;
A systematic study on parameter correlations in large-scale duplicate document detection,"Shaozhi Ye (University of California, Davis);Ji-Rong Wen (Microsoft);Wei-Ying Ma (Microsoft);","2713061243,2066159008,2134693834","Although much work has been done on duplicate document detection (DDD) and its applications, we observe the absence of a systematic study on the performance and scalability of large-scale DDD algorithms. It is still unclear how various parameters in DDD correlate mutually, such as similarity threshold, precision/recall requirement, sampling ratio, and document size. This paper explores the correlations among several most important parameters in DDD and the impact of sampling ratio is of most interest since it heavily affects the accuracy and scalability of DDD algorithms. An empirical analysis is conducted on a million HTML documents from the TREC .GOV collection. Experimental results show that even when using the same sampling ratio, the precision of DDD varies greatly on documents with different sizes. Based on this observation, we propose an adaptive sampling strategy for DDD, which minimizes the sampling ratio with the constraint of a given precision requirement. We believe that the insights from our analysis are helpful for guiding the future large-scale DDD work.",2008,Knowledge and Information Systems volume 14 issue 2 pp 217-232,cluster analysis;speech recognition;data mining;database;machine learning;statistics;computer science;
Parallel randomized sampling for support vector machine (SVM) and support vector regression (SVR),"Yumao Lu (Yahoo!);Vwani P. Roychowdhury (University of California, Los Angeles);","2637217224,286655925","A parallel randomized support vector machine (PRSVM) and a parallel randomized support vector regression (PRSVR) algorithm based on a randomized sampling technique are proposed in this paper. The proposed PRSVM and PRSVR have four major advantages over previous methods. (1) We prove that the proposed algorithms achieve an average convergence rate that is so far the fastest bounded convergence rate, among all SVM decomposition training algorithms to the best of our knowledge. The fast average convergence bound is achieved by a unique priority based sampling mechanism. (2) Unlike previous work (Provably fast training algorithm for support vector machines, 2001) the proposed algorithms work for general linear-nonseparable SVM and general non-linear SVR problems. This improvement is achieved by modeling new LP-type problems based on Karush–Kuhn–Tucker optimality conditions. (3) The proposed algorithms are the first parallel version of randomized sampling algorithms for SVM and SVR. Both the analytical convergence bound and the numerical results in a real application show that the proposed algorithm has good scalability. (4) We present demonstrations of the algorithms based on both synthetic data and data obtained from a real word application. Performance comparisons with SVMlight show that the proposed algorithms may be efficiently implemented.",2008,Knowledge and Information Systems volume 14 issue 2 pp 233-247,randomized algorithms as zero sum games;support vector machine;sampling;linear programming;econometrics;machine learning;mathematical optimization;statistics;algorithm;computer science;mathematics;
An Algorithm for Business Process Compliance,Guido Governatori (NICTA);Antonino Rotolo (University of Bologna);,"2108494256,2024031949",-,2008,Knowledge and Information Systems,artifact centric business process model;new business development;semantics of business vocabulary and business rules;business architecture;business transformation;business activity monitoring;business process model and notation;business rule;process mining;business process discovery;business case;business process modeling;business process management;business process;business model;process modeling;business analysis;process management;computer science;
A Process Model of Legal Argument with Hypotheticals,Kevin D. Ashley (University of Pittsburgh);Collin Lynch (University of Pittsburgh);Niels Pinkwart (Clausthal University of Technology);Vincent Aleven (Carnegie Mellon University);,"718266907,2126850028,133666533,2008829840",-,2008,Knowledge and Information Systems,process modeling;empirical evidence;natural language processing;knowledge management;artificial intelligence;computer science;
Top 10 algorithms in data mining,Xindong Wu (University of Vermont);Vipin Kumar (University of Minnesota);J. Ross Quinlan;Joydeep Ghosh (University of Texas at Austin);Qiang Yang 0001 (Hong Kong University of Science and Technology);Hiroshi Motoda (Osaka University);Geoffrey J. McLachlan (University of Queensland);Angus F. M. Ng (Griffith University);Bing Liu 0001 (University of Illinois at Chicago);Philip S. Yu (IBM);Zhi-Hua Zhou (Nanjing University);Michael Steinbach (University of Minnesota);David J. Hand (Imperial College London);Dan Steinberg (University of Salford);,"2123651450,2161062602,2685249985,2148168557,2109031554,323689644,2058818181,2574198912,2244698799,2125104194,2286237009,2102135651,2175518357,2189564309","This paper presents the top 10 data mining algorithms identified by the IEEE International Conference on Data Mining (ICDM) in December 2006: C4.5, k-Means, SVM, Apriori, EM, PageRank, AdaBoost, kNN, Naive Bayes, and CART. These top 10 algorithms are among the most influential data mining algorithms in the research community. With each algorithm, we provide a description of the algorithm, discuss the impact of the algorithm, and review current and further research on the algorithm. These 10 algorithms cover classification,",2007,Knowledge and Information Systems volume 14 issue 1 pp 1-37,fsa red algorithm;k means clustering;supervised learning;information extraction;data mining;artificial intelligence;machine learning;statistics;computer science;
Algorithms for unimodal segmentation with applications to unimodality detection,Niina Haiminen (University of Helsinki);Aristides Gionis (University of Helsinki);Kari Laasonen (University of Helsinki);,"2711929780,2593877498,2602336950","We study the problem of segmenting a sequence into k pieces so that the resulting segmentation satisfies monotonicity or unimodality constraints. Unimodal functions can be used to model phenomena in which a measured variable first increases to a certain level and then decreases. We combine a well-known unimodal regression algorithm with a simple dynamic-programming approach to obtain an optimal quadratic-time algorithm for the problem of unimodal k-segmentation. In addition, we describe a more efficient greedy-merging heuristic that is experimentally shown to give solutions very close to the optimal. As a concrete application of our algorithms, we describe methods for testing if a sequence behaves unimodally or not. The methods include segmentation error comparisons, permutation testing, and a BIC-based scoring scheme. Our experimental evaluation shows that our algorithms and the proposed unimodality tests give very intuitive results, for both real-valued and binary data.",2007,Knowledge and Information Systems volume 14 issue 1 pp 39-57,unimodality;combinatorics;mathematical optimization;statistics;mathematics;
Mining follow-up correlation patterns from time-related databases,Shichao Zhang (Guangxi Normal University);Zifang Huang (Beihang University);Jilian Zhang (Guangxi Normal University);Xiaofeng Zhu (Guangxi Normal University);,"2695727709,2311610499,2109511667,2609202506","Research on traditional association rules has gained a great attention during the past decade. Generally, an association rule A → B is used to predict that B likely occurs when A occurs. This is a kind of strong correlation, and indicates that the two events will probably happen simultaneously. However, in real world applications such as bioinformatics and medical research, there are many follow-up correlations between itemsets A and B, such as, B is likely to occur n times after A has occurred m times. That is, the correlative itemsets do not belong to the same transaction. We refer to this relation as a follow-up correlation pattern (FCP). The task of mining FCP patterns brings more challenges on efficient processing than normal pattern discovery because the number of potentially interesting patterns becomes extremely large as the length limit of transactions no longer exists. In this paper, we develop an efficient algorithm to identify FCP patterns in time-related databases. We also experimentally evaluate our approach, and provide extensive results on mining this new kind of patterns.",2007,Knowledge and Information Systems volume 14 issue 1 pp 81-100,transaction processing;regular expression;association rule learning;relational database;association;correlation;data analysis;information extraction;data mining;database;artificial intelligence;algorithm;computer science;
Mining gene–sample–time microarray data: a coherent gene cluster discovery approach,Daxin Jiang (Nanyang Technological University);Jian Pei (Simon Fraser University);Murali Ramanathan (University at Buffalo);Chuan Lin (University at Buffalo);Chun Tang (University at Buffalo);Aidong Zhang (University at Buffalo);,"2123654898,2126330539,2154842653,2728481681,2130157064,2228514421","Extensive studies have shown that mining microarray data sets is important in bioinformatics research and biomedical applications. In this paper, we explore a novel type of gene–sample–time microarray data sets that records the expression levels of various genes under a set of samples during a series of time points. In particular, we propose the mining of coherent gene clusters from such data sets. Each cluster contains a subset of genes and a subset of samples such that the genes are coherent on the samples along the time series. The coherent gene clusters may identify the samples corresponding to some phenotypes (e.g., diseases), and suggest the candidate genes correlated to the phenotypes. We present two efficient algorithms, namely the Sample-Gene Search and the Gene–Sample Search, to mine the complete set of coherent gene clusters. We empirically evaluate the performance of our approaches on both a real microarray data set and synthetic data sets. The test results have shown that our approaches are both efficient and effective to find meaningful coherent gene clusters.",2007,Knowledge and Information Systems volume 13 issue 3 pp 305-335,cluster analysis;information extraction;bioinformatics;data mining;computer science;
Modeling semantics in composite Web service requests by utility elicitation,Qianhui Althea Liang (Singapore Management University);Jen-Yao Chung (IBM);Steven Miller (Singapore Management University);,"2305321689,2474381830,2154241972","When meeting the challenges in automatic and semi-automatic Web service composition, capturing the user’s service demand and preferences is as important as knowing what the services can do. This paper discusses the idea of semantic service requests for composite services, and presents a multi-attribute utility theory (MAUT) based model of composite service requests. Service requests are modeled as user preferences and constraints. Two preference structures, additive independence and generalized additive independence, are utilized in calculating the expected utilities of service composition outcomes. The model is also based on an iterative and incremental scheme meant to better capture requirements in accordance with service consumers’ needs. OWL-S markup vocabularies and associated inference mechanism are used as a means to bring semantics to service requests. Ontology conceptualizations and language constructs are added to OWL-S as uniform representations of possible aspects of the requests. This model of semantics in service requests enables unambiguous understanding of the service needs and more precise generation of the desired compositions. An application scenario is presented to illustrate how the proposed model can be applied in the real business world.",2007,Knowledge and Information Systems volume 13 issue 3 pp 367-394,service level requirement;service level objective;service design;service delivery framework;consumer;conceptualization;semantic web;web service;differentiated service;expected utility hypothesis;utility;iterative method;systems modeling;ontology;semantics;world wide web;data mining;database;artificial intelligence;computer science;
CPU load shedding for binary stream joins,Bugra Gedik (IBM);Kun Lung Wu (IBM);Philip S. Yu (IBM);Ling Liu (Georgia Institute of Technology College of Computing);,"1814817956,2095612124,2125104194,2125988131","We present an adaptive load shedding approach for windowed stream joins. In contrast to the conventional approach of dropping tuples from the input streams, we explore the concept ofselective processing for load shedding. We allow stream tuples to be stored in the windows and shed excessive CPU load by performing the join operations, not on the entire set of tuples within the windows, but on a dynamically changing subset of tuples that are learned to be highly beneficial. We support such dynamic selective processing through three forms of runtimeadaptations: adaptation to input stream rates, adaptation to time correlation between the streams and adaptation to join directions. Our load shedding approach enables us to integrateutility-based load shedding withtime correlation-based load shedding. Indexes are used to further speed up the execution of stream joins. Experiments are conducted to evaluate our adaptive load shedding in terms of output rate and utility. The results show that our selective processing approach to load shedding is very effective and significantly outperforms the approach that drops tuples from the input streams.",2007,Knowledge and Information Systems volume 13 issue 3 pp 271-303,data stream mining;parallel computing;database;real time computing;machine learning;computer science;
Evolution of faceted taxonomies and CTCA expressions,Yannis Tzitzikas (University of Crete);,64415330,"A faceted taxonomy is a set of taxonomies each describing the application domain from a different (preferably orthogonal) point of view. CTCA is an algebra that allows specifying the set of meaningful compound terms (meaningful conjunctions of terms) over a faceted taxonomy in a flexible and efficient manner. However, taxonomy updates may turn a CTCA expression e not well-formed and may turn the compound terms specified by e to no longer reflect the domain knowledge originally expressed in e. This paper shows how we can revise e after a taxonomy update and reach an expression e′ that is both well-formed and whose semantics (compound terms defined) is as close as possible to the semantics of the original expression e before the update. Various cases are analyzed and the revising algorithms are given. The proposed technique can enhance the robustness and usability of systems that are based on CTCA and allows optimizing several other tasks where CTCA can be used (including mining and compressing).",2007,Knowledge and Information Systems volume 13 issue 3 pp 337-365,domain knowledge;usability;robustness;taxonomy;semantics;data mining;database;artificial intelligence;algorithm;computer science;
A multimodal virtual reality interface for 3D interaction with VTK,Arjan J. F. Kok (Open University in the Netherlands);Robert van Liere (College of Western Idaho);,"2111583020,245899705","The object-oriented visualization Toolkit (VTK) is widely used for scientific visualization. VTK is a visualization library that provides a large number of functions for presenting three-dimensional data. Interaction with the visualized data is controlled with two-dimensional input devices, such as mouse and keyboard. Support for real three-dimensional and multimodal input is non-existent. This paper describes VR-VTK: a multimodal interface to VTK on a virtual environment. Six degree of freedom input devices are used for spatial 3D interaction. They control the 3D widgets that are used to interact with the visualized data. Head tracking is used for camera control. Pedals are used for clutching. Speech input is used for application commands and system control. To address several problems specific for spatial 3D interaction, a number of additional features, such as more complex interaction methods and enhanced depth perception, are discussed. Furthermore, the need for multimodal input to support interaction with the visualization is shown. Two existing VTK applications are ported using VR-VTK to run in a desktop virtual reality system. Informal user experiences are presented.",2007,Knowledge and Information Systems volume 13 issue 2 pp 197-219,visualization;virtual reality;human computer interaction;computer graphics images;simulation;computer science;
Toward understanding the importance of gesture in distributed scientific collaboration,Brian Corrie (University of Victoria);Margaret-Anne D. Storey (University of Victoria);,"2138340080,2123850919","In this paper, we explore the importance of gesture in distributed, scientific collaboration. In particular, we are interested in the impact that distance has when remote collaborators are working together with digital artifacts that are complex (and often visual) in form, such as data that results from complex scientific simulations. We call this artifact-centric collaboration. In order to understand such collaborations, we performed a longitudinal ethnographic study of a group of collaborating scientific researchers. We observed a single research group during its regular research meetings, performing over 18 h of observations spanning a 5-month period. In this paper, we present a detailed analysis of two meetings where artifact interaction is prominent, one where all participants are collocated and the other where participants are distributed. Our analysis consists of a detailed coding of the artifact-centric interactions in both meetings as well as an analysis of these interactions. We conclude the paper with a summary of our findings, including a set of guidelines that can be used to inform the design of collaboration software that supports distributed, artifact-centric collaboration.",2007,Knowledge and Information Systems volume 13 issue 2 pp 143-171,visualization;scientific visualization;knowledge management;human computer interaction;multimedia;computer science;
Visual transformation for interactive spatiotemporal data mining,Yang Cai 0002 (Carnegie Mellon University);Richard P. Stumpf (National Oceanic and Atmospheric Administration);Timothy Wynne (National Oceanic and Atmospheric Administration);Michelle C. Tomlinson (National Oceanic and Atmospheric Administration);Daniel Sai Ho Chung (Carnegie Mellon University);Xavier Boutonnier (Carnegie Mellon University);Matthias Ihmig (Carnegie Mellon University);Rafael Franco (Carnegie Mellon University);Nathaniel Bauernfeind (Carnegie Mellon University);,"2686463739,2064648739,2529647516,2123685899,2287898341,41218394,2274254254,2126775158,2044362962","Analytical models intend to reveal inner structure, dynamics, or relationship of things. However, they are not necessarily intuitive to humans. Conventional scientific visualization methods are intuitive, but limited by depth, dimension, and resolution. The purpose of this study is to bridge the gap with transformation algorithms for mapping the data from an abstract space to an intuitive one, which include shape correlation, periodicity, multiphysics, and spatial Bayesian. We tested this approach with the oceanographic case study. We found that the interactive visualization increases robustness in object tracking and positive detection accuracy in object prediction. We also found that the interactive method enables the user to process the image data at less than 1 min per image versus 30 min per image manually. As a result, our test system can handle at least 10 times more data sets than traditional manual analyses. The results also suggest that minimal human interactions with appropriate computational transformations or cues may significantly increase the overall productivity.",2007,Knowledge and Information Systems volume 13 issue 2 pp 119-142,visualization;interactive visualization;information extraction;computer vision;data mining;statistics;computer science;
On the role of individual human abilities in the design of adaptive user interfaces for scientific problem solving environments,Elena V. Zudilova-Seinstra (University of Amsterdam);,1968218127,"A scientific problem solving environment should be built in such a way that users (scientists) might exploit underlying technologies without a specialised knowledge about available tools and resources. An adaptive user interface can be considered as an opportunity in addressing this challenge. This paper explores the importance of individual human abilities in the design of adaptive user interfaces for scientific problem solving environments. In total, seven human factors (gender, learning abilities, locus of control, attention focus, cognitive strategy and verbal and nonverbal IQs) have been evaluated regarding their impact on interface adjustments done manually by users. People’s preferences for different interface configurations have been investigated. The experimental study suggests criteria for the inclusion of human factors into the user model guiding and controlling the adaptation process. To provide automatic means of adaptation, the Intelligent System for User Modelling has been developed.",2007,Knowledge and Information Systems volume 13 issue 2 pp 243-270,locus of control;user modeling;attention;sex;user interface;systems modeling;human factors and ergonomics;human computer interaction;artificial intelligence;simulation;programming language;computer science;
A typology of course of motion in simulated environments based on Bézier curve analysis,Corina Sas (Lancaster University);Nikita Schmidt (University College Dublin);,"2094470252,2149030029","This paper proposes a novel method of analysing trajectories followed by people while they perform navigational tasks. The results indicate that modelling trajectories with Bezier curves provides a basis for the diagnosis of navigational patterns. The method offers five indicators: goodness of fit, average curvature, number of inflexion points, lengths of straight line segments, and area covered. Study results, obtained in a virtual environment show that these indicators carry important information about user performance, specifically spatial knowledge acquisition.",2007,Knowledge and Information Systems volume 13 issue 2 pp 173-196,line segment;bezier curve;typology;virtual machine;trajectory;curvature;goodness of fit;navigation;systems modeling;human factors and ergonomics;artificial intelligence;simulation;computer science;
Exploration of directional-predictive sounds for nonvisual interaction with graphs,Tatiana G. Evreinova (University of Tampere);Leena K. Vesterinen (University of Tampere);Grigori Evreinov (University of Tampere);Roope Raisamo (University of Tampere);,"2029552009,23678755,1386924463,2323039776","Sonification of stylus movements accompanied with kinesthetic feedback is one of possible techniques to develop cross-modal coordination in the absence of visual information. The investigated problems are the following: how to minimize the number of sounds while increasing the information they contain and how to choose a natural sonification grammar which would not require extra cognitive efforts. We demonstrate two case studies of employing directional-predictive sounds (DPS). Stylus movements were sonified through three sound signals taking into account the exploration behavior and the concept of the capture radius. The performance of eight subjects was evaluated in terms of the stylus deviation in relation to the points of the virtual graph, a length of the scanpaths, and the task completion time. When stylus movements were accompanied with the DPS signals within four capture radiuses, the deviation of the stylus from the graph inspected was always less than one capture radius. The scanpaths were 24–40% shorter in length and the task completion times decreased by 20–25%. We also demonstrate the game application which was designed to optimize an exploration behavior enhanced by the DPS. The results of the proposed sonification technique based on the model of the exploration behavior are discussed.",2007,Knowledge and Information Systems volume 13 issue 2 pp 221-241,sonification;stylus;grammar;behavioral modeling;computer vision;artificial intelligence;simulation;computer science;
Visualisation and interaction for scientific exploration and knowledge discovery,Elena V. Zudilova-Seinstra (University of Amsterdam);Tony Adriaansen (Commonwealth Scientific and Industrial Research Organisation);,"1968218127,2009635208",-,2007,Knowledge and Information Systems volume 13 issue 2 pp 115-117,knowledge extraction;data science;bioinformatics;data mining;computer science;
"A quantitative analysis of product categorization standards: content, coverage, and maintenance of eCl@ss, UNSPSC, eOTD, and the RosettaNet Technical Dictionary",Martin Hepp (University of Innsbruck);Joerg Leukel (University of Hohenheim);Volker Schmitz (University of Duisburg-Essen);,"2121886309,2086242892,2128956465","Many e-business scenarios require the integration of product-related data into target applications or target documents at the recipient’s side. Such tasks can be automated much better if the textual descriptions are augmented by a machine-feasible representation of the product semantics. For this purpose, categorization standards for products and services, like UNSPSC, eCl@ss, the ECCMA Open Technical Dictionary (eOTD), or the RosettaNet Technical Dictionary (RNTD) are available, but they vary in terms of structural properties and content. In this paper, we present metrics for assessing the content quality and maturity of such standards and apply these metrics to eCl@ss, UNSPSC, eOTD, and RNTD. Our analysis shows that (1) the amount of content is very unevenly spread over top-level categories, which contradicts the promise of a broad scope implicitly made by the existence of a large number of top-level categories, and that (2) more expressive structural features exist only for parts of these standards. Additionally, we (3) measure the amount of maintenance in the various top-level categories, which helps identify the actively maintained subject areas as compared to those which ones are rather dead branches. Finally, we show how our approach can be used (4) by enterprises for selecting an appropriate standard, and (5) by standards bodies for monitoring the maintenance of a standard as a whole.",2007,Knowledge and Information Systems volume 13 issue 1 pp 77-114,maturity;ramification;branching;ontology;information integration;categorization;metrics;relational database;metric;quantitative analysis;biological classification;ontology;semantics;world wide web;data mining;database;artificial intelligence;computer science;
Interactive visual exploration of association rules with rule-focusing methodology,Julien Blanchard (École polytechnique de l'université de Nantes);Fabrice Guillet (École polytechnique de l'université de Nantes);Henri Briand (École polytechnique de l'université de Nantes);,"2494037063,2424244730,2664857644","On account of the enormous amounts of rules that can be produced by data mining algorithms, knowledge post-processing is a difficult stage in an association rule discovery process. In order to find relevant knowledge for decision making, the user (a decision maker specialized in the data studied) needs to rummage through the rules. To assist him/her in this task, we here propose the rule-focusing methodology, an interactive methodology for the visual post-processing of association rules. It allows the user to explore large sets of rules freely by focusing his/her attention on limited subsets. This new approach relies on rule interestingness measures, on a visual representation, and on interactive navigation among the rules. We have implemented the rule-focusing methodology in a prototype system called ARVis. It exploits the user's focus to guide the generation of the rules by means of a specific constraint-based rule-mining algorithm.",2007,Knowledge and Information Systems volume 13 issue 1 pp 43-75,association rule learning;interactive visualization;information extraction;knowledge base;expert system;image processing;data mining;database;artificial intelligence;computer science;
Generalizing the notion of confidence,Michael Steinbach (University of Minnesota);Vipin Kumar (University of Minnesota);,"2102135651,2161062602","In this paper, we explore extending association analysis to non-traditional types of patterns and non-binary data by generalizing the notion of confidence. We begin by describing a general framework that measures the strength of the connection between two association patterns by the extent to which the strength of one association pattern provides information about the strength of another. Although this framework can serve as the basis for designing or analyzing measures of association, the focus in this paper is to use the framework as the basis for extending the traditional concept of confidence to error-tolerant itemsets (ETIs) and continuous data. To that end, we provide two examples. First, we (1) describe an approach to defining confidence for ETIs that preserves the interpretation of confidence as an estimate of a conditional probability, and (2) show how association rules based on ETIs can have better coverage (at an equivalent confidence level) than rules based on traditional itemsets. Next, we derive a confidence measure for continuous data that agrees with the standard confidence measure when applied to binary transaction data. Further analysis of this result exposes some of the important issues involved in constructing a confidence measure for continuous data.",2007,Knowledge and Information Systems volume 12 issue 3 pp 279-299,confidence distribution;association rule learning;confidence interval;information extraction;knowledge base;expert system;data mining;database;pattern recognition;artificial intelligence;statistics;computer science;mathematics;
Answering ad hoc aggregate queries from data streams using prefix aggregate trees,Moonjung Cho (University at Buffalo);Jian Pei (Simon Fraser University);Ke Wang (Simon Fraser University);,"2120300352,2126330539,2626264286","In some business applications such as trading management in financial institutions, it is required to accurately answer ad hoc aggregate queries over data streams. Materializing and incrementally maintaining a full data cube or even its compression or approximation over a data stream is often computationally prohibitive. On the other hand, although previous studies proposed approximate methods for continuous aggregate queries, they cannot provide accurate answers. In this paper, we develop a novel prefix aggregate tree (PAT) structure for online warehousing data streams and answering ad hoc aggregate queries. Often, a data stream can be partitioned into the historical segment, which is stored in a traditional data warehouse, and the transient segment, which can be stored in a PAT to answer ad hoc aggregate queries. The size of a PAT is linear in the size of the transient segment, and only one scan of the data stream is needed to create and incrementally maintain a PAT. Although the query answering using PAT costs more than the case of a fully materialized data cube, the query answering time is still kept linear in the size of the transient segment. Our extensive experimental results on both synthetic and real data sets illustrate the efficiency and the scalability of our design.",2007,Knowledge and Information Systems volume 12 issue 3 pp 301-329,materialized view;cube;data cube;transient response;scalability;tree structure;time complexity;information system;data warehouse;world wide web;data mining;database;computer science;
An efficient method for estimating null values in relational databases,Jia-Wen Wang (National Yunlin University of Science and Technology);Ching-Hsue Cheng (National Yunlin University of Science and Technology);,"2425863611,2161354717","Generally, a database system containing null value attributes will not operate properly. This study proposes an efficient and systematic approach for estimating null values in a relational database which utilizes clustering algorithms to cluster data, and a regression coefficient to determine the degree of influence between different attributes. Two databases are used to verify the proposed method: (1) Human resource database; and (2) Waugh's database. Furthermore, the mean of absolute error rate (MAER) and average error are used as evaluation criteria to compare the proposed method with other methods. It demonstrates that the proposed method is superior to existing methods for estimating null values in relational database systems.",2007,Knowledge and Information Systems volume 12 issue 3 pp 379-394,relational database management system;human resource management;data mining;database;statistics;computer science;
Intelligent systems in the automotive industry: applications and trends,Oleg Yu. Gusikhin (Ford Motor Company);Nestor Rychtyckyj (Ford Motor Company);Dimitar Filev (Ford Motor Company);,"2408110031,2088676570,2182069676","There is a common misconception that the automobile industry is slow to adapt new technologies, such as artificial intelligence (AI) and soft computing. The reality is that many new technologies are deployed and brought to the public through the vehicles that they drive. This paper provides an overview and a sampling of many of the ways that the automotive industry has utilized AI, soft computing and other intelligent system technologies in such diverse domains like manufacturing, diagnostics, on-board systems, warranty analysis and design.",2007,Knowledge and Information Systems volume 12 issue 2 pp 147-168,automotive industry;soft computing;operations research;artificial intelligence;computer science;
Knowledge and information distribution leveraged by intelligent agents,Ramón F. Brena (Monterrey Institute of Technology and Higher Education);José-Luis Aguirre (Monterrey Institute of Technology and Higher Education);Carlos Iván Chesñevar (Universidad Nacional del Sur);Eduardo H. Ramírez (Monterrey Institute of Technology and Higher Education);Leonardo Garrido (Monterrey Institute of Technology and Higher Education);,"2162079529,2297288793,85652889,2136033820,2142891466","Knowledge and Information distribution is indeed one of the main processes in Knowledge Management. Today, most Information Technology tools for supporting this distribution are based on repositories accessed through Web-based systems. This approach has, however, many practical limitations, mainly due to the strain they put on the user, who is responsible of accessing the right Knowledge and Information at the right moments. As a solution for this problem, we have proposed an alternative approach which is based on the notion of delegation of distribution tasks to synthetic agents, which become responsible of taking care of the organization's as well as the individuals' interests. In this way, many Knowledge and Information distribution tasks can be performed on the background, and the agents can recognize relevant events as triggers for distributing the right information to the right users at the right time. In this paper, we present the JITIK approach to model knowledge and information distribution, giving a high-level account of the research made around this project, emphasizing two particular aspects: a sophisticated argument-based mechanism for deciding among conflicting distribution policies, and the embedding of JITIK agents in enterprises using the service-oriented architecture paradigm. It must be remarked that a JITIK-based application is currently being implemented for one of the leading industries in Mexico.",2007,Knowledge and Information Systems volume 12 issue 2 pp 203-227,delegation;aspect oriented programming;service oriented architecture;embedding;the internet;systems modeling;knowledge engineering;intelligent agent;information technology;knowledge management;database;artificial intelligence;simulation;computer science;
A hierarchical approach for the redesign of chemical processes,Ivan López-Arévalo (CINVESTAV);René Bañares-Alcántara (University of Oxford);Arantza Aldea (Oxford Brookes University);A. Rodríguez-Martínez (Universidad Autónoma del Estado de Morelos);,"220145877,106308539,2118911644,1829902532","An approach to improve the management of complexity during the redesign of technical processes is proposed. The approach consists of two abstract steps. In the first step, model-based reasoning is used to generate automatically alternative representations of an existing process at several levels of abstraction. In the second step, process alternatives are generated through the application of case-based reasoning. The key point of our framework is the modeling approach, which is an extension of the Multimodeling and Multilevel Flow Modeling methodologies. These, together with a systematic design methodology, are used to represent a process hierarchically, thus improving the identification of analogous equipment/sections from different processes. The hierarchical representation results in sets of equipment/sections organized according to their functions and intentions. A case-based reasoning system then retrieves from a library of cases similar equipment/sections to the one selected by the user. The final output is a set of equipment/sections ordered according to their similarity. Human intervention is necessary to adapt the most promising case within the original process.",2007,Knowledge and Information Systems volume 12 issue 2 pp 169-201,alternative process;chemical reactor;case based reasoning;design process;similitude;similarity;abstraction;engineering design process;artificial intelligence;algorithm;computer science;
Decision-making on pipe stress analysis enabled by knowledge-based systems,Matías Alvarado (CINVESTAV);Miguel A. Rodríguez-Toral (Mexican Institute of Petroleum);Armando Rosas (Mexican Institute of Petroleum);Sergio Ayala (Mexican Institute of Petroleum);,"2124820900,2661646179,2105354842,2528584301","This paper presents engineering decision-making on pipe stress analysis through the application of knowledge-based systems (KBS). Stress analysis, as part of the design and analysis of process pipe networks, serves to identify whether a given pipe arrangement can cope with weight, thermal, and pressure stress at safe operation levels. An iterative process of design and analysis cycle is done routinely by engineers while analyzing the existing networks or while designing the process pipe networks. In our proposal, the KBS establishes a bidirectional communication with the current engineering software for pipe stress analysis, so that the user benefits from this integration. The stress analysis knowledge base is constructed by registering the senior engineers’ know-how. The engineers’ overall strategy to follow up during the pipe stress analysis, to some extent contained by the KBS, is presented. Advantages in saving engineering man-hours and usefulness in guiding experts in pipe stress analysis are the major services for the process industry.",2007,Knowledge and Information Systems volume 12 issue 2 pp 255-278,canalisation;pipe;iterative and incremental development;design process;stress strain analysis;weight;stress;knowledge based systems;operations research;artificial intelligence;computer science;
Artificial intelligence methodologies for agile refining: an overview,Rajagopalan Srinivasan_aff1n2 (National University of Singapore);,2497491944,"Agile manufacturing is the capability to prosper in a competitive environment of continuous and unpredictable changes by reacting quickly and effectively to the changing markets and other exogenous factors. Agility of petroleum refineries is determined by two factors – ability to control the process and ability to efficiently manage the supply chain. In this paper, we outline some challenges faced by refineries that seek to be lean, nimble, and proactive. These problems, which arise in supply chain management and operations management are seldom amenable to traditional, monolithic solutions. As discussed here using several examples, methodologies drawn from artificial intelligence – software agents, pattern recognition, expert systems – have a role to play in this path toward agility.",2007,Knowledge and Information Systems volume 12 issue 2 pp 129-145,software agent;supply chain management;intelligent agent;expert system;process control;artificial intelligence;simulation;computer science;
Ontology-driven intelligent service for configuration support in networked organizations,Alexander V. Smirnov (Russian Academy of Sciences);Nikolay Shilov (Russian Academy of Sciences);Tatiana Levashova (Russian Academy of Sciences);Leonid Sheremetov (Mexican Institute of Petroleum);Miguel Contreras (Mexican Institute of Petroleum);,"2112255007,1965127585,2025678467,2622728798,2584427646","Nowadays, organizations must continually adapt to market and organizational changes to achieve their most important goals. Migration to business services and service-oriented architectures provides a valuable opportunity to attain the organization objectives. This migration causes evolution both in organizational structure and in technology-enabling businesses to dynamically change vendors and services. One of the forms of organizational structures is the form of networked organization. Technologies of business intelligence and Web intelligence effectively support business processes within the networked organizations. While business intelligence focuses on development of services for consumer needs recognition, information search, and evaluation of alternatives; Web intelligence addresses advancement of Web-empowered systems, services, and environments. The paper proposes a technological ontology-driven framework for configuration support as applied to networked organization. The framework integrates concepts of business intelligence and Web intelligence into a collaboration environment of a networked organization on the base of attainment of knowledge logistics purposes. This framework referred to as KSNet is based on the integration of software agent technology and Web services. Knowledge logistics functions of KSNet are complemented by technological functions of knowledge-gathering agents. The services of these agents are implemented with CAPNET, a FIPA compliant agent platform. CAPNET allows consuming services of agents in a service-oriented way. Applicability of the approach is illustrated through a “Binni scenario”-based case study of a portable field hospital configuration.",2007,Knowledge and Information Systems volume 12 issue 2 pp 229-253,services computing;web service;software agent;web intelligence;competitive intelligence;ontology;knowledge base;business intelligence;knowledge management;database;artificial intelligence;simulation;computer science;
Current challenges and trends in intelligent computing and knowledge management in industry,Matías Alvarado (CINVESTAV);Leonid Sheremetov (American Petroleum Institute);René Bañares-Alcántara (University of Oxford);Francisco Cantú-Ortiz (Monterrey Institute of Technology and Higher Education);,"2124820900,2045632445,106308539,2020699900","Matias Alvarado is currently a Research Scientist at the Centre of Research and Advanced Studies (CINVESTAV-IPN, Mexico). He got a Ph.D. degree in computer science from the Technical University of Catalonia, with a major in artificial intelligence. He received the B.Sc. degree in mathematics from the National Autonomous University of Mexico. His interests in research and technological applications include knowledge management and decision making; autonomous agents and multiagent systems for supply chain disruption management; concurrency control, pattern recognition and computational logic. He is the author of about 50 scientific papers, a Journal Special Issues Guest Editor on topics of artificial intelligence and knowledge management for the oil industry; an academic, invited to the National University of Singapore, Technical University of Catalonia, University of Oxford, University of Utrecht, and Benemerita Universidad Autonoma de Puebla. Leonid Sheremetov received the Ph.D. degree in computer science in 1990 from St. Petersburg Institute for Informatics and Automation of the Russian Academy of Sciences, where he has worked as a Research Fellow and a Senior Research Fellow from 1982. Now he is a Principal Investigator of the Research Program on Applied Mathematics and Computing of the Mexican Petroleum Institute, where he leads the Distributed Intelligent Systems Group, and a part-time professor of the Artificial Intelligence Laboratory of the Centre for Computing Research of the National Polytechnic Institute (CIC-IPN), Mexico. His current research interests include multiagent systems, semantic WEB, decision support systems, and enterprise information integration. His group developed CAPNET agent platform and has been involved in several projects for the energy industry ranging from petroleum exploration and production to knowledge management with special focus on industrial exploitation of agent technology. He is also a member of the Editorial Boards of several journals. Rene Banares-Alcantara has worked in the University of Oxford from October 2003 and is now a Reader in engineering science at the Department of Engineering Science and a Fellow in engineering at New College. He previously held a readership at the University of Edinburgh and lectureships in Spain and at the Universidad Nacional Autonoma de Mexico (UNAM). He obtained his undergraduate degree from UNAM and the M.S. and Ph.D. degrees from Carnegie Mellon University (CMU). Starting with his work at CMU, his research interests have been in the area of process systems engineering, in particular chemical process design and synthesis. He has developed a strong relationship with computer science/artificial intelligence research groups in different universities and research institutes, with current research also linking to social and biological modeling. He has (co)authored more than 100 refereed publications and has been a Principal Investigator and a Researcher in several EPSRC and European Union projects. Francisco Cantu-Ortiz obtained the Ph.D. degree in artificial intelligence from the University of Edinburgh, United Kingdom and the Bachelor's degree in computer systems engineering from the Tecnologico de Monterrey (ITESM), Mexico. He is a Full Professor of artificial intelligence at Tecnologico de Monterey and is also the Dean of research and graduate Studies. He has been the Head of the Center for Artificial Intelligence and of the Informatics Research Center. Dr. Cantu-Ortiz has been the General Chair of about 15 international conferences in artificial intelligence and expert system and was a Local Chair of the International Joint Conference on Artificial Intelligence in 2003. His research interests include knowledge based systems and inference, machine learning, and data mining using Bayesian and statistical techniques for business intelligence, technology management, and entrepreneurial science. More recently, his interests have extended to epistemology and philosophy of science. He was the President of the Mexican Society for Artificial Intelligence and is a member of the IEEE Computer Society and the ACM.",2007,Knowledge and Information Systems volume 12 issue 2 pp 117-127,operations research;data mining;artificial intelligence;computer science;
Stability of feature selection algorithms: a study on high-dimensional spaces,Alexandros Kalousis (University of Geneva);Julien Prados (University of Geneva);Melanie Hilario (University of Geneva);,"1835498038,2314201037,2159401048","With the proliferation of extremely high-dimensional data, feature selection algorithms have become indispensable components of the learning process. Strangely, despite extensive work on the stability of learning algorithms, the stability of feature selection algorithms has been relatively neglected. This study is an attempt to fill that gap by quantifying the sensitivity of feature selection algorithms to variations in the training set. We assess the stability of feature selection algorithms based on the stability of the feature preferences that they express in the form of weights-scores, ranks, or a selected feature subset. We examine a number of measures to quantify the stability of feature preferences and propose an empirical way to estimate them. We perform a series of experiments with several feature selection algorithms on a set of proteomics datasets. The experiments allow us to explore the merits of each stability measure and create stability profiles of the feature selection algorithms. Finally, we show how stability profiles can support the choice of a feature selection algorithm.",2007,Knowledge and Information Systems volume 12 issue 1 pp 95-116,minimum redundancy feature selection;clustering high dimensional data;curse of dimensionality;dimensionality reduction;feature;feature selection;data mining;pattern recognition;artificial intelligence;machine learning;computer science;mathematics;
Fast agglomerative hierarchical clustering algorithm using Locality-Sensitive Hashing,Hisashi Koga (University of Electro-Communications);Tetsuo Ishibashi (University of Electro-Communications);Toshinori Watanabe (University of Electro-Communications);,"2239996905,2274821608,2619553030","The single linkage method is a fundamental agglomerative hierarchical clustering algorithm. This algorithm regards each point as a single cluster initially. In the agglomeration step, it connects a pair of clusters such that the distance between the nearest members is the shortest. This step is repeated until only one cluster remains. The single linkage method can efficiently detect clusters in arbitrary shapes. However, a drawback of this method is a large time complexity of O(n 2), where n represents the number of data points. This time complexity makes this method infeasible for large data. This paper proposes a fast approximation algorithm for the single linkage method. Our algorithm reduces the time complexity to O(nB) by rapidly finding the near clusters to be connected by Locality-Sensitive Hashing, a fast algorithm for the approximate nearest neighbor search. Here, B represents the maximum number of points going into a single hash entry and it practically diminishes to a small constant as compared to n for sufficiently large hash tables. Experimentally, we show that (1) the proposed algorithm obtains clustering results similar to those obtained by the single linkage method and (2) it runs faster for large data than the single linkage method.",2007,Knowledge and Information Systems volume 12 issue 1 pp 25-53,nearest neighbor chain algorithm;canopy clustering algorithm;complete linkage clustering;single linkage clustering;locality sensitive hashing;outlier;hash function;hash table;hierarchical clustering;time complexity;cluster analysis;mathematical chemistry;approximation algorithm;pattern recognition;machine learning;algorithm;computer science;mathematics;
Non-redundant data clustering,David Gondek (Brown University);Thomas Hofmann (Brown University);,"2673988360,2240984066","Data clustering is a popular approach for automatically finding classes, concepts, or groups of patterns. In practice, this discovery process should avoid redundancies with existing knowledge about class structures or groupings, and reveal novel, previously unknown aspects of the data. In order to deal with this problem, we present an extension of the information bottleneck framework, called coordinated conditional information bottleneck, which takes negative relevance information into account by maximizing a conditional mutual information score subject to constraints. Algorithmically, one can apply an alternating optimization scheme that can be used in conjunction with different types of numeric and non-numeric attributes. We discuss extensions of the technique to the tasks of semi-supervised classification and enumeration of successive non-redundant clusterings. We present experimental results for applications in text mining and computer vision.",2007,Knowledge and Information Systems volume 12 issue 1 pp 1-24,cluster analysis;information extraction;text mining;data mining;database;pattern recognition;artificial intelligence;machine learning;computer science;
S-Club: an overlay-based efficient service discovery mechanism in CROWN Grid,Chunming Hu (Beihang University);Yanmin Zhu (Hong Kong University of Science and Technology);Jinpeng Huai (Beihang University);Yunhao Liu (Hong Kong University of Science and Technology);Lionel M. Ni (Hong Kong University of Science and Technology);,"2098362168,2130173690,1966937424,2130996983,2135854099","Information service plays a key role in grid system, handles resource discovery and management process. Employing existing information service architectures suffers from poor scalability, long search response time, and large traffic overhead. In this paper, we propose a service club mechanism, called S-Club, for efficient service discovery. In S-Club, an overlay based on existing Grid Information Service (GIS) mesh network of CROWN is built, so that GISs are organized as service clubs. Each club serves for a certain type of service while each GIS may join one or more clubs. S-Club is adopted in our CROWN Grid and the performance of S-Club is evaluated by comprehensive simulations. The results show that S-Club scheme significantly improves search performance and outperforms existing approaches.",2007,Knowledge and Information Systems volume 12 issue 1 pp 55-75,service discovery;type of service;grid;mesh networking;web service;response time;grid computing;scalability;resource allocation;geographic information system;resource management;world wide web;telecommunications;computer security;computer science;
Generalized regression model for sequence matching and clustering,Hansheng Lei (University of Texas at Brownsville);Venu Govindaraju (State University of New York System);,"2158122691,2102146905","Linear relation has been found to be valuable in rule discovery of stocks, such as if stock X goes up a, stock Y will go down b. The traditional linear regression models the linear relation of two sequences faithfully. However, if a user requires clustering of stocks into groups where sequences have high linearity or similarity with each other, it is prohibitively expensive to compare sequences one by one. In this paper, we present generalized regression model (GRM) to match the linearity of multiple sequences at a time. GRM also gives strong heuristic support for graceful and efficient clustering. The experiments on the stocks in the NASDAQ market mined interesting clusters of stock trends efficiently.",2007,Knowledge and Information Systems volume 12 issue 1 pp 77-94,linear regression;eigenvalues and eigenvectors;regression analysis;knowledge extraction;calculus;econometrics;machine learning;statistics;computer science;mathematics;
Handicapping attacker's confidence: an alternative to k -anonymization,Ke Wang (Simon Fraser University);Benjamin C. M. Fung (Simon Fraser University);Philip S. Yu (IBM);,"2626264286,2015549731,2125104194","We present an approach of limiting the confidence of inferring sensitive properties to protect against the threats caused by data mining abilities. The problem has dual goals: preserve the information for a wanted data analysis request and limit the usefulness of unwanted sensitive inferences that may be derived from the release of data. Sensitive inferences are specified by a set of “privacy templates"". Each template specifies the sensitive property to be protected, the attributes identifying a group of individuals, and a maximum threshold for the confidence of inferring the sensitive property given the identifying attributes. We show that suppressing the domain values monotonically decreases the maximum confidence of such sensitive inferences. Hence, we propose a data transformation that minimally suppresses the domain values in the data to satisfy the set of privacy templates. The transformed data is free of sensitive inferences even in the presence of data mining algorithms. The prior k-anonymization k has been italicized consistently throughout this article. focuses on personal identities. This work focuses on the association between personal identities and sensitive properties.",2007,Knowledge and Information Systems volume 11 issue 3 pp 345-368,anonymity;data transformation;personal identity;confidentiality;confidence;shared memory;biological classification;satisfiability;data analysis;information extraction;world wide web;computer security;data mining;database;artificial intelligence;statistics;computer science;
CanTree: a canonical-order tree for incremental frequent-pattern mining,Carson Kai-Sang Leung (University of Manitoba);Quamrul I. Khan (University of Manitoba);Zhan Li (University of Manitoba);Tariqul Hoque (University of Manitoba);,"2128845036,2151360548,2288016208,2129784588","Since its introduction, frequent-pattern mining has been the subject of numerous studies, including incremental updating. Many existing incremental mining algorithms are Apriori-based, which are not easily adoptable to FP-tree-based frequent-pattern mining. In this paper, we propose a novel tree structure, called CanTree (canonical-order tree), that captures the content of the transaction database and orders tree nodes according to some canonical order. By exploiting its nice properties, the CanTree can be easily maintained when database transactions are inserted, deleted, and/or modified. For example, the CanTree does not require adjustment, merging, and/or splitting of tree nodes during maintenance. No rescan of the entire updated database or reconstruction of a new tree is needed for incremental updating. Experimental results show the effectiveness of our CanTree in the incremental mining of frequent patterns. Moreover, the applicability of CanTrees is not confined to incremental mining; CanTrees can also be applicable to other frequent-pattern mining tasks including constrained mining and interactive mining.",2007,Knowledge and Information Systems volume 11 issue 3 pp 287-311,incremental decision tree;transaction processing;decision tree learning;tree structure;data stream mining;information extraction;data mining;database;algorithm;computer science;
Making clustering in delay-vector space meaningful,Jason Robert Chen (Australian National University);,2116401202,"Sequential time series clustering is a technique used to extract important features from time series data. The method can be shown to be the process of clustering in the delay-vector space formalism used in the Dynamical Systems literature. Recently, the startling claim was made that sequential time series clustering is meaningless. This has important consequences for a significant amount of work in the literature, since such a claim invalidates these work’s contribution. In this paper, we show that sequential time series clustering is not meaningless, and that the problem highlighted in these works stem from their use of the Euclidean distance metric as the distance measure in the delay-vector space. As a solution, we consider quite a general class of time series, and propose a regime based on two types of similarity that can exist between delay vectors, giving rise naturally to an alternative distance measure to Euclidean distance in the delay-vector space. We show that, using this alternative distance measure, sequential time series clustering can indeed be meaningful. We repeat a key experiment in the work on which the “meaningless” claim was based, and show that our method leads to a successful clustering outcome.",2007,Knowledge and Information Systems volume 11 issue 3 pp 369-385,correlation clustering;constrained clustering;cure data clustering algorithm;fuzzy clustering;clustering high dimensional data;cluster analysis;dynamical system;time series;consensus clustering;combinatorics;calculus;artificial intelligence;machine learning;statistics;mathematics;
Efficient query filtering for streaming time series with applications to semisupervised learning of time series classifiers,"Li Wei (University of California, Riverside);Eamonn J. Keogh (University of California, Riverside);Helga Van Herle (University of California, Los Angeles);Agenor Mafra-Neto (University of California, Riverside);Russell J. Abbott (The Aerospace Corporation);","2325104268,2170070822,2429037425,206648872,2146211256","In this paper, we define time series query filtering, the problem of monitoring the streaming time series for a set of predefined patterns. This problem is of great practical importance given the massive volume of streaming time series available through sensors, medical patient records, financial indices and space telemetry. Since the data may arrive at a high rate and the number of predefined patterns can be relatively large, it may be impossible for the comparison algorithm to keep up. We propose a novel technique that exploits the commonality among the predefined patterns to allow monitoring at higher bandwidths, while maintaining a guarantee of no false dismissals. Our approach is based on the widely used envelope-based lower-bounding technique. As we will demonstrate on extensive experiments in diverse domains, our approach achieves tremendous improvements in performance in the offline case, and significant improvements in the fastest possible arrival rate of the data stream that can be processed with guaranteed no false dismissals. As a further demonstration of the utility of our approach, we demonstrate that it can make semisupervised learning of time series classifiers tractable.",2007,Knowledge and Information Systems volume 11 issue 3 pp 313-344,medical record;streams;filter;bandwidth;time series;upper and lower bounds;data analysis;information extraction;speech recognition;data mining;database;artificial intelligence;statistics;computer science;
Automatic outlier detection for time series: an application to sensor data,Sabyasachi Basu;Martin Meckesheimer;,"2723808208,2685423083",In this article we consider the problem of detecting unusual values or outliers from time series data where the process by which the data are created is difficult to model. The main consideration is the fact that data closer in time are more correlated to each other than those farther apart. We propose two variations of a method that uses the median from a neighborhood of a data point and a threshold value to compare the difference between the median and the observed data value. Both variations of the method are fast and can be used for data streams that occur in quick succession such as sensor data on an airplane.,2007,Knowledge and Information Systems volume 11 issue 2 pp 137-154,outlier;time series;anomaly detection;econometrics;data mining;statistics;
Solving multi-instance problems with classifier ensemble based on constructive clustering,Zhi-Hua Zhou (Nanjing University);Min-Ling Zhang (Nanjing University);,"2286237009,2310402581","In multi-instance learning, the training set is composed of labeled bags each consists of many unlabeled instances, that is, an object is represented by a set of feature vectors instead of only one feature vector. Most current multi-instance learning algorithms work through adapting single-instance learning algorithms to the multi-instance representation, while this paper proposes a new solution which goes at an opposite way, that is, adapting the multi-instance representation to single-instance learning algorithms. In detail, the instances of all the bags are collected together and clustered into d groups first. Each bag is then re-represented by d binary features, where the value of the ith feature is set to one if the concerned bag has instances falling into the ith group and zero otherwise. Thus, each bag is represented by one feature vector so that single-instance classifiers can be used to distinguish different classes of bags. Through repeating the above process with different values of d, many classifiers can be generated and then they can be combined into an ensemble for prediction. Experiments show that the proposed method works well on standard as well as generalized multi-instance problems.",2007,Knowledge and Information Systems volume 11 issue 2 pp 155-170,cluster analysis;ensemble learning;data mining;pattern recognition;artificial intelligence;machine learning;computer science;mathematics;
The pairwise attribute noise detection algorithm,Jason D. Van Hulse (Florida Atlantic University);Taghi M. Khoshgoftaar (Florida Atlantic University);Haiying Huang (Florida Atlantic University);,"2166804830,2660029024,2488127740","Analyzing the quality of data prior to constructing data mining models is emerging as an important issue. Algorithms for identifying noise in a given data set can provide a good measure of data quality. Considerable attention has been devoted to detecting class noise or labeling errors. In contrast, limited research work has been devoted to detecting instances with attribute noise, in part due to the difficulty of the problem. We present a novel approach for detecting instances with attribute noise and demonstrate its usefulness with case studies using two different real-world software measurement data sets. Our approach, called Pairwise Attribute Noise Detection Algorithm (PANDA), is compared with a nearest neighbor, distance-based outlier detection technique (denoted DM) investigated in related literature. Since what constitutes noise is domain specific, our case studies uses a software engineering expert to inspect the instances identified by the two approaches to determine whether they actually contain noise. It is shown that PANDA provides better noise detection performance than the DM algorithm.",2007,Knowledge and Information Systems volume 11 issue 2 pp 171-190,sound quality;software measurement;outlier;data modeling;data quality;quality control;k nearest neighbors algorithm;data analysis;information extraction;data mining;database;machine learning;statistics;computer science;
Editorial: Special issue on mining low-quality data,Xingquan Zhu (Florida Atlantic University);Taghi M. Khoshgoftaar (Florida Atlantic University);Ian Davidson (State University of New York System);Shichao Zhang (Guangxi Normal University);,"2618356905,23511057,2560595684,2695727709",-,2007,Knowledge and Information Systems volume 11 issue 2 pp 131-136,-
Data quality awareness: a case study for cost optimal association rule mining,Laure Berti-Équille (University of Rennes);,192351227,"The quality of discovered association rules is commonly evaluated by interestingness measures (commonly support and confidence) with the purpose of supplying indicators to the user in the understanding and use of the new discovered knowledge. Low-quality datasets have a very bad impact over the quality of the discovered association rules, and one might legitimately wonder if a so-called “interesting” rule noted LHS→ RHS is meaningful when 30% of the LHS data are not up-to-date anymore, 20% of the RHS data are not accurate, and 15% of the LHS data come from a data source that is well-known for its bad credibility. This paper presents an overview of data quality characterization and management techniques that can be advantageously employed for improving the quality awareness of the knowledge discovery and data mining processes. We propose to integrate data quality indicators for quality aware association rule mining. We propose a cost-based probabilistic model for selecting legitimately interesting rules. Experiments on the challenging KDD-Cup-98 datasets show that variations on data quality have a great impact on the cost and quality of discovered association rules and confirm our approach for the integrated management of data quality indicators into the KDD process that ensure the quality of data mining results.",2007,Knowledge and Information Systems volume 11 issue 2 pp 191-215,association rule learning;data quality;quality management;information extraction;data science;data mining;database;computer science;
Inference of abduction theories for handling incompleteness in first-order learning,F. Esposito (University of Bari);S. Ferilli (University of Bari);T. M. A. Basile (University of Bari);N. Di Mauro (University of Bari);,"2122401555,1970347819,1989179004,2159375903","In real-life domains, learning systems often have to deal with various kinds of imperfections in data such as noise, incompleteness and inexactness. This problem seriously affects the knowledge discovery process, specifically in the case of traditional Machine Learning approaches that exploit simple or constrained knowledge representations and are based on single inference mechanisms. Indeed, this limits their capability of discovering fundamental knowledge in those situations. In order to broaden the investigation and the applicability of machine learning schemes in such particular situations, it is necessary to move on to more expressive representations which require more complex inference mechanisms. However, the applicability of such new and complex inference mechanisms, such as abductive reasoning, strongly relies on a deep background knowledge about the specific application domain. This work aims at automatically discovering the meta-knowledge needed to abduction inference strategy to complete the incoming information in order to handle cases of missing knowledge.",2007,Knowledge and Information Systems volume 11 issue 2 pp 217-242,abductive reasoning;knowledge engineering;knowledge base;knowledge based systems;data mining;artificial intelligence;machine learning;computer science;
Rule-Based Composite Event Queries: The Language XChangeEQ and its Semantics (Extended Version with Appendix),Francois Bry (Ludwig Maximilian University of Munich);Michael P. Eckert (Ludwig Maximilian University of Munich);,"870525885,2121111018",-,2007,Knowledge and Information Systems,object language;rdf query language;data control language;web search query;complex event processing;semantic web rule language;operational semantics;formal semantics;web service;query language;computational semantics;rule based system;natural language processing;database;programming language;computer science;
Finding the most unusual time series subsequence: algorithms and applications,"Eamonn J. Keogh (University of California, Riverside);Jessica Lin 0001 (George Mason University);Sang-Hee Lee (University of California, Riverside);Helga Van Herle (University of California, Davis);","2170070822,2100950507,2168330173,1534603910","In this work we introduce the new problem of finding time seriesdiscords. Time series discords are subsequences of longer time series that are maximally different to all the rest of the time series subsequences. They thus capture the sense of the most unusual subsequence within a time series. While discords have many uses for data mining, they are particularly attractive as anomaly detectors because they only require one intuitive parameter (the length of the subsequence) unlike most anomaly detection algorithms that typically require many parameters. While the brute force algorithm to discover time series discords is quadratic in the length of the time series, we show a simple algorithm that is three to four orders of magnitude faster than brute force, while guaranteed to produce identical results. We evaluate our work with a comprehensive set of experiments on diverse data sources including electrocardiograms, space telemetry, respiration physiology, anthropological and video datasets.",2006,Knowledge and Information Systems volume 11 issue 1 pp 1-27,cluster analysis;anomaly detection;information extraction;data mining;artificial intelligence;machine learning;statistics;algorithm;computer science;mathematics;
"Capabilities of outlier detection schemes in large datasets, framework and methodologies",Jian Tang (Memorial University of Newfoundland);Zhixiang Chen (University of Texas–Pan American);Ada Waichee Fu (The Chinese University of Hong Kong);David W. Cheung (University of Hong Kong);,"2619761154,2223943589,2165551961,1979772396","Outlier detection is concerned with discovering exceptional behaviors of objects. Its theoretical principle and practical implementation lay a foundation for some important applications such as credit card fraud detection, discovering criminal behaviors in e-commerce, discovering computer intrusion, etc. In this paper, we first present a unified model for several existing outlier detection schemes, and propose a compatibility theory, which establishes a framework for describing the capabilities for various outlier formulation schemes in terms of matching users'intuitions. Under this framework, we show that the density-based scheme is more powerful than the distance-based scheme when a dataset contains patterns with diverse characteristics. The density-based scheme, however, is less effective when the patterns are of comparable densities with the outliers. We then introduce a connectivity-based scheme that improves the effectiveness of the density-based scheme when a pattern itself is of similar density as an outlier. We compare density-based and connectivity-based schemes in terms of their strengths and weaknesses, and demonstrate applications with different features where each of them is more effective than the other. Finally, connectivity-based and density-based schemes are comparatively evaluated on both real-life and synthetic datasets in terms of recall, precision, rank power and implementation-free metrics.",2006,Knowledge and Information Systems volume 11 issue 1 pp 45-84,justice;outlier;compatibility;unified model;metric;systems modeling;e commerce;anomaly detection;econometrics;data mining;database;statistics;computer science;
Detecting anomalies in cross-classified streams: a Bayesian approach,Deepak Agarwal (Yahoo!);,2591515730,"We consider the problem of detecting anomalies in data that arise as multidimensional arrays with each dimension corresponding to the levels of a categorical variable. In typical data mining applications, the number of cells in such arrays are usually large. Our primary focus is detecting anomalies by comparing information at the current time to historical data. Naive approaches advocated in the process control literature do not work well in this scenario due to the multiple testing problem—performing multiple statistical tests on the same data produce excessive number of false positives. We use an empirical Bayes method which works by fitting a two-component Gaussian mixture to deviations at current time. The approach is scalable to problems that involve monitoring massive number of cells and fast enough to be potentially useful in many streaming scenarios. We show the superiority of the method relative to a naive “per component error rate” procedure through simulation. A novel feature of our technique is the ability to suppress deviations that are merely the consequence of sharp changes in the marginal distributions. This research was motivated by the need to extract critical application information and business intelligence from the daily logs that accompany large-scale spoken dialog systems. We illustrate our method on one such system.",2006,Knowledge and Information Systems volume 11 issue 1 pp 29-44,marginal distribution;active learning;anomaly;type i and type ii errors;natural language;multiple comparisons problem;gaussian process;word error rate;empirical research;bayesian probability;statistical hypothesis testing;data analysis;competitive intelligence;information extraction;process control;business intelligence;econometrics;artificial intelligence;statistics;computer science;mathematics;
Node similarity in the citation graph,Wangzhong Lu (Dalhousie University);Jeannette C. M. Janssen (Dalhousie University);Evangelos E. Milios (Dalhousie University);Nathalie Japkowicz (University of Ottawa);Yongzheng Zhang (Dalhousie University);,"2121250272,2110639033,2231563531,195975349,2153269437","Published scientific articles are linked together into a graph, the citation graph, through their citations. This paper explores the notion of similarity based on connectivity alone, and proposes several algorithms to quantify it. Our metrics take advantage of the local neighborhoods of the nodes in the citation graph. Two variants of link-based similarity estimation between two nodes are described, one based on the separate local neighborhoods of the nodes, and another based on the joint local neighborhood expanded from both nodes at the same time. The algorithms are implemented and evaluated on a subgraph of the citation graph of computer science in a retrieval context. The results are compared with text-based similarity, and demonstrate the complementarity of link-based and text-based retrieval.",2006,Knowledge and Information Systems volume 11 issue 1 pp 105-129,complement graph;graph database;random geometric graph;clustering coefficient;graph;case based reasoning;similitude;similarity;connectivity;metric space;metric;digital library;theoretical computer science;world wide web;information retrieval;data mining;database;artificial intelligence;computer science;mathematics;
Handling non-canonical software requirements based on Annotated Predicate Calculus,Kedian Mu (Peking University);Zhi Jin (Chinese Academy of Sciences);Ruqian Lu (Chinese Academy of Sciences);Yan Peng (Peking University);,"2126191483,2155874124,2310521963,2662467895","Eliciting requirements for a proposed system inevitably involves the problem of handling undesirable information about customer's needs, including inconsistency, vagueness, redundancy, or incompleteness. We term the requirements statements involved in the undesirable information non-canonical software requirements. In this paper, we propose an approach to handling non-canonical software requirements based on Annotated Predicate Calculus (APC). Informally, by defining a special belief lattice appropriate for representing the stakeholder's belief in requirements statements, we construct a new form of APC to formalize requirements specifications. We then show how the APC can be employed to characterize non-canonical requirements. Finally, we show how the approach can be used to handle non-canonical requirements through a case study.",2006,Knowledge and Information Systems volume 11 issue 1 pp 85-104,non functional testing;requirement prioritization;system requirements specification;paraconsistent logic;requirements management;requirements traceability;software requirements specification;software requirements;requirement;belief;software development;redundancy;lattice;first order logic;data mining;artificial intelligence;algorithm;mathematics;
Using discriminant analysis for multi-class classification: an experimental investigation,Tao Li (Florida International University);Shenghuo Zhu (NEC);Mitsunori Ogihara (University of Rochester);,"2472069284,2096537073,2123444417","Many supervised machine learning tasks can be cast as multi-class classification problems. Support vector machines (SVMs) excel at binary classification problems, but the elegant theory behind large-margin hyperplane cannot be easily extended to their multi-class counterparts. On the other hand, it was shown that the decision hyperplanes for binary classification obtained by SVMs are equivalent to the solutions obtained by Fisher's linear discriminant on the set of support vectors. Discriminant analysis approaches are well known to learn discriminative feature transformations in the statistical pattern recognition literature and can be easily extend to multi-class cases. The use of discriminant analysis, however, has not been fully experimented in the data mining literature. In this paper, we explore the use of discriminant analysis for multi-class classification problems. We evaluate the performance of discriminant analysis on a large collection of benchmark datasets and investigate its usage in text categorization. Our experiments suggest that discriminant analysis provides a fast, efficient yet accurate alternative for general multi-class classification problems.",2006,Knowledge and Information Systems volume 10 issue 4 pp 453-472,optimal discriminant analysis;multiple discriminant analysis;kernel fisher discriminant analysis;support vector machine;supervised learning;information extraction;linear discriminant analysis;data mining;pattern recognition;machine learning;computer science;
Efficient string matching with wildcards and length constraints,Gong Chen (University of Vermont);Xindong Wu (University of Vermont);Xingquan Zhu (University of Vermont);Abdullah N. Arslan (University of Vermont);Yu He (University of Vermont);,"2701566564,2123651450,2618356905,2091854989,2290524399","This paper defines a challenging problem of pattern matching between a pattern P and a text T, with wildcards and length constraints, and designs an efficient algorithm to return each pattern occurrence in an online manner. In this pattern matching problem, the user can specify the constraints on the number of wildcards between each two consecutive letters of P and the constraints on the length of each matching substring in T. We design a complete algorithm, SAIL that returns each matching substring of P in T as soon as it appears in T in an O(n+klmg) time with an O(lm) space overhead, where n is the length of T, k is the frequency of P's last letter occurring in T, l is the user-specified maximum length for each matching substring, m is the length of P, and g is the maximum difference between the user-specified maximum and minimum numbers of wildcards allowed between two consecutive letters in P.",2006,Knowledge and Information Systems volume 10 issue 4 pp 399-419,substring index;wildcard character;letter;string searching algorithm;pattern matching;combinatorics;pattern recognition;programming language;algorithm;computer science;mathematics;
XCQ: A queriable XML compression system,"Wilfred Ng (Hong Kong University of Science and Technology);Wai Yeung Lam (Hong Kong University of Science and Technology);Peter T. Wood (Birkbeck, University of London);Mark Levene (Birkbeck, University of London);","2170178419,2277363527,2301950271,2164185460","XML has already become the de facto standard for specifying and exchanging data on the Web. However, XML is by nature verbose and thus XML documents are usually large in size, a factor that hinders its practical usage, since it substantially increases the costs of storing, processing, and exchanging data. In order to tackle this problem, many XML-specific compression systems, such as XMill, XGrind, XMLPPM, and Millau, have recently been proposed. However, these systems usually suffer from the following two inadequacies: They either sacrifice performance in terms of compression ratio and execution time in order to support a limited range of queries, or perform full decompression prior to processing queries over compressed documents.In this paper, we address the above problems by exploiting the information provided by a Document Type Definition (DTD) associated with an XML document. We show that a DTD is able to facilitate better compression as well as generate more usable compressed data to support querying. We present the architecture of the XCQ, which is a compression and querying tool for handling XML data. XCQ is based on a novel technique we have developed called DTD Tree and SAX Event Stream Parsing (DSP). The documents compressed by XCQ are stored in Partitioned Path-Based Grouping (PPG) data streams, which are equipped with a Block Statistics Signature (BSS) indexing scheme. The indexed PPG data streams support the processing of XML queries that involve selection and aggregation, without the need for full decompression. In order to study the compression performance of XCQ, we carry out comprehensive experiments over a set of XML benchmark datasets.",2006,Knowledge and Information Systems volume 10 issue 4 pp 421-452,xml validation;efficient xml interchange;xml framework;xml encryption;streaming xml;xml signature;simple api for xml;xml database;xml schema;xml;data compression;computational geometry;data processing;world wide web;data mining;database;statistics;computer science;
Finding centroid clusterings with entropy-based criteria,Tianming Hu (East China Normal University);Sam Yuan Sung (South Texas College);,"2270319918,2251526302","We investigate the following problem: Given a set of candidate clusterings for a common set of objects, find a centroid clustering that is most compatible to the input set. First, we propose a series of entropy-based distance functions for comparing various clusterings. Such functions enable us to directly select the local centroid from the candidate set. Second, we present two combining methods for the global centroid. The selected/combined centroid clustering is likely to be a good choice, i.e., top or middle ranked in terms of closeness to the true clustering. Finally, we evaluate their effectiveness on both artificial and real data sets.",2006,Knowledge and Information Systems volume 10 issue 4 pp 505-514,metric;consensus clustering;data mining;pattern recognition;machine learning;statistics;computer science;mathematics;
Applying language modeling to session identification from database trace logs,Xiangji Huang (York University);Qingsong Yao (York University);Aijun An (York University);,"2100624012,2303969520,2156668854","A database session is a sequence of requests presented to the database system by a user or an application to achieve a certain task. Session identification is an important step in discovering useful patterns from database trace logs. The discovered patterns can be used to improve the performance of database systems by prefetching predicted queries, rewriting the current query or conducting effective cache replacement.In this paper, we present an application of a new session identification method based on statistical language modeling to database trace logs. Several problems of the language modeling based method are revealed in the application, which include how to select values for the parameters of the language model, how to evaluate the accuracy of the session identification result and how to learn a language model without well-labeled training data. All of these issues are important in the successful application of the language modeling based method for session identification. We propose solutions to these open issues. In particular, new methods for determining an entropy threshold and the order of the language model are proposed. New performance measures are presented to better evaluate the accuracy of the identified sessions. Furthermore, three types of learning methods, namely, learning from labeled data, learning from semi-labeled data and learning from unlabeled data, are introduced to learn language models from different types of training data. Finally, we report experimental results that show the effectiveness of the language model based method for identifying sessions from the trace logs of an OLTP database application and the TPC-C Benchmark.",2006,Knowledge and Information Systems volume 10 issue 4 pp 473-504,data control language;data definition language;rewriting;database schema;data manipulation language;database design;cpu cache;query language;regular expression;data type;systems modeling;language model;supervised learning;theoretical computer science;data mining;database;artificial intelligence;machine learning;programming language;computer science;
Mining user access patterns with traversal constraint for predicting web page requests,Mei-Ling Shyu (University of Miami);Choochart Haruechaiyasak (NECTEC);Shu-Ching Chen (Florida International University);,"2041921851,2307867087,2118447230","The recent increase in HyperText Transfer Protocol (HTTP) traffic on the World Wide Web (WWW) has generated an enormous amount of log records on Web server databases. Applying Web mining techniques on these server log records can discover potentially useful patterns and reveal user access behaviors on the Web site. In this paper, we propose a new approach for mining user access patterns for predicting Web page requests, which consists of two steps. First, the Minimum Reaching Distance (MRD) algorithm is applied to find the distances between the Web pages. Second, the association rule mining technique is applied to form a set of predictive rules, and the MRD information is used to prune the results from the association rule mining process. Experimental results from a real Web data set show that our approach improved the performance over the existing Markov-model approach in precision, recall, and the reduction of user browsing time.",2006,Knowledge and Information Systems volume 10 issue 4 pp 515-528,web log analysis software;web analytics;static web page;data web;clickstream;ajax;web mapping;web server;web service;web page;association rule learning;web mining;world wide web;data mining;database;computer science;
Catch the moment: maintaining closed frequent itemsets over a data stream sliding window,"Yun Chi (University of California, Los Angeles);Haixun Wang (IBM);Philip S. Yu (IBM);Richard R. Muntz (University of California, Los Angeles);","2696493159,2116756368,2125104194,2033368653","This paper considers the problem of mining closed frequent itemsets over a data stream sliding window using limited memory space. We design a synopsis data structure to monitor transactions in the sliding window so that we can output the current closed frequent itemsets at any time. Due to time and memory constraints, the synopsis data structure cannot monitor all possible itemsets. However, monitoring only frequent itemsets will make it impossible to detect new itemsets when they become frequent. In this paper, we introduce a compact data structure, the closed enumeration tree (CET), to maintain a dynamically selected set of itemsets over a sliding window. The selected itemsets contain a boundary between closed frequent itemsets and the rest of the itemsets. Concept drifts in a data stream are reflected by boundary movements in the CET. In other words, a status change of any itemset (e.g., from non-frequent to frequent) must occur through the boundary. Because the boundary is relatively stable, the cost of mining closed frequent itemsets over a sliding window is dramatically reduced to that of mining transactions that can possibly cause boundary movements in the CET. Our experiments show that our algorithm performs much better than representative algorithms for the sate-of-the-art approaches.",2006,Knowledge and Information Systems volume 10 issue 3 pp 265-294,concept drift;enumeration;sliding window protocol;transaction processing;data structure;data mining;database;real time computing;computer science;
A collaborative filtering framework based on fuzzy association rules and multiple-level similarity,Cane Wing-ki Leung (Hong Kong Polytechnic University);Stephen Chi-fai Chan (Hong Kong Polytechnic University);Fu-lai Chung (Hong Kong Polytechnic University);,"2712545342,2167014661,2469582652","The rapid development of Internet technologies in recent decades has imposed a heavy information burden on users. This has led to the popularity of recommender systems, which provide advice to users about items they may like to examine. Collaborative Filtering (CF) is the most promising technique in recommender systems, providing personalized recommendations to users based on their previously expressed preferences and those of other similar users. This paper introduces a CF framework based on Fuzzy Association Rules and Multiple-level Similarity (FARAMS). FARAMS extended existing techniques by using fuzzy association rule mining, and takes advantage of product similarities in taxonomies to address data sparseness and nontransitive associations. Experimental results show that FARAMS improves prediction quality, as compared to similar approaches.",2006,Knowledge and Information Systems volume 10 issue 3 pp 357-381,collaborative filtering;similitude;association rule learning;similarity;fuzzy logic;association;input output;the internet;recommender system;world wide web;data mining;database;artificial intelligence;machine learning;computer science;
"Detecting outlying subspaces for high-dimensional data: the new task, algorithms, and performance",Ji Zhang (Dalhousie University);Hai H. Wang (Saint Mary's University);,"2109058337,2723335329","In this paper, we identify a new task for studying the outlying degree (OD) of high-dimensional data, i.e. finding the subspaces (subsets of features) in which the given points are outliers, which are called their outlying subspaces. Since the state-of-the-art outlier detection techniques fail to handle this new problem, we propose a novel detection algorithm, called High-Dimension Outlying subspace Detection (HighDOD), to detect the outlying subspaces of high-dimensional data efficiently. The intuitive idea of HighDOD is that we measure the OD of the point using the sum of distances between this point and itsknearest neighbors. Two heuristic pruning strategies are proposed to realize fast pruning in the subspace search and an efficient dynamic subspace search method with a sample-based learning process has been implemented. Experimental results show that HighDOD is efficient and outperforms other searching alternatives such as the naive top–down, bottom–up and random search methods, and the existing outlier detection methods cannot fulfill this new task effectively.",2006,Knowledge and Information Systems volume 10 issue 3 pp 333-355,top down and bottom up design;anomaly detection;data mining;machine learning;statistics;algorithm;computer science;mathematics;
Singular value decomposition based data distortion strategy for privacy protection,Shuting Xu (Virginia State University);Jun Zhang (University of Kentucky);Dianwei Han (University of Kentucky);Jie Wang (University of Kentucky);,"2098312141,2429199220,2161611520,2659454712","Privacy-preserving is a major concern in the application of data mining techniques to datasets containing personal, sensitive, or confidential information. Data distortion is a critical component to preserve privacy in security-related data mining applications, such as in data mining-based terrorist analysis systems. We propose a sparsified Singular Value Decomposition (SVD) method for data distortion. We also put forth a few metrics to measure the difference between the distorted dataset and the original dataset and the degree of the privacy protection. Our experimental results using synthetic and real world datasets show that the sparsified SVD method works well in preserving privacy as well as maintaining utility of the datasets.",2006,Knowledge and Information Systems volume 10 issue 3 pp 383-397,forth;terrorism;singular value decomposition;metric;data analysis;information extraction;computer security;data mining;database;computer science;
Tree-based partitioning of date for association rule mining,Shakil Ahmed (University of Liverpool);Frans Coenen (University of Liverpool);Paul H. Leng (University of Liverpool);,"2425628954,1934644436,2139721226","The most computationally demanding aspect of Association Rule Mining is the identification and counting of support of the frequent sets of items that occur together sufficiently often to be the basis of potentially interesting rules. The task increases in difficulty with the scale of the data and also with its density. The greatest challenge is posed by data that is too large to be contained in primary memory, especially when high data density and/or low support thresholds give rise to very large numbers of candidates that must be counted. In this paper, we consider strategies for partitioning the data to deal effectively with such cases. We describe a partitioning approach which organises the data into tree structures that can be processed independently. We present experimental results that show the method scales well for increasing dimensions of data and performs significantly better than alternatives, especially when dealing with dense data and low support thresholds.",2006,Knowledge and Information Systems volume 10 issue 3 pp 315-331,association rule learning;data structure;data mining;machine learning;programming language;algorithm;computer science;mathematics;
Exploring overlapping clusters using dynamic re-scaling and sampling,Mei Kobayashi (IBM);Masaki Aono (Toyohashi University of Technology);,"2255525341,2330481481","Until recently, the aim of most text-mining work has been to understand major topics and clusters. Minor topics and clusters have been relatively neglected even though they may represent important information on rare events. We present a novel method for exploring overlapping clusters of heterogeneous sizes, which is based on vector space modeling, covariance matrix analysis, random sampling, and dynamic re-weighting of document vectors in massive databases. Our system addresses a combination of difficult issues in database analysis, such as synonymy and polysemy, identification of minor clusters, accommodation of cluster overlap, automatic labeling of clusters based on their document contents, and the user-controlled trade-off between speed of computation and quality of results. We conducted implementation studies with new articles from the Reuters and LA Times TREC data sets and artificially generated data with a known cluster structure to demonstrate the effectiveness of our system.",2006,Knowledge and Information Systems volume 10 issue 3 pp 295-313,cluster analysis;text mining;data mining;pattern recognition;machine learning;statistics;computer science;mathematics;
Real-time classification of variable length multi-attribute motions,Chuanjun Li (University of Texas at Dallas);Latifur Khan (University of Texas at Dallas);Balakrishnan Prabhakaran (University of Texas at Dallas);,"2151845877,2155983610,2092083035","Multi-attribute motion data can be generated in many applications/ devices, such as motion capture devices and animations. It can have dozens of attributes, thousands of rows, and even similar motions can have different durations and different speeds at corresponding parts. There are no row-to-row correspondences between data matrices of two motions. To be classified and recognized, multi-attribute motion data of different lengths are reduced to feature vectors by using the properties of singular value decomposition (SVD) of motion data. The reduced feature vectors of similar motions are close to each other, while reduced feature vectors are different from each other if their motions are different. By applying support vector machines (SVM) to the feature vectors, we efficiently classify and recognize real-world multi-attribute motion data. With our data set of more than 300 motions with different lengths and variations, SVM outperforms classification by related similarity measures, in terms of accuracy and CPU time. The performance of our approach shows its feasibility of real-time applications to real-world data.",2006,Knowledge and Information Systems volume 10 issue 2 pp 163-183,motion estimation;support vector machine;speech recognition;computer vision;pattern recognition;machine learning;computer science;mathematics;
Web image indexing by using associated texts,Zhiguo Gong (University of Macau);U Leong Hou (University of Macau);Chan Wa Cheang (University of Macau);,"2159588081,2167796918,2150390275","In order to index Web images, the whole associated texts are partitioned into a sequence of text blocks, then the local relevance of a term to the corresponding image is calculated with respect to both its local occurrence in the block and the distance of the block to the image. Thus, the overall relevance of a term is determined as the sum of all its local weight values multiplied by the corresponding distance factors of the text blocks. In the present approach, the associated text of a Web image is firstly partitioned into three parts, including a page-oriented text (TM), a link-oriented text (LT), and a caption-oriented text (BT). Since the big size and semantic divergence, the caption-oriented text is further partitioned into finer blocks based on the tree structure of the tag elements within the BT text. During the processing, all heading nodes are pulled up in order to correlate with their semantic scopes, and a collapse algorithm is also exploited to remove the empty blocks. In our system, the relevant factors of the text blocks are determined by using a greedy Two-Way-Merging algorithm.",2006,Knowledge and Information Systems volume 10 issue 2 pp 243-264,greedy algorithm;search engine indexing;segmentation;image segmentation;tree structure;block code;the internet;theoretical computer science;world wide web;data mining;database;artificial intelligence;computer science;
Discovering mappings in hierarchical data from multiple sources using the inherent structure,K. Selçuk Candan (Arizona State University);Jong Wook Kim (Arizona State University);Huan Liu (Arizona State University);Reshma Suvarna (Arizona State University);,"674992784,2143703869,2122391114,2157047617","Unprecedented amounts of media data are publicly accessible. However, it is increasingly difficult to integrate relevant media from multiple and diverse sources for effective applications. The functioning of a multimodal integration system requires metadata, such as ontologies, that describe media resources and media components. Such metadata are generally application-dependent and this can cause difficulties when media needs to be shared across application domains. There is a need for a mechanism that can relate the common and uncommon terms and media components. In this paper, we develop an algorithm to mine and automatically discover mappings in hierarchical media data, metadata, and ontologies, using the structural information inherent in these types of data. We evaluate the performance of this algorithm for various parameters using both synthetic and real-world data collections and show that the structure-based mining of relationships provides high degrees of precision.",2006,Knowledge and Information Systems volume 10 issue 2 pp 185-210,ontology;metadata;xml;hierarchical database model;data type;data collection;ontology;information extraction;semantics;information retrieval;data mining;database;computer science;
CoMMA: a framework for integrated multimedia mining using multi-relational associations,Ankur M. Teredesai (Rochester Institute of Technology);Muhammad A. Ahmad (Rochester Institute of Technology);Juveria Kanodia (Rochester Institute of Technology);Roger S. Gaborski (Rochester Institute of Technology);,"387590160,2295258951,1491521420,1676728679","Generating captions or annotations automatically for still images is a challenging task. Traditionally, techniques involving higher-level (semantic) object detection and complex feature extraction have been employed for scene understanding. On the basis of this understanding, corresponding text descriptions are generated for a given image. In this paper, we pose the auto-annotation problem as that of multi-relational association rule mining where the relations exist between image-based features, and textual annotations. The central idea is to combine low-level image features such as color, orientation, intensity, etc. and corresponding text annotations to generate association rules across multiple tables using multi-relational association mining. Subsequently, we use these association rules to auto-annotate test images.In this paper we also present a multi-relational extension to the FP-tree algorithm to accomplish the association rule mining task effectively. The motivation for using multi-relational association rule mining for multimedia data mining is to exhibit the potential accorded by multiple descriptions for the same image (such as multiple people labeling the same image differently). Moreover, multi-relational association rule mining can also benefit the auto-annotation process by pruning the number of trivial associations that are generated if text and image features were combined in a single table through a join. In this paper, we discuss these issues and the results of our auto-annotation experiments on different test sets. Another contribution of this paper is highlighting a need to develop robust evaluation metrics for the image annotation task. We propose several applicable scoring techniques and then evaluate the performance of the different algorithms to study the utility of these techniques. A detailed analysis of the datasets used and the performance results are presented to conclude the paper.",2006,Knowledge and Information Systems volume 10 issue 2 pp 135-162,association rule learning;information extraction;image analysis;information retrieval;data mining;database;pattern recognition;artificial intelligence;machine learning;computer science;
Extending metric index structures for efficient range query processing,Karin Kailing (Ludwig Maximilian University of Munich);Hans-Peter Kriegel (Ludwig Maximilian University of Munich);Martin Pfeifle (Ludwig Maximilian University of Munich);Stefan Schönauer (Ludwig Maximilian University of Munich);,"276841381,1919135125,2108078646,2042460568","Databases are getting more and more important for storing complex objects from scientific, engineering, or multimedia applications. Examples for such data are chemical compounds, CAD drawings, or XML data. The efficient search for similar objects in such databases is a key feature. However, the general problem of many similarity measures for complex objects is their computational complexity, which makes them unusable for large databases. In this paper, we combine and extend the two techniques of metric index structures and multi-step query processing to improve the performance of range query processing. The efficiency of our methods is demonstrated in extensive experiments on real-world data including graphs, trees, and vector sets.",2006,Knowledge and Information Systems volume 10 issue 2 pp 211-227,sargable;range query;xml;cluster analysis;computational complexity theory;information retrieval;data mining;database;machine learning;computer science;
Multiple-camera people localization in an indoor environment,Valery A. Petrushin (Accenture);Gang Wei (Accenture);Anatole V. Gershman (Accenture);,"212253149,2310322507,1826085721","With the rapid proliferation of video cameras in public places, the ability to identify and track people and other objects creates tremendous opportunities for business and security applications. This paper presents the Multiple Camera Indoor Surveillance project which is devoted to using multiple cameras, agent-based technology and knowledge-based techniques to identify and track people and summarize their activities. We also describe a people localization system, which identifies and localizes people in an indoor environment. The system uses low-level color features – a color histogram and average vertical color – for building people models and the Bayesian decision-making approach for people localization. The results of a pilot experiment that used 32 h of data (4 days × 8 h) showed the average recall and precision values of 68 and 59% respectively. Augmenting the system with domain knowledge, such as location of working places in cubicles, doors and passages, increased the average recall to 87% and precision to 73%.",2006,Knowledge and Information Systems volume 10 issue 2 pp 229-241,color histogram;local system;domain knowledge;bayesian inference;knowledge base;computer vision;artificial intelligence;simulation;computer science;
Human-centered ontology engineering: The HCOME methodology,Konstantinos Kotis (University of the Aegean);A. Vouros (University of the Aegean);,"2075035682,2580589730","The fast emergent and continuously evolving areas of the Semantic Web and Knowledge Management make the incorporation of ontology engineering tasks in knowledge-empowered organizations and in the World Wide Web more than necessary. In such environments, the development and evolution of ontologies must be seen as a dynamic process that has to be supported through the entire ontology life cycle, resulting to living ontologies. The aim of this paper is to present the Human-Centered Ontology Engineering Methodology (HCOME) for the development and evaluation of living ontologies in the context of communities of knowledge workers. The methodology aims to empower knowledge workers to continuously manage their formal conceptualizations in their day-to-day activities and shape their information space by being actively involved in the ontology life cycle. The paper also demonstrates the Human Centered ONtology Engineering Environment, HCONE, which can effectively support this methodology.",2006,Knowledge and Information Systems volume 10 issue 1 pp 109-131,idef5;ontology based data integration;process ontology;upper ontology;owl s;ontology alignment;suggested upper merged ontology;ontology;conceptualization;semantic web;knowledge engineering;ontology;knowledge management;data mining;database;artificial intelligence;computer science;
Fast and exact out-of-core and distributed k -means clustering,Ruoming Jin (Kent State University);Anjan Goswami (Ohio State University);Gagan Agrawal (Ohio State University);,"2119237514,2083651318,2154982890","Clustering has been one of the most widely studied topics in data mining and k-means clustering has been one of the popular clustering algorithms. K-means requires several passes on the entire dataset, which can make it very expensive for large disk-resident datasets. In view of this, a lot of work has been done on various approximate versions of k-means, which require only one or a small number of passes on the entire dataset.In this paper, we present a new algorithm, called fast and exact k-means clustering (FEKM), which typically requires only one or a small number of passes on the entire dataset and provably produces the same cluster centres as reported by the original k-means algorithm. The algorithm uses sampling to create initial cluster centres and then takes one or more passes over the entire dataset to adjust these cluster centres. We provide theoretical analysis to show that the cluster centres thus reported are the same as the ones computed by the original k-means algorithm. Experimental results from a number of real and synthetic datasets show speedup between a factor of 2 and 4.5, as compared with k-means.This paper also describes and evaluates a distributed version of FEKM, which we refer to as DFEKM. This algorithm is suitable for analysing data that is distributed across loosely coupled machines. Unlike the previous work in this area, DFEKM provably produces the same results as the original k-means algorithm. Our experimental results show that DFEKM is clearly better than two other possible options for exact clustering on distributed data, which are down loading all data and running sequential k-means or running parallel k-means on a loosely coupled configuration. Moreover, even in a tightly coupled environment, DFEKM can outperform parallel k-means if there is a significant load imbalance.",2006,Knowledge and Information Systems volume 10 issue 1 pp 17-40,k medians clustering;canopy clustering algorithm;correlation clustering;constrained clustering;data stream clustering;cure data clustering algorithm;fuzzy clustering;k means clustering;clustering high dimensional data;cluster analysis;information extraction;data mining;machine learning;statistics;algorithm;computer science;
Visual information extraction,Yonatan Aumann (Bar-Ilan University);Ronen Feldman (Bar-Ilan University);Yair Liberzon;Benjamin Rosenfeld (Bar-Ilan University);Jonathan Schler (Bar-Ilan University);,"361301894,2159469398,2306485189,2096069757,103510680","Typographic and visual information is an integral part of textual documents. Most information extraction (IE) systems ignore most of this visual information, processing the text as a linear sequence of words. Thus, much valuable information is lost. In this paper, we show how to make use of this visual information for IE. We present an algorithm that allows to automatically extract specific fields of the document (such as the title, author, etc.) based exclusively on the visual formatting of the document, without any reference to the semantic content. The algorithm employs a machine learning approach, whereby the system is first provided with a set of training documents in which the target fields are manually tagged and automatically learns how to extract these fields in future documents. We implemented the algorithm in a system for automatic analysis of documents in PDF format. We present experimental results of applying the system on a set of financial documents, extracting nine different target fields. Overall, the system achieved a 90% accuracy.",2006,Knowledge and Information Systems volume 10 issue 1 pp 1-15,information system;information extraction;information processing;image processing;information retrieval;data mining;artificial intelligence;machine learning;computer science;
"Exploiting edge semantics in citation graphs using efficient, vertical ARM",Imad Rahal (St. John's University);Dongmei Ren (North Dakota State University);Weihua Wu (North Dakota State University);Anne Denton (North Dakota State University);Christopher Besemann (North Dakota State University);William Perrizo (North Dakota State University);,"2608590266,2501379271,2307456403,2126706684,1182558981,9761453","Graphs are increasingly becoming a vital source of information within which a great deal of semantics is embedded. As the size of available graphs increases, our ability to arrive at the embedded semantics grows into a much more complicated task. One form of important hidden semantics is that which is embedded in the edges of directed graphs. Citation graphs serve as a good example in this context. This paper attempts to understand temporal aspects in publication trends through citation graphs, by identifying patterns in the subject matters of scientific publications using an efficient, vertical association rule mining model. Such patterns can (a) indicate subject-matter evolutionary history, (b) highlight subject-matter future extensions, and (c) give insights on the potential effects of current research on future research. We highlight our major differences with previous work in the areas of graph mining, citation mining, and Web-structure mining, propose an efficient vertical data representation model, introduce a new subjective interestingness measure for evaluating patterns with a special focus on those patterns that signify strong associations between properties of cited papers and citing papers, and present an efficient algorithm for the purpose of discovering rules of interest followed by a detailed experimental analysis.",2006,Knowledge and Information Systems volume 10 issue 1 pp 57-91,association rule learning;citation analysis;information extraction;evolutionary algorithm;semantics;data science;data mining;database;artificial intelligence;machine learning;computer science;
An ontology-based procedure for generating object model from text description,Waralak Vongdoiwang (Asian Institute of Technology);N. Batanov (Asian Institute of Technology);,"2288549631,2292551473","The main objective of the procedure proposed in this paper is to use ontologies to convert a problem domain text description into an object model. The object model of a system consists of objects, identified from the text description and structural linkages corresponding to existing or established relationships. The ontologies provide metadata schemas, offering a controlled vocabulary of concepts. At the center of both object models and ontologies are objects within a given problem domain. The difference is that while the object model should contain explicitly shown structural dependencies between objects in a system, including their properties, relationships, events, and processes, the ontologies are based on related terms only. On the other hand, the object model refers to the collections of concepts used to describe the generic characteristics of objects in object-oriented languages. Because ontology is accepted as a formal, explicit specification of a shared conceptualization, we can naturally link ontologies with object models, which represent a system-oriented map of related objects, described as Abstract Data Types (ADTs). This paper addresses ontologies as a basis of a complete methodology for object modeling, including available tools, particularly CORPORUM OntoExtract and VisualText, which can help the conversion process. This paper describes how the developers can implement this methodology on the basis of an illustrative example.",2006,Knowledge and Information Systems volume 10 issue 1 pp 93-108,idef5;object oriented design;object model;object oriented programming;method;ontology;natural language processing;data mining;database;programming language;computer science;
Improving the performance of 1D object classification by using the Electoral College,Liang Chen (University of Northern British Columbia);Ruoyu Chen (Jilin University);Sharmin Nilufar (University of Northern British Columbia);,"2419924165,2413489314,2305052034","It has been proven that districted matching schemes (e.g., the US presidential election scheme, also called the Electoral College) are more stable than undistricted matching schemes (e.g., the popular voting scheme for selecting a governor in California), and that the theory can be used in pattern classification applications, such as image classification, where by its nature an object to be classified consists of elements distributed in a bounded 2D space. However, the objects of some pattern classification applications consist of features/values of elements lying on a limited 1D line segment. This paper will prove that districted matching scheme can still outperform undistricted matching scheme in these applications, and the improved performance of districted vote scheme is even more substantial for these 1D objects than for 2D objects. The theoretical result suggests the use of districted matching schemes for pattern recognition of 1D objects. We verified the theoretical analysis through artificial neural network-based approaches for the prediction of start codons of nucleotide sequences.",2006,Knowledge and Information Systems volume 10 issue 1 pp 41-56,start codon;contextual image classification;stability;nucleic acid sequence;artificial neural network;data mining;database;artificial intelligence;machine learning;computer science;
SLOM: a new measure for local spatial outliers,Sanjay Chawla (University of Sydney);Pei Sun (University of Sydney);,"2201421368,2117885702","We propose a measure, spatial local outlier measure (SLOM), which captures the local behaviour of datum in their spatial neighbourhood. With the help of SLOM, we are able to discern local spatial outliers that are usually missed by global techniques, like ""three standard deviations away from the mean"". Furthermore, the measure takes into account the local stability around a data point and suppresses the reporting of outliers in highly unstable areas, where data are too heterogeneous and the notion of outliers is not meaningful. We prove several properties of SLOM and report experiments on synthetic and real data sets that show that our approach is novel and scalable to large datasets.",2006,Knowledge and Information Systems volume 9 issue 4 pp 412-429,r tree;outlier;complexity;standard deviation;oscillation;econometrics;data mining;statistics;computer science;mathematics;
Higher order feature selection for text classification,Jan Bakus (University of Waterloo);Mohamed S. Kamel (University of Waterloo);,"2008388982,2096364092","In this paper, we present the MIFS-C variant of the mutual information feature-selection algorithms. We present an algorithm to find the optimal value of the redundancy parameter, which is a key parameter in the MIFS-type algorithms. Furthermore, we present an algorithm that speeds up the execution time of all the MIFS variants. Overall, the presented MIFS-C has comparable classification accuracy (in some cases even better) compared with other MIFS algorithms, while its running time is faster. We compared this feature selector with other feature selectors, and found that it performs better in most cases. The MIFS-C performed especially well for the breakeven and F-measure because the algorithm can be tuned to optimise these evaluation measures.",2006,Knowledge and Information Systems volume 9 issue 4 pp 468-491,mutual information;redundancy;higher order logic;biological classification;content analysis;feature selection;data mining;artificial intelligence;machine learning;statistics;computer science;
Using cocitation information to estimate political orientation in web documents,Miles Efron;,2627525351,"This paper introduces a simple method for estimating cultural orientation, the affiliation of online entities in a polarized field of discourse. In particular, cocitation information is used to estimate the political orientation of hypertext documents. A type of cultural orientation, the political orientation of a document is the degree to which it participates in traditionally left- or fight-wing beliefs. Estimating documents' political orientation is of interest for personalized information retrieval and recommender systems. In its application to politics, the method uses a simple probabilistic model to estimate the strength of association between a document and left- and right-wing communities. The model estimates the likelihood of cocitation between a document of interest and a small number of documents of known orientation. The model is tested on three sets of data, 695 partisan web documents, 162 political weblogs, and 198 nonpartisan documents. Accuracy above 90% is obtained from the cocitation model, outperforming lexically based classifiers at statistically significant levels.",2006,Knowledge and Information Systems volume 9 issue 4 pp 492-511,biology and political orientation;belief;hypertext;information system;statistical model;systems modeling;biological classification;statistical significance;recommender system;public policy;world wide web;information retrieval;data mining;database;statistics;computer science;
Mining the meaningful term conjunctions from materialised faceted taxonomies: algorithms and complexity,Yannis Tzitzikas (University of Crete);Anastasia Analyti (Foundation for Research & Technology – Hellas);,"64415330,2282574431","A materialised faceted taxonomy is an information source where the objects of interest are indexed according to a faceted taxonomy. This paper shows how from a materialised faceted taxonomy, we can mine an expression of the Compound Term Composition Algebra that specifies exactly those compound terms (conjunctions of terms) that have non-empty interpretation. The mined expressions can be used for encoding in a very compact form (and subsequently reusing), the domain knowledge that is stored in existing materialised faceted taxonomies. A distinctive characteristic of this mining task is that the focus is given on minimising the storage space requirements of the mined set of compound terms. This paper formulates the problem of expression mining, gives several algorithms for expression mining, analyses their computational complexity, provides techniques for optimisation, and discusses several novel applications that now become possible.",2006,Knowledge and Information Systems volume 9 issue 4 pp 430-467,materialized view;reuse;domain knowledge;scalability;complexity;computational complexity theory;knowledge extraction;data mining;artificial intelligence;algorithm;computer science;
Manipulation of qualitative degrees to handle uncertainty: formal models and applications,Isis Truck;Herman Akdag (University of Paris);,"296162073,1927925818","In this article, qualitative, symbolic and linguistic models for knowledge representation are presented as well as their applications. Such models are useful in decision making problems when information from the experts' knowledge is expressed through different heterogeneous types such as numerical, interval-valued, symbolic, linguistic ... The whole work proposed here takes place in a given many-valued logic. First, as an alternative to classic probabilities, a method using qualitative degrees is described and an application in supervised learning is proposed. Then we study the transformation of these degrees when they are subjected to a modification: thus we present the Generalized Symbolic Modifiers. These tools are defined as manipulations computed on a pair (degree, scale). They are grouped together into several families and thus offer many possibilities to handle uncertainty. An application in colorimetrics is described and shows the feasibility of the approach. The last point addressed in this article is the data combination. An operator called the Symbolic Weighted Median gives a summary of several qualitative degrees with associated weights. One particularity is that this median is constructed on the Generalized Symbolic Modifiers. Finally we explain how the Symbolic Weighted Median is exploited in the internal mechanism of the application in colorimetrics.",2006,Knowledge and Information Systems volume 9 issue 4 pp 385-411,many valued logic;symbolic data analysis;combination;median;formal specification;qualitative property;qualitative research;uncertainty;systems modeling;supervised learning;artificial intelligence;machine learning;statistics;algorithm;computer science;mathematics;
Constraining and summarizing association rules in medical data,Carlos Ordonez (Teradata);Norberto F. Ezquerra (Georgia Institute of Technology);Cesar A. Santana (Emory University);,"2165938062,2034954149,2172154375","Association rules are a data mining technique used to discover frequent patterns in a data set. In this work, association rules are used in the medical domain, where data sets are generally high dimensional and small. The chief disadvantage about mining association rules in a high dimensional data set is the huge number of patterns that are discovered, most of which are irrelevant or redundant. Several constraints are proposed for filtering purposes, since our aim is to discover only significant association rules and accelerate the search process. A greedy algorithm is introduced to compute rule covers in order to summarize rules having the same consequent. The significance of association rules is evaluated using three metrics: support, confidence and lift. Experiments focus on discovering association rules on a real data set to predict absence or existence of heart disease. Constraints are shown to significantly reduce the number of discovered rules and improve running time. Rule covers summarize a large number of rules by producing a succinct set of rules with high-quality metrics.",2006,Knowledge and Information Systems volume 9 issue 3 pp 1-2,association rule learning;information extraction;data mining;database;artificial intelligence;machine learning;statistics;computer science;mathematics;
Multi-step density-based clustering,Stefan Brecheisen (Ludwig Maximilian University of Munich);Hans-Peter Kriegel (Ludwig Maximilian University of Munich);Martin Pfeifle (Ludwig Maximilian University of Munich);,"2018129953,1919135125,2108078646","Data mining in large databases of complex objects from scientific, engineering or multimedia applications is getting more and more important. In many areas, complex distance measures are first choice but also simpler distance functions are available which can be computed much more efficiently. In this paper, we will demonstrate how the paradigm of multi-step query processing which relies on exact as well as on lower-bounding approximated distance functions can be integrated into the two density-based clustering algorithms DBSCAN and OPTICSE resulting in a considerable efficiency boost. Our approach tries to confine itself to e-range queries on the simple distance functions and carries out complex distance computations only at that stage of the clustering algorithm where they are compulsory to compute the correct clustering result. Furthermore, we will show how our approach can be used for approximated clustering allowing the user to find an individual trade-off between quality and efficiency. In order to assess the quality of the resulting clusterings, we introduce suitable quality measures which can be used generally for evaluating the quality of approximated partitioning and hierarchical clusterings. In a broad experimental evaluation based on real-world test data sets, we demonstrate that our approach accelerates the generation of exact density-based clusterings by more than one order of magnitude. Furthermore, we show that our approximated clustering approach results in high quality clusterings where the desired quality is scalable with respect to (w.r.t.) the overall number of exact distance computations.",2006,Knowledge and Information Systems volume 9 issue 3 pp 284-308,k medians clustering;complex valued function;range query;fuzzy clustering;hierarchical clustering;quality control;metric;cluster analysis;upper and lower bounds;data analysis;information extraction;computational geometry;data mining;machine learning;algorithm;computer science;mathematics;
Quality assessment of individual classifications in machine learning and data mining,Matjaž Kukar (University of Ljubljana);,2053626489,"Although in the past machine learning algorithms have been successfully used in many problems, their serious practical use is affected by the fact that often they cannot produce reliable and unbiased assessments of their predictions' quality. In last few years, several approaches for estimating reliability or confidence of individual classifiers have emerged, many of them building upon the algorithmic theory of randomness, such as (historically ordered) transduction-based confidence estimation, typicalness-based confidence estimation, and transductive reliability estimation. Unfortunately, they all have weaknesses: either they are tightly bound with particular learning algorithms, or the interpretation of reliability estimations is not always consistent with statistical confidence levels. In the paper we describe typicalness and transductive reliability estimation frameworks and propose a joint approach that compensates the above-mentioned weaknesses by integrating typicalness-based confidence estimation and transductive reliability estimation into a joint confidence machine. The resulting confidence machine produces confidence values in the statistical sense. We perform series of tests with several different machine learning algorithms in several problem domains. We compare our results with that of a proprietary method as well as with kernel density estimation. We show that the proposed method performs as well as proprietary methods and significantly outperforms density estimation methods.",2006,Knowledge and Information Systems volume 9 issue 3 pp 364-384,confidence interval;algorithmics;information extraction;econometrics;artificial intelligence;statistics;computer science;
Finding centric local outliers in categorical/numerical spaces,Jeffrey Xu Yu (The Chinese University of Hong Kong);Weining Qian (Fudan University);Hongjun Lu (Hong Kong University of Science and Technology);Aoying Zhou (Fudan University);,"2119358208,2344638167,2131290459,2620616608","Outlier detection techniques are widely used in many applications such as credit-card fraud detection, monitoring criminal activities in electronic commerce, etc. These applications attempt to identify outliers as noises, exceptions, or objects around the border. The existing density-based local outlier detection assigns the degree to which an object is an outlier in a numerical space. In this paper, we propose a novel mutual-reinforcement-based local outlier detection approach. Instead of detecting local outliers as noise, we attempt to identify local outliers in the center, where they are similar to some clusters of objects on one hand, and are unique on the other. Our technique can be used for bank investment to identify a unique body, similar to many good competitors, in which to invest. We attempt to detect local outliers in categorical, ordinal as well as numerical data. In categorical data, the challenge is that there are many similar but different ways to specify relationships among the data items. Our mutual-reinforcement-based approach is stable, with similar but different user-defined relationships. Our technique can reduce the burden for users to determine the relationships among data items, and find the explanations why the outliers are found. We conducted extensive experimental studies using real datasets.",2006,Knowledge and Information Systems volume 9 issue 3 pp 309-338,cluster analysis;anomaly detection;information extraction;data science;computer security;data mining;machine learning;computer science;
Effective classification of noisy data streams with attribute-oriented dynamic classifier selection,Xingquan Zhu (University of Vermont);Xindong Wu (University of Vermont);Ying Yang (Monash University);,"2618356905,2123651450,2140940508","Recently, mining from data streams has become an important and challenging task for many real-world applications such as credit card fraud protection and sensor networking. One popular solution is to separate stream data into chunks, learn a base classifier from each chunk, and then integrate all base classifiers for effective classification. In this paper, we propose a new dynamic classifier selection (DCS) mechanism to integrate base classifiers for effective mining from data streams. The proposed algorithm dynamically selects a single “best” classifier to classify each test instance at run time. Our scheme uses statistical information from attribute values, and uses each attribute to partition the evaluation set into disjoint subsets, followed by a procedure that evaluates the classification accuracy of each base classifier on these subsets. Given a test instance, its attribute values determine the subsets that the similar instances in the evaluation set have constructed, and the classifier with the highest classification accuracy on those subsets is selected to classify the test instance. Experimental results and comparative studies demonstrate the efficiency and efficacy of our method. Such a DCS scheme appears to be promising in mining data streams with dramatic concept drifting or with a significant amount of noise, where the base classifiers are likely conflictive or have low confidence.",2006,Knowledge and Information Systems volume 9 issue 3 pp 339-363,quadratic classifier;confidence;wireless sensor network;signal to noise ratio;biological classification;comparative research;data analysis;information extraction;data mining;artificial intelligence;machine learning;statistics;computer science;
Learning accurate and concise naïve Bayes classifiers from attribute value taxonomies and data,Jun Zhang 0002 (Iowa State University);Dae-Ki Kang (Iowa State University);Adrian Silvescu (Iowa State University);Vasant Honavar (Pennsylvania State University);,"2303329600,2133513039,203236475,2069172874","In many application domains, there is a need for learning algorithms that can effectively exploit attribute value taxonomies (AVT)--hierarchical groupings of attribute values--to learn compact, comprehensible and accurate classifiers from data--including data that are partially specified. This paper describes AVT-NBL, a natural generalization of the naive Bayes learner (NBL), for learning classifiers from AVT and data. Our experimental results show that AVT-NBL is able to generate classifiers that are substantially more compact and more accurate than those produced by NBL on a broad range of data sets with different percentages of partially specified values. We also show that AVT-NBL is more efficient in its use of training data: AVT-NBL produces classifiers that outperform those produced by NBL using substantially fewer training examples.",2006,Knowledge and Information Systems volume 9 issue 2 pp 157-179,bayes classifier;biological classification;taxonomy;text mining;data mining;database;pattern recognition;artificial intelligence;machine learning;statistics;computer science;
On condensed representations of constrained frequent patterns,Francesco Bonchi (Institute for Scientific Interchange);Claudio Lucchese (Istituto di Scienza e Tecnologie dell'Informazione);,"2176652147,1989507918","Constrained frequent patterns and closed frequent patterns are two paradigms aimed at reducing the set of extracted patterns to a smaller, more interesting, subset. Although a lot of work has been done with both these paradigms, there is still confusion around the mining problem obtained by joining closed and constrained frequent patterns in a unique framework. In this paper, we shed light on this problem by providing a formal definition and a thorough characterisation. We also study computational issues and show how to combine the most recent results in both paradigms, providing a very efficient algorithm that exploits the two requirements (satisfying constraints and being closed) together at mining time in order to reduce the computation as much as possible.",2006,Knowledge and Information Systems volume 9 issue 2 pp 180-201,constraint satisfaction;formal specification;satisfiability;data mining;database;algorithm;computer science;mathematics;
Merging uncertain information with semantic heterogeneity in XML,Anthony Hunter (University College London);Weiru Liu (Queen's University Belfast);,"2105909224,2148409606","Semistructured information can be merged in a logic-based framework [6,7]. This framework has been extended to deal with uncertainty, in the form of probability values, degrees of beliefs, or necessity measures, associated with leaves (i.e. textentries) in the XML documents [3]. In this paper we further extend this approach to modelling and merging uncertain information that is defined at different levels of granularity of XML textentries, and to modelling and reasoning with XML documents that contain semantically heterogeneous uncertain information on more complex elements in XML subtrees. We present the formal definitions for modelling, propagating and merging semantically heterogeneous uncertain information and explain how they can be handled using logic-based fusion techniques.",2006,Knowledge and Information Systems volume 9 issue 2 pp 230-258,xml schema editor;xml validation;xml framework;document structure description;xml schema;xml;semantics;natural language processing;data mining;database;programming language;computer science;
"Call classification using recurrent neural networks, support vector machines and finite state automata",Sheila Garfield (University of Sunderland);Stefan Wermter (University of Sunderland);,"2115923086,2308014113","Our objective is spoken-language classification for helpdesk call routing using a scanning understanding and intelligent-system techniques. In particular, we examine simple recurrent networks, support-vector machines and finite-state transducers for their potential in this spoken-language-classification task and we describe an approach to classification of recorded operator-assistance telephone utterances. The main contribution of the paper is a comparison of a variety of techniques in the domain of call routing. Support-vector machines and transducers are shown to have some potential for spoken-language classification, but the performance of the neural networks indicates that a simple recurrent network performs best for helpdesk call routing.",2006,Knowledge and Information Systems volume 9 issue 2 pp 131-156,recurrent neural network;support vector machine;finite state machine;artificial intelligence;machine learning;algorithm;computer science;
A geometrical solution to time series searching invariant to shifting and scaling,Mi Zhou (The Chinese University of Hong Kong);Man Hon Wong (The Chinese University of Hong Kong);Kelvin Kam Wing Chu (The Chinese University of Hong Kong);,"2631787402,2230488577,2304625684","The technique of searching for similar patterns among time series data is very useful in many applications. The problem becomes difficult when shifting and scaling are considered. We find that we can treat the problem geometrically and the major contribution of this paper is that a uniform geometrical model that can analyze the existing related methods is proposed. Based on the analysis, we conclude that the angle between two vectors after the Shift-Eliminated Transformation is a more intrinsical similarity measure invariant to shifting and scaling. We then enhance the original conical index to adapt to the geometrical properties of the problem and compare its performance with that of sequential search and R*-tree. Experimental results show that the enhanced conical index achieves larger improvement on R*-tree and sequential search in high dimension. It can also keep a steady performance as the selectivity increases.",2006,Knowledge and Information Systems volume 9 issue 2 pp 202-229,time series;combinatorics;geometry;statistics;computer science;mathematics;
Query indexing with containment-encoded intervals for efficient stream processing,Kun-Lung Wu (IBM);Shyh-Kwei Chen (IBM);S. Yu (IBM);,"2095612124,2150933152,2702287749","Many continual range queries can be issued against data streams. To efficiently evaluate continual queries against a stream, a main memory-based query index with a small storage cost and a fast search time is needed, especially if the stream is rapid. In this paper, we study a CEI-based query index that meets both criteria for efficient processing of continual interval queries. This new query index is an indirect indexing approach. It centres around a set of predefined virtual containment-encoded intervals, or CEIs. The CEIs are used to first decompose query intervals and then perform efficient search operations. The CEIs are defined and labeled such that containment relationships among them are encoded in their IDs. The containment encoding makes decomposition and search operations efficient; from the encoding of the smallest CEI containing a data point, the encodings of other containing CEIs can be easily derived. Closed-form formulae for the bounds of the average index storage cost are derived. Simulations are conducted to evaluate the effectiveness of the CEI-based query index and to compare it with alternative approaches. The results show that the CEI-based query index significantly outperforms existing approaches in terms of both storage cost and search time.",2006,Knowledge and Information Systems volume 9 issue 1 pp 62-90,sargable;web query classification;query expansion;range query;query optimization;computational geometry;theoretical computer science;data mining;database;computer science;
Multiple labels associative classification,Abdeljaber Thabtah (University of Bradford);Peter Cowling (University of Bradford);Yonghong Peng (University of Bradford);,"2286802689,174968694,2131925740","Building fast and accurate classifiers for large-scale databases is an important task in data mining. There is growing evidence that integrating classification and association rule mining can produce more efficient and accurate classifiers than traditional techniques. In this paper, the problem of producing rules with multiple labels is investigated, and we propose a multi-class, multi-label associative classification approach (MMAC). In addition, four measures are presented in this paper for evaluating the accuracy of classification approaches to a wide range of traditional and multi-label classification problems. Results for 19 different data sets from the UCI data collection and nine hyperheuristic scheduling runs show that the proposed approach is an accurate and effective classification technique, highly competitive and scalable if compared with other traditional and associative classification approaches.",2006,Knowledge and Information Systems volume 9 issue 1 pp 109-129,one class classification;association rule learning;information extraction;data science;data mining;database;pattern recognition;statistics;computer science;
TEG—a hybrid approach to information extraction,Ronen Feldman (Bar-Ilan University);Benjamin Rosenfeld (Bar-Ilan University);Moshe Fresko (Bar-Ilan University);,"2159469398,2096069757,1257047566","This paper describes a hybrid statistical and knowledge-based information extraction model, able to extract entities and relations at the sentence level. The model attempts to retain and improve the high accuracy levels of knowledge-based systems while drastically reducing the amount of manual labour by relying on statistics drawn from a training corpus. The implementation of the model, called TEG (trainable extraction grammar), can be adapted to any IE domain by writing a suitable set of rules in a SCFG (stochastic context-free grammar)-based extraction language and training them using an annotated corpus. The system does not contain any purely linguistic components, such as PoS tagger or shallow parser, but allows to using external linguistic components if necessary. We demonstrate the performance of the system on several named entity extraction and relation extraction tasks. The experiments show that our hybrid approach outperforms both purely statistical and purely knowledge-based systems, while requiring orders of magnitude less manual rule writing and smaller amounts of training data. We also demonstrate the robustness of our system under conditions of poor training-data quality.",2006,Knowledge and Information Systems volume 9 issue 1 pp 1-18,stochastic context free grammar;entity relationship model;context free grammar;data quality;robustness;systems modeling;hidden markov model;natural language processing;speech recognition;data mining;artificial intelligence;machine learning;computer science;
On efficiently summarizing categorical databases,Jianyong Wang (Tsinghua University);George Karypis (University of Minnesota);,"2105625159,219814910","Frequent itemset mining was initially proposed and has been studied extensively in the context of association rule mining. In recent years, several studies have also extended its application to transaction or document clustering. However, most of the frequent itemset based clustering algorithms need to first mine a large intermediate set of frequent itemsets in order to identify a subset of the most promising ones that can be used for clustering. In this paper, we study how to directly find a subset of high quality frequent itemsets that can be used as a concise summary of the transaction database and to cluster the categorical data. By exploring key properties of the subset of itemsets that we are interested in, we proposed several search space pruning methods and designed an efficient algorithm called SUMMARY. Our empirical results show that SUMMARY runs very fast even when the minimum support is extremely low and scales very well with respect to the database size, and surprisingly, as a pure frequent itemset mining algorithm it is very effective in clustering the categorical data and summarizing the dense transaction databases.",2006,Knowledge and Information Systems volume 9 issue 1 pp 19-37,association rule learning;cluster analysis;document clustering;distributed database;information extraction;data mining;database;pattern recognition;machine learning;computer science;
Improving Legal Document Summarization Using Graphical Models,M. Saravanan (Indian Institute of Technology Madras);Balaraman Ravindran (Indian Institute of Technology Madras);Sanjay Raman;,"2250970876,1956086128,2718973932",-,2006,Knowledge and Information Systems,active learning;semantic memory;conditional random field;exponential growth;domain knowledge;graphical model;probability distribution;multi document summarization;automatic summarization;natural language processing;information retrieval;pattern recognition;machine learning;computer science;
Query-learning-based iterative feature-subset selection for learning from high-dimensional data sets,Hiroshi Mamitsuka (Kyoto University);,278261381,"We propose a new data-mining method that is effective for learning from extremely high-dimensional data sets. Our proposed method selects a subset of features from a high-dimensional data set by a process of iterative refinement. Our selection of a feature-subset has two steps. The first step selects a subset of instances, to which predictions by hypotheses previously obtained are most unreliable, from the data set. The second step selects a subset of features whose values in the selected instances vary the most from those in all instances of the database. We empirically evaluate the effectiveness of the proposed method by comparing its performance with those of four other methods, including one of the latest feature-subset selection methods. The evaluation was performed on a real-world data set with approximately 140,000 features. Our results show that the performance of the proposed method exceeds those of the other methods in terms of prediction accuracy, precision at a certain recall value, and computation time to reach a certain prediction accuracy. We have also examined the effect of noise in the data and found that the advantage of the proposed method becomes more pronounced for larger noise levels. Extended abstracts of parts of the work presented in this paper have appeared in Mamitsuka [14] and Mamitsuka [15].",2006,Knowledge and Information Systems volume 9 issue 1 pp 91-108,iterative and incremental development;drug design;clustering high dimensional data;iterative method;uncertainty;data analysis;sampling;information extraction;data mining;artificial intelligence;machine learning;statistics;computer science;
Semi-supervised learning with an imperfect supervisor,R. Amini;Patrick Gallinari (Pierre-and-Marie-Curie University);,"2444573576,2235456028","Real-life applications may involve huge data sets with misclassified or partially classified training data. Semi-supervised learning and learning in the presence of label noise have recently emerged as new paradigms in the machine learning community to cope with this kind of problems. This paper describes a new discriminant algorithm for semi-supervised learning. This algorithm optimizes the classification maximum likelihood (CML) of a set of labeled–unlabeled data, using a discriminant extension of the Classification Expectation Maximization algorithm. We further propose to extend this algorithm by modeling imperfections in the estimated class labels for unlabeled data. The parameters of this label-error model are learned together with the semi-supervised classifier parameters. We demonstrate the effectiveness of the approach using extensive experiments on different datasets.",2005,Knowledge and Information Systems volume 8 issue 4 pp 385-413,population based incremental learning;stability;wake sleep algorithm;multi task learning;generalization error;expectation maximization algorithm;active learning;semi supervised learning;supervised learning;unsupervised learning;pattern recognition;artificial intelligence;machine learning;statistics;computer science;mathematics;
Web data extraction based on structural similarity,Zhao Li (Nanyang Technological University);Wee Keong Ng (Nanyang Technological University);Aixin Sun (Nanyang Technological University);,"2344975965,2120738522,2124989948","Web data-extraction systems in use today mainly focus on the generation of extraction rules, i.e., wrapper induction. Thus, they appear ad hoc and are difficult to integrate when a holistic view is taken. Each phase in the data-extraction process is disconnected and does not share a common foundation to make the building of a complete system straightforward. In this paper, we demonstrate a holistic approach to Web data extraction. The principal component of our proposal is the notion of a document schema. Document schemata are patterns of structures embedded in documents. Once the document schemata are obtained, the various phases (e.g. training set preparation, wrapper induction and document classification) can be easily integrated. The implication of this is improved efficiency and better control over the extraction procedure. Our experimental results confirmed this. More importantly, because a document can be represented as avector of schema, it can be easily incorporated into existing systems as the fabric for integration.",2005,Knowledge and Information Systems volume 8 issue 4 pp 438-461,design document listing;principal component analysis;world wide web;data mining;database;machine learning;computer science;
A supervised clustering algorithm for computer intrusion detection,Xiangyang Li (University of Michigan);Nong Ye (Arizona State University);,"2716461682,2692640335","We previously developed a clustering and classification algorithm—supervised (CCAS) to learn patterns of normal and intrusive activities and to classify observed system activities. Here we further enhance the robustness of CCAS to the presentation order of training data and the noises in training data. This robust CCAS adds data redistribution, a supervised hierarchical grouping of clusters and removal of outliers as the postprocessing steps.",2005,Knowledge and Information Systems volume 8 issue 4 pp 498-509,intrusion detection system;cluster analysis;supervised learning;data mining;pattern recognition;machine learning;computer science;
A graph-based approach for extracting terminological properties from information sources with heterogeneous formats,Luigi Palopoli (University of Calabria);Domenico Rosaci;Giorgio Terracina (University of Calabria);Domenico Ursino;,"2294106506,20636212,2071483913,2040027413","The problem of handling both the integration and the cooperation of a large number of information sources characterised by heterogeneous representation formats is a challenging issue. In this context, a central role can be played by the knowledge about the semantic relationships holding between concepts belonging to different information sources (intersource properties). In this paper, we propose a semiautomatic approach for extracting two kinds of intersource properties, namely synonymies and homonymies, from heterogeneous information sources. In order to carry out the extraction task, we introduce both a conceptual model, for representing involved sources, and a metrics, for measuring the strength of the semantic relationships holding among concepts represented within the same source.",2005,Knowledge and Information Systems volume 8 issue 4 pp 462-497,homonym;conceptual model;information retrieval;data mining;database;computer science;
A study of the model and algorithms for handling location-dependent continuous queries,Manish Gupta 0008 (University of Texas at Dallas);Manghui Tu (University of Texas at Dallas);Latifur Khan (University of Texas at Dallas);Farokh B. Bastani (University of Texas at Dallas);I-Ling Yen (University of Texas at Dallas);,"2302862417,2700222325,2155983610,349810649,2083279976","Advances in wireless and mobile computing environments allow a mobile user to access a wide range of applications. For example, mobile users may want to retrieve data about unfamiliar places or local life styles related to their location. These queries are called location-dependent queries. Furthermore, a mobile user may be interested in getting the query results repeatedly, which is called location-dependent continuous querying. This continuous query emanating from a mobile user may retrieve information from a single-zone (single-ZQ) or from multiple neighbouring zones (multiple-ZQ). We consider the problem of handling location-dependent continuous queries with the main emphasis on reducing communication costs and making sure that the user gets correct current-query result. The key contributions of this paper include: (1) Proposing a hierarchical database framework (tree architecture and supporting continuous query algorithm) for handling location-dependent continuous queries. (2) Analysing the flexibility of this framework for handling queries related to single-ZQ or multiple-ZQ and propose intelligent selective placement of location-dependent databases. (3) Proposing an intelligent selective replication algorithm to facilitate time- and space-efficient processing of location-dependent continuous queries retrieving single-ZQ information. (4) Demonstrating, using simulation, the significance of our intelligent selective placement and selective replication model in terms of communication cost and storage constraints, considering various types of queries.",2005,Knowledge and Information Systems volume 8 issue 4 pp 414-437,spatial query;spatial database;replication;mobile computing;world wide web;data mining;database;computer science;
2005 KAIS Reviewers,Springer London Ltd;,2280719261,-,2005,Knowledge and Information Systems volume 8 issue 4 pp 510-511,-
Generative model-based document clustering: a comparative study,Shi Zhong (Florida Atlantic University);Joydeep Ghosh (University of Texas at Austin);,"2671309347,2148168557","This paper presents a detailed empirical study of 12 generative approaches to text clustering, obtained by applying four types of document-to-cluster assignment strategies (hard, stochastic, soft and deterministic annealing (DA) based assignments) to each of three base models, namely mixtures of multivariate Bernoulli, multinomial, and von Mises-Fisher (vMF) distributions. A large variety of text collections, both with and without feature selection, are used for the study, which yields several insights, including (a) showing situations wherein the vMF-centric approaches, which are based on directional statistics, fare better than multinomial model-based methods, and (b) quantifying the trade-off between increased performance of the soft and DA assignments and their increased computational demands. We also compare all the model-based algorithms with two state-of-the-art discriminative approaches to document clustering based, respectively, on graph partitioning (CLUTO) and a spectral coclustering method. Overall, DA and CLUTO perform the best but are also the most computationally expensive. The vMF models provide good performance at low cost while the spectral coclustering algorithm fares worse than vMF-based methods for a majority of the datasets.",2005,Knowledge and Information Systems volume 8 issue 3 pp 374-384,document structure description;graph partition;empirical research;cluster analysis;comparative research;document clustering;data mining;pattern recognition;machine learning;statistics;computer science;
Supporting virtual enterprise systems using agent coordination,Nacereddine Zarour;Mahmoud Boufaïda (University of Mentouri);Lionel Seinturier (university of lille);Pascal Estraillier (University of La Rochelle);,"2608851787,2110124950,1986010036,1265152420","Open environments like the Internet or corporate intranets enable a large number of interested enterprises to access, filter, process and present information on an as-needed basis. These environments support modern applications, such as virtual enterprises and interorganisational workflow management systems, which involve a number of heterogeneous resources, services and processes. However, any execution of a virtual enterprise system would yield to disjoining and error-prone behaviour without appropriate techniques to coordinate the various business processes. This paper reports on the design and implementation of a flexible agent-based framework for supporting the coordination of virtual enterprises and workflow management systems. The paper also shows how an agent coordination infrastructure, which is explained by social constraints, can impact on the engineering of highly dynamic virtual enterprises and workflow management systems by presenting a simple case study.",2005,Knowledge and Information Systems volume 8 issue 3 pp 330-349,workflow technology;workflow engine;workflow management system;business process;workflow;user interface;the internet;knowledge management;simulation;computer science;
A binary neural k -nearest neighbour technique,Victoria J. Hodge (University of York);Jim Austin (University of York);,"2039493643,2108286228","K-Nearest Neighbour (k-NN) is a widely used technique for classifying and clustering data. K-NN is effective but is often criticised for its polynomial run-time growth as k-NN calculates the distance to every other record in the data set for each record in turn. This paper evaluates a novel k-NN classifier with linear growth and faster run-time built from binary neural networks. The binary neural approach uses robust encoding to map standard ordinal, categorical and real-valued data sets onto a binary neural network. The binary neural network uses high speed pattern matching to recall the k-best matches. We compare various configurations of the binary approach to a conventional approach for memory overheads, training speed, retrieval speed and retrieval accuracy. We demonstrate the superior performance with respect to speed and memory requirements of the binary approach compared to the standard approach and we pinpoint the optimal configurations.",2005,Knowledge and Information Systems volume 8 issue 3 pp 276-291,pattern matching;categorical variable;covariance matrix;artificial neural network;data mining;pattern recognition;machine learning;statistics;computer science;mathematics;
A content-search information retrieval process based on conceptual graphs,David Genest (University of Angers);Michel Chein (University of Montpellier);,"2295052987,2151401873","An intelligent information retrieval system is presented in this paper. In our approach, which complies with the logical view of information retrieval, queries, document contents and other knowledge are represented by expressions in a knowledge representation language based on the conceptual graphs introduced by Sowa. In order to take the intrinsic vagueness of information retrieval into account, i.e. to search documents imprecisely and incompletely represented in order to answer a vague query, different kinds of probabilistic logic are often used. The search process described in this paper uses graph transformations instead of probabilistic notions. This paper is focused on the content-based retrieval process, and the cognitive facet of information retrieval is not directly addressed. However, our approach, involving the use of a knowledge representation language for representing data and a search process based on a combinatorial implementation of van Rijsbergen’s logical uncertainty principle, also allows the representation of retrieval situations. Hence, we believe that it could be implemented at the core of an operational information retrieval system. Two applications, one dealing with academic libraries and the other concerning audiovisual documents, are briefly presented.",2005,Knowledge and Information Systems volume 8 issue 3 pp 292-309,cognitive models of information retrieval;divergence from randomness model;human computer information retrieval;adversarial information retrieval;concept search;query expansion;conceptual graph;vector space model;metadata;data retrieval;relevance;uncertainty principle;probabilistic logic;search engine;information system;question answering;content analysis;document retrieval;information processing;natural language processing;information retrieval;data mining;database;computer science;
Optimizing complex queries based on similarities of subqueries,Qiang Zhu (University of Michigan);Yingying Tao (University of Michigan);Calisto Zuzarte (IBM);,"2292350935,2126653289,2139751229","As database technology is applied to more and more application domains, user queries are becoming increasingly complex (e.g. involving a large number of joins and a complex query structure). Query optimizers in existing database management systems (DBMS) were not developed for efficiently processing such queries and often suffer from problems such as intolerably long optimization time and poor optimization results. To tackle this challenge, we present a new similarity-based approach to optimizing complex queries in this paper. The key idea is to identify similar subqueries that often appear in a complex query and share the optimization result among similar subqueries in the query. Different levels of similarity for subqueries are introduced. Efficient algorithms to identify similar queries in a given query and optimize the query based on similarity are presented. Related issues, such as choosing good starting nodes in a query graph, evaluating identified similar subqueries and analyzing algorithm complexities, are discussed. Our experimental results demonstrate that the proposed similarity-based approach is quite promising in optimizing complex queries with similar subqueries in a DBMS.",2005,Knowledge and Information Systems volume 8 issue 3 pp 350-373,sargable;spatial query;query optimization;query language;computational complexity theory;management information systems;world wide web;data mining;database;computer science;
Visualising hierarchical associations,Aaron John Ceglar (Flinders University);John Francis Roddick (Flinders University);Paul Robert Calder (Flinders University);Chris Rainsford (Flinders University);,"2014168631,2275978167,2201169625,1245253535","Recent association-mining research has led to the development of techniques that allow the accommodation of concept hierarchies within the mining process. This extension results in the discovery of rules which associate not only groups of items but which are also influenced by the hierarchies within which an item may reside. Given this, there then arises a need for techniques whereby such hierarchical associations can be presented to the user. Current association rule visualisation techniques are limited, as they do not effectively incorporate or enable the visualisation of hierarchical semantics. This paper presents a review of current hierarchical and association visualisation techniques and introduces a novel technique for visualising hierarchical association rules.",2005,Knowledge and Information Systems volume 8 issue 3 pp 257-275,visualization;knowledge extraction;data science;data mining;artificial intelligence;computer science;
Inference in the SOOKAT object-oriented knowledge acquisition tool,Päivikki Parpola (Helsinki University of Technology);,51649834,"The SOOKAT (structured object-oriented knowledge acquisition tool) knowledge acquisition (KA) tool, supporting the SeSKA (seamless structured knowledge acquisition) methodology, integrates phases of KA through seamless transformations between object-oriented (OO) models.The integration of constructing a knowledge base (KB) can be extended beyond the KA process by performing inferences in instantiations of models constructed during the KA process.The models, constructed during the KA process, form a framework for performing inferences in instantiations of the models.Inferences performed in instantiations of OO models are guided by control objects (CO). Messages are sent between COs and components of the inference structure. A specific CO, possibly using subordinate COs, can be specified for each inference strategy.There exists a mutual CO for forward and backward chaining that can also be used when reasoning according to protocols. In addition, COs for problem-solving methods (PSMs), such as cover-and-differentiate or propose-and-revise, can be used.Mechanisms for importing PSMs over the Internet, as well as for generating specific COs for PSMs, are under development.",2005,Knowledge and Information Systems volume 8 issue 3 pp 310-329,object orientation;object oriented programming;the internet;knowledge engineering;knowledge base;artificial intelligence;computer science;
Clustering of time-series subsequences is meaningless: implications for previous and future research,"Eamonn J. Keogh (University of California, Riverside);Jessica Lin 0001 (University of California, Riverside);","2170070822,2100950507","Given the recent explosion of interest in streaming data and online algorithms, clustering of time-series subsequences, extracted via a sliding window, has received much attention. In this work, we make a surprising claim. Clustering of time-series subsequences is meaningless. More concretely, clusters extracted from these time series are forced to obey a certain constraint that is pathologically unlikely to be satisfied by any dataset, and because of this, the clusters extracted by any clustering algorithm are essentially random. While this constraint can be intuitively demonstrated with a simple illustration and is simple to prove, it has never appeared in the literature. We can justify calling our claim surprising because it invalidates the contribution of dozens of previously published papers. We will justify our claim with a theorem, illustrative examples, and a comprehensive set of experiments on reimplementations of previous work. Although the primary contribution of our work is to draw attention to the fact that an apparent solution to an important problem is incorrect and should no longer be used, we also introduce a novel method that, based on the concept of time-series motifs, is able to meaningfully cluster subsequences on some time-series datasets.",2005,Knowledge and Information Systems volume 8 issue 2 pp 154-177,cluster analysis;data mining;database;artificial intelligence;machine learning;statistics;algorithm;computer science;mathematics;
Mining border descriptions of emerging patterns from dataset pairs,Guozhu Dong (Wright State University);Jinyan Li (Nanyang Technological University);,"2164298414,2190567876","The mining of changes or differences or other comparative patterns from a pair of datasets is an interesting problem. This paper is focused on the mining of one type of comparative pattern called emerging patterns. Emerging patterns are denoted by EPs and are defined as patterns for which support increases from one dataset to the other with a big ratio. The number of EPs is sometimes huge. To provide a good structure for and to reduce the size of mining results, we use borders to concisely describe large collections of EPs in a lossless way. Such a border consists of only the minimal (under set inclusion) and the maximal EPs in the collection. We also present an algorithm for efficiently computing the borders of some desired EPs by manipulating the input borders only. Our experience with many datasets in the UCI Repository and recent cancer diagnosis datasets demonstrated that: Both the EP pattern type and our algorithm are useful for building accurate classifiers and useful for mining multifactor interactions, for example, minimal gene groups potentially responsible for the development of cancer.",2005,Knowledge and Information Systems volume 8 issue 2 pp 178-202,contrast;association rule learning;medical diagnosis;biological classification;data mining;artificial intelligence;machine learning;computer science;
Canonical forms for labelled trees and their applications in frequent subtree mining,"Yun Chi (University of California, Los Angeles);Yirong Yang (University of California, Los Angeles);Richard R. Muntz (University of California, Los Angeles);","2696493159,2126614047,2033368653","Tree structures are used extensively in domains such as computational biology, pattern recognition, XML databases, computer networks, and so on. In this paper, we first present two canonical forms for labelled rooted unordered trees–the breadth-first canonical form (BFCF) and the depth-first canonical form (DFCF). Then the canonical forms are applied to the frequent subtree mining problem. Based on the BFCF, we develop a vertical mining algorithm, RootedTreeMiner, to discover all frequently occurring subtrees in a database of labelled rooted unordered trees. The RootedTreeMiner algorithm uses an enumeration tree to enumerate all (frequent) labelled rooted unordered subtrees. Next, we extend the definition of the DFCF to labelled free trees and present an Apriori-like algorithm, FreeTreeMiner, to discover all frequently occurring subtrees in a database of labelled free trees. Finally, we study the performance and the scalability of our algorithms through extensive experiments based on both synthetic data and datasets from real applications.",2005,Knowledge and Information Systems volume 8 issue 2 pp 203-234,xml database;xml;isomorphism;canonical form;synthetic data;scalability;tree structure;distributed database;data mining;database;algorithm;computer science;
Evolution of mathematical models of chaotic systems based on multiobjective genetic programming,Katya Rodríguez-Vázquez (National Autonomous University of Mexico);Peter J. Fleming (University of Sheffield);,"2049821487,2095989000","This work is concerned with the identification of models for nonlinear dynamical systems using multiobjective evolutionary algorithms. Systems modelling involves the processes of structure selection, parameter estimation, model performance and model validation and involves a complex solution space. Evolutionary Algorithms (EAs) are search and optimisation tools founded on the principles of natural evolution and genetics, which are suitable for a wide range of application areas. Due to the versatility of these tools and motivated by the versatility of genetic programming (GP), this evolutionary paradigm is proposed for this modelling problem. GP is then combined with a multiobjective function definition scheme. Multiobjective genetic programming (MOGP) is applied to multiple, conflicting objectives and yields a set of candidate parsimonious and valid models, which reproduce the original system behaviour. The MOGP approach is then demonstrated as being applicable for system modelling with chaotic dynamics. The circuit introduced by Chua, being one of the most popular benchmarks for studying nonlinear oscillations, and the Duffing---Holmes oscillator are the systems to test the evolutionary-based modelling approach introduced in this paper.",2005,Knowledge and Information Systems volume 8 issue 2 pp 235-256,evolutionary programming;regression model validation;genetic programming;search algorithm;system identification;oscillation;estimation theory;mathematical model;evolutionary computation;evolutionary algorithm;artificial intelligence;machine learning;mathematical optimization;statistics;algorithm;computer science;mathematics;
Efficient breadth-first mining of frequent pattern with monotone constraints,Francesco Bonchi (Institute for Scientific Interchange);Fosca Giannotti (Istituto di Scienza e Tecnologie dell'Informazione);Alessio Mazzanti;Dino Pedreschi (University of Pisa);,"2176652147,1205765909,1994304057,7769909","The key point of this article is that, in frequent pattern mining, the most appropriate way of exploiting monotone constraints in conjunction with frequency is to use them in order to reduce the input data; this reduction in turn induces a stronger pruning of the search space of the problem. Following this intuition, we introduce ExAMiner, a breadth-first algorithm that exploits the real synergy of antimonotone and monotone constraints: the total benefit is greater than the sum of the two individual benefits. ExAMiner generalizes the basic idea of the preprocessing algorithm ExAnte (Bonchi et al. 2003(b)), embedding such ideas at all levels of an Apriori-like computation. The resulting algorithm is the generalization of the Apriori algorithm when a conjunction of monotone constraints is conjoined to the frequency antimonotone constraint. Experimental results confirm that this is, so far, the most efficient way of attacking the computational problem in analysis.",2005,Knowledge and Information Systems volume 8 issue 2 pp 131-153,search problem;data reduction;data mining;machine learning;algorithm;computer science;mathematics;
Privacy-preserving clustering with distributed EM mixture modeling,Xiaodong Lin (University of Cincinnati);Chris Clifton (Purdue University);Michael Y. Zhu (Purdue University);,"2117834427,2158488542,2150398521","Privacy and security considerations can prevent sharing of data, derailing data mining projects. Distributed knowledge discovery can alleviate this problem. We present a technique that uses EM mixture modeling to perform clustering on distributed data. This method controls data sharing, preventing disclosure of individual data items or any results that can be traced to an individual site.",2005,Knowledge and Information Systems volume 8 issue 1 pp 68-81,cluster analysis;information privacy;knowledge extraction;information security;world wide web;data mining;database;machine learning;computer science;
Best terms: an efficient feature-selection algorithm for text categorization,Dimitris Fragoudis (University of Patras);Dimitris Meretakis (Novartis);Spiridon Likothanassis_aff1n3 (University of Patras);,"2086079524,2701404058,2277958010","In this paper, we propose a new feature-selection algorithm for text classification, called best terms (BT). The complexity of BT is linear in respect to the number of the training-set documents and is independent from both the vocabulary size and the number of categories. We evaluate BT on two benchmark document collections, Reuters-21578 and 20-Newsgroups, using two classification algorithms, naive Bayes (NB) and support vector machines (SVM). Our experimental results, comparing BT with an extensive and representative list of feature-selection algorithms, show that (1) BT is faster than the existing feature-selection algorithms; (2) BT leads to a considerable increase in the classification accuracy of NB and SVM as measured by the F1 measure; (3) BT leads to a considerable improvement in the speed of NB and SVM; in most cases, the training time of SVM has dropped by an order of magnitude; (4) in most cases, the combination of BT with the simple, but very fast, NB algorithm leads to classification accuracy comparable with SVM while sometimes it is even more accurate.",2005,Knowledge and Information Systems volume 8 issue 1 pp 16-33,independent set;support vector machine;content analysis;feature selection;data mining;pattern recognition;machine learning;computer science;mathematics;
Demand-driven frequent itemset mining using pattern structures,Haixun Wang (IBM);Chang-Shing Perng (IBM);Sheng Ma (IBM);Philip S. Yu (IBM);,"2116756368,2127489856,2116699166,2125104194","Frequent itemset mining aims at discovering patterns the supports of which are beyond a given threshold. In many applications, including network event management systems, which motivated this work, patterns are composed of items each described by a subset of attributes of a relational table. As it involves an exponential mining space, the efficient implementation of user preferences and mining constraints becomes the first priority for a mining algorithm. User preferences and mining constraints are often expressed using patterns’ attribute structures. Unlike traditional methods that mine all frequent patterns indiscriminately, we regard frequent itemset mining as a two-step process: the mining of the pattern structures and the mining of patterns within each pattern structure. In this paper, we present a novel architecture that uses pattern structures to organize the mining space. In comparison with the previous techniques, the advantage of our approach is two-fold: (i) by exploiting the interrelationships among pattern structures, execution times for mining can be reduced significantly; and (ii) more importantly, it enables us to incorporate high-level simple user preferences and mining constraints into the mining process efficiently. These advantages are demonstrated by our experiments using both synthetic and real-life datasets.",2005,Knowledge and Information Systems volume 8 issue 1 pp 82-102,k optimal pattern discovery;molecule mining;association rule learning;concept mining;data stream mining;information extraction;data science;data mining;database;computer science;
Use of RDF for expertise matching within academia,Ping Liu (University of Leeds);Jayne Curson (University of Leeds);Peter M. Dew (University of Leeds);,"2309114278,2032880234,936992688","Organisations have realized that effective development and management of their organisational knowledge base is critical to survival in today’s competitive business environment. Employees, as a special knowledge asset, also attract the interest of many researchers because only through people communicating with one another can they really share their tacit knowledge and skills, which are often more valuable than sharing explicit documentations. The need to be able to quickly locate experts among the heterogeneous data sources stored in the organisational memory has been recognized by many researchers. This paper examines the advantages of using RDF (resource description framework) for expertise matching—the process of finding an individual with the required knowledge and skills. The major challenge is to semantically integrate multiple expertise indications from heterogeneous data sources stored in the organisational memory in order to facilitate users to locate the right expert(s). To understand the issues, a case study has been performed that involves building a RDF-based expertise broker that helps Ph.D. applicants locate potential supervisors before they formally apply to a university. An evaluation of the brokering system has been conducted through an experiment and the key results are presented. An extended brokering system that would support expertise matching in a multidisciplinary context is also described and further research identified.",2005,Knowledge and Information Systems volume 8 issue 1 pp 103-130,rdf;knowledge engineering;knowledge base;knowledge management;data mining;database;artificial intelligence;computer science;
A history-driven approach at evolving views under meta data changes,Andreas Koeller (Montclair State University);Elke A. Rundensteiner (Worcester Polytechnic Institute);,"2130978780,2289414603","Views over distributed information sources, such as data warehouses, rely on the stability of the schemas of underlying databases. In the event of meta data changes in the sources, such as the deletion of a table or column, such views may become undefined. Using meta data about information redundancy, views can be evolved as necessary to remain well defined after source meta data changes.Previous work in view synchronization focused only on deletions of schema elements. We now offer an approach that makes use of additions also. Our algorithm returns view definitions to previous versions by using knowledge about the history of views and meta data. This technology enables us to adapt views to temporary meta data changes by canceling out opposite changes. It also allows undo/redo operations on meta data. Last, in many cases, the resulting evolved views even have an improved information quality. In this paper, we give a formal taxonomy of schema and constraint changes and a full description of the proposed history-driven view-synchronization algorithm for this taxonomy. We also prove the history-driven view-synchronization algorithm to be correct. Our approach falls in the global-as-view category of data integration solutions but, unlike prior solutions in this category, it now also deals with changes in the information space rather than requiring source schemas to remain constant over time.",2005,Knowledge and Information Systems volume 8 issue 1 pp 34-67,information schema;synchronization;data warehouse;information retrieval;data mining;database;artificial intelligence;computer science;
New voting strategies designed for the classification of nucleic sequences,Mourad Elloumi (Tunis University);Mondher Maddouri;,"988120282,2302735669","Biological macromolecules, i.e. DNA, RNA and proteins, are coded by strings, called primary structures. During the last decades, the number and the complexity of primary structures are growing exponentially. Analyzing this huge volume of data to extract pertinent knowledge is a challenging task. Data mining approaches can be helpful to reach this goal. In this paper, we present a new data mining approach, called Disclass, based on vote strategies to do classification of primary structures: Let f1,f2,...,fn be families that represent, respectively, n samples of n sets S1,S2,...,Sn of primary structures. Let us consider now a new primary structure w that is assumed to belong to one of the n sets S1,S2,...,Sn. By using our data mining approach Disclass, the decision to assign the new primary structure w to one of the sets S1,S2,...,Sn is taken as follows: (i) During the first step, for each family fi, 1≤i≤n, we construct the ambiguously discriminant and minimal substrings (ADMS) associated with this family. Because the family fi, 1≤i≤n, is a sample of the set Si, the obtained ADMS are considered also to be associated with the whole set Si. During the classification process, the ADMS associated with the set Si, that are approximate substrings of the new primary structure w, will vote with weighted voices for the set Si. (ii) During the second step, we compute according to a vote strategy, the voice weights of the different ADMS, constructed during the first step. (iii) Finally, during the last step, the set that has the maximum weight of voices is the set to which we assign the new primary structure w.",2005,Knowledge and Information Systems volume 8 issue 1 pp 1-15,string;biological classification;rna;dna;data analysis;information extraction;data mining;database;artificial intelligence;machine learning;statistics;algorithm;computer science;mathematics;
Random-data perturbation techniques and privacy-preserving data mining,"Hillol Kargupta (University of Maryland, Baltimore County);Souptik Datta (University of Maryland, Baltimore County);Qi Wang (Washington State University);Krishnamoorthy Sivakumar (Washington State University);","539587773,2134013234,2660746163,2264914315","Privacy is becoming an increasingly important issue in many data-mining applications. This has triggered the development of many privacy-preserving data-mining techniques. A large fraction of them use randomized data-distortion techniques to mask the data for preserving the privacy of sensitive data. This methodology attempts to hide the sensitive data by randomly modifying the data values often using additive noise. This paper questions the utility of the random-value distortion technique in privacy preservation. The paper first notes that random matrices have predictable structures in the spectral domain and then it develops a random matrix-based spectral-filtering technique to retrieve original data from the dataset distorted by adding random values. The proposed method works by comparing the spectrum generated from the observed data with that of random matrices. This paper presents the theoretical foundation and extensive experimental results to demonstrate that, in many cases, random-data distortion preserves very little data privacy. The analytical framework presented in this paper also points out several possible avenues for the development of new privacy-preserving data-mining techniques. Examples include algorithms that explicitly guard against privacy breaches through linear transformations, exploiting multiplicative and colored noise for preserving privacy in data mining applications.",2005,Knowledge and Information Systems volume 7 issue 4 pp 387-414,random matrix;information extraction;theoretical computer science;computer security;data mining;database;machine learning;statistics;computer science;
TSP: Mining top-k closed sequential patterns,Petre Tzvetkov (University of Illinois at Urbana–Champaign);Xifeng Yan (University of Illinois at Urbana–Champaign);Jiawei Han (University of Illinois at Urbana–Champaign);,"2281041638,2116657824,2121939561","Sequential pattern mining has been studied extensively in the data mining community. Most previous studies require the specification of a min_support threshold for mining a complete set of sequential patterns satisfying the threshold. However, in practice, it is difficult for users to provide an appropriate min_support threshold. To overcome this difficulty, we propose an alternative mining task: mining top-k frequent closed sequential patterns of length no less than min_l, where k is the desired number of closed sequential patterns to be mined and min_l is the minimal length of each pattern. We mine the set of closed patterns because it is a compact representation of the complete set of frequent patterns. An efficient algorithm, called TSP, is developed for mining such patterns without min_support. Starting at (absolute) min_support=1, the algorithm makes use of the length constraint and the properties of top-k closed sequential patterns to perform dynamic support raising and projected database pruning. Our extensive performance study shows that TSP has high performance. In most cases, it outperforms the efficient closed sequential pattern-mining algorithm, CloSpan, even when the latter is running with the best tuned min_support threshold. Thus, we conclude that, for sequential pattern mining, mining top-k frequent closed sequential patterns without min_support is more preferable than the traditional min_support-based mining.",2005,Knowledge and Information Systems volume 7 issue 4 pp 438-457,sequential pattern mining;sequence;satisfiability;data analysis;information extraction;data mining;machine learning;algorithm;computer science;mathematics;
Reliable detection of episodes in event sequences,Robert Gwadera (Purdue University);Mikhail J. Atallah (Purdue University);Wojciech Szpankowski (Purdue University);,"2308751569,1984264859,2006075200","Suppose one wants to detect bad or suspicious subsequences in event sequences. Whether an observed pattern of activity (in the form of a particular subsequence) is significant and should be a cause for alarm depends on how likely it is to occur fortuitously. A long-enough sequence of observed events will almost certainly contain any subsequence, and setting thresholds for alarm is an important issue in a monitoring system that seeks to avoid false alarms. Suppose a long sequence, T, of observed events contains a suspicious subsequence pattern, S, within it, where the suspicious subsequence S consists of m events and spans a window of size w within T. We address the fundamental problem: Is a certain number of occurrences of a particular subsequence unlikely to be generated by randomness itself (i.e. indicative of suspicious activity)? If the probability of an occurrence generated by randomness is high and an automated monitoring system flags it as suspicious anyway, then such a system will suffer from generating too many false alarms. This paper quantifies the probability of such an S occurring in T within a window of size w, the number of distinct windows containing S as a subsequence, the expected number of such occurrences, its variance, and establishes its limiting distribution that allows setting up an alarm threshold so that the probability of false alarms is very small. We report on experiments confirming the theory and showing that we can detect bad subsequences with low false alarm rate.",2005,Knowledge and Information Systems volume 7 issue 4 pp 415-437,constant false alarm rate;probabilistic analysis of algorithms;pattern matching;data analysis;information extraction;data mining;pattern recognition;algorithm;computer science;mathematics;
On the use of hierarchical information in sequential mining-based XML document similarity computation,Ho-pong Leung (Hong Kong Polytechnic University);Fu-lai Chung (Hong Kong Polytechnic University);Stephen Chi-fai Chan (Hong Kong Polytechnic University);,"2164483261,2469582652,2167014661","Measuring the structural similarity among XML documents is the task of finding their semantic correspondence and is fundamental to many web-based applications. While there exist several methods to address the problem, the data mining approach seems to be a novel, interesting and promising one. It explores the idea of extracting paths from XML documents, encoding them as sequences and finding the maximal frequent sequences using the sequential pattern mining algorithms. In view of the deficiencies encountered by ignoring the hierarchical information in encoding the paths for mining, a new sequential pattern mining scheme for XML document similarity computation is proposed in this paper. It makes use of a preorder tree representation (PTR) to encode the XML tree’s paths so that both the semantics of the elements and the hierarchical structure of the document can be taken into account when computing the structural similarity among documents. In addition, it proposes a postprocessing step to reuse the mined patterns to estimate the similarity of unmatched elements so that another metric to qualify the similarity between XML documents can be introduced. Encouraging experimental results were obtained and reported.",2005,Knowledge and Information Systems volume 7 issue 4 pp 476-498,well formed document;xml validation;xml framework;simple api for xml;xml schema;xml;information extraction;world wide web;data mining;database;computer science;
Efficient mining of sequential patterns with time constraints by delimited pattern growth,Ming-Yen Lin (Feng Chia University);Suh-Yin Lee (National Chiao Tung University);,"2135906478,2251835087","An active research topic in data mining is the discovery of sequential patterns, which finds all frequent subsequences in a sequence database. The generalized sequential pattern (GSP) algorithm was proposed to solve the mining of sequential patterns with time constraints, such as time gaps and sliding time windows. Recent studies indicate that the pattern-growth methodology could speed up sequence mining. However, the capabilities to mine sequential patterns with time constraints were previously available only within the Apriori framework. Therefore, we propose the DELISP (delimited sequential pattern) approach to provide the capabilities within the pattern-growth methodology. DELISP features in reducing the size of projected databases by bounded and windowed projection techniques. Bounded projection keeps only time-gap valid subsequences and windowed projection saves nonredundant subsequences satisfying the sliding time-window constraint. Furthermore, the delimited growth technique directly generates constraint-satisfactory patterns and speeds up the pattern growing process. The comprehensive experiments conducted show that DELISP has good scalability and outperforms the well-known GSP algorithm in the discovery of sequential patterns with time constraints.",2005,Knowledge and Information Systems volume 7 issue 4 pp 499-514,sequential pattern mining;scalability;satisfiability;data analysis;information extraction;data mining;database;artificial intelligence;algorithm;computer science;
A high-performance distributed algorithm for mining association rules,Assaf Schuster (Technion – Israel Institute of Technology);Ran Wolff (Technion – Israel Institute of Technology);Dan Trock (Technion – Israel Institute of Technology);,"2135728993,2096432594,2000178603","We present a new distributed association rule mining (D-ARM) algorithm that demonstrates superlinear speed-up with the number of computing nodes. The algorithm is the first D-ARM algorithm to perform a single scan over the database. As such, its performance is unmatched by any previous algorithm. Scale-up experiments over standard synthetic benchmarks demonstrate stable run time regardless of the number of computers. Theoretical analysis reveals a tighter bound on error probability than the one shown in the corresponding sequential algorithm. As a result of this tighter bound and by utilizing the combined memory of several computers, the algorithm generates far fewer candidates than comparable sequential algorithms--the same order of magnitude as the optimum.",2005,Knowledge and Information Systems volume 7 issue 4 pp 458-475,suurballe s algorithm;suzuki kasami algorithm;fsa red algorithm;population based incremental learning;freivalds algorithm;gsp algorithm;nondeterministic algorithm;association rule learning;parallel algorithm;information extraction;distributed algorithm;data mining;artificial intelligence;algorithm;computer science;
Exact indexing of dynamic time warping,"Eamonn Keogh (University of California, Riverside);Chotirat Ann Ratanamahatana (University of California, Riverside);","2170070822,2091934636","The problem of indexing time series has attracted much interest. Most algorithms used to index time series utilize the Euclidean distance or some variation thereof. However, it has been forcefully shown that the Euclidean distance is a very brittle distance measure. Dynamic time warping (DTW) is a much more robust distance measure for time series, allowing similar shapes to match even if they are out of phase in the time axis. Because of this flexibility, DTW is widely used in science, medicine, industry and finance. Unfortunately, however, DTW does not obey the triangular inequality and thus has resisted attempts at exact indexing. Instead, many researchers have introduced approximate indexing techniques or abandoned the idea of indexing and concentrated on speeding up sequential searches. In this work, we introduce a novel technique for the exact indexing of DTW. We prove that our method guarantees no false dismissals and we demonstrate its vast superiority over all competing approaches in the largest and most comprehensive set of time series indexing experiments ever undertaken.",2005,Knowledge and Information Systems volume 7 issue 3 pp 358-386,dynamic time warping;linear search;search engine indexing;euclidean distance;time series;upper and lower bounds;data mining;database;artificial intelligence;machine learning;mathematical optimization;statistics;algorithm;computer science;mathematics;
Semisupervised learning from different information sources,Tao Li (University of Rochester);Mitsunori Ogihara (University of Rochester);,"2472069284,2123444417","This paper studies the use of a semisupervised learning algorithm from different information sources. We first offer a theoretical explanation as to why minimising the disagreement between individual models could lead to the performance improvement. Based on the observation, this paper proposes a semisupervised learning approach that attempts to minimise this disagreement by employing a co-updating method and making use of both labeled and unlabeled data. Three experiments to test the effectiveness of the approach are presented in this paper: (i) webpage classification from both content and hyperlinks; (ii) functional classification of gene using gene expression data and phylogenetic data and (iii) machine self-maintaining from both sensory and image data. The results show the effectiveness and efficiency of our approach and suggest its application potentials.",2005,Knowledge and Information Systems volume 7 issue 3 pp 289-309,support vector machine;semi supervised learning;data mining;artificial intelligence;machine learning;computer science;
Using Markov chain Monte Carlo and dynamic programming for event sequence data,Marko Salmenkivi (University of Helsinki);Heikki Mannila (University of Helsinki);,"1028827500,310734946","Sequences of events are a common type of data in various scientific and business applications, e.g. telecommunication network management, study of web access logs, biostatistics and epidemiology. A natural approach to modelling event sequences is using time-dependent intensity functions, indicating the expected number of events per time unit. In Bayesian modelling, piecewise constant functions can be utilized to model continuous intensities, if the number of segments is a model parameter. The reversible jump Markov chain Monte Carlo (RJMCMC) methods can be exploited in the data analysis. With very large quantities, these approaches may be too slow. We study dynamic programming algorithms for finding the best fitting piecewise constant intensity function, given a number of pieces. We introduce simple heuristics for pruning the number of the potential change points of the functions. Empirical evidence from trials on real and artificial data sets is provided, showing that the developed methods yield high performance and they can be applied to very large data sets. We also compare the RJMCMC and dynamic programming approaches and show that the results correspond closely. The methods are applied to fault-alarm sequences produced by large telecommunication networks.",2005,Knowledge and Information Systems volume 7 issue 3 pp 267-288,markov chain monte carlo;dynamic programming;information extraction;epidemiology;econometrics;simulation;statistics;computer science;
Architecture for knowledge discovery and knowledge management,S. K. Gupta (Indian Institutes of Technology);Vasudha Bhatnagar (University of Delhi);Siri Krishan Wasan (Jamia Millia Islamia);,"2128855107,2590701236,2045482139","In this paper, we propose I-MIN model for knowledge discovery and knowledge management in evolving databases. The model splits the KDD process into three phases. The schema designed during the first phase, abstracts the generic mining requirements of the KDD process and provides a mapping between the generic KDD process and (user) specific KDD subprocesses. The generic process is executed periodically during the second phase and windows of condensed knowledge called knowledge concentrates are created. During the third phase, which corresponds to actual mining by the end users, specific KDD subprocesses are invoked to mine knowledge concentrates. The model provides a set of mining operators for the development of mining applications to discover and renew, preserve and reuse, and share knowledge for effective knowledge management. These operators can be invoked by either using a declarative query language or by writing applications.The architectural proposal emulates a DBMS like environment for the managers, administrators and end users in the organization. Knowledge management functions, like sharing and reuse of the discovered knowledge among the users and periodic updating of the discovered knowledge are supported. Complete documentation and control of all the KDD endeavors in an organization are facilitated by the I-MIN model. This helps in structuring and streamlining the KDD operations in an organization.",2005,Knowledge and Information Systems volume 7 issue 3 pp 310-336,declarative programming;body of knowledge;query language;knowledge engineering;knowledge extraction;knowledge management;data mining;database;artificial intelligence;computer science;
CVA file: an index structure for high-dimensional datasets,Jiyuan An (University of Tsukuba);Hanxiong Chen (University of Tsukuba);Kazutaka Furuse (University of Tsukuba);Nobuo Ohbo (University of Tsukuba);,"2260082823,2096892702,1467882240,253148662","Similarity search is important in information-retrieval applications where objects are usually represented as vectors of high dimensionality. This paper proposes a new dimensionality-reduction technique and an indexing mechanism for high-dimensional datasets. The proposed technique reduces the dimensions for which coordinates are less than a critical value with respect to each data vector. This flexible datawise dimensionality reduction contributes to improving indexing mechanisms for high-dimensional datasets that are in skewed distributions in all coordinates. To apply the proposed technique to information retrieval, a CVA file (compact VA file), which is a revised version of the VA file is developed. By using a CVA file, the size of index files is reduced further, while the tightness of the index bounds is held maximally. The effectiveness is confirmed by synthetic and real data.",2005,Knowledge and Information Systems volume 7 issue 3 pp 337-357,zipf s law;data structure;data science;data mining;database;statistics;computer science;
Multiknowledge for decision making,Qingxiang Wu (Ulster University);David A. Bell (Queen's University);T. Martin McGinnity (Ulster University);,"2112592308,2655831120,2041547936","The representation of knowledge has an important effect on automated decision-making. In this paper, vector spaces are used to describe a condition space and a decision space, and knowledge is represented by a mapping from the condition space to the decision space. Many such mappings can be obtained from a training set. A set of mappings, which are created from multiple reducts in the training set, is defined as multiknowledge. In order to get a good reduct and find multiple reducts, the WADF (worst-attribute-drop-first) algorithm is developed through analysis of the properties of decision systems using rough set theory. An approach that combines multiknowledge and the naive Bayes classifier is applied to make decisions for unseen instances or for instances with missing attribute values. Benchmark data sets from the UCI Machine Learning Repository are used to test the algorithms. The experimental results are encouraging; the prediction accuracy for unseen instances by using the algorithms is higher than by using other approaches based on a single body of knowledge.",2005,Knowledge and Information Systems volume 7 issue 2 pp 246-266,rough set;decision support system;data mining;pattern recognition;artificial intelligence;machine learning;computer science;mathematics;
A quantitative analysis of the robustness of knowledge-based systems through degradation studies,Perry Groot (VU University Amsterdam);Annette ten Teije (VU University Amsterdam);Frank van Harmelen (VU University Amsterdam);,"2106856679,2297608794,333553561","The overall aim of this paper is to provide a general setting for quantitative quality measures of knowledge-based system behaviour that is widely applicable to many knowledge-based systems. We propose a general approach that we call degradation studies: an analysis of how system output changes as a function of degrading system input, such as incomplete or incorrect data or knowledge. To show the feasibility of our approach, we have applied it in a case study. We have taken a large and realistic vegetation-classification system, and have analysed its behaviour under various varieties of incomplete and incorrect input. This case study shows that degradation studies can reveal interesting and surprising properties of the system under study.",2005,Knowledge and Information Systems volume 7 issue 2 pp 224-245,complete information;missing data;quality control;quantitative analysis;robustness;knowledge based systems;operations research;data mining;artificial intelligence;computer science;
An evolutionary approach for automatically extracting intelligible classification rules,I. De Falco (National Research Council);A. Della Cioppa (University of Salerno);A. Iazzetta (National Research Council);E. Tarantino (National Research Council);,"2128030985,372004123,2054677911,1901590524","The process of automatically extracting novel, useful and ultimately comprehensible information from large databases, known as data mining, has become of great importance due to the ever-increasing amounts of data collected by large organizations. In particular, the emphasis is devoted to heuristic search methods able to discover patterns that are hard or impossible to detect using standard query mechanisms and classical statistical techniques. In this paper an evolutionary system capable of extracting explicit classification rules is presented. Special interest is dedicated to find easily interpretable rules that may be used to make crucial decisions. A comparison with the findings achieved by other methods on a real problem, the breast cancer diagnosis, is performed.",2005,Knowledge and Information Systems volume 7 issue 2 pp 179-201,search algorithm;data collection;biological classification;data analysis;evolutionary algorithm;data mining;artificial intelligence;statistics;algorithm;computer science;
Accelerating EM clustering to find high-quality solutions,Carlos Ordonez (NCR Corporation);Edward Omiecinski (Georgia Institute of Technology);,"2165938062,2174003361","Clustering is one of the most important techniques used in data mining. This article focuses on the EM clustering algorithm. Two fundamental aspects are studied: achieving faster convergence and finding higher quality clustering solutions. This work introduces several improvements to the EM clustering algorithm, being periodic M steps during initial iterations, reseeding of low-weight clusters and splitting of high-weight clusters the most important. These improvements lead to two important parameters. The first parameter is the number of M steps per iteration and the second one, a weight threshold to reseed low-weight clusters. Experiments show how frequently the M step must be executed and what weight threshold values make EM reach higher quality solutions. In general, the improved EM clustering algorithm finds higher quality solutions than the classical EM algorithm and converges in fewer iterations.",2005,Knowledge and Information Systems volume 7 issue 2 pp 135-157,k medians clustering;canopy clustering algorithm;correlation clustering;constrained clustering;data stream clustering;cure data clustering algorithm;affinity propagation;cluster analysis;information extraction;data mining;machine learning;statistics;algorithm;computer science;mathematics;
A statistical framework for mining substitution rules,Wei-Guang Teng (National Taiwan University);Ming-Jyh Hsieh (National Taiwan University);Ming-Syan Chen (National Taiwan University);,"1985097037,2119755336,2122365371","In this paper, a new mining capability, called mining of substitution rules, is explored. A substitution refers to the choice made by a customer to replace the purchase of some items with that of others. The mining of substitution rules in a transaction database, the same as that of association rules, will lead to very valuable knowledge in various aspects, including market prediction, user behaviour analysis and decision support. The process of mining substitution rules can be decomposed into two procedures. The first procedure is to identify concrete itemsets among a large number of frequent itemsets, where a concrete itemset is a frequent itemset whose items are statistically dependent. The second procedure is then on the substitution rule generation. In this paper, we first derive theoretical properties for the model of substitution rule mining and devise a technique on the induction of positive itemset supports to improve the efficiency of support counting for negative itemsets. Then, in light of these properties, the SRM (substitution rule mining) algorithm is designed and implemented to discover the substitution rules efficiently while attaining good statistical significance. Empirical studies are performed to evaluate the performance of the SRM algorithm proposed. It is shown that the SRM algorithm not only has very good execution efficiency but also produces substitution rules of very high quality.",2005,Knowledge and Information Systems volume 7 issue 2 pp 158-178,integration by substitution;transaction processing;association rule learning;association;empirical research;statistical significance;decision support system;data mining;artificial intelligence;algorithm;computer science;
MotifMiner: Efficient discovery of common substructures in biochemical molecules,Matt Coatney (Ohio State University);Srinivasan Parthasarathy (Ohio State University);,"2071321552,2106796124","Biochemical research often involves examining structural relationships in molecules since scientists strongly believe in the causal relationship between structure and function. Traditionally, researchers have identified these patterns, or motifs, manually using domain expertise. However, with the massive influx of new biochemical data and the ability to gather data for very large molecules, there is great need for techniques that automatically and efficiently identify commonly occurring structural patterns in molecules. Previous automated substructure discovery approaches have each introduced variations of similar underlying techniques and have embedded domain knowledge. While doing so improves performance for the particular domain, this complicates extensibility to other domains. Also, they do not address scalability or noise, which is critical for macromolecules such as proteins. In this paper, we present MotifMiner, a general framework for efficiently identifying common motifs in most scientific molecular datasets. The approach combines structure-based frequent-pattern discovery with search space reduction and coordinate noise handling. We describe both the framework and several algorithms as well as demonstrate the flexibility of our system by analyzing protein and drug biochemical datasets.",2005,Knowledge and Information Systems volume 7 issue 2 pp 202-223,macromolecule;cheminformatics;data science;bioinformatics;data mining;database;artificial intelligence;machine learning;computer science;
Building Minority Language Corpora by Learning to Generate Web Search Queries,Rayid Ghani (Accenture);Rosie Jones (Carnegie Mellon University);Dunja Mladenic (Carnegie Mellon University);,"2117205193,2128530851,2282522104","The Web is a source of valuable information, but the process of collecting, organizing, and effectively utilizing the resources it contains is difficult. We describe CorpusBuilder, an approach for automatically generating Web search queries for collecting documents matching a minority concept. The concept used for this paper is that of text documents belonging to a minority natural language on the Web. Individual documents are automatically labeled as relevant or nonrelevant using a language filter, and the feedback is used to learn what query lengths and inclusion/exclusion term-selection methods are helpful for finding previously unseen documents in the target language. Our system learns to select good query terms using a variety of term scoring methods. Using odds ratio scores calculated over the documents acquired was one of the most consistently accurate query-generation methods. To reduce the number of estimated parameters, we parameterize the query length using a Gamma distribution and present empirical results with learning methods that vary the time horizon used when learning from the results of past queries. We find that our system performs well whether we initialize it with a whole document or with a handful of words elicited from a user. Experiments applying the same approach to multiple languages are also presented showing that our approach generalizes well across several languages regardless of the initial conditions.",2005,Knowledge and Information Systems volume 7 issue 1 pp 56-83,web search query;web query classification;gamma distribution;query language;odds ratio;natural language;initial value problem;the internet;web mining;world wide web;information retrieval;data mining;database;artificial intelligence;machine learning;statistics;computer science;
Collaborative Filtering Using a Regression-Based Approach,Slobodan Vucetic (Temple University);Zoran Obradovic (Temple University);,"2012499344,2029694244","The task of collaborative filtering is to predict the preferences of an active user for unseen items given preferences of other users. These preferences are typically expressed as numerical ratings. In this paper, we propose a novel regression-based approach that first learns a number of experts describing relationships in ratings between pairs of items. Based on ratings provided by an active user for some of the items, the experts are combined by using statistical methods to predict the user’s preferences for the remaining items. The approach was designed to efficiently address the problem of data sparsity and prediction latency that characterise collaborative filtering. Extensive experiments on Eachmovie and Jester benchmark collaborative filtering data show that the proposed regression-based approach achieves improved accuracy and is orders of magnitude faster than the popular neighbour-based alternative. The difference in accuracy was more evident when the number of ratings provided by an active user was small, as is common for real-life recommendation systems. Additional benefits were observed in predicting items with large rating variability. To provide a more detailed characterisation of the proposed algorithm, additional experiments were performed on synthetic data with second-order statistics similar to that of the Eachmovie data. Strong experimental evidence was obtained that the proposed approach can be applied to data over a large range of sparsity scenarios and is superior to non-personalised predictors even when ratings data are very sparse.",2005,Knowledge and Information Systems volume 7 issue 1 pp 1-22,collaborative filtering;synthetic data;regression;recommender system;world wide web;information retrieval;data mining;database;artificial intelligence;machine learning;statistics;computer science;
Designing Triggers with Trigger-By-Example,"Dongwon Lee (Pennsylvania State University);Wenlei Mao (University of California, Los Angeles);Henry Chiu (IBM);Wesley W. Chu (University of California, Los Angeles);","2141172858,2142801800,2102657940,2168131383","One of the obstacles that hinder database trigger systems from their wide deployment is the lack of tools that aid users in creating trigger rules. Similar to understanding and specifying database queries in SQL3, it is difficult to visualize the meaning of trigger rules. Furthermore, it is even more difficult to write trigger rules using such text-based trigger rule languages as SQL3. In this paper, we propose TBE (Trigger-By-Example) to remedy such problems in writing trigger rules visually by using QBE (Query-By-Example) ideas. TBE is a visual trigger rule composition system that helps the users understand and specify active database triggers. TBE retains benefits of QBE while extending features to support triggers. Hence, TBE is a useful tool for novice users to create simple triggers in a visual and intuitive manner. Further, since TBE is designed to hide the details of underlying trigger systems from users, it can be used as a universal trigger interface.",2005,Knowledge and Information Systems volume 7 issue 1 pp 110-134,query by example;knowledge base;world wide web;data mining;database;computer science;
ENDL: A Logical Framework for Verifying Secure Transaction Protocols,"Qingfeng Chen (University of Technology, Sydney);Chengqi Zhang (University of Technology, Sydney);Shichao Zhang (University of Technology, Sydney);","2641684909,2166080598,2713654059","This paper proposes a new logic for verifying secure transaction protocols. We have named this logic the ENDL (extension of non-monotonic dynamic logic). In this logic, timestamps and signed certificates are used for protecting against replays of old keys or the substitution of bogus keys. The logic is useful for verifying the authentication properties of secure protocols, and especially for protecting data integrity. To evaluate the logic, three practical instances of secure protocols are illustrated. This evaluation demonstrates that the ENDL is effective and promising.",2005,Knowledge and Information Systems volume 7 issue 1 pp 84-109,authentication;e commerce;information security;computer security;data mining;database;algorithm;computer science;
Emergence of Cooperative Internet Server Sharing Among Internet Search Agents Caught in the n-Person Prisoner's Dilemma Game,Jae C. Oh (Syracuse University);,2098251512,"Information on the Internet can be collected by autonomous agents that send out queries to the servers that may have the information sought. From a single agent’s perspective, sending out as many queries as possible maximizes the chances of finding the information sought. However, if every agent does the same, the servers will be overloaded. The first major contribution of this paper is proving mathematically that the agents situated in such environments play the n -Person Prisoner’s Dilemma Game. The second is mathematically deriving the notion of effectiveness of cooperation among the agents in such environments and then presenting the optimal interval for the number of information sites for a given number of information-seeking agents. When the optimal interval is satisfied, cooperation among agents is effective, meaning that resources (e.g., servers) are optimally shared. Experimental results suggest that agents can better share available servers through the kinship-based cooperation without explicitly knowing about the entire environment. This paper also identifies difficulties of promoting cooperation in such environments and presents possible solutions. The long-term goal of this research is to elucidate the understanding of massively distributed multiagent environments such as the Internet and to identify valuable design principles of software agents in similar environments.",2005,Knowledge and Information Systems volume 7 issue 1 pp 23-55,autonomous system;autonomous agent;software agent;prisoner s dilemma;emergence;cooperation;the internet;satisfiability;intelligent agent;computer security;artificial intelligence;simulation;computer science;
The k -Nearest Neighbour Join: Turbo Charging the KDD Process,Christian Böhm (Ludwig Maximilian University of Munich);Florian Krebs (Ludwig Maximilian University of Munich);,"2486446532,2078393825","The similarity join has become an important database primitive for supporting similarity searches and data mining. A similarity join combines two sets of complex objects such that the result contains all pairs of similar objects. Two types of the similarity join are well-known, the distance range join, in which the user defines a distance threshold for the join, and the closest pair query or k -distance join, which retrieves the k most similar pairs. In this paper, we propose an important, third similarity join operation called the k -nearest neighbour join, which combines each point of one point set with its k nearest neighbours in the other set. We discover that many standard algorithms of Knowledge Discovery in Databases (KDD) such as k -means and k -medoid clustering, nearest neighbour classification, data cleansing, postprocessing of sampling-based data mining, etc. can be implemented on top of the k -nn join operation to achieve performance improvements without affecting the quality of the result of these algorithms. We propose a new algorithm to compute the k -nearest neighbour join using the multipage index (MuX), a specialised index structure for the similarity join. To reduce both CPU and I/O costs, we develop optimal loading and processing strategies.",2004,Knowledge and Information Systems volume 6 issue 6 pp 728-749,sort merge join;hash join;join dependency;k means clustering;k nearest neighbors algorithm;information extraction;data mining;database;machine learning;computer science;mathematics;
Characterizing and Mining the Citation Graph of the Computer Science Literature,Yuan An (University of Toronto);Jeannette C. M. Janssen (Dalhousie University);Evangelos E. Milios (Dalhousie University);,"2122150120,2110639033,2231563531","Citation graphs representing a body of scientific literature convey measures of scholarly activity and productivity. In this work we present a study of the structure of the citation graph of the computer science literature. Using a web robot we built several topic-specific citation graphs and their union graph from the digital library ResearchIndex. After verifying that the degree distributions follow a power law, we applied a series of graph theoretical algorithms to elicit an aggregate picture of the citation graph in terms of its connectivity. We discovered the existence of a single large weakly-connected and a single large biconnected component, and confirmed the expected lack of a large strongly-connected component. The large components remained even after removing the strongest authority nodes or the strongest hub nodes, indicating that such tight connectivity is widespread and does not depend on a small subset of important nodes. Finally, minimum cuts between authority papers of different areas did not result in a balanced partitioning of the graph into areas, pointing to the need for more sophisticated algorithms for clustering the graph.",2004,Knowledge and Information Systems volume 6 issue 6 pp 664-678,complement graph;spqr tree;null graph;critical graph;graph;strongly connected component;minimum cut;degree distribution;connectivity;power law;the internet;information system;digital library;graph theory;theoretical computer science;data mining;database;artificial intelligence;machine learning;computer science;mathematics;
Document Similarity Using a Phrase Indexing Graph Model,Khaled M. Hammouda (University of Waterloo);Mohamed S. Kamel (University of Waterloo);,"1268727595,2096364092","Document clustering techniques mostly rely on single term analysis of text, such as the vector space model. To better capture the structure of documents, the underlying data model should be able to represent the phrases in the document as well as single terms. We present a novel data model, the Document Index Graph, which indexes Web documents based on phrases rather than on single terms only. The semistructured Web documents help in identifying potential phrases that when matched with other documents indicate strong similarity between the documents. The Document Index Graph captures this information, and finding significant matching phrases between documents becomes easy and efficient with such model. The model is flexible in that it could revert to a compact representation of the vector space model if we choose not to index phrases. However, using phrase indexing yields more accurate document similarity calculations. The similarity between documents is based on both single term weights and matching phrase weights. The combined similarities are used with standard document clustering techniques to test their effect on the clustering quality. Experimental results show that our phrase-based similarity, combined with single-term similarity measures, gives a more accurate measure of document similarity and thus significantly enhances Web document clustering quality.",2004,Knowledge and Information Systems volume 6 issue 6 pp 710-727,document structure description;vector space model;service quality;search engine indexing;data model;the internet;document clustering;graph theory;world wide web;information retrieval;data mining;database;pattern recognition;computer science;
An Efficient Density-based Approach for Data Mining Tasks,"Carlotta Domeniconi (George Mason University);Dimitrios Gunopulos (University of California, Riverside);","45678088,1656275121","We propose a locally adaptive technique to address the problem of setting the bandwidth parameters for kernel density estimation. Our technique is efficient and can be performed in only two dataset passes. We also show how to apply our technique to efficiently solve range query approximation, classification and clustering problems for very large datasets. We validate the efficiency and accuracy of our technique by presenting experimental results on a variety of both synthetic and real datasets.",2004,Knowledge and Information Systems volume 6 issue 6 pp 750-770,kernel density estimation;information extraction;computational geometry;econometrics;data mining;machine learning;statistics;computer science;
Managing Multiuser Database Buffers Using Data Mining Techniques,Ling Feng (University of Twente);Hongjun Lu (Hong Kong University of Science and Technology);,"2707183811,2131290459","In this paper, we propose a data-mining-based approach to public buffer management for a multiuser database system, where database buffers are organized into two areas – public and private . While the private buffer areas contain pages to be updated by particular users, the public buffer area contains pages shared among different users. Unlike traditional buffer management strategies where limited knowledge of user access patterns is used, the proposed approach discovers knowledge from page access sequences of user transactions and uses it to guide public buffer placement and replacement . A prefetch strategy is exploited based on the discovered page access knowledge. In practice, to make such a data-mining-based buffer management approach tractable, we present a soft variation to approximate our absolute best buffer replacement solution. The knowledge to be discovered and the discovery methods are discussed in the paper. The effectiveness of the proposed approach was investigated through a simulation study. The results indicate that with the help of the discovered knowledge, the public buffer hit ratio can be improved significantly, while the added computational complexity, compared to the achievement in buffer hit ratio, is less. In some situations, the time cost of the data-mining-based buffer management policy is even lower than that of the simplest buffer management policy.",2004,Knowledge and Information Systems volume 6 issue 6 pp 679-709,instruction prefetch;placement;bicarbonate buffering system;transaction processing;computational complexity theory;data analysis;knowledge engineering;information extraction;world wide web;data mining;database;computer science;
Convex Hull Ensemble Machine for Regression and Classification,Yongdai Kim (Ewha Womans University);Jinseog Kim;,"2657282289,2662343321","We propose a new ensemble algorithm called Convex Hull Ensemble Machine (CHEM). CHEM in Hilbert space is first developed and modified for regression and classification problems. We prove that the ensemble model converges to the optimal model in Hilbert space under regularity conditions. Empirical studies reveal that, for classification problems, CHEM has a prediction accuracy similar to that of boosting, but CHEM is much more robust with respect to output noise and never overfits datasets even when boosting does. For regression problems, CHEM is competitive with other ensemble methods such as gradient boosting and bagging.",2004,Knowledge and Information Systems volume 6 issue 6 pp 645-663,gradient boosting;bootstrap aggregating;boosting;convex hull;ensembl;hilbert space;regression;empirical research;biological classification;regression analysis;ensemble learning;pattern recognition;machine learning;mathematical optimization;statistics;computer science;mathematics;
Editorial: Introducing the New Editor-in-Chief,Xindong Wu (University of Vermont);,2123651450,-,2004,Knowledge and Information Systems volume 6 issue 6 pp 643-644,-
Mining condensed frequent-pattern bases,Jian Pei (University at Buffalo);Guozhu Dong (Wright State University);Wei Zou (University of Illinois at Urbana–Champaign);Jiawei Han (University of Illinois at Urbana–Champaign);,"2126330539,2164298414,2309569033,2121939561","Frequent-pattern mining has been studied extensively and has many useful applications. However, frequent-pattern mining often generates too many patterns to be truly efficient or effective. In many applications, it is sufficient to generate and examine frequent patterns with a sufficiently good approximation of the support frequency instead of in full precision. Such a compact but ""close-enough"" frequent-pattern base is called a condensed frequent-pattern base.In this paper, we propose and examine several alternatives for the design, representation, and implementation of such condensed frequent-pattern bases. Several algorithms for computing such pattern bases are proposed. Their effectiveness at pattern compression and methods for efficiently computing them are investigated. A systematic performance study is conducted on different kinds of databases, and demonstrates the effectiveness and efficiency of our approach in handling frequent-pattern mining in large databases.",2004,Knowledge and Information Systems volume 6 issue 5 pp 570-594,compression;approximation;data compression;theoretical computer science;data mining;database;algorithm;computer science;
Query-sensitive similarity measures for information retrieval,Anastasios Tombros (University of Glasgow);C.J. van Rijsbergen (University of Glasgow);,"1901725433,1999182239","The application of document clustering to information retrieval has been motivated by the potential effectiveness gains postulated by the cluster hypothesis. The hypothesis states that relevant documents tend to be highly similar to each other and therefore tend to appear in the same clusters. In this paper we propose an axiomatic view of the hypothesis by suggesting that documents relevant to the same query (co-relevant documents) display an inherent similarity to each other that is dictated by the query itself. Because of this inherent similarity, the cluster hypothesis should be valid for any document collection. Our research describes an attempt to devise means by which this similarity can be detected. We propose the use of query-sensitive similarity measures that bias interdocument relationships toward pairs of documents that jointly possess attributes expressed in a query. We experimentally tested three query-sensitive measures against conventional ones that do not take the query into account, and we also examined the comparative effectiveness of the three query-sensitive measures. We calculated interdocument relationships for varying numbers of top-ranked documents for six document collections. Our results show a consistent and significant increase in the number of relevant documents that become nearest neighbors of any given relevant document when query-sensitive measures are used. These results suggest that the effectiveness of a cluster-based information retrieval system has the potential to increase through the use of query-sensitive similarity measures.",2004,Knowledge and Information Systems volume 6 issue 5 pp 617-642,ranking;cluster analysis;document clustering;information retrieval;data mining;pattern recognition;machine learning;computer science;mathematics;
Using artificial anomalies to detect unknown and known network intrusions,Wei Fan (IBM);Matthew Miller (Columbia University);Salvatore J. Stolfo (Columbia University);Wenke Lee (Georgia Institute of Technology);Philip K. Chan (Florida Institute of Technology);,"2491264568,2257985829,2021877992,2155626625,2098876098","Intrusion detection systems (IDSs) must be capable of detecting new and unknown attacks, or anomalies. We study the problem of building detection models for both pure anomaly detection and combined misuse and anomaly detection (i.e., detection of both known and unknown intrusions). We show the necessity of artificial anomalies by discussing the failure to use conventional inductive learning methods to detect anomalies. We propose an algorithm to generate artificial anomalies to coerce the inductive learner into discovering an accurate boundary between known classes (normal connections and known intrusions) and anomalies. Empirical studies show that our pure anomaly-detection model trained using normal and artificial anomalies is capable of detecting more than 77% of all unknown intrusion classes with more than 50% accuracy per intrusion class. The combined misuse and anomaly-detection models are as accurate as a pure misuse detection model in detecting known intrusions and are capable of detecting at least 50% of unknown intrusion classes with accuracy measurements between 75 and 100% per class.",2004,Knowledge and Information Systems volume 6 issue 5 pp 507-527,anomaly based intrusion detection system;intrusion detection system;anomaly detection;computer security;artificial intelligence;machine learning;computer science;
Logical comparison of inconsistent perspectives using scoring functions,Anthony Hunter (University College London);,2105909224,"The language for describing inconsistency is underdeveloped. If a database (a set of formulae) is inconsistent, there is usually no qualification of that inconsistency. Yet, it would seem useful to be able to say how inconsistent a database is, or to say whether one database is ""more inconsistent"" than another database. In this paper, we provide a more general characterization of inconsistency in terms of a scoring function for each database Δ. A scoring function S is from the power set of Δ into the natural numbers defined so that S(Γ) gives the number of minimally inconsistent subsets of Δ that would be eliminated if the subset Γ was removed from Δ. This characterization offers an expressive and succinct means for articulating, in general terms, the nature of inconsistency in a set of formulae. We then compare databases using their scoring functions. This gives an intuitive ordering relation over databases that we can describe as ""more inconsistent than"". These techniques are potentially useful in a wide range of problems including monitoring progress in negotiations between a number of participants, and in comparing heterogeneous sources of information.",2004,Knowledge and Information Systems volume 6 issue 5 pp 528-543,score;conflict resolution;data mining;database;artificial intelligence;statistics;algorithm;computer science;mathematics;
Towards personalised web intelligence,"Ah-Hwee Tan (Agency for Science, Technology and Research);Hwee-Leng Ong (Agency for Science, Technology and Research);Hong Pan (Agency for Science, Technology and Research);Jamie Ng (Agency for Science, Technology and Research);Qiu-Xiang Li (Agency for Science, Technology and Research);","2099104211,2608560740,2609246297,2692458183,2142782369","The Flexible Organizer for Competitive Intelligence (FOCI) is a personalised web intelligence system that provides an integrated platform for gathering, organising, tracking, and disseminating competitive information on the web. FOCI builds personalised information portfolios through a novel method called User-Configurable Clustering, which allows a user to personalise his/her portfolios in terms of the content as well as the organisational structure. This paper outlines the key challenges we face in personalised information management and gives a detailed account of FOCI's underlying personalisation mechanism. For a quantitative evaluation of the system's performance, we propose a set of performance indices based on information entropy that measures the degree of matching between a system-generated cluster structure and a user-preferred category organisation. Experimental results of a case study show that FOCI's personalisation increases the degree of matching tremendously after a reasonable number of operations. In addition, the personalised portfolios can be used to track and organise new information with a good level of performance.",2004,Knowledge and Information Systems volume 6 issue 5 pp 595-616,personalization;web intelligence;competitive intelligence;information management;multimedia;world wide web;data mining;database;artificial intelligence;machine learning;computer science;
Incorporating background knowledge for better prediction of cycle phases,Ursula Garczarek;Claus Weihs (Technical University of Dortmund);,"2672135748,197391645","When predicting the state of a system, we sometimes know that the succession of states is cyclic. This is for example true for the prediction of business cycle phases, where an upswing is always followed by upper turning points and the subsequent downswing passes via lower turning points over to the next upswing and so on. We present several ideas of how to implement this background knowledge in popular static classification methods. Additionally, we present a full dynamic model. The usefulness for the prediction of business cycles is investigated, revealing pitfalls and potential benefits of ideas.",2004,Knowledge and Information Systems volume 6 issue 5 pp 544-569,business cycle;bayesian network;prediction;biological classification;hidden markov model;operations research;artificial intelligence;machine learning;computer science;
Ontology Evolution: Not the Same as Schema Evolution,Natalya Fridman Noy (Stanford University);Michel C. A. Klein (VU University Amsterdam);,"2021138233,2132991267","As ontology development becomes a more ubiquitous and collaborative process, ontology versioning and evolution becomes an important area of ontology research. The many similarities between database-schema evolution and ontology evolution will allow us to build on the extensive research in schema evolution. However, there are also important differences between database schemas and ontologies. The differences stem from different usage paradigms, the presence of explicit semantics and different knowledge models. A lot of problems that existed only in theory in database research come to the forefront as practical problems in ontology evolution. These differences have important implications for the development of ontology-evolution frameworks: The traditional distinction between versioning and evolution is not applicable to ontologies. There are several dimensions along which compatibility between versions must be considered. The set of change operations for ontologies is different. We must develop automatic techniques for finding similarities and differences between versions.",2004,Knowledge and Information Systems volume 6 issue 4 pp 428-440,ontology components;ontology based data integration;process ontology;upper ontology;open knowledge base connectivity;ontology alignment;suggested upper merged ontology;ontology;knowledge interchange format;evolution;satisfiability;ontology;knowledge management;data mining;database;artificial intelligence;computer science;
Ontologies: How can They be Built?,Helena Sofia Andrade N. P. Pinto (Instituto Superior Técnico);João Pavão Martins (Instituto Superior Técnico);,"2163874071,2475760377","Ontologies are an important component in many areas, such as knowledge management and organization, electronic commerce and information retrieval and extraction. Several methodologies for ontology building have been proposed. In this article, we provide an overview of ontology building. We start by characterizing the ontology building process and its life cycle. We present the most representative methodologies for building ontologies from scratch, and the proposed techniques, guidelines and methods to help in the construction task. We analyze and compare these methodologies. We describe current research issues in ontology reuse. Finally, we discuss the current trends in ontology building and its future challenges, namely, the new issues for building ontologies for the Semantic Web.",2004,Knowledge and Information Systems volume 6 issue 4 pp 441-464,bibliographic ontology;idef5;ontology based data integration;process ontology;upper ontology;ontology alignment;suggested upper merged ontology;ontology;semantic web;software development process;biological life cycle;information system;ontology;knowledge management;world wide web;data mining;database;computer science;
Ontologies for Knowledge Management: An Information Systems Perspective,Igor Jurisica (University of Toronto);John Mylopoulos (University of Toronto);Eric S. K. Yu (University of Toronto);,"164646474,2161153869,2159496315","Knowledge management research focuses on concepts, methods, and tools supporting the management of human knowledge. The main objective of this paper is to survey basic concepts that have been used in computer science for the representation of knowledge and summarize some of their advantages and drawbacks. A secondary objective is to relate these techniques to information science theory and practice.The survey classifies the concepts used for knowledge representation into four broad ontological categories. Static ontologies describe static aspects of the world, i.e., what things exist, their attributes and relationships. A dynamic ontology, on the other hand, describes the changing aspects of the world in terms of states, state transitions and processes. Intentional ontologies encompass the world of things agents believe in, want, prove or disprove, and argue about. Finally, social ontologies cover social settings – agents, positions, roles, authority, permanent organizational structures or shifting networks of alliances and interdependencies.",2004,Knowledge and Information Systems volume 6 issue 4 pp 380-401,information system;ontology;information science;information technology;data processing;knowledge management;data mining;artificial intelligence;computer science;
Addressing the Ontology Acquisition Bottleneck Through Reverse Ontological Engineering,Debbie Richards (Macquarie University);,2156772009,"The use of ontologies in knowledge engineering arose as a solution to the difficulties associated with acquiring knowledge, commonly referred to as the knowledge acquisition bottleneck. The knowledge-level model represented in an ontology provides a much more structured and principled approach compared with earlier transfer-of-symbolic-knowledge approaches but brings with it a new problem, which can be termed the ontology-acquisition (and maintenance) bottleneck. Each ontological approach offers a different structure, different terms and different meanings for those terms. The unifying theme across approaches is the considerable effort associated with developing, validating and connecting ontologies. We propose an approach to engineering ontologies by retrospectively and automatically discovering them from existing data and knowledge sources in the organization. The method offered assists in the identification of similar and different terms and includes strategies for developing a shared ontology. The approach uses a human-centered, concept-based knowledge processing technique, known as formal concept analysis, to generate an ontology from examples. To assist classification of examples and to identify the salient features of the example, we use a rapid and incremental knowledge acquisition and representation technique, known as ripple-down rules. The method can be used as an alternative or complement to other approaches.",2004,Knowledge and Information Systems volume 6 issue 4 pp 402-427,idef5;process ontology;upper ontology;formal concept analysis;body of knowledge;ontology;case based reasoning;knowledge based systems;knowledge management;data mining;database;artificial intelligence;computer science;
PowerDB-IR --- Scalable Information Retrieval and Storage with a Cluster of Databases,Torsten Grabs (ETH Zurich);Klemens Böhm (Karlsruhe Institute of Technology);Hans-Jörg Schek (ETH Zurich);,"2289987644,2245547659,2050071469","Our objective is a scalable infrastructure for information retrieval (IR) with up-to-date retrieval results in the presence of updates. Timely processing of updates is important with novel application domains such as e-commerce. These issues are challenging, given the additional requirement that the system must scale well. We have built PowerDB-IR, a system that has the characteristics sought. This article describes its design, implementation, and evaluation. We follow a three-tier architecture with a database cluster as the bottom layer for storage management. The rationale for a database cluster is to ‘scale out’, i.e., to add further cluster nodes, whenever necessary for better performance. The middle tier provides IR-specific retrieval and update services. We deploy state-of-the-art middleware software to coordinate the cluster and to invoke IR-specific components. PowerDB-IR extends the middleware layer with service decomposition and parallelisation. PowerDB-IR has the following features: It supports state-of-the-art retrieval models such as vector space retrieval. It allows documents to be inserted and retrieved concurrently and ensures up-to-date retrieval results with almost no overhead. PowerDB-IR ensures the correctness of global concurrency and recovery. Alternative physical data organisation schemes and respective query processing techniques provide adequate performance for different workloads and database sizes. Scaling out the database cluster yields higher throughput and lower response times. We have run extensive experiments with PowerDB-IR using several commercial database systems as well as different middleware products. Further experiments have quantified the effect of transactional guarantees on performance. The main result is that PowerDB-IR shows surprisingly good scalability and low response times.",2004,Knowledge and Information Systems volume 6 issue 4 pp 465-505,middleware;vector space;e commerce;world wide web;data mining;database;computer science;
Editorial: Ontology Challenges: A Thumbnail Historical Perspective,James Geller (New Jersey Institute of Technology);Yehoshua Perl (New Jersey Institute of Technology);Jintae Lee (University of Colorado Boulder);,"2098545123,1239058640,2704604172",-,2004,Knowledge and Information Systems volume 6 issue 4 pp 375-379,data science;multimedia;data mining;
Discovering High-Order Periodic Patterns,Jiong Yang (University of Illinois at Urbana–Champaign);Wei Wang (University of North Carolina at Chapel Hill);Philips S. Yu (IBM);,"2115276518,2315689540,2495548024","Discovery of periodic patterns in time series data has become an active research area with many applications. These patterns can be hierarchical in nature, where a higher-level pattern may consist of repetitions of lower-level patterns. Unfortunately, the presence of noise may prevent these higher-level patterns from being recognized in the sense that two portions (of a data sequence) that support the same (high-level) pattern may have different layouts of occurrences of basic symbols. There may not exist any common representation in terms of raw symbol combinations; and hence such (high-level) pattern may not be expressed by any previous model (defined on raw symbols or symbol combinations) and would not be properly recognized by any existing method. In this paper, we propose a novel model, namely meta-pattern, to capture these high-level patterns. As a more flexible model, the number of potential meta-patterns could be very large. A substantial difficulty lies in how to identify the proper pattern candidates. However, the well-known Apriori property is not able to provide sufficient pruning power. A new property, namely component location property, is identified and used to conduct the candidate generation so that an efficient computation-based mining algorithm can be developed. Last, but not least, we apply our algorithm to some real and synthetic sequences and some interesting patterns are discovered.",2004,Knowledge and Information Systems volume 6 issue 3 pp 243-268,time series;data mining;artificial intelligence;statistics;algorithm;mathematics;
A Linguistic Approach to Extracting Acronym Expansions from Text,Manuel Zahariev (Simon Fraser University);,2002271664,"We propose a linguistically aided approach for the extraction of acronyms from text, with implications at the discourse, lexicon, morphologic and phonologic levels. A system implemented using this approach achieves excellent performance ($f=95.22$%) on a limited corpus. Our work indicates the adequacy of linguistic methods for acronym discovery.",2004,Knowledge and Information Systems volume 6 issue 3 pp 366-373,acronym;information extraction;natural language processing;speech recognition;computer science;
Fuzzy Conditional Probability Relations and their Applications in Fuzzy Information Systems,Rolly Intan (Meiji University);Masao Mukaidono (Meiji University);,"2127129671,370444231","In our real-world applications, data may be imprecise in which levels or degrees of preciseness of data are intuitively different. In this case, fuzzy set expressions are considered as an alternative to represent the imprecise data. In general, the degree of similarity relationship between two fuzzy (imprecise) data in real-world applications may not necessarily be symmetric or transitive. In order to provide such a degree of similarity between two fuzzy data, we introduced the fuzzy conditional probability relation. The concept of a fuzzy conditional probability relation may be considered as a concrete example of weak similarity relation which in turn is a special type of fuzzy binary relation generalizing similarity relation. Two important applications concerning the application of Knowledge Discovery and Data Mining (KDD) in the presence of a fuzzy data table (usually called fuzzy information system), namely removing redundant objects and recognizing partial or total dependency of (domain) attributes, are considered induced by the fuzzy conditional probability relation. Here, the fuzzy information system contains precise as well as imprecise data (fuzzy values) about objects of interest characterized by some attributes. Related to the dependency of attributes, we introduce the fuzzy functional dependency that satisfies Armstrong’s Axioms. In addition, we also discuss some interesting applications such as approximate data reduction and projection, approximate data querying and approximate joining in order to extend the query system.",2004,Knowledge and Information Systems volume 6 issue 3 pp 345-365,fuzzy associative matrix;type 2 fuzzy sets and systems;fuzzy subalgebra;fuzzy set operations;fuzzy measure theory;fuzzy mathematics;defuzzification;adaptive neuro fuzzy inference system;fuzzy classification;fuzzy number;fuzzy clustering;membership function;neuro fuzzy;fuzzy logic;fuzzy set;knowledge extraction;fuzzy control system;discrete mathematics;data mining;database;artificial intelligence;machine learning;computer science;mathematics;
Alternative strategies for Performing Spatial Joins on Web Sources,Cyrus Shahabi (University of Southern California);Mohammad R. Kolahdouzan (University of Southern California);Maytham Safar (Kuwait University);,"240820708,311041565,1984546891","With the current information explosion on the Web, numerous applications require access to a collection of different but related pieces of distributed geospatial data. In this paper, we focus on one set of such applications that requires efficient support of spatial operations (specifically, spatial join) on distributed non-database sources. The main challenge with this environment is that remote sources are usually read-only and/or do not support spatial queries. Moreover, several of these Web-based applications can tolerate either some level of inaccuracy or progressively filtered (or polished) results. Therefore, conventional distributed spatial join strategies are not applicable or efficient in this environment. To address these challenges, we first break down the process of distributed spatial join operation into three steps: (1) local to remote transfer, (2) remote spatial selection, and (3) local refinement. Then, for each step, we propose and study alternative techniques and by varying their combinations, we generate several query plans. Each plan strives to strike a compromise between efficiency and accuracy. Since the techniques proposed for the first step have significant impact on the overall performance of the query, we specially focus our attention on this step. We propose two heuristics for the first step to reduce either the number of selection queries or the area covered by each selection query. Within a realistic experimental set-up, we show that one heuristic is more appropriate with fast networks and a powerful local server, while the other one is superior in the opposite situation. Our experiments also show that both heuristics outperform approaches based on transmitting either the actual spatial objects or their bounding boxes. Note that the intention of this paper is not to propose a query optimizer to choose one plan over the others. Instead, it serves as a first step towards the design of such an optimizer by concentrating on the design and evaluation of several alternative plans within a realistic experimental set-up.",2004,Knowledge and Information Systems volume 6 issue 3 pp 290-314,spatial query;query optimization;web application;distributed database;world wide web;data mining;database;artificial intelligence;machine learning;computer science;
Vocabulary Use in XML Standards in the Financial Market Domain,Jian Qin (Syracuse University School of Information Studies);Mary D. Taffet (Syracuse University School of Information Studies);,"2644380501,2565215730","Vocabulary issues are central to XML-based e-commerce standards. The rapid increase in the number of XML standards has made the vocabulary issues a critical area of XML applications. This paper reports the result from a study of the vocabulary use in XML standards. Choosing five DTDs from the financial and capital markets domain, we examined a total of 1346 elements in nine categories. Inconsistent naming conventions and varying practices were major problems among other findings. We also observed a number of linguistic and semantic patterns in element names. Such patterns show a potential to serve as a foundation for building ontology-based conceptual models for XML schemas. The authors propose an ontology-driven, dynamic coordination approach in developing schema elements.",2004,Knowledge and Information Systems volume 6 issue 3 pp 269-289,xml schema editor;xml schema;xml catalog;cxml;xml validation;binary xml;document type definition;document structure description;xml schema;data modeling;ontology;world wide web;data mining;database;computer science;
Contextual Partitioning for Comprehension of OODB Schemas,Huanying Gu (University of Medicine and Dentistry of New Jersey);Yehoshua Perl (New Jersey Institute of Technology);James Geller (New Jersey Institute of Technology);Erich J. Neuhold (Fraunhofer Society);,"2308432819,1239058640,2098545123,434753560","Object-oriented databases (OODBs) have been utilized for complex modeling tasks within a variety of application domains. The OODB schema, typically expressed in a graphical notation, can serve as a useful presentation tool for the information contained in the underlying OODB. However, such a schema can be a large, complex network of classes and relationships. This may greatly hinder its effectiveness in helping users gain an understanding of the OODB’s contents and data organization. To facilitate this orientation process, a theoretical framework is presented that guides the refinement\/ of an existing schema’s subclass-of\/ relationship hierarchy – the backbone of any OODB. The framework sets forth three rules which, when satisfied, lead to the establishment of a collection of contexts, each of which exhibits an internal subclass-of\/ tree structure. A formal proof of this result is presented. An algorithmic methodology, involving a human–computer interaction, describes how the approach can be applied to a given OODB schema. An application of the methodology to an example OODB schema is included.",2004,Knowledge and Information Systems volume 6 issue 3 pp 315-344,comprehension;complex network;tree structure;satisfiability;theoretical computer science;data mining;database;artificial intelligence;computer science;
Collective Mining of Bayesian Networks from Distributed Heterogeneous Data,"Rong Chen (Washington State University);Krishnamoorthy Sivakumar (Washington State University);Hillol Kargupta (University of Maryland, Baltimore County);","2569468258,2264914315,539587773","We present a collective approach to learning a Bayesian network from distributed heterogeneous data. In this approach, we first learn a local Bayesian network at each site using the local data. Then each site identifies the observations that are most likely to be evidence of coupling between local and non-local variables and transmits a subset of these observations to a central site. Another Bayesian network is learnt at the central site using the data transmitted from the local site. The local and central Bayesian networks are combined to obtain a collective Bayesian network, which models the entire data. Experimental results and theoretical justification that demonstrate the feasibility of our approach are presented.",2004,Knowledge and Information Systems volume 6 issue 2 pp 164-187,variable order bayesian network;dynamic bayesian network;bayesian network;web mining;distributed database;data science;data mining;machine learning;computer science;
WAR: Weighted Association Rules for Item Intensities,"Wei Wang (University of California, Los Angeles);Jiong Yang (Case Western Reserve University);Philip S. Yu (IBM);","2315689540,2115276518,2125104194","In this paper, we extend the traditional association rule problem by allowing a weight to be associated with each item in a transaction to reflect the interest/intensity of each item within the transaction. In turn, this provides us with an opportunity to associate a weight parameter with each item in a resulting association rule; we call them weighted association rules (WAR). One example of such a rule might be 80% of people buying more than three bottles of soda will also be likely to buy more than four packages of snack food, while a conventional association rule might just be 60% of people buying soda will be also be likely to buy snack food. Thus WARs cannot only improve the confidence of the rules, but also provide a mechanism to do more effective target marketing by identifying or segmenting customers based on their potential degree of loyalty or volume of purchases. Our approach mines WARs by first ignoring the weight and finding the frequent itemsets (via a traditional frequent itemset discovery algorithm), followed by introducing the weight during the rule generation. Specifically, the rule generation is achieved by partitioning the weight domain space of each frequent itemset into fine grids, and then identifying the popular regions within the domain space to derive WARs. This approach does not only support the batch mode mining, i.e., finding WARs for the dataset, but also supports the interactive mode, i.e., finding and refining WARs for a given (set) of frequent itemset(s).",2004,Knowledge and Information Systems volume 6 issue 2 pp 203-229,association rule learning;density;knowledge extraction;data mining;database;artificial intelligence;machine learning;computer science;
Mining Navigation Patterns Using a Sequence Alignment Method,Birgit Hay;Geert Wets (University of Hasselt);Koen Vanhoof (University of Hasselt);,"2715327746,2088723926,2242613764","In this article, a new method is illustrated for mining navigation patterns on a web site. Instead of clustering patterns by means of a Euclidean distance measure, in this approach users are partitioned into clusters using a non-Euclidean distance measure called the Sequence Alignment Method (SAM). This method partitions navigation patterns according to the order in which web pages are requested and handles the problem of clustering sequences of different lengths. The performance of the algorithm is compared with the results of a method based on Euclidean distance measures. SAM is validated by means of user-traffic data of two different web sites. Empirical results show that SAM identifies sequences with similar behavioral patterns not only with regard to content, but also considering the order of pages visited in a sequence.",2004,Knowledge and Information Systems volume 6 issue 2 pp 150-163,web page;euclidean distance;sequence analysis;the internet;information system;cluster analysis;sequence alignment;web mining;world wide web;bioinformatics;data mining;database;computer science;
Learning Feature Weights from Customer Return-Set Selections,L. Karl Branting (University of Wyoming);,2089530433,"This paper describes LCW, a procedure for learning customer preferences represented as feature weights by observing customers’ selections from return sets. An empirical evaluation on simulated customer behavior indicated that uninformed hypotheses about customer weights lead to low ranking accuracy unless customers place some importance on almost all features or the total number of features is quite small. In contrast, LCW’s estimate of the mean preferences of a customer population improved as the number of customers increased, even for larger numbers of features of widely differing importance. This improvement in the estimate of mean customer preferences led to improved prediction of individual customers’ rankings, irrespective of the extent of variation among customers and whether a single or multiple retrievals were permitted. The experimental results suggest that the return set that optimizes benefit may be smaller for customer populations with little variation than for customer populations with wide variation.",2004,Knowledge and Information Systems volume 6 issue 2 pp 188-202,customer lifetime value;case based reasoning;the internet;e commerce;consumer behaviour;data mining;artificial intelligence;machine learning;computer science;
Discovering and Analyzing World Wide Web Collections,Sougata Mukherjea (IBM);,2129317713,"With the explosive growth of the World Wide Web, it is becoming increasingly difficult for users to discover Web pages that are relevant to a topic. To address this problem we are developing a system that allows the collection and analysis of Web pages related to a particular topic. In this paper we present the system’s overall architecture and introduce the focused crawler used by the system. We also discuss the various techniques we use to allow the user to analyze and gain useful insights about a collection. Finally, we present some statistics on the collections.",2004,Knowledge and Information Systems volume 6 issue 2 pp 230-241,web modeling;focused crawler;web standards;web mapping;web navigation;web page;search engine;information system;web intelligence;web engineering;web mining;world wide web;information retrieval;data mining;computer science;
An Intelligent Information System for Organizing Online Text Documents,Han-joon Kim (Seoul National University);Sang-goo Lee (Seoul National University);,"2165662526,2126044724","This paper describes an intelligent information system for effectively managing huge amounts of online text documents (such as Web documents) in a hierarchical manner. The organizational capabilities of this system are able to evolve semi-automatically with minimal human input. The system starts with an initial taxonomy in which documents are automatically categorized, and then evolves so as to provide a good indexing service as the document collection grows or its usage changes. To this end, we propose a series of algorithms that utilize text-mining technologies such as document clustering, document categorization, and hierarchy reorganization. In particular, clustering and categorization algorithms have been intensively studied in order to provide evolving facilities for hierarchical structures and categorization criteria. Through experiments using the Reuters-21578 document collection, we evaluate the performance of the proposed clustering and categorization methods by comparing them to those of well-known conventional methods.",2004,Knowledge and Information Systems volume 6 issue 2 pp 125-149,information system;document clustering;taxonomy;text mining;information retrieval;data mining;database;machine learning;computer science;
A genetic selection algorithm for OLAP data cubes,Wen-Yang Lin (I-Shou University);I-Chung Kuo (I-Shou University);,"2238964325,2098233435","Multidimensional data analysis, as supported by OLAP (online analytical processing) systems, requires the computation of many aggregate functions over a large volume of historically collected data. To decrease the query time and to provide various viewpoints for the analysts, these data are usually organized as a multidimensional data model, called data cubes. Each cell in a data cube corresponds to a unique set of values for the different dimensions and contains the metric of interest. The data cube selection problem is, given the set of user queries and a storage space constraint, to select a set of materialized cubes from the data cubes to minimize the query cost and/or the maintenance cost. This problem is known to be an NP-hard problem. In this study, we examined the application of genetic algorithms to the cube selection problem. We proposed a greedy-repaired genetic algorithm, called the genetic greedy method. According to our experiments, the solution obtained by our genetic greedy method is superior to that found using the traditional greedy method. That is, within the same storage constraint, the solution can greatly reduce the amount of query cost as well as the cube maintenance cost.",2004,Knowledge and Information Systems volume 6 issue 1 pp 83-102,data cube;greedy algorithm;genetic algorithm;data warehouse;data mining;database;algorithm;computer science;mathematics;
"LetSum, an automatic Legal Text Summarizing system",Atefeh Farzindar (Université de Montréal);Guy Lapalme (Université de Montréal);,"186380074,2113271024","This paper presents our work on the development of a new methodology for automatic summarization of justice decision. We describe LetSum (Legal text Sum- marizer), a prototype system, which determines the thematic structure of a judgment in four themes INTRODUCTION, CONTEXT, JURIDICAL ANALYSIS and CONCLUSION. Then it identifies the relevant sentences for each theme. We discuss the evaluation of produced summaries with statistical method and also human evaluation based on jurist judgment. The results so far indicate good performance of the system when compared with other summarization technologies.",2004,Knowledge and Information Systems,text graph;multi document summarization;automatic summarization;data science;information retrieval;data mining;computer science;
Agent Models and Different User Ontologies for an Electronic Market Place,Marcel Albers;Catholijn M. Jonker (VU University Amsterdam);Mehrzad Karami;Jan Treur (VU University Amsterdam);,"2681963829,2016708809,2500628775,2006738124","In this paper the agent-based electronic market architecture GEMS is described. The market incorporates different user perspectives: consumers, retailers, and producers. Ontologies for the different user perspectives are included. Knowledge is included to relate information from the different perspectives; for example, evaluation knowledge that can be used to derive product evaluations in terms of user ontology from product information based on producer ontology. Agent models are used as a high-level design structure for the architecture. It is shown how this combination of agent models, ontologies and knowledge provides an adequate approach to the distributed and knowledge-intensive character of the application.",2004,Knowledge and Information Systems volume 6 issue 1 pp 1-41,electronics;knowledge engineering;ontology;knowledge management;data mining;database;artificial intelligence;
Electronic roads: intelligent navigation through multi-contextual information,Georgios John Fakas (University of Cyprus);Antonis C. Kakas (University of Cyprus);Christos Schizas (University of Cyprus);,"1216114803,130847482,2263782775","This paper proposes a model for intelligent navigation through multi-contextual information that could form electronic roads in the information society. This paper aims to address the problem of electronic information roads, define their notion and the technical form they can take as well as present the tools developed for implementing such a system. The main objective of the proposed model is to give the traveler the capability of exploring the information space in a natural way where the information offered will remain continuously interesting. The system offers links to information in a dynamic and adaptive way. This is achieved by employing intelligent navigation techniques, which combine user profiling and meta-data. Electronic roads emphasize the presentation of multi-contextual information, i.e., information that is semantically related but of different nature at different locations and time. An electronic road is the user’s navigation path through a series of information units. Information units are the building blocks of the available cultural information content.",2004,Knowledge and Information Systems volume 6 issue 1 pp 103-124,information processor;information filtering system;turn by turn navigation;information mapping;information quality;metadata;automated information system;management information systems;expert system;multimedia;world wide web;data mining;database;artificial intelligence;computer science;
An approach for deriving a global representation of data sources having different formats and structures,Domenico Rosaci;Giorgio Terracina (University of Calabria);Domenico Ursino;,"20636212,2071483913,2040027413","In this paper we propose an approach for deriving a global representation of data sources having different formats and structures. The proposed approach is based on the exploitation of a particular conceptual model for both uniformly representing such data sources and reconstructing both their intra-source and their inter-source semantics. Along with the global representation, our approach returns two support structures which improve the access transparency to stored information, namely, a set of mappings, encoding the transformations carried out during the construction of the global representation, and a set of views, allowing to obtain instances of the concepts of the global representation from instances of the concepts of the input data sources. The paper also describes a prototype which implements the proposed approach.",2004,Knowledge and Information Systems volume 6 issue 1 pp 42-82,metadata;conceptual model;information system;theoretical computer science;world wide web;data mining;database;computer science;
Cell histograms versus color histograms for image representation and retrieval,Renato O. Stehling (State University of Campinas);Mario A. Nascimento (University of Alberta);Alexandre X. Falcão (State University of Campinas);,"1934594247,2118674688,2170165422","This paper presents a new approach for content-based image retrieval, which is based on the well-known and widely used color histograms. Previous approaches have used a single global color histogram (GCH) for the whole image, or local color histograms (LCHs) for cells within a grid of fixed size. Our approach is also based on a grid of cells, but unlike the latter it uses a cell histogram for each of the colors actually present in the images, representing how that color is distributed among the image cells - hence the name Cell/Color Histograms. Our experiments have shown that the actual number of colors present in images is often low. Thus we are able to achieve performance comparable to using LCHs within a grid, but with a much smaller space overhead. Furthermore, the proposed approach is very flexible in the sense that the user has alternative ways to calibrate the trade-off between space overhead and retrieval effectiveness. In fact, we have been able to outperform GCHs (typically a compact representation) in terms of effectiveness requiring less storage space.",2003,Knowledge and Information Systems volume 5 issue 3 pp 315-336,color quantization;color histogram;histogram;image retrieval;computer graphics images;information retrieval;computer vision;statistics;computer science;
Modeling spatial-temporal data with a short observation history,Dragoljub Pokrajac (Temple University);Reed L. Hoskinson;Zoran Obradovic (Temple University);,"2105028114,2642015216,2029694244","A novel method is proposed for forecasting spatial-temporal data with a short observation history sampled on a uniform grid. The method is based on spatial-temporal autoregressive modeling where the predictions of the response at the subsequent temporal layer are obtained using the response values from a recent history in a spatial neighborhood of each sampling point. Several modeling aspects such as covariance structure and sampling, as well as identification, model estimation and forecasting issues, are discussed. Extensive experimental evaluation is performed on synthetic and real-life data. The proposed forecasting models were shown capable of providing a near optimal prediction accuracy on simulated stationary spatial-temporal data in the presence of additive noise and a correlated model error. Results on a spatial-temporal agricultural dataset indicate that the proposed methods can provide useful prediction on complex real-life data with a short observation history.",2003,Knowledge and Information Systems volume 5 issue 3 pp 368-386,temporal database;autoregressive model;data processing;econometrics;data mining;statistics;computer science;
Web site synthesis based on computational logic,João M. B. Cavalcanti (University of Edinburgh);David Robertson (University of Edinburgh);,"2108826213,2144159400","Web site design and maintenance has become a challenging problem due to the increase in volume and complexity of information presented in this way. Much attention has been given to the deployment of Web sites but little thought has been given to methods for their design and maintenance. Web site applications can also benefit from systematic approaches to development that make design more methodical and maintenance less time consuming. One way to tackle this problem is via automated synthesis, automatically deriving a Web site from a high-level application description. Computational logic is well suited to this problem because of its support of a uniform view of data and computation, allowing reasoning with both specification and program via meta-programming.",2003,Knowledge and Information Systems volume 5 issue 3 pp 263-287,web modeling;metaprogramming;visualization;web engineering;systems architecture;artificial intelligence;simulation;programming language;computer science;
Database technologies for L-system simulations in virtual plant applications on bioinformatics,"Yi-Ping Phoebe Chen (Faculty of Information Technology, University Džemal Bijedić of Mostar);Robert M. Colomb (University of Queensland);","2150183633,700536811","One of the most important advantages of database systems is that the underlying mathematics is rich enough to specify very complex operations with a small number of statements in the database language. This research covers an aspect of biological informatics that is the marriage of information technology and biology, involving the study of real-world phenomena using virtual plants derived from L-systems simulation. L-systems were introduced by Aristid Lindenmayer as a mathematical model of multicellular organisms. Not much consideration has been given to the problem of persistent storage for these simulations. Current procedures for querying data generated by L-systems for scientific experiments, simulations and measurements are also inadequate. To address these problems the research in this paper presents a generic process for data-modeling tools (L-DBM) between L-systems and database systems. This paper shows how L-system productions can be generically and automatically represented in database schemas and how a database can be populated from the L-system strings. This paper further describes the idea of pre-computing recursive structures in the data into derived attributes using compiler generation. A method to allow a correspondence between biologists' terms and compiler-generated terms in a biologist computing environment is supplied. Once the L-DBM gets any specific L-systems productions and its declarations, it can generate the specific schema for both simple correspondence terminology and also complex recursive structure data attributes and relationships.",2003,Knowledge and Information Systems volume 5 issue 3 pp 288-314,l system;view;database schema;database design;data model;information technology;data mining;database;artificial intelligence;algorithm;computer science;
Compositional verification of knowledge-based task models and problem-solving methods,Frank Cornelissen (VU University Amsterdam);Catholijn M. Jonker (VU University Amsterdam);Jan Treur (VU University Amsterdam);,"2153957977,2016708809,2006738124","In this paper a compositional verification method for task models and problem-solving methods for knowledge-based systems is introduced. Required properties of a system are formally verified by deriving them from assumptions that themselves are properties of sub-components, which in their turn may be derived from assumptions on sub-sub-components, and so on. The method is based on properties that are formalized in terms of temporal semantics; both static and dynamic properties are covered. The compositional verification method imposes structure on the verification process. Because of the possibility of focusing at one level of abstraction (information and process hiding), compositional verification provides transparency and limits the complexity per level. Since verification proofs are structured in a compositional manner, they can be reused in the event of reuse of models or modification of an existing system. The method is illustrated for a generic model for diagnostic reasoning.",2003,Knowledge and Information Systems volume 5 issue 3 pp 337-367,verification and validation of computer simulation models;runtime verification;functional verification;formal verification;knowledge based systems;theoretical computer science;artificial intelligence;algorithm;computer science;
Feature Weighting and Instance Selection for Collaborative Filtering: An Information-Theoretic Approach,Kai Yu (Siemens);Xiaowei Xu (Siemens);Martin Ester (Ludwig Maximilian University of Munich);Hans-Peter Kriegel (Ludwig Maximilian University of Munich);,"2159960587,2645375510,2067196623,1919135125","Collaborative filtering (CF) employing a consumer preference database to make personal product recommendations is achieving widespread success in E-commerce. However, it does not scale well to the ever-growing number of consumers. The quality of the recommendation also needs to be improved in order to gain more trust from consumers. This paper attempts to improve the accuracy and efficiency of collaborative filtering. We present a unified information-theoretic approach to measure the relevance of features and instances. Feature weighting and instance selection methods are proposed for collaborative filtering. The proposed methods are evaluated on the well-known EachMovie data set and the experimental results demonstrate a significant improvement in accuracy and efficiency.",2003,Knowledge and Information Systems volume 5 issue 2 pp 201-224,collaborative filtering;recommender system;information retrieval;data mining;database;machine learning;computer science;
Data Mining: How Research Meets Practical Development?,Xindong Wu (University of Vermont);S. Yu (University of Vermont);Gregory Piatetsky-Shapiro;Nick Cercone (University of Waterloo);Y. Lin (San Jose State University);Ramamohanarao Kotagiri (University of Melbourne);W. Wah (University of Illinois at Urbana–Champaign);,"2123651450,2514783008,2703252382,76047928,2507380549,1995540086,2652024432","At the 2001 IEEE International Conference on Data Mining in San Jose, California, on November 29 to December 2, 2001, there was a panel discussion on how data mining research meets practical development. One of the motivations for organizing the panel discussion was to provide useful advice for industrial people to explore their directions in data mining development. Based on the panel discussion, this paper presents the views and arguments from the panel members, the Conference Chair and the Program Committee Co-Chairs. These people as a group have both academic and industrial experiences in different data mining related areas such as databases, machine learning, and neural networks. We will answer questions such as (1) how far data mining is from practical development, (2) how data mining research differs from practical development, and (3) what are the most promising areas in data mining for practical development.",2003,Knowledge and Information Systems volume 5 issue 2 pp 248-261,computer data storage;data processing;data science;operations research;data mining;database;artificial intelligence;machine learning;computer science;
An Agenda- and Justification-Based Framework for Discovery Systems,Gary R. Livingston (University of Pittsburgh);John M. Rosenberg (University of Pittsburgh);Bruce G. Buchanan (University of Pittsburgh);,"2063601550,2002142829,2513848785","We propose and evaluate an agenda- and justification-based architecture for discovery systems that selects the next tasks to perform, as well as heuristics for use in discovery systems. This framework has many desirable properties: (1) it selects its own tasks to perform based upon how plausible they are judged to be; (2) it facilitates the encoding of general discovery strategies using a variety of background knowledge; and (3) it tailors its behavior toward a user’s interests. Many experiments with a prototype discovery program called HAMB demonstrate that both reasons and estimates of interestingness contribute to performance in the domains of protein crystallization and patient rehabilitation data. The program’s heuristics provide good initial solutions to problems encountered when implementing fully autonomous discovery systems.",2003,Knowledge and Information Systems volume 5 issue 2 pp 133-161,protein crystallization;architecture;information system;knowledge extraction;data science;data mining;artificial intelligence;computer science;
A Methodology for Structural Conflict Resolution in the Integration of Entity-Relationship Schemas,Li Lee (National University of Singapore);Wang Ling (National University of Singapore);,"2655430528,2442095395","The integration ofinf ormation systems is becoming increasingly important. A common requirement in distributed data-intensive applications, such as data warehousing and data mining, is that the various databases involved be joined in a process called schema integration. The entity-relationship (ER) model or a variant of the ER model is often used as the common data model. To aid the schema conforming, merging and restructuring phases of the integration process, various transformations have been defined to map between various equivalent ER representations. In this paper, we describe a different approach to integrate ER schemas. We focus on the resolution of structural conflicts, that is, when related real-world concepts are modeled using different constructs in different schemas. Unlike previous work, our approach proposes to resolve the structural conflict between an entity type in one schema and an attribute in another schema and show that the other structural conflicts are automatically resolved. This reduces the manual effort required in integration. We give a detailed algorithm to transform an attribute in one schema into an equivalent entity type in another schema without any loss of semantics, that is, our transformation is both information preserving and constraint preserving.",2003,Knowledge and Information Systems volume 5 issue 2 pp 225-247,semi structured model;three schema approach;schema migration;star schema;conceptual schema;entity relationship model;conflict resolution;semantics;data mining;database;artificial intelligence;computer science;
Necessary and Sufficient Pre-processing in Numerical Range Discretization,Tapio Elomaa (University of Helsinki);Juho Rousu (University of Helsinki);,"2303288923,2228020513","The time complexities of class-driven numerical range discretization algorithms depend on the number of cut point candidates. Previous analysis has shown that only a subset of all cut points - the segment borders - have to be taken into account in optimal discretization with respect to many goodness criteria. In this paper we show that inspecting segment borders alone suffices in optimizing any convex evaluation function. For strictly convex evaluation functions inspecting all of them also is necessary, since the placement of neighboring cut points affects the optimality of a segment border. With the training set error function, which is not strictly convex, it suffices to inspect an even smaller set of cut point candidates, called alternations, when striving for optimal partition. On the other hand, we prove that failing to check an alternation may lead to suboptimal discretization. We present a linear-time algorithm for finding all alternation points. The number of alternation points is typically much lower than the total number of cut points. In our experiments running the discretization algorithm over the sequence of alternation points led to a significant speed-up.",2003,Knowledge and Information Systems volume 5 issue 2 pp 162-182,discretization error;discretization;time complexity;discrete mathematics;combinatorics;algorithm;computer science;mathematics;
Discovering Similar Patterns for Characterizing Time Series in a Medical Domain,Fernando Alonso (Technical University of Madrid);P. Caraça-Valente;Loïc Martínez (Technical University of Madrid);César Montes (Technical University of Madrid);,"2173072484,2487413353,2164385520,2157323916","In this article, we describe the process of discovering similar patterns in time series and creating reference models for population groups in a medical domain, and particularly in the field of physiotherapy, using data mining techniques on a set of isokinetic data. The discovered knowledge was evaluated against the expertise of a physician specialized in isokinetic techniques, and applied in the I4 (Intelligent Interpretation of Isokinetic Information) project developed in conjunction with the Spanish National Center for Sports Research and Sciences for muscular diagnosis and rehabilitation, injury prevention, training evaluation and planning, etc., of elite athletes and ordinary people.",2003,Knowledge and Information Systems volume 5 issue 2 pp 183-200,reference model;time series;injury prevention;knowledge extraction;data mining;artificial intelligence;simulation;computer science;
The unified problem-solving method development language UPML,Dieter Fensel (VU University Amsterdam);Enrico Motta (Open University);Frank van Harmelen (VU University Amsterdam);V. Richard Benjamins (University of Amsterdam);Monica Crubézy (Stanford University);Stefan Decker (Karlsruhe Institute of Technology);Mauro Gaspari (University of Bologna);Rix Groenboom (University of Groningen);William E. Grosso (Stanford University);Mark A. Musen (Stanford University);Enric Plaza;Guus Schreiber (University of Amsterdam);Rudi Studer (Karlsruhe Institute of Technology);Bob J. Wielinga (University of Amsterdam);,"2303458062,2106913481,333553561,2138792078,57577535,2182667553,2144939910,423601442,348457372,2169034151,2711771922,2177918909,2105886198,1966931212","Problem-solving methods provide reusable architectures and components for implementing the reasoning part of knowledge-based systems. The Unified Problem-Solving Method Description Language (UPML) has been developed to describe and implement such architectures and components to facilitate their semi-automatic reuse and adaptation. In a nutshell, UPML is a framework for developing knowledge-intensive reasoning systems based on libraries of generic problem-solving components. The paper describes the components and adapters, architectural constraints, development guidelines, and tools provided by UPML. UPML is developed as part of the IBROW project, which provides an Internet-based brokering service for reusing problem-solving methods.",2003,Knowledge and Information Systems volume 5 issue 1 pp 83-131,semantic web;software architecture;knowledge engineering;semantics;database;artificial intelligence;algorithm;computer science;
Knowledge-based system explanation: the ripple-down rules alternative,Debbie Richards (Macquarie University);,2156772009,The ability to provide explanations has been seen as a key feature of expert systems (ES) typically not offered by other types of computer systems. ES need to offer explanations because of imprecise domains and the use of heuristics. Verification is not enough. ES need to justify and be accountable. Explanation is seen as an important activity for knowledge-based systems as it satisfies the user's need to decide whether to accept or reject a recommendation. In this paper we review explanation in first-generation and second-generation ES. An alternative is offered to the main approaches which uses multiple classification ripple-down rules and challenges even the goals of explanation. Instead of trying to give explanations which provide a meaningful line of reasoning and which are tailored to suit the individual it may be just as useful to provide the user with sufficient information and browsing tools to develop their own line of reasoning. The type of information that can assist understanding is the context in which the recommendation applies (which is provided through the display of relevant cases and exception rule history) and the ability to explore an abstraction hierarchy of the rules using formal concept analysis. An explanation toolkit aimed at putting the user in control is described and evaluated in this paper.,2003,Knowledge and Information Systems volume 5 issue 1 pp 2-25,formal concept analysis;satisfiability;knowledge based systems;data mining;artificial intelligence;machine learning;algorithm;computer science;
Cost estimation for queries experiencing multiple contention states in dynamic multidatabase environments,Qiang Zhu (University of Michigan);Satyanarayana Motheramgari (University of Michigan);Yu Sun (University of Michigan);,"2292350935,2430890577,2425910509","Accurate query cost estimation is crucial to query optimization in a multidatabase system. Several estimation techniques for a static environment have been suggested in the literature. To develop a cost model for a dynamic environment, we recently introduced a multistate query-sampling method. It has been shown that this technique is promising in estimating the cost of a query run in any given contention state for a dynamic environment. In this paper, we study a new problem on how to estimate the cost of a large query that may experience multiple contention states. Following the discussion of limitations for two simple approaches, i.e., single state analysis and average cost analysis, we propose two novel techniques to tackle this challenge. The first one, called fractional analysis, is suitable for a gradually and smoothly changing environment, while the second one, called the probabilistic approach, is developed for a rapidly and randomly changing environment. The former estimates a query cost by analyzing its fractions, and the latter estimates a query cost based on Markov chain theory. The related issues including cost formula development, error analysis, and comparison among different approaches are discussed. Experiments demonstrate that the proposed techniques are quite promising in solving the new problem.",2003,Knowledge and Information Systems volume 5 issue 1 pp 26-49,query optimization;cost estimate;markov chain;sampling;theoretical computer science;data mining;database;computer science;
Maintaining views in object-relational databases,Jixue Liu (University of South Australia);Millist W. Vincent (University of South Australia);Mukesh K. Mohania (Western Michigan University);,"2149643224,2339232969,2661585128","View materialization is an important way of improving the performance of query processing. When an update occurs to the source data from which a materialized view is derived, the materialized view has to be updated so that it is consistent with the source data. This update process is called view maintenance . The incremental method of view maintenance, which computes the new view using the old view and the update to the source data, is widely preferred to full view recomputation when the update is small in size. In this paper we investigate how to incrementally maintain views in object-relational (OR) databases. The investigation focuses on maintaining views defined in OR-SQL, a language containing the features of object referencing, inheritance, collection, and aggregate functions including user-defined set aggregate functions. We propose an architecture and algorithms for incremental OR view maintenance. We implement all algorithms and analyze the performance of them in comparison with full view recomputation. The analysis shows that the algorithms significantly reduce the cost of updating a view when the size of an update to the source data is relatively small.",2003,Knowledge and Information Systems volume 5 issue 1 pp 50-82,view;materialized view;data modeling;query language;computer data storage;relational database;data warehouse;information retrieval;data mining;database;computer science;
Editorial: online first publication,Xindong Wu;Christiane Notarmarco (Springer Science+Business Media);,"2668984283,2478401275",-,2003,Knowledge and Information Systems volume 5 issue 1 pp 1-1,-
A general approach to clustering in large databases with noise,Alexander Hinneburg (Martin Luther University of Halle-Wittenberg);Daniel A. Keim (AT&T);,"293376274,2147343253","Several clustering algorithms can be applied to clustering in large multimedia databases. The effectiveness and efficiency of the existing algorithms, however, are somewhat limited, since clustering in multimedia databases requires clustering of high-dimensional feature vectors and because multimedia databases often contain large amounts of noise. In this paper, we therefore introduce a new Kernel Density Estimation-based algorithm for clustering in large multimedia databases called DENCLUE (DENsity-based CLUstEring). Kernel Density Estimation (KDE) models the overall point density analytically as the sum of kernel (or influence) functions of the data points. Clusters can then be identified by determining density attractors and clusters of arbitrary shape can be easily described by a simple equation of the overall density function. The advantages of our KDE-based DENCLUE approach are: (1) it has a firm mathematical basis; (2) it has good clustering properties in data sets with large amounts of noise; (3) it allows a compact mathematical description of arbitrarily shaped clusters in high-dimensional data sets; and (4) it is significantly faster than existing algorithms. To demonstrate the effectiveness and efficiency of DENCLUE, we perform a series of experiments on a number of different data sets from CAD and molecular biology. A comparison with k-Means, DBSCAN, and BIRCH shows the superiority of our new algorithm.",2003,Knowledge and Information Systems volume 5 issue 4 pp 387-415,k medians clustering;subclu;canopy clustering algorithm;determining the number of clusters in a data set;dbscan;correlation clustering;constrained clustering;data stream clustering;cure data clustering algorithm;fuzzy clustering;kernel density estimation;clustering high dimensional data;cluster analysis;knowledge extraction;theoretical computer science;data mining;database;machine learning;statistics;computer science;mathematics;
A Heuristic Genetic Algorithm for Solving Resource Allocation Problems,Zne-Jung Lee (National Taiwan University of Science and Technology);Shun-Feng Su (National Taiwan University of Science and Technology);Chou-Yuan Lee (National Taiwan University of Science and Technology);Yao-Shan Hung;,"2491679397,2237112505,2137630821,2704417534","In the paper, a heuristic genetic algorithm for solving resource allocation problems is proposed. The resource allocation problems are to allocate resources to activities so that the fitness becomes as optimal as possible. The objective of this paper is to develop an efficient algorithm to solve resource allocation problems encountered in practice. Various genetic algorithms are studied and a heuristic genetic algorithm is proposed to ameliorate the rate of convergence for resource allocation problems. Simulation results show that the proposed algorithm gives the best performance.",2003,Knowledge and Information Systems volume 5 issue 4 pp 503-511,population based incremental learning;heuristics;rate of convergence;resource allocation;optimization problem;genetic algorithm;mathematical economics;mathematical optimization;computer science;
HDM: A Client/Server/Engine Architecture for Real-Time Web Usage Mining,Florent Masseglia (French Institute for Research in Computer Science and Automation);Maguelonne Teisseire (Centre national de la recherche scientifique);Pascal Poncelet (Mines ParisTech);,"2440503352,2141863019,733374064","The behavior of the users of a website may change so quickly that it becomes a real challenge to attempt to make predictions according to the frequent patterns coming from the analysis of an access log file. In order to reduce the obsolescence of behavioral patterns as much as possible, the ideal method would provide frequent patterns in real time, making the result immediately available. In this paper, we propose a method for finding frequent behavioral patterns in real time, whatever the number of connected users. Considering how fast frequent behavior patterns may have changed since the time the access log file was analyzed, this result thus provides completely appropriate navigation schemata for predicting user behavior. Based on a distributed heuristic, our method also tackles and provides answers to several problems within the framework of data mining: the discovery of ‘interesting zones’ (a large number of frequent patterns concentrated over a period of time, or ‘super-frequent’ patterns), discovering very long sequential patterns and interactive data mining (‘on-the-fly’ modification of the minimum support).",2003,Knowledge and Information Systems volume 5 issue 4 pp 439-465,client server model;the internet;heuristic;world wide web;data mining;database;computer science;
An Approximate Median Polish Algorithm for Large Multidimensional Data Sets,Daniel Barbará;Xintao Wu (University of North Carolina at Charlotte);,"2710711701,2623789330","Exploratory data analysis is a widely used technique to determine which factors have the most influence on data values in a multi-way table, or which cells in the table can be considered anomalous with respect to the other cells. In particular, median polish is a simple yet robust method to perform exploratory data analysis. Median polish is resistant to holes in the table (cells that have no values), but it may require many iterations through the data. This factor makes it difficult to apply median polish to large multidimensional tables, since the I/O requirements may be prohibitive. This paper describes a technique that uses median polish over an approximation of a datacube, easing the burden of I/O. The cube approximation is achieved by fitting log-linear models to the data. The results obtained are tested for quality, using a variety of measures. The technique scales to large datacubes and proves to give a good approximation of the results that would have been obtained by median polish in the original data.",2003,Knowledge and Information Systems volume 5 issue 4 pp 416-438,median polish;cube;data cube;log linear model;exploratory data analysis;data analysis;data structure;econometrics;data mining;statistics;computer science;mathematics;
A method for developing component-oriented applications: A use-context driven approach toward Component Coordination,Mathee Olarnsakul (Asian Institute of Technology);Dentcho N. Batanov (Asian Institute of Technology);,"2473379113,180789722","In this paper, we propose a methodology for the development of component-oriented applications based on the Component Coordination Model (CCM) that is embedded into applications during software architecture design. The method is general enough to support the conceptual level of component-based software development. It provides a set of guidelines that ease developers during analysis and design phases. These guidelines are defined as processes for developing increasingly detailed models of constructing software systems. The CCM is driven by the use-context model, which directly reflects the role model that abstracts system behaviors as a computational organization comprising various role relationships. It then focuses on exposing the design of software components to be separated from their execution contexts. These separate concerns, including computation, coordination, and policies imposed on a given use-context, form the principal concept of our approach. A case study using the proposed method has been demonstrated in order to provide the feasibility of the introduced approach.",2003,Knowledge and Information Systems volume 5 issue 4 pp 466-502,component based software engineering;software architecture;database;simulation;computer science;
Mining frequent rooted trees and free trees using canonical forms,"Yun Chi (University of California, Los Angeles);Yang Richard Yang;Richard Muntz (University of California, Los Angeles);","2696493159,2429013724,2033368653",-,2003,Knowledge and Information Systems,a b tree;weight balanced tree;k ary tree;2 3 tree;xml database;search tree;binary tree;canonical form;tree structure;discrete mathematics;data mining;database;pattern recognition;computer science;mathematics;
Knowledge Management for Legislative Drafting in an International Setting,Alexander Boer (University of Amsterdam);Radboud Winkels (University of Amsterdam);Rinke J. Hoekstra (VU University Amsterdam);Engers van Tom M;,"2148868092,1996130453,2144494049,2121922256",-,2003,Knowledge and Information Systems,homology modeling;management science;knowledge management;computer science;
A Use Case Analysis of Legal Knowledge-Based Systems,Thomas F. Gordon (Fraunhofer Society);Fraunhofer Fokus;,"2280312759,210032777","A number of important use cases for public administration (""e-governance"") applications of legal knowledge-based systems are identified. For each use case, a data flow model is developed showing processes and document types sufficient to imple- ment the use case. Our main goal is to identify document types which, if standardized, would enable a modular set of interoperable components for building legal knowledge- based systems.",2003,Knowledge and Information Systems,use case analysis;use case;data flow diagram;knowledge based systems;knowledge management;data mining;database;computer science;
Findout: finding outliers in very large datasets,Dantong Yu (University at Buffalo);Gholamhosein Sheikholeslami (University at Buffalo);Aidong Zhang (University at Buffalo);,"2309580202,2065786411,2228514421","Finding the rare instances or the outliers is important in many KDD (knowledge discovery and data-mining) applications, such as detecting credit card fraud or finding irregularities in gene expressions. Signal-processing techniques have been introduced to transform images for enhancement, filtering, restoration, analysis, and reconstruction. In this paper, we present a new method in which we apply signal-processing techniques to solve important problems in data mining. In particular, we introduce a novel deviation (or outlier) detection approach, termed FindOut, based on wavelet transform. The main idea in FindOut is to remove the clusters from the original data and then identify the outliers. Although previous research showed that such techniques may not be effective because of the nature of the clustering, FindOut can successfully identify outliers from large datasets. Experimental results on very large datasets are presented which show the efficiency and effectiveness of the proposed approach.",2002,Knowledge and Information Systems volume 4 issue 4 pp 387-412,outlier;signal processing;data science;data mining;pattern recognition;statistics;computer science;
A pattern decomposition algorithm for data mining of frequent patterns,"Qinghua Zou (University of California, Los Angeles);Wesley W. Chu (University of California, Los Angeles);David B. Johnson (University of California, Los Angeles);Henry Chiu (IBM);","2050846921,2168131383,2602605904,2102657940","Efficient algorithms to mine frequent patterns are crucial to many tasks in data mining. Since the Apriori algorithm was proposed in 1994, there have been several methods proposed to improve its performance. However, most still adopt its candidate set generation-and-test approach. In addition, many methods do not generate all frequent patterns, making them inadequate to derive association rules. We propose a pattern decomposition (PD) algorithm that can significantly reduce the size of the dataset on each pass, making it more efficient to mine all frequent patterns in a large dataset. The proposed algorithm avoids the costly process of candidate set generation and saves time by reducing the size of the dataset. Our empirical evaluation shows that the algorithm outperforms Apriori by one order of magnitude and is faster than FP-tree algorithm.",2002,Knowledge and Information Systems volume 4 issue 4 pp 466-482,fsa red algorithm;gsp algorithm;apriori algorithm;association rule learning;data mining;pattern recognition;machine learning;computer science;
Knowledge discovery with second-order relations,Rattikorn Hewett (Florida Institute for Human and Machine Cognition);John H. Leuchner (Florida Institute for Human and Machine Cognition);,"1990641796,1253072439","This paper presents an induction technique that discovers a set of classification rules, from a set of examples, using second-order relations as a representational model. Second-order relations are database relations in which tuples have sets of atomic values as components. Using sets of values, which are interpreted as disjunctions, provides compact representations that facilitate efficient management and enhance comprehensibility. The second-order relational framework is based on theoretical foundations that link relational database theory, machine learning, and logic synthesis. The rule induction technique can be viewed as a second-order relation compression problem in which the original relation, representing training data, is transformed into a second-order relation with fewer tuples by merging tuples in ways that preserve consistency with the training data. This problem is closely related to two-level Boolean function minimization in logic synthesis. We describe a rule-mining system, SORCER, and compare its performance to two state-of-the-art classification systems: C4.5 and CBA. Experimental results based on the average of error rates ove 26 data sets show that SORCER, using a simple compression scheme, outperforms C4.5 and is competitive to CBA. Using a slightly more sophisticated compression scheme, SORCER outperforms both C4.5 and CBA.",2002,Knowledge and Information Systems volume 4 issue 4 pp 413-439,database theory;boolean function;logic synthesis;relational database;word error rate;second order logic;biological classification;knowledge extraction;data mining;database;artificial intelligence;machine learning;statistics;algorithm;computer science;mathematics;
Qualitative reasoning under ignorance and information-relevant extraction,Mohamed Chachoua (University of Angers);Daniel Pacholczyk (University of Angers);,"2085769979,267423159","This paper is devoted to qualitative reasoning under ignorance. We show how to represent conditional ignorance and informational relevance in the symbolic entropy theory that we have developed in our previous work. This theory allows us to represent uncertainty, in the ignorance form, as in common-sense reasoning, by using the linguistic expressions of the interval [Certain, Completely uncertain]. We recall this theory, then we introduce the notions of conditional ignorance and of informational relevance. Finally we present some theorems of qualitative reasoning with uncertain knowledge. Particularly, we show how to extract the best relevant information in order to treat some problems under ignorance.",2002,Knowledge and Information Systems volume 4 issue 4 pp 483-506,qualitative reasoning;entropy;knowledge representation and reasoning;information theory;data mining;artificial intelligence;machine learning;algorithm;computer science;mathematics;
High-dimensional nearest neighbor search with remote data centers,Changzhou Wang;Xiaoyang Sean Wang (George Mason University);,"2709038557,2698941210","Many data centers have archived a tremendous amount of data and begun to publish them on the Web. Due to limited resources and large amount of service requests, data centers usually do not directly support high-cost queries. On the other hand, users are often overwhelmed by the huge data volume and cannot afford to download the whole data sets and search them locally. To support high-dimensional nearest neighbor searches in this environment, the paper develops a multi-level approximation scheme. The coarsest-level approximations are stored locally and searched first. The result is then refined gradually via accesses to remote data centers. Data centers need only to deliver data items or their precomputed finer level approximations by their identifiers.The searching process is usually long in this environment, since it involves remote sites. This paper describes an online search process: the system periodically reports a data item and a positive integer M. The reported item is guaranteed to be one of the M nearest neighbors of the query one. The paper proposes two algorithms to minimize M in each period. Experiments show that one of them performs similarly as a theoretical a posteriori algorithm and significantly outperforms the online extensions of two state-of-the-art nearest neighbor search methods.",2002,Knowledge and Information Systems volume 4 issue 4 pp 440-465,best bin first;cover tree;online algorithm;multilevel model;data center;nearest neighbor search;clustering high dimensional data;computer data storage;k nearest neighbors algorithm;data mining;database;machine learning;computer science;
Content-based retrieval of video data with flexibly managed attributes,Sung I. Yong (Yonsei University);Won S. Lee (Yonsei University);,"2283337082,2240115339","Various types of information can be mixed in a continuous video stream without any clear boundary. For this reason, there seems to be no simple solution to support content-based queries in a video database. The meaning of a video scene can be interpreted by multiple levels of abstraction and its description can be varied among different users. Therefore, it is important for a user to be able to describe a scene flexibly while the description given by different users should be maintained consistently. This paper proposes an effective way to represent the different types of video information in the conventional relational database model. Flexibly defined attributes and their values are organized as tree-structured dictionaries while the description of video data is stored in a fixed database schema. In order to assist a user, two browsing methods are introduced. The dictionary browser simplifies the annotation process as well as the querying process of a user while the result browser can help a user analyze the results of a query in terms of various combinations of query conditions.",2002,Knowledge and Information Systems volume 4 issue 4 pp 507-519,video tracking;search engine indexing;tree structure;relational database;multimedia;world wide web;data mining;database;computer science;
A knowledge management framework for the support of decision making in humanitarian assistance/disaster relief,Dongsong Zhang (University of Arizona);Lina Zhou (University of Arizona);Jay F. Nunamaker (University of Arizona);,"2619658290,2140606150,2148746491","The major challenge in current humanitarian assistance/disaster relief (HA/DR) efforts is that diverse information and knowledge are widely distributed and owned by different organizations. These resources are not efficiently organized and utilized during HA/DR operations. We present a knowledge management framework that integrates multiple information technologies to collect, analyze, and manage information and knowledge for supporting decision making in HA/DR. The framework will help identify the information needs, be aware of a disaster situation, and provide decision-makers with useful relief recommendations based on past experience. A comprehensive, consistent and authoritative knowledge base within the framework will facilitate knowledge sharing and reuse. This framework can also be applied to other similar real-time decision-making environments, such as crisis management and emergency medical assistance.",2002,Knowledge and Information Systems volume 4 issue 3 pp 370-385,situation awareness;disaster;case based reasoning;information extraction;knowledge base;information technology;management science;knowledge management;data mining;artificial intelligence;computer science;
Schema evolution in data warehouses,Zohra Bellahsene (Centre national de la recherche scientifique);,2707062715,"In this paper, we address the issues related to the evolution and maintenance of data warehousing systems, when underlying data sources change their schema capabilities. These changes can invalidate views at the data warehousing system. We present an approach for dynamically adapting views according to schema changes arising on source relations. This type of maintenance concerns both the schema and the data of the data warehouse. The main issue is to avoid the view recomputation from scratch especially when views are defined from multiple sources. The data of the data warehouse is used primarily in organizational decision-making and may be strategic. Therefore, the schema of the data warehouse can evolve for modeling new requirements resulting from analysis or data-mining processing. Our approach provides means to support schema evolution of the data warehouse independently of the data sources.",2002,Knowledge and Information Systems volume 4 issue 3 pp 283-304,semi structured model;three schema approach;information schema;dimensional modeling;database schema;containment;conceptual schema;data exchange;data integration;computer data storage;data analysis;data warehouse;data science;data mining;database;computer science;
Agents in E-commerce: state of the art,Minghua He (The Chinese University of Hong Kong);Ho-fung Leung (The Chinese University of Hong Kong);,"2653691818,2132605483","This paper surveys the state of the art of agent-mediated electronic commerce (e-commerce), especially in business-to-consumer (B2C) e-commerce and business-to-business (B2B) e-commerce. From the consumer buying behaviour perspective, the roles of agents in B2C e-commerce are: product brokering, merchant brokering, and negotiation. The applications of agents in B2B e-commerce are mainly in supply chain management. Mobile agents, evolutionary agents, and data-mining agents are some special techniques which can be applied in agent-mediated e-commerce. In addition, some technologies for implementation are briefly reviewed. Finally, we conclude this paper by discussions on the future directions of agent-mediated e-commerce.",2002,Knowledge and Information Systems volume 4 issue 3 pp 257-282,negotiation;e commerce;computer science;
Attribute generation based on association rules,Masahiro Terabe (Mitsubishi Research Institute);Takashi Washio (Osaka University);Hiroshi Motoda (Osaka University);Osamu Katai (Kyoto University);Tetsuo Sawaragi (Kyoto University);,"2027276237,1794038515,323689644,1723066037,463175782","A decision tree is considered to be appropriate (1) if the tree can classify the unseen data accurately, and (2) if the size of the tree is small. One of the approaches to induce such a good decision tree is to add new attributes and their values to enhance the expressiveness of the training data at the data pre-processing stage. There are many existing methods for attribute extraction and construction, but constructing new attributes is still an art. These methods are very time consuming, and some of them need a priori knowledge of the data domain. They are not suitable for data mining dealing with large volumes of data. We propose a novel approach that the knowledge on attributes relevant to the class is extracted as association rules from the training data. The new attributes and the values are generated from the association rules among the originally given attributes. We elaborate on the method and investigate its feature. The effectiveness of our approach is demonstrated through some experiments.",2002,Knowledge and Information Systems volume 4 issue 3 pp 329-349,variable and attribute;incremental decision tree;association rule learning;data processing;data mining;pattern recognition;machine learning;computer science;
A meteorological fuzzy expert system incorporating subjective user input,Michael Hadjimichael (United States Naval Research Laboratory);Arunas P. Kuciauskas (United States Naval Research Laboratory);Paul M. Tag (United States Naval Research Laboratory);Richard L. Bankert (United States Naval Research Laboratory);James E. Peak (Computer Sciences Corporation);,"2064279480,1967569912,2475795946,2034996778,2055397565","We present a fuzzy expert system, MEDEX, for forecasting gale-force winds in the Mediterranean basin. The most successful local wind forecasting in this region is achieved by an expert human forecaster with access to numerical weather prediction products. That forecaster's knowledge is expressed as a set of 'rules-of-thumb'. Fuzzy set methodologies have proved well suited for encoding the forecaster's knowledge, and for accommodating the uncertainty inherent in the specification of rules, as well as in subjective and objective input. MEDEX uses fuzzy set theory in two ways: as a fuzzy rule base in the expert system, and for fuzzy pattern matching to select dominant wind circulation patterns as one input to the expert system. The system was developed, tuned, and verified over a two-year period, during which the weather conditions from 539 days were individually analyzed. Evaluations of MEDEX performance for both the onset and cessation of winter and summer winds are presented, and demonstrate that MEDEX has forecasting skill competitive with the US Navy's regional forecasting center in Rota, Spain.",2002,Knowledge and Information Systems volume 4 issue 3 pp 350-369,fuzzy set;expert system;operations research;data mining;artificial intelligence;computer science;
Efficient transitive closure reasoning in a combined class/part/containment hierarchy,Yugyung Lee (University of Missouri–Kansas City);James Geller (New Jersey Institute of Technology);,"2113943634,2098545123","Class hierarchies form the backbone of many implemented knowledge representation and reasoning systems. They are used for inheritance, classification and transitive closure reasoning. Part hierarchies are also important in artificial intelligence. Other hierarchies, e.g. containment hierarchies, have received less attention in artificial intelligence. This paper presents an architecture and an implementation of a hierarchy reasoner that integrates a class hierarchy, a part hierarchy, and a containment hierarchy into one structure. In order to make an implemented reasoner useful, it needs to operate at least at speeds comparable to human reasoning. As real-world hierarchies are always large, special techniques need to be used to achieve this. We have developed a set of parallel algorithms and a data representation called maximally reduced tree cover for that purpose. The maximally reduced tree cover is an improvement of a materialized transitive closure representation which has appeared in the literature. Our experiments with a medical vocabulary show that transitive closure reasoning for combined class/part/containment hierarchies in near constant time is possible for a fixed hardware configuration.",2002,Knowledge and Information Systems volume 4 issue 3 pp 305-328,transitive closure;reason;parallel algorithm;external data representation;information system;biological classification;knowledge representation and reasoning;discrete mathematics;data mining;database;artificial intelligence;machine learning;algorithm;computer science;mathematics;
Learning the Search Range for Evolutionary Optimization in Dynamic Environments,Eik Fun Khor (National University of Singapore);Kay Chen Tan (National University of Singapore);Tong Heng Lee (National University of Singapore);,"1985073907,2137721914,2141241064","Conventional evolutionary algorithms operate in a fixed search space with limiting parameter range, which is often predefined via a priori knowledge or trial and error in order to `guess' a suitable region comprising the global optimal solution. This requirement is hard, if not impossible, to fulfil in many real-world optimization problems since there is often no clue of where the desired solutions are located in these problems. Thus, this paper proposes an inductive---deductive learning approach for single- and multi-objective evolutionary optimization. The method is capable of directing evolution towards more promising search regions even if these regions are outside the initial predefined space. For problems where the global optimum is included in the initial search space, it is capable of shrinking the search space dynamically for better resolution in genetic representation to facilitate the evolutionary search towards more accurate optimal solutions. Validation results based on benchmark optimization problems show that the proposed inductive---deductive learning is capable of handling different fitness landscapes as well as distributing nondominated solutions uniformly along the final trade-offs in multi-objective optimization, even if there exist many local optima in a high-dimensional search space or the global optimum is outside the predefined search region.",2002,Knowledge and Information Systems volume 4 issue 2 pp 228-255,incremental heuristic search;beam search;local optimum;fitness landscape;directed evolution;global optimum;a priori and a posteriori;local search;multi objective optimization;simulated annealing;optimization problem;metaheuristic;evolutionary algorithm;artificial intelligence;machine learning;mathematical optimization;computer science;mathematics;
Editorial: Introduction to Web Information Systems Engineering,Qing Li (City University of Hong Kong);Yanchun Zhang (University of Southern Queensland);,"2490701366,2717032267",-,2002,Knowledge and Information Systems volume 4 issue 2 pp 129-131,web 2 0;semantic web stack;web modeling;data web;web standards;web accessibility initiative;web mapping;web application security;web development;web design;web navigation;web server;semantic web;web service;web page;information system;web intelligence;web mining;world wide web;information retrieval;data mining;computer science;
For the Automated Mark-Up of Italian Legislative Texts in XML,Andrea Bolioli;Luca Dini;Pietro Mercatali (National Research Council);Francesco Romano (National Research Council);,"2619967232,2647785689,173666153,2617912915","In this paper we will present a method for mining information within legal texts, in particular in regards to corpora of statutes. Text mining, or more in general Information Extraction, can provide a valuable help to people involved in research about the linguistic structure of statutes, and, as a side effect can be the seed for a new generation of applications for the validation and conversion in the legislative domain.",2002,Knowledge and Information Systems,side effect;information extraction;text mining;data science;information retrieval;data mining;database;computer science;
Distributed mining of classification rules,Vincent Cho (Hong Kong Polytechnic University);Beat Wüthrich (UBS);,"2012529463,2700738606","Many successful data-mining techniques and systems have been developed. These techniques usually apply to centralized databases with less restricted requirements on learning and response time. Not so much effort has yet been put into mining distributed databases and real-time issues. In this paper, we investigate issues of fast-distributed data mining. We assume that merging the distributed databases into a single one would either be too costly (distributed case) or the individual fragments would be non-uniform so that mining only one fragment would bias the result (fragmented case). The goal is to classify the objects O of the database into one of several mutually exclusive classes Ci. Our approach to make mining fast and feasible is as follows. From each data site or fragment dbk, only a single rule rik is generated for each class Ci. A small subset {ri1,.....,rih} of these individual rules is selected to form a rule set Ri for each class Ci. These rule subsets represent adequately the hidden knowledge of the entire database. Various selection criteria to form Ri are discussed, both theoretically and experimentally.",2002,Knowledge and Information Systems volume 4 issue 1 pp 1-30,confidence;support;consensus;biological classification;distributed database;data science;data mining;database;artificial intelligence;machine learning;computer science;
Ontologies for Legal Information Serving and Knowledge Management,Joost Breuker (University of Amsterdam);Abdullatif A. O. Elhag (University of Amsterdam);Emil Petkov (University of Amsterdam);Radboud Winkels (University of Amsterdam);,"2584656890,2124614683,2050138304,1996130453",-,2002,Knowledge and Information Systems,idef5;semantic web stack;process ontology;mathematical knowledge management;upper ontology;social semantic web;personal information management;body of knowledge;ontology;semantic web;domain knowledge;personal knowledge management;knowledge extraction;information management;data management;knowledge base;knowledge based systems;knowledge management;data mining;database;computer science;
Agent-based infrastructure and an application to internet information gathering,Shyh-Horng Jou (National Chung Hsing University);Shang-Juh Kao (National Chung Hsing University);,"2658041221,2632420631","Pragmatic applications and studies of agent-based software engineering have evolved over the last decade. In order to explore how an agent is organized and applied, in this paper an agent framework is presented and applied to Internet information gathering. Agent systems are classified as micro or macro perspectives and agent applications are characterized by the four feature dimensions: behavior (user), knowledge (task), safety (time), and cooperation (social). An agent itself can be modeled according to the information, behavior, and organization aspects of the agent's functional modules as proposed in this paper. A three-tier multi-agent and JAVA-implemented system, which coordinates information-gathering activities using KQML for inter-agent communication, is developed on the basis of the proposed architectural modules. Finally, we explore possible areas for future study.",2002,Knowledge and Information Systems volume 4 issue 1 pp 80-95,software agent;agent based social simulation;intelligent agent;computer security;artificial intelligence;simulation;computer science;
Evaluating interface esthetics,"David Chek Ling Ngo (Faculty of Information Technology, University Džemal Bijedić of Mostar);Lian Seng Teo (Faculty of Information Technology, University Džemal Bijedić of Mostar);John G. Byrne (Trinity College);","2632236697,2125666560,2182850360","Gestalt psychologists promulgated the principles of visual organization in the early twentieth century. These principles have been discussed and re-emphasized, and their importance and relevance to user interface design are understood. However, a limited number of systems represent and make adequate use of this knowledge in the form of a design tool that supports certain aspects of the user interface design process. The graphic design rules that these systems use are extremely rudimentary and often vastly oversimplified. Most of them have no concept of design basics such as visual balance or rhythm. In this paper, we attempt to synthesize the guidelines and empirical data related to the formatting of screen layouts into a well-defined model. Fourteen esthetic characteristics have been selected for the purpose. The results of our exercise suggest that these characteristics are important to prospective viewers.",2002,Knowledge and Information Systems volume 4 issue 1 pp 46-79,graphic design;user interface design;interface;design;artificial intelligence;simulation;programming language;computer science;
Learning premises of fuzzy rules for knowledge acquisition in classification problems,N. Xiong (Kaiserslautern University of Technology);Lothar Litz (Kaiserslautern University of Technology);Habtom W. Ressom (University of Maine);,"2172138631,2130767103,1913284061","A key issue in building fuzzy classification systems is the specification of rule conditions, which determine the structure of a knowledge base. This paper presents a new approach to automatically extract classification knowledge from numerical data by means of premise learning. A genetic algorithm is employed to search for premise structure in combination with parameters of membership functions of input fuzzy sets to yield optimal conditions of classification rules. The major advantage of our work is that a parsimonious knowledge base with a low number of rules can be achieved. The practical applicability of the proposed method is examined by computer simulations on two well-known benchmark problems of Iris Data and Cancer Data classification.",2002,Knowledge and Information Systems volume 4 issue 1 pp 96-111,fuzzy classification;membership function;fuzzy set;genetic algorithm;knowledge base;knowledge based systems;computer simulation;data mining;artificial intelligence;machine learning;computer science;mathematics;
Knowledge discovery through self-organizing maps: data visualization and query processing,Shouhong Wang (University of Massachusetts Dartmouth);Hai Wang (University of Toronto);,"2673553516,2664086495","In data mining, the usefulness of a data pattern depends on the user of the database and does not solely depend on the statistical strength of the pattern. Based on the premise that heuristic search in combinatorial spaces built on computer and human cognitive theories is useful for effective knowledge discovery, this study investigates how the use of self-organizing maps as a tool of data visualization in data mining plays a significant role in human-computer interactive knowledge discovery. This article presents the conceptual foundations of the integration of data visualization and query processing for knowledge discovery, and proposes a set of query functions for the validation of self-organizing maps in data mining.",2002,Knowledge and Information Systems volume 4 issue 1 pp 31-45,k optimal pattern discovery;software mining;self organizing map;self organization;cluster analysis;data stream mining;information visualization;knowledge extraction;data visualization;data science;information retrieval;data mining;database;machine learning;computer science;
Correlation-based Web-Document Clustering for Web Interface Design,Zhong Su (IBM);Qiang Yang (Hong Kong University of Science and Technology);H.J. Zhang (Microsoft);X.W. Xu (Siemens);Y.H. Hu (University of Wisconsin-Madison);S.P. Ma (Tsinghua University);,"2305826094,2109031554,2107003637,2703172766,2105997349,2657801236",-,2002,Knowledge and Information Systems volume 4 pp 141,web 2 0;semantic web stack;web modeling;social semantic web;static web page;web api;data web;web standards;web based simulation;web mapping;web development;web design;web navigation;web server;web service;web page;user interface;web mining;world wide web;computer science;
A fuzzy petri net for pattern recognition: application to dynamic classes,Veyis Gunes;Pierre Loonis;Michel Ménard;,"2032370888,2711001665,2486382394","When involving evolutionary natural objects, the modeling of dynamic classes is the main issue for a pattern recognition system. This problem can be avoided by making dynamic the system of pattern recognition which can then enter into various states according to the evolution of the classes. We propose a dynamic recognition system founded on two types of learning. The static aspect of the learning is ensured by classifiers or systems of classifiers, while the dynamic aspect is translated by the learning of the planning of the various states by a fuzzy Petri net. The method is successfully applied to a synthetic data set.",2002,Knowledge and Information Systems volume 4 issue 1 pp 112-128,membership function;synthetic data;petri net;fuzzy set;knowledge representation and reasoning;artificial intelligence;machine learning;algorithm;computer science;
The Effect of Formal Representation Formats on the Quality of Legal Decision-Making,Tom M. Van Engers (University of Amsterdam);Liesbeth van Driel;Margherita R. Boekenoogen;,"2103107147,2149994299,1970982729",-,2002,Knowledge and Information Systems,legal expert system;open knowledge base connectivity;body of knowledge;information system;knowledge extraction;knowledge base;knowledge based systems;knowledge management;data mining;artificial intelligence;computer science;
METAlex: Legislation in XML,Alexander Boer (University of Amsterdam);Robert J. Hoekstra;R. G. F. Winkels (University of Amsterdam);Engers van Tom M;,"2148868092,2603255147,1996130453,2121922256",-,2002,Knowledge and Information Systems,computer science;
Distributed clustering using collective principal component analysis,"Hillol Kargupta (University of Maryland, Baltimore);Weiyun Huang (Washington State University);Krishnamoorthy Sivakumar (Washington State University);Erik L. Johnson (Washington State University);","539587773,2306743686,2264914315,2159546752","This paper considers distributed clustering of high-dimensional heterogeneous data using a distributed principal component analysis (PCA) technique called the collective PCA. It presents the collective PCA technique, which can be used independent of the clustering application. It shows a way to integrate the Collective PCA with a given off-the-shelf clustering algorithm in order to develop a distributed clustering technique. It also presents experimental results using different test data sets including an application for web mining.",2001,Knowledge and Information Systems volume 3 issue 4 pp 422-448,correlation clustering;cure data clustering algorithm;fuzzy clustering;principal component analysis;cluster analysis;data mining;pattern recognition;machine learning;computer science;
Multivariate discretization for set mining,"Stephen D. Bay (University of California, Irvine);",2719938533,"Many algorithms in data mining can be formulated as a set-mining problem where the goal is to find conjunctions (or disjunctions) of terms that meet user-specified constraints. Set-mining techniques have been largely designed for categorical or discrete data where variables can only take on a fixed number of values. However, many datasets also contain continuous variables and a common method of dealing with these is to discretize them by breaking them into ranges. Most discretization methods are univariate and consider only a single feature at a time (sometimes in conjunction with a class variable). We argue that this is a suboptimal approach for knowledge discovery as univariate discretization can destroy hidden patterns in data. Discretization should consider the effects on all variables in the analysis and that two regions X and Y should only be in the same interval after discretization if the instances in those regions have similar multivariate distributions (Fx∼Fy) across all variables and combinations of variables. We present a bottom-up merging algorithm to discretize continuous variables based on this rule. Our experiments indicate that the approach is feasible, that it will not destroy hidden patterns and that it will generate meaningful intervals.",2001,Knowledge and Information Systems volume 3 issue 4 pp 491-512,discretization of continuous features;discretization;multivariate normal distribution;top down and bottom up design;knowledge extraction;data mining;machine learning;statistics;computer science;mathematics;
Arbitrating among competing classifiers using learned referees,Julio Ortega (IBM);Moshe Koppel (Bar-Ilan University);Shlomo Argamon (Bar-Ilan University);,"2660220891,2163015493,77528956","The situation in which the results of several different classifiers and learning algorithms are obtainable for a single classification problem is common. In this paper, we propose a method that takes a collection of existing classifiers and learning algorithms, together with a set of available data, and creates a combined classifier that takes advantage of all of these sources of knowledge. The basic idea is that each classifier has a particular subdomain for which it is most reliable. Therefore, we induce a referee for each classifier, which describes its area of expertise. Given such a description, we arbitrate between the component classifiers by using the most reliable classifier for the examples in each subdomain. In experiments in several domains, we found such arbitration to be significantly more effective than various voting techniques which do not seek out subdomains of expertise. Our results further suggest that the more fine grained the analysis of the areas of expertise of the competing classifiers, the more effectively they can be combined. In particular, we find that classification accuracy increases greatly when using intermediate subconcepts from the classifiers themselves as features for the induction of referees.",2001,Knowledge and Information Systems volume 3 issue 4 pp 470-490,cascading classifiers;random subspace method;decision tree;data mining;pattern recognition;artificial intelligence;machine learning;computer science;
Parallel and sequential algorithms for data mining using inductive logic,David B. Skillicorn (Queen's University);Yu Wang (IBM);,"2031406638,2424762841","Inductive logic is a research area in the intersection of machine learning and logic programming, and has been increasingly applied to data mining. Inductive logic studies learning from examples, within the framework provided by clausal logic. It provides a uniform and expressive means of representation: examples, background knowledge, and induced theories are all expressed in first-order logic. Such an expressive representation is computationally expensive, so it is natural to consider improving the performance of inductive logic data mining using parallelism. We present a parallelization technique for inductive logic, and implement a parallel version of a core inductive logic programming system: Progol. The technique provides perfect partitioning of computation and data access and communication requirements are small, so almost linear speedup is readily achieved. However, we also show why the information flow of the technique permits superlinear speedup over the standard sequential algorithm. Performance results on several datasets and platforms are reported. The results have wider implications for the design on parallel and sequential data-mining algorithms.",2001,Knowledge and Information Systems volume 3 issue 4 pp 405-421,bunched logic;multimodal logic;logic optimization;horn clause;inductive bias;binary space partitioning;bulk synchronous parallel;description logic;data access;information flow;logic synthesis;first order logic;inductive programming;statistical relational learning;theoretical computer science;data mining;database;artificial intelligence;machine learning;algorithm;computer science;
Making use of the most expressive jumping emerging patterns for classification,Jinyan Li (University of Melbourne);Guozhu Dong (Wright State University);Kotagiri Ramamohanarao (University of Melbourne);,"2190567876,2164298414,123309386","Classification aims to discover a model from training data that can be used to predict the class of test instances. In this paper, we propose the use of jumping emerging patterns (JEPs) as the basis for a new classifier called the JEP-Classifier. Each JEP can capture some crucial difference between a pair of datasets. Then, aggregating all JEPs of large supports can produce a more potent classification power. Procedurally, the JEP-Classifier learns the pair-wise features (sets of JEPs) contained in the training data, and uses the collective impacts contributed by the most expressive pair-wise features to determine the class labels of the test data. Using only the most expressive JEPs in the JEP-Classifier strengthens its resistance to noise in the training data, and reduces its complexity (as there are usually a very large number of JEPs). We use two algorithms for constructing the JEP-Classifier which are both scalable and efficient. These algorithms make use of the border representation to efficiently store and manipulate JEPs. We also present experimental results which show that the JEP-Classifier achieves much higher testing accuracies than the association-based classifier of (Liu et al, 1998), which was reported to outperform C4.5 in general.",2001,Knowledge and Information Systems volume 3 issue 2 pp 131-145,biological classification;data mining;database;artificial intelligence;machine learning;computer science;
Multipass algorithms for mining association rules in text databases,John D. Holt (Wright State University);Soon Myoung Chung (Wright State University);,"2137722529,2145880949","In this paper, we propose two new algorithms for mining association rules between words in text databases. The characteristics of text databases are quite different from those of retail transaction databases, and existing mining algorithms cannot handle text databases efficiently because of the large number of itemsets (i.e., words) that need to be counted. Two well-known mining algorithms, Apriori algorithm and Direct Hashing and Pruning (DHP) algorithm, are evaluated in the context of mining text databases, and are compared with the new proposed algorithms named Multipass-Apriori (M-Apriori) and Multipass-DHP (M-DHP). It has been shown that the proposed algorithms have better performance for large text databases.",2001,Knowledge and Information Systems volume 3 issue 2 pp 168-183,apriori algorithm;association rule learning;concept mining;text mining;information retrieval;data mining;database;computer science;
C-Net: a method for generating non-deterministic and dynamic multivariate decision trees,Hussein A. Abbass (University of New South Wales);Michael Towsey (Queensland University of Technology);Gerard D. Finn (Queensland University of Technology);,"1702763375,2020534464,2138016595","Despite the fact that artificial neural networks (ANNs) are universal function approximators, their black box nature (that is, their lack of direct interpretability or expressive power) limits their utility. In contrast, univariate decision trees (UDTs) have expressive power, although usually they are not as accurate as ANNs. We propose an improvement, C-Net, for both the expressiveness of ANNs and the accuracy of UDTs by consolidating both technologies for generating multivariate decision trees (MDTs). In addition, we introduce a new concept, recurrent decision trees, where C-Net uses recurrent neural networks to generate an MDT with a recurrent feature. That is, a memory is associated with each node in the tree with a recursive condition which replaces the conventional linear one. Furthermore, we show empirically that, in our test cases, our proposed method achieves a balance of comprehensibility and accuracy intermediate between ANNs and UDTs. MDTs are found to be intermediate since they are more expressive than ANNs and more accurate than UDTs. Moreover, in all cases MDTs are more compact (i.e., smaller tree size) than UDTs.",2001,Knowledge and Information Systems volume 3 issue 2 pp 184-197,recurrent neural network;expressive power;function approximation;decision tree;artificial neural network;artificial intelligence;machine learning;algorithm;computer science;
XML indexing and retrieval with a hybrid storage model: 252,Dongwook Shin (National Institutes of Health);,2305755335,"XML DTD (Document Type Declaration) puts two distinctive entities (attribute and element content) together into one framework for representing different document features. The notion of attribute in the XML DTD is similar to the field representation in the database, whereas the element content corresponds to the full text. In this paper, we view these two entities as different, each of which requires a different model for storage and retrieval. Attributes are stored in a database system, whereas the element contents and their indices are saved in files. We present a technique that puts together those two in an efficient way and builds an XML retrieval system on top of that. Such a system can achieve a reasonable trade-off between performance and cost in indexing and retrieval.",2001,Knowledge and Information Systems volume 3 issue 2 pp 252-261,xml schema editor;xml catalog;well formed document;xml validation;efficient xml interchange;xml framework;xml namespace;xml encryption;streaming xml;xml signature;simple api for xml;document type definition;xml database;document structure description;xml schema;vector space model;data retrieval;xml;information retrieval;data mining;database;computer science;
ActiveCBR: an agent system that integrates case-based reasoning and active databases,Sheng Li (Simon Fraser University);Qiang Yang (Simon Fraser University);,"2492249607,2109031554","Case-based reasoning (CBR) is an artificial intelligence (AI) technique for problem solving that uses previous similar examples to solve a current problem. Despite its success, most current CBR systems are passive: they require human users to activate them manually and to provide information about the incoming problem explicitly. In this paper, we present an integrated agent system that integrates CBR systems with an active database system. Active databases, with the support of active rules, can perform event detection, condition monitoring, and event handling (action execution) in an automatic manner. The integrated ActiveCBR system consists of two layers. In the lower layer, the active database is rule-driven; in the higher layer, the result of action execution of active rules is transformed into feature–value pairs required by the CBR subsystem. The layered architecture separates CBR from sophisticated rule-based reasoning, and improves the traditional passive CBR system with the active property. The system has both real-time response and is highly exible in knowledge management as well as autonomously in response to events that a passive CBR system cannot handle. We demonstrate the system efficiency and effectiveness through empirical tests.",2001,Knowledge and Information Systems volume 3 issue 2 pp 225-251,case based reasoning;rule based system;data mining;database;artificial intelligence;computer science;
A description length-based decision criterion for default knowledge in the ripple down rules method,Takuya Wada (Osaka University);Tadashi Horiuchi (Osaka University);Hiroshi Motoda (Osaka University);Takashi Washio (Osaka University);,"2142885196,2050587694,323689644,1794038515","The ‘Ripple Down Rules (RDR)’ method is a promising approach to directly acquiring and encoding knowledge from human experts. It requires data to be supplied incrementally to the knowledge base being constructed, each new piece of knowledge being added as an exception to the existing knowledge base. Because of this patching principle, the knowledge acquired depends strongly on what is given as the default knowledge, used as an implicit outcome when inference fails. Therefore, it is important to choose good default knowledge for constructing an accurate and compact knowledge base. Further, real-world data are often noisy and we want the RDR to be noise resistant. This paper reports experimental results about the effect of the selection of default knowledge and the amount of noise in data on the performance of RDR, using a simulated expert in place of a human expert. The best default knowledge is characterized as the class knowledge that maximizes the description length of encoding rules and misclassified cases. We confirmed by extensive experimentation that this criterion is indeed valid and useful in constructing an accurate and compact knowledge base. We also ascertained that the same criterion holds when the data are noisy.",2001,Knowledge and Information Systems volume 3 issue 2 pp 146-167,knowledge base;knowledge based systems;knowledge management;data mining;artificial intelligence;machine learning;computer science;
A hybrid fragmentation approach for disrtributed deductive database systems,Seung Jin Lim (Brigham Young University);Yiu-Kai Ng (Brigham Young University);,"2109819818,2135939557","Fragmentation of base relations in distributed database management systems increases the level of concurrency and therefore system throughput for query processing. Algorithms for horizontal and vertical fragmentation of relations in relational, object-oriented and deductive databases exist; however, hybrid fragmentation techniques based on variable bindings appearing in user queries and query-access-rule dependency are lacking for deductive database systems. In this paper, we propose a hybrid fragmentation approach for distributed deductive database systems. Our approach first considers the horizontal partition of base relations according to the bindings imposed on user queries, and then generates vertical fragments of the horizontally partitioned relations and clusters rules using affinity of attributes and access frequency of queries and rules. The proposed fragmentation technique facilitates the design of distributed deductive database systems.",2001,Knowledge and Information Systems volume 3 issue 2 pp 198-224,database theory;management system;object oriented programming;distributed database;theoretical computer science;distributed computing;database;computer science;
Parallel data mining for association rules on shared memory systems,Srinivasan Parthasarathy (Ohio State University);Mohammed Javeed Zaki (Rensselaer Polytechnic Institute);Mitsunori Ogihara (University of Rochester);Wei Li 0015 (Intel);,"2106796124,2165917828,2123444417,2607728121","In this paper we present a new parallel algorithm for data mining of association rules on shared-memory multiprocessors. We study the degree of parallelism, synchronization, and data locality issues, and present optimizations for fast frequency computation. Experiments show that a significant improvement of performance is achieved using our proposed optimizations. We also achieved good speed-up for the parallel algorithm.",2001,Knowledge and Information Systems volume 3 issue 1 pp 1-29,association rule learning;data structure;theoretical computer science;parallel computing;data mining;database;programming language;computer science;
Zipf's law for Web surfers,"Mark Levene (University College London);José Luis Cabral de Moura Borges (University College London);George Loizou (Birkbeck, University of London);","2164185460,2124974212,2216061209","One of the main activities of Web users, known as ‘surfing’, is to follow links. Lengthy navigation often leads to disorientation when users lose track of the context in which they are navigating and are unsure how to proceed in terms of the goal of their original query. Studying navigation patterns of Web users is thus important, since it can lead us to a better understanding of the problems users face when they are surfing. We derive Zipf's rank frequency law (i.e., an inverse power law) from an absorbing Markov chain model of surfers' behavior assuming that less probable navigation trails are, on average, longer than more probable ones. In our model the probability of a trail is interpreted as the relevance (or ‘value’) of the trail. We apply our model to two scenarios: in the first the probability of a user terminating the navigation session is independent of the number of links he has followed so far, and in the second the probability of a user terminating the navigation session increases by a constant each time the user follows a link. We analyze these scenarios using two sets of experimental data sets showing that, although the first scenario is only a rough approximation of surfers' behavior, the data is consistent with the second scenario and can thus provide an explanation of surfers' behavior.",2001,Knowledge and Information Systems volume 3 issue 1 pp 120-129,absorbing markov chain;web navigation;zipf s law;power law;markov chain;world wide web;data mining;artificial intelligence;simulation;statistics;computer science;
On similarity measures for multimedia database applications,K. Selçuk Candan (NEC Corporation of America);Wen-Syan Li (NEC Corporation of America);,"674992784,2251660241","A multimedia database query consists of a set of fuzzy and boolean (or crisp) predicates, constants, variables, and conjunction, disjunction, and negation operators. The fuzzy predicates are evaluated based on different media criteria, such as color, shape, layout, keyword. Since media-based evaluation yields similarity values, results to such a query is defined as an ordered set. Since many multimedia applications require partial matches, query results also include tuples which do not satisfy all predicates. Hence, any fuzzy semantics which extends the boolean semantics of conjunction in a straight forward manner may not be desirable for multimedia databases. In this paper, we focus on the problem of ‘given a multimedia query which consists of multiple fuzzy and crisp predicates, how to provide the user with a meaningful overall ranking.’ More specifically, we study the problem of merging similarity values in queries with multiple fuzzy predicates. We describe the essential multimedia retrieval semantics, compare these with the known approaches, and propose a semantics which captures the retrieval requirements in multimedia databases.",2001,Knowledge and Information Systems volume 3 issue 1 pp 30-51,fuzzy set operations;fuzzy number;score;fuzzy logic;satisfiability;information retrieval;data mining;database;statistics;computer science;
Fuzzy user modeling for information retrieval on the World Wide Web,Robert Ivor John (De Montfort University);Gabrielle J. Mooney (De Montfort University);,"2166922526,2133972089","Information retrieval from the World Wide Web through the use of search engines is known to be unable to capture effectively the information needs of users. The approach taken in this paper is to add intelligence to information retrieval from the World Wide Web, by the modeling of users to improve the interaction between the user and information retrieval systems. In other words, to improve the performance of the user in retrieving information from the information source. To effect such an improvement, it is necessary that any retrieval system should somehow make inferences concerning the information the user might want. The system then can aid the user, for instance by giving suggestions or by adapting any query based on predictions furnished by the model. So, by a combination of user modeling and fuzzy logic a prototype system has been developed (the Fuzzy Modeling Query Assistant (FMQA)) which modifies a user's query based on a fuzzy user model. The FMQA was tested via a user study which clearly indicated that, for the limited domain chosen, the modified queries are better than those that are left unmodified.",2001,Knowledge and Information Systems volume 3 issue 1 pp 81-95,cognitive models of information retrieval;human computer information retrieval;information filtering system;information mapping;user interface design;query expansion;user;user modeling;information needs;relevance;fuzzy logic;search engine;document retrieval;world wide web;information retrieval;data mining;database;computer science;
An artificial network simulating cause-to-effect reasoning: cancellation interactions and numerical studies,Lotfi Ben Romdhane (Université de Sherbrooke);Béchir el Ayeb (Université de Sherbrooke);Shengrui Wang (Université de Sherbrooke);,"1988434177,242779350,2106818440","During the last few decades, a variety of models have been proposed to address causal reasoning (known also as abduction); most of these dealt with a probabilistic or a logical framework. Recently, a few models have been proposed within a neural framework. The investigation of neural approaches is mainly motivated by the computational burden of the causal reasoning task and by the satisfactory results given by neural networks in solving hard problems in general. A particular class of causal reasoning that raises several difficulties is the cancellation class. From an abstract point of view, cancellation occurs when two causes (hypotheses) cancel each other's explanation capabilities with respect to a given effect (observation). The present work is twofold. First, we extend an existing neural model to handle cancellation interactions. Second, we test the model on a large database and propose objective criteria to quantitatively evaluate the scenarios (explanations) produced. Simulation results show good performance and stability of the model.",2001,Knowledge and Information Systems volume 3 issue 1 pp 96-119,logical framework;causal reasoning;network simulation;competition;artificial neural network;data mining;artificial intelligence;machine learning;simulation;programming language;computer science;
Repesenting and reasoning on database conceptual schemas,Mohand-Said Hacid (University of Lyon);Jean-Marc Petit (Blaise Pascal University);Farouk Toumani (Blaise Pascal University);,"2044619081,2226135823,2283235115","In this paper we address the problem of reasoning about database conceptual schemas by exploiting the possibility of using a description logic. We develop an approach by using as a foundation an entity-relationship model that displays features such as ISA, disjointness and cardinality constraints. We propose an equivalence-preserving transformation of entity relationship schemas into terminologies in a description logic. This equivalence, based on the measure of information capacity, ensures that the semantics of entity-relationship schemas is accurately captured by the corresponding terminologies. As a consequence, reasoning on entity-relationship schemas is appropriately reduced to reasoning on terminologies in a description logic.",2001,Knowledge and Information Systems volume 3 issue 1 pp 52-80,deductive reasoning;conceptual graph;entity relationship model;description logic;natural language processing;data mining;database;artificial intelligence;computer science;
Cost Complexity-Based Pruning of Ensemble Classifiers,Andreas L. Prodromidis (Columbia University);Salvatore J. Stolfo (Columbia University);,"222714987,2021877992","In this paper we study methods that combine multiple classification models learned over separate data sets. Numerous studies posit that such approaches provide the means to efficiently scale learning to large data sets, while also boosting the accuracy of individual classifiers. These gains, however, come at the expense of an increased demand for run-time system resources. The final ensemble meta-classifier may consist of a large collection of base classifiers that require increased memory resources while also slowing down classification throughput. Here, we describe an algorithm for pruning (i.e., discarding a subset of the available base classifiers) the ensemble meta-classifier as a means to reduce its size while preserving its accuracy and we present a technique for measuring the trade-off between predictive performance and available run-time system resources. The algorithm is independent of the method used initially when computing the meta-classifier. It is based on decision tree pruning methods and relies on the mapping of an arbitrary ensemble meta-classifier to a decision tree model. Through an extensive empirical study on meta-classifiers computed over two real data sets, we illustrate our pruning algorithm to be a robust and competitive approach to discarding classification models without degrading the overall predictive performance of the smaller ensemble computed over those that remain after pruning.",2001,Knowledge and Information Systems volume 3 issue 4 pp 449-469,pruning;random subspace method;pruning;decision tree;empirical research;ensemble learning;distributed database;data mining;database;pattern recognition;machine learning;computer science;
Distributed Web Log Mining Using Maximal Large Itemsets,Mehmet Sayal (Hewlett-Packard);Peter Scheuermann (Northwestern University);,"2131501168,797615088","We introduce a partitioning-based distributed document-clustering algorithm using user access patterns from multi-server web sites. Our algorithm makes it possible to exploit simultaneously adaptive document replication and persistent connections, two techniques that are most effective in decreasing the response time that is observed by web users. The algorithm first distributes the user access data evenly among the servers by using a hash function. Then, each server generates a local clustering on its fair share of the user sessions records by employing a traditional single-machine document-clustering algorithm. Finally, those local clustering results are combined together by using a novel procedure that generates maximal large itemsets of web documents. We present preliminary experimental results and discuss alternative approaches to be pursued in the future.",2001,Knowledge and Information Systems volume 3 issue 4 pp 389-404,web log analysis software;hash function;document clustering;world wide web;data mining;database;computer science;
Dimensionality reduction for fast similarity search in large time series databases,"Eamonn J. Keogh (University of California, Irvine);Kaushik Chakrabarti (University of Illinois at Urbana–Champaign);Michael J. Pazzani (University of California, Irvine);Sharad Mehrotra (University of California, Irvine);","2170070822,2124676925,1996789426,2201039448","The problem of similarity search in large time series databases has attracted much attention recently. It is a non-trivial problem because of the inherent high dimensionality of the data. The most promising solutions involve first performing dimensionality reduction on the data, and then indexing the reduced data with a spatial access method. Three major dimensionality reduction techniques have been proposed: Singular Value Decomposition (SVD), the Discrete Fourier transform (DFT), and more recently the Discrete Wavelet Transform (DWT). In this work we introduce a new dimensionality reduction technique which we call Piecewise Aggregate Approximation (PAA). We theoretically and empirically compare it to the other techniques and demonstrate its superiority. In addition to being competitive with or faster than the other methods, our approach has numerous other advantages. It is simple to understand and to implement, it allows more flexible distance measures, including weighted Euclidean queries, and the index can be built in linear time.",2001,Knowledge and Information Systems volume 3 issue 3 pp 263-286,diffusion map;discrete wavelet transform;discrete fourier transform;singular value decomposition;dimensionality reduction;time complexity;time series;data mining;pattern recognition;machine learning;statistics;computer science;mathematics;
Temporal Defeasible Reasoning,Juan Carlos Augusto (Universidad Nacional del Sur);Guillermo Ricardo Simari (Universidad Nacional del Sur);,"2590216878,2245549248","An argumentation system that allows temporal reasoning using the notions of instant and interval is presented. Previous proposals just considered either instants or intervals. A many-sorted logic is used to represent temporal knowledge at the monotonic level. The logic considers how to formalize knowledge about explicit temporal references, events, properties and actions. The argumentation system provides a non-monotonic layer in which to reason about the justification of truths in the system. The proposal is illustrated showing how to solve well-known problems of the literature.",2001,Knowledge and Information Systems volume 3 issue 3 pp 287-318,reasoning system;defeasible reasoning;interval temporal logic;deductive reasoning;non monotonic logic;model based reasoning;knowledge representation and reasoning;natural language processing;artificial intelligence;algorithm;computer science;mathematics;
Efficient Graph-Based Algorithms for Discovering and Maintaining Association Rules in Large Databases,Guanling Lee (National Tsing Hua University);K. L. Lee (National Tsing Hua University);Arbee L. P. Chen (National Tsing Hua University);,"2114215785,2699874638,2211440602","In this paper, we study the issues of mining and maintaining association rules in a large database of customer transactions. The problem of mining association rules can be mapped into the problems of finding large itemsets which are sets of items brought together in a sufficient number of transactions. We revise a graph-based algorithm to further speed up the process of itemset generation. In addition, we extend our revised algorithm to maintain discovered association rules when incremental or decremental updates are made to the databases. Experimental results show the efficiency of our algorithms. The revised algorithm is a significant improvement over the original one on mining association rules. The algorithms for maintaining association rules are more efficient than re-running the mining algorithms for the whole updated database and outperform previously proposed algorithms that need multiple passes over the database.",2001,Knowledge and Information Systems volume 3 issue 3 pp 338-355,apriori algorithm;bit array;association rule learning;data mining;database;machine learning;programming language;computer science;
Generalized Affinity-Based Association Rule Mining for Multimedia Database Queries,Mei-Ling Shyu (University of Miami);Shu-Ching Chen (Florida International University);Rangasami L. Kashyap (Purdue University);,"2041921851,2118447230,2346493798","The recent progress in high-speed communication networks and large-capacity storage devices has led to a tremendous increase in the number of databases and the volume of data in them. This has created a need to discover structural equivalence relationships from the databases since queries tend to access information from structurally equivalent media objects residing in different databases. The more databases there are, the more query-processing performance improvement can be achieved when the structural equivalence relationships are automatically discovered. In response to such a demand, association rule mining has emerged and proven to be a highly successful technique for discovering knowledge from large databases. In this paper, we explore a generalized affinity-based association rule mining approach to discover the quasi-equivalence relationships from a network of databases. The algorithm is implemented and two empirical studies on real databases are conducted. The results show that the proposed generalized affinity-based association rule mining approach not only correctly exploits the set of quasi-equivalent media objects from the databases, but also outperforms the basic association rule mining approach in the discovery of the quasi-equivalent media object pairs.",2001,Knowledge and Information Systems volume 3 issue 3 pp 319-337,k optimal pattern discovery;apriori algorithm;database theory;association rule learning;empirical research;data science;data mining;database;computer science;
SAFE: An Efficient Feature Extraction Technique,Ujjwal Maulik (Kalyani Government Engineering College);Sanghamitra Bandyopadhyay (Indian Statistical Institute);John C. Trinder (University of New South Wales);,"2232264627,2148373778,2124806996","This paper proposes an efficient window-based semi-automatic feature extraction technique which uses simulated annealing for minimizing the energy of an active contour within a specified image region. The energy is computed based on a chamfer image, in which pixel values are a function of distance to image edges. A user places a number of control points close to the feature of interest. B-spline fitted to these points provides an initial approximation of the contour. A window containing both the initial contour and the feature of interest is considered. The contour with minimum energy inside the window provides the final delineation. Comparison of the performance of the proposed algorithm with traditional snake, a popular feature extraction technique based on energy minimization, demonstrates the superiority of the SAFE technique.",2001,Knowledge and Information Systems volume 3 issue 3 pp 374-387,b spline;active contour model;energy minimization;edge detection;feature extraction;simulated annealing;computer vision;pattern recognition;computer science;
Discovering Text Databases with Neural Nets,Yong S. Choi (National Center for Science Education);,2653989559,"Since documents on the Web are naturally partitioned into many text databases, the efficient document retrieval process requires identifying the text databases that are most likely to provide relevant documents to the query and then searching for the identified text databases. In this paper, we propose a neural net based approach to such an efficient document retrieval. First, we present a neural net agent that learns about underlying text databases from the user's relevance feedback. For a given query, the neural net agent, which is sufficiently trained on the basis of the BPN learning mechanism, discovers the text databases associated with the relevant documents and retrieves those documents effectively. In order to scale our approach with the large number of text databases, we also propose the hierarchical organization of neural net agents which reduces the total training cost at the acceptable level. Finally, we evaluate the performance of our approach by comparing it to those of the conventional well-known approaches.",2001,Knowledge and Information Systems volume 3 issue 3 pp 356-373,full text search;document retrieval;information retrieval;data mining;database;machine learning;computer science;
Principal component analysis for dimension reduction in massive distributed data sets,"Hillol Kargupta (University of Maryland, Baltimore County);W. Huang (Washington State University);Krishnamoorthy Sivakumar (Washington State University);Erik L. Johnson (Washington State University);","539587773,2306743686,2264914315,2159546752",-,2001,Knowledge and Information Systems,multilinear principal component analysis;multiple correspondence analysis;kernel principal component analysis;dimensionality reduction;principal component analysis;pattern recognition;machine learning;statistics;computer science;
Contrary to Duty Obligations A Study in Legal Ontology,"Jaap C. Hage (Faculty of Law, University of Colombo);",2102370249,-,2001,Knowledge and Information Systems,deontic logic;knowledge management;computer security;computer science;
Intentions in the coordinated generation of graphics and text from tabular data,Massimo Fasciano;Guy Lapalme (Université de Montréal);,"1993190316,2113271024","To use graphics efficiently in an automatic report generation system, one has to model messages and how they go from the writer (intention) to the reader (interpretation). This paper describes PostGraphe, a system which generates a report integrating graphics and text from a set of writer's intentions. The system is given the data in tabular form as might be found in a spreadsheet; also input is a declaration of the types of values in the columns of the table. The user then indicates the intentions to be conveyed in the graphics (e.g., compare two variables or show the evolution of a set of variables) and the system generates a report in LATEX with the appropriate PostScript graphic files. PostGraphe uses the same information to generate the accompanying text that helps the reader to focus on the important points of the graphics. We also describe how these ideas have been embedded to create a new Chart Wizard for Microsoft Excel.",2000,Knowledge and Information Systems volume 2 issue 3 pp 310-339,computer graphics metafile;graphics software;multimedia;computer graphics images;world wide web;data mining;database;artificial intelligence;machine learning;computer science;
An intelligent decision support system for investment analysis,Kim-Leng Poh (National University of Singapore);,2165045243,"We present an intelligent decision support system that combines decision analysis and traditional investment evaluation and analysis. The system brings to the ordinary user expertise in both decision analysis and investment evaluation techniques, as well as domain knowledge about the market. The system supports the entire decision analysis cycle and provides facilities and tools for decision modeling, probability assessment, model evaluation, and sensitivity analysis. An example based on the Shanghai Stock Market is presented. We demonstrate how an investment decision model for the stock market is constructed by the system and how the optimal decision is obtained. Sensitivity and value of information analysis were also carried out by the system.",2000,Knowledge and Information Systems volume 2 issue 3 pp 340-358,r cast;decision analysis cycle;business decision mapping;optimal decision;evidential reasoning approach;influence diagram;decision model;value of information;domain knowledge;sensitivity analysis;decision rule;decision tree;decision engineering;intelligent decision support system;decision analysis;decision support system;management science;knowledge management;data mining;artificial intelligence;computer science;
Spatio-temporal analysis with the self-organizing feature map,Susan E. George (University of South Australia);,2153332452,Spatio-temporal pattern recognition problems are particularly challenging. They typically involve detecting change that occurs over time in two-dimensional patterns. Analytic techniques devised for temporal data must take into account the spatial relationships among data points. An artificial neural network known as the self-organizing feature map (SOM) has been used to analyze spatial data. This paper further investigates the use of the SOM with spatio-temporal pattern recognition. The principles of the two-dimensional SOM are developed into a novel three-dimensional network and experiments demonstrate that (i) the three-dimensional network makes a better topological ordering and (ii) there is a difference in terms of the spatio-temporal analysis that can be made with the three-dimensional network.,2000,Knowledge and Information Systems volume 2 issue 3 pp 359-372,temporal database;spatial relation;spatial analysis;visualization;three dimensional space;artificial neural network;computer vision;data mining;machine learning;computer science;
A Mathematical Foundation for Improved Reduct Generation in Information Systems,Janusz A. Starzyk (Ohio University);Dale E. Nelson (Air Force Research Laboratory);Kirk Sturtz;,"2134759901,2301114172,2072996038","When data sets are analyzed, statistical pattern recognition is often used to find the information hidden in the data. Another approach to information discovery is data mining. Data mining is concerned with finding previously undiscovered relationships in data sets. Rough set theory provides a theoretical basis from which to find these undiscovered relationships. We define a new theoretical concept, strong compressibility, and present the mathematical foundation for an efficient algorithm, the Expansion Algorithm, for generation of all reducts of an information system. The process of finding reducts has been proven to be NP-hard. Using the elimination method, problems of size 13 could be solved in reasonable times. Using our Expansion Algorithm, the size of problems that can be solved has grown to 40. Further, by using the strong compressibility property in the Expansion Algorithm, additional savings of up to 50% can be achieved. This paper presents this algorithm and the simulation results obtained from randomly generated information systems.",2000,Knowledge and Information Systems volume 2 issue 2 pp 131-146,fsa red algorithm;rough set;data mining;machine learning;algorithm;computer science;mathematics;
An Index Structure for Data Mining and Clustering,Xiong Wang (New Jersey Institute of Technology);Jason Tsong-Li Wang (New Jersey Institute of Technology);King-Ip Lin (University of Memphis);Dennis E. Shasha (Courant Institute of Mathematical Sciences);Bruce A. Shapiro (National Institutes of Health);Kaizhong Zhang (University of Western Ontario);,"2140924016,2167133797,2099166163,2082974086,2120862559,2117210734","In this paper we present an index structure, called MetricMap, that takes a set of objects and a distance metric and then maps those objects to a k-dimensional space in such a way that the distances among objects are approximately preserved. The index structure is a useful tool for clustering and visualization in data-intensive applications, because it replaces expensive distance calculations by sum-of-square calculations. This can make clustering in large databases with expensive distance metrics practical. We compare the index structure with another data mining index structure, FastMap, recently proposed by Faloutsos and Lin, according to two criteria: relative error and clustering accuracy. For relative error, we show that (i) FastMap gives a lower relative error than MetricMap for Euclidean distances, (ii) MetricMap gives a lower relative error than FastMap for non-Euclidean distances (i.e., general distance metrics), and (iii) combining the two reduces the error yet further. A similar result is obtained when comparing the accuracy of clustering. These results hold for different data sizes. The main qualitative conclusion is that these two index structures capture complementary information about distance metrics and therefore can be used together to great benefit. The net effect is that multi-day computations can be done in minutes.",2000,Knowledge and Information Systems volume 2 issue 2 pp 161-184,k medians clustering;approximation error;hierarchical clustering;explained sum of squares;visualization;euclidean distance;metric;cluster analysis;information engineering;knowledge extraction;data science;data mining;machine learning;statistics;computer science;mathematics;
Reinvestigating Dempster's Idea on Evidence Combination,Weiru Liu (Ulster University);Jun Hong (Ulster University);,"2148409606,2125000923","In this paper, we investigate the problem encountered by Dempster's combination rule in view of Dempster's original combination framework. We first show that the root of Dempster's combination rule (defined and named by Shafer) is Dempster's original idea on evidence combination. We then argue that Dempster's original idea on evidence combination is, in fact, richer than what has been formulated in the rule. We conclude that, by strictly following what Dempster has suggested, there should be no counterintuitive results when combining evidence.",2000,Knowledge and Information Systems volume 2 issue 2 pp 223-241,evidential reasoning approach;dempster shafer theory;data mining;pattern recognition;artificial intelligence;computer science;mathematics;
Pest Control Expert System for Tomato (PCEST),El-Sayed El-Azhary;Hesham A. Hassan (Cairo University);Ahmed A. Rafea (American University in Cairo);,"2035487061,2396915727,2134534628","This paper presents a real-life pest control expert system for tomato. The system involves two main subtasks, namely: ‘diagnose’ and ‘treat’. The ‘diagnose’ subtask finds out the causes of the growers' complaints, while the ‘treat’ subtask finds out a treatment plan for these causes. CommonKADS methodology has been used to develop the system. Dependency network is used as one of our knowledge representation schemes in both subtasks. An expert system evaluation methodology has been suggested and applied to the developed system.",2000,Knowledge and Information Systems volume 2 issue 2 pp 242-257,pest control;expert system;data mining;artificial intelligence;computer science;
Comparison between Lamarckian and Darwinian Evolution on a Model Using Neural Networks and Genetic Algorithms,Takahiro Sasaki (Keio University);Mario Tokoro (Keio University);,"2110433836,2190019005","In this paper, we study the relationship between learning and evolution in a simple abstract model, where neural networks capable of learning are evolved using genetic algorithms (GAs). Each individual tries to acquire a proper behavior under a given environment through its lifetime learning, and the best individuals are selected to reproduce offspring, which then conduct lifetime learning in the succeeding generation. The connective weights of individuals' neural networks undergo modification, i.e., certain characters will be acquired, through their lifetime learning. By setting various rates for the heritability of acquired characters, which control the strength of ‘Lamarckian’ strategy, we observe adaptational processes of populations over successive generations. By taking the degree of environmental changes into consideration, we show the following results. Under static environments, populations with higher rates of heritability adapt themselves more quickly toward the environments, and thus perform well. On the other hand, under nonstationary environments, populations with lower rates of heritability not only show more stable behavior against environmental changes, but also maintain greater adaptability with respect to such changing environments. Consequently, the population with zero heritability, i.e., the Darwinian population, attains the highest level of adaptation towards dynamic environments.",2000,Knowledge and Information Systems volume 2 issue 2 pp 201-222,genetic algorithm;artificial neural network;artificial intelligence;machine learning;computer science;
Pattern Analysis and Analogy in Shogi: Predicting Shogi Moves from Prior Experience,Steven Walczak (University of Colorado Denver);Reijer Grimbergen (Yamagata University);,"2145377897,2127868676","As a research paradigm, pattern analysis has been shown to be an effective tool for analyzing complex game situations in both chess and go. We extend the prior pattern analysis research in chess to the domain of shogi. Shogi is computationally more complex than chess and should realize greater benefits than the chess domain from pattern recognition and pattern exploitation research. The IAM program, which has accurately predicted up to 28% of the moves for a specific chess player, is redesigned to operate in the domain of shogi. Results similar to those achieved for the domain of chess are achieved in shogi.",2000,Knowledge and Information Systems volume 2 issue 2 pp 185-200,analogy;artificial intelligence;machine learning;simulation;computer science;
Fast association discovery in derivative transaction collections,Li Shen (Dartmouth College);Hong Shen (Griffith University);Ling Cheng (Dartmouth College);Paul Andrew Pritchard (Griffith University);,"2121150543,2194483685,2126628614,2619820006","Association discovery from a transaction collection is an important data-mining task. We study a new problem in this area whose solution can provide users with valuable association rules in some relevant collections: association discovery in derivative transaction collections. In this problem, we are given association rules in two transaction collections D1 and D2, and aim to find new association rules in derivative transaction collections D1∖D2, D1∩D2, D2∖D1 and D1∪D2. Direct application of existing algorithms can solve this problem, but in an expensive way. We propose an efficient solution through making full use of already discovered information, taking advantage of the relationships existing among relevant collections, and avoiding unnecessary but expensive support-counting operations by scanning databases. Experiments on well-known synthetic data show that our solution consistently outperforms the naive solution by factors from 2 to 3 in most cases. We also propose an efficient parallelization of our approach, as parallel algorithms are often interesting and necessary in the area of data mining.",2000,Knowledge and Information Systems volume 2 issue 2 pp 147-160,synthetic data;association rule learning;parallel algorithm;data science;data mining;database;computer science;
Nesting and defoliation of index expressions for information retrieval,Bernd Wondergem (Radboud University Nijmegen);Patrick van Bommel (Radboud University Nijmegen);Theo P. van der Weide (Radboud University Nijmegen);,"1215463233,2142719163,2130187599","In this article, a formalisation of index expressions is presented. Index expressions are more expressive than keywords while maintaining a comprehensible complexity. Index expressions are well-known in Information Retrieval (IR), where they are used for characterising document contents, formulation of user interests, and matching mechanisms. In addition, index expressions have found both practical and theoretical applicability in 2-level hypermedia systems for IR. In these applications, properties of (the structure of) index expressions are heavily relied upon. However, the presupposed mathematical formalisation of index expressions and their properties still lacks. Our formalism is based on the structural notation of index expressions. It is complete in the sense that several notions of subexpressions and defoliation of index expressions are also formalised. Defoliation, which plays an important role in defining properties of index expressions, is provided as a recursively defined operator. Finally, two other representational formalisms for index expressions are compared to ours.",2000,Knowledge and Information Systems volume 2 issue 1 pp 33-52,theoretical computer science;data mining;database;computer science;
XML and Industrial Standards for Electronic Commerce,Haifei Li (University of Florida);,2697172441,"With the rapid adoption of XML as the meta data format for electronic commerce applications, many XML-based industrial standards for electronic commerce have been proposed. Since XML only defines a standardized syntax that introduces new tags used to represent data semantics, the task of defining a common set of tags and organizing tags into Data Type Definitions (DTDs) is left to developers of XML applications. XML applications do not interact with each other if their tags and DTDs are different. In this paper, we discuss and compare seven industrial standards for electronic commerce based on XML. They are BizTalk, CBL, cXML, IOTP, OAGIS, OCF, and RETML. Each standard is categorized according to its coverage of business activities. The following observations are made after examining industrial standards from different perspectives. Firstly, XML has a big impact on industrial standards for electronic commerce. Secondly, most industrial standards are not quite mature at the current stage, and there are no apparent leaders among the competitors. Thirdly, the complexity of their DTDs varies quantitatively and qualitatively. Fourthly, the integration of industrial standards is imperative if applications based on different standards intend to exchange XML documents smoothly.",2000,Knowledge and Information Systems volume 2 issue 4 pp 487-497,xml schema editor;xml base;xml schema;xml catalog;well formed document;cxml;xml validation;efficient xml interchange;xml framework;xml encryption;streaming xml;xml signature;binary xml;ebxml;xml database;document structure description;xml schema;xml;e commerce;world wide web;data mining;database;programming language;computer science;
Reference Ontology and (ONTO)2 Agent: The Ontology Yellow Pages,Julio César Arpírez (Technical University of Madrid);Asunción Gómez-Pérez (Technical University of Madrid);Adolfo Lozano-Tello;Helena Sofia Andrade N. P. Pinto (Instituto Superior Técnico);,"2058102267,2247736927,2636008889,2163874071","Knowledge reuse by means of ontologies faces three important problems at present: (1) there are no standardized identifying features that characterize ontologies from the user point of view; (2) there are no web sites using the same logical organization, presenting relevant information about ontologies; and (3) the search for appropriate ontologies is hard, time-consuming and usually fruitless. To solve the above problems, we present: (1) a living set of features that allow us to characterize ontologies from the user point of view and have the same logical organization; (2) a living domain ontology about ontologies (called Reference Ontology) that gathers, describes and has links to existing ontologies; and (3) (ONTO)2 Agent, the ontology-based WWW broker about ontologies that uses Reference Ontology as a source of its knowledge and retrieves descriptions of ontologies that satisfy a given set of constraints.",2000,Knowledge and Information Systems volume 2 issue 4 pp 387-412,ontology components;bibliographic ontology;idef5;ontology based data integration;process ontology;upper ontology;open biomedical ontologies;ontology language;ontology alignment;suggested upper merged ontology;web ontology language;ontology inference layer;ontology;satisfiability;information retrieval;data mining;database;computer science;
The Impact of Payoff Function and Local Interaction on the N-Player Iterated Prisoner's Dilemma,Yeon-Gyu Seo (Yonsei University);Sung-Bae Cho (Yonsei University);Xin Yao (University of Birmingham);,"2148197969,2647181681,2106268727","The N-player iterated prisoner's dilemma (NIPD) game has been widely used to study the evolution of cooperation in social, economic and biological systems. This paper studies the impact of dierent payo functions and local interactions on the NIPD game. The evolutionary approach is used to evolve game-playing strategies starting from a population of random strategies. The dierent payo functions used in our study describe dierent behaviors of cooperation and defection among a group of players. Local interaction introduces neighborhoods into the NIPD game. A player does not play against every other player in a group any more. He only interacts with his neighbors. We investigate the impact of neighborhood size on the evolution of cooperation in the NIPD game and the generalization ability of evolved strategies.",2000,Knowledge and Information Systems volume 2 issue 4 pp 461-478,superrationality;simultaneous game;strong reciprocity;generalization;repeated game;artificial intelligence;simulation;
Knowledge Bases with Output,Nicolae Ţăndăreanu (University of Craiova);,31793909,"In this paper we shall propose a knowledge representation method, the deduction process in a knowledge representation and reasoning system using this method and therefore the answer function of such a system. This is an algebraic method because some concepts of universal algebra are used. More precisely, we introduce the concept of knowledge base with output (KBO), the central structure being that of labeled stratified graph. For such a base we distinguish a structure and two kinds of computations. The structure is described by a labeled stratified graph and an output space. The computations in a KBO are represented by two levels: syntactic and semantic. Both the syntactic and the semantic computations in a KBO are described in this paper. Finally, we exemplify all the concepts and we apply the method to solve a given problem in travel scheduling. All the theoretical results are proved in a separate section.",2000,Knowledge and Information Systems volume 2 issue 4 pp 438-460,morphism;universal algebra;knowledge representation and reasoning;knowledge based systems;theoretical computer science;discrete mathematics;artificial intelligence;machine learning;computer science;mathematics;
Incremental view maintenance for mobile databases,Ken C. K. Lee (Hong Kong Polytechnic University);Hong Va Leong (Hong Kong Polytechnic University);Antonio Si (Oracle Corporation);,"2630595792,2233290192,2557286592","In a mobile environment, querying a database at a stationary server from a mobile client is expensive due to the limited bandwidth of a wireless channel and the instability of the wireless network. We address this problem by maintaining a materialized view in a mobile client's local storage. Such a materialized view can be considered as a data warehouse. The materialized view contains results of common queries in which the mobile client is interested. In this paper, we address the view update problem for maintaining the consistency between a materialized view at a mobile client and the database server. The content of a materialized view could become incoherent with that at the database server when the content of the database server and/or when the location of the client is changed. Existing view update mechanisms are ‘push-based’. The server is responsible for notifying all clients whose views might be affected by the changes in database or the mobility of the client. This is not appropriate in a mobile environment due to the frequent wireless channel disconnection. Furthermore, it is not easy for a server to keep track of client movements to update individual client location-dependent views. We propose a ‘pull-based’ approach that allows a materialized view to be updated at a client in an incremental manner, requiring a client to request changes to its view from the server. We demonstrate the feasibility of our approach with experimental results.",2000,Knowledge and Information Systems volume 2 issue 4 pp 413-437,mobile database;view;materialized view;data warehouse;world wide web;data mining;database;computer science;
Towards a Model of Learning through Communication,Nadim Obeid (University of Jordan);,2107019284,"Communication is an interactive, complex, structured process involving agents that are capable of drawing conclusions from the information they have available about some real-life situations. Such situations are generally characterized as being imperfect. In this paper, we aim to address learning from the perspective of the communication between agents. To learn a collection of propositions concerning some situation is to incorporate it within one's knowledge about that situation. That is, the key factor in this activity is for the goal agent, where agents may switch role if appropriate, to integrate the information offered with what it already knows. This may require a process of belief revision, which suggests that the process of incorporation of new information should be modeled nonmonotonically. We shall employ for reasoning a three-valued based nonmonotonic logic that formalizes some aspects of revisable reasoning and it is accessible to implementation. The logic is sound and complete. A theorem-prover of the logic has successfully been implemented.",2000,Knowledge and Information Systems volume 2 issue 4 pp 498-508,non monotonic logic;artificial intelligence;machine learning;algorithm;computer science;mathematics;
A Note on the Refinement of Nonmonotonic Knowledge Bases,G. Antoniou (Griffith University);Cara MacNish (University of Western Australia);N. Y. Foo (University of New South Wales);,"2048729876,2296418942,76983209","Refinement of logical theories plays an important role in various application domains, notably in software engineering. This note introduces and studies refinement notions for nonmonotonic knowledge bases in default logic. The paper motivates and proposes refinement concepts, discusses their relationship, and establishes sufficient conditions for refinement.",2000,Knowledge and Information Systems volume 2 issue 4 pp 479-486,non monotonic logic;refinement;data mining;artificial intelligence;algorithm;computer science;mathematics;
The State of the Art in Agent Communication Languages,Mamadou Tadiou Kone (Japan Advanced Institute of Science and Technology);Akira Shimazu (Japan Advanced Institute of Science and Technology);Tatsuo Nakajima (Waseda University);,"1244148654,684514524,2246046809","Like societies of humans, there is a need for agents in a multi-agent system to rely on one another, enlist the support of peers in order to solve complex tasks. Agents will be able to cooperate only through a meaningful communication language that can bear correctly their mental states and convey precisely the content of their messages. In search for the ideal agent communication language (ACL), several initiatives like the pioneering work of the Knowledge Sharing Effort and the Foundation for Intelligent Physical Agents (FIPA) are paving the way for a platform where all agents would be able to interact regardless of their implementation environment. ACL is a new field of study that could gain from a survey in expanding its application areas. For this purpose, we examine in this paper the state of the art in ACL design and suggest some principles for building a generalized ACL framework. We then evaluate some existing ACL models, and present the current issues in ACL research, and new perspectives.",2000,Knowledge and Information Systems volume 2 issue 3 pp 259-284,multi agent system;artificial intelligence;simulation;computer science;
A Probe-Based Technique to Optimize Join Queries in Distributed Internet Databases,Cyrus Shahabi (University of Southern California);Latifur Khan (University of Southern California);Dennis McLeod (University of Southern California);,"240820708,2155983610,2097719264","An adaptive probe-based optimization technique is developed and demonstrated in the context of an Internet-based distributed database environment. More and more common are database systems which are distributed across servers communicating via the Internet where a query at a given site might require data from remote sites. Optimizing the response time of such queries is a challenging task due to the unpredictability of server performance and network traffic at the time of data shipment; this may result in the selection of an expensive query plan using a static query optimizer. We constructed an experimental setup consisting of two servers running the same database management system connected via the Internet. Concentrating on join queries, we demonstrate how a static query optimizer might choose an expensive plan by mistake. This is due to the lack of a priori knowledge of the run-time environment, inaccurate statistical assumptions in size estimation, and neglecting the cost of remote method invocation. These shortcomings are addressed collectively by proposing a probing mechanism. An implementation of our run-time optimization technique for join queries was constructed in the Java language and incorporated into an experimental setup. The results demonstrate the superiority of our probe-based optimization over a static optimization.",2000,Knowledge and Information Systems volume 2 issue 3 pp 373-385,spatial query;view;database theory;query optimization;a priori and a posteriori;query language;distributed database;distributed computing;data mining;database;computer science;
Simulating the Ecology of Oligopolistic Competition with Genetic Algorithms,Shu-Heng Chen (National Chengchi University);Chih-Chi Ni (National Chengchi University);,"2658731097,2715258468","In economics, the n-person oligopoly game and the n-person IPD game are often considered close in spirit. Our analytical framework shows that this is not the case owing to the path dependence of the pay-off matrix of the oligopoly game. By simulating the evolution of a three-person oligopoly game with genetic algorithms, we explore the significance of the path dependence property to the rich ecology of oligopoly. The emergent behavior of oligopolists in the simulations indicates how the path dependence nature may shed light on the phenotypes and genotypes coming into existence. The features shown in this research can be further exploited in more practical contexts so that nontrivial policy issues in industrial economics can be seriously tackled.",2000,Knowledge and Information Systems volume 2 issue 3 pp 285-309,genetic algorithm;oligopoly;artificial intelligence;computer science;
Database intergration using neural networks: implementation and experiences,Wen-Syan Li (NEC Corporation of America);Chris Clifton (Mitre Corporation);Shu-Yao Liu (Oracle Corporation);,"2251660241,2158488542,2280525933","Applications in a wide variety of industries require access to multiple heterogeneous distributed databases. One step in heterogeneous database integration is semantic integration: identifying corresponding attributes in different databases that represent the same real world concept. The rules of semantic integration can not be ‘pre-programmed’ since the information to be accessed is heterogeneous and attribute correspondences could be fuzzy. Manually comparing all possible pairs of attributes is an unreasonably large task. We have applied artificial neural networks (ANNs) to this problem. Metadata describing attributes is automatically extracted from a database to represent their ‘signatures’. The metadata is used to train neural networks to find similar patterns of metadata describing corresponding attributes from other databases. In our system, the rules to determine corresponding attributes are discovered through machine learning. This paper describes how we applied neural network techniques in a database integration problem and how we represent an attribute with its metadata as discriminators. This paper focuses on our experiments on effectiveness of neural networks and each discriminator. We also discuss difficulties of using neural networks for this problem and our wish list for the Machine Learning community.",2000,Knowledge and Information Systems volume 2 issue 1 pp 73-96,database testing;semantic integration;data integration;time delay neural network;deep learning;artificial neural network;data mining;database;machine learning;computer science;
FANNC: a fast adaptive neural network classifier,Zhihua Zhou (Nanjing University);Shifu Chen (Nanjing University);Zhaoqian Chen (Nanjing University);,"2286237009,2149194538,2135089442","In this paper, a fast adaptive neural network classifier named FANNC is proposed. FANNC exploits the advantages of both adaptive resonance theory and field theory. It needs only one-pass learning, and achieves not only high predictive accuracy but also fast learning speed. Besides, FANNC has incremental learning ability. When new instances are fed, it does not need to retrain the whole training set. Instead, it could learn the knowledge encoded in those instances through slightly adjusting the network topology when necessary, that is, adaptively appending one or two hidden units and corresponding connections to the existing network. This characteristic makes FANNC fit for real-time online learning tasks. Moreover, since the network architecture is adaptively set up, the disadvantage of manually determining the number of hidden units of most feed-forward neural networks is overcome. Benchmark tests show that FANNC is a preferable neural network classifier, which is superior to several other neural algorithms on both predictive accuracy and learning speed.",2000,Knowledge and Information Systems volume 2 issue 1 pp 115-129,types of artificial neural networks;stability;wake sleep algorithm;stochastic neural network;adaptive resonance theory;probabilistic neural network;time delay neural network;recurrent neural network;competitive learning;network architecture;feedforward neural network;network topology;field theory;learning classifier system;deep learning;unsupervised learning;artificial neural network;pattern recognition;artificial intelligence;machine learning;computer science;
Expert system support and juridical quality,Marga M. Groothuis;Jörgen S. Svensson;,"2630393521,2674810789",This article discusses the use of expert systems as a means of achieving juridical quality within administrative organisations. Do these systems really improve the quality of decision making and provide the desired guarantees with respect to the correct treatment of clients?,2000,Knowledge and Information Systems,legal expert system;subject matter expert;management science;knowledge management;data mining;computer science;
Integrating database and dialogue design,Klaus-Dieter Schewe (Massey University);Bettina Schewe;,"2632317859,2155697851",The user interface of data-intensive information systems may be described as a collection of semi-independent dialogues with underlying procedures accessing databases. It will be shown how to conceptualize dialogues via dialogue objects. These are organized in an object oriented fashion by dialogue classes modelled as extended database views. The description is based on an object oriented model which enables a smooth integration of dialogues with the underlying database schema.,2000,Knowledge and Information Systems volume 2 issue 1 pp 1-32,database model;view;database schema;conceptual schema;database theory;database design;object oriented programming;user interface;information system;intelligent database;knowledge management;data mining;database;computer science;
The Consultancy Game,Alexander Boer (University of Amsterdam);,2148868092,-,2000,Knowledge and Information Systems pp 99-112,computer science;
Using Pattern Decomposition Methods for Finding All Frequent Patterns in Large Datasets,"Qinghua Zou (University of California, Los Angeles);Henry Chiu (IBM);Wesley W. Chu (University of California, Los Angeles);","2050846921,2102657940,2168131383",-,2000,Knowledge and Information Systems,decomposition method;association rule learning;data mining;pattern recognition;computer science;
Combining deontic and action logics for collective agency,Lamber M. M. Royakkers;,2722743524,-,2000,Knowledge and Information Systems,accessibility relation;deontic logic;dynamic logic;artificial intelligence;algorithm;computer science;
Neural network learning using entropy cycle,Geok See Ng (Nanyang Technological University);Khue Hiang Chan (Nanyang Technological University);Sevki S. Erdogan (Nanyang Technological University);Harcharan Singh (Nanyang Technological University);,"2181485039,2097178335,2330356158,2574676505","In this paper, an additional entropy penalty term is used to steer the direction of the hidden node's activation in the process of learning. A state with minimum entropy means that most nodes are operating in the non-linear zones (i.e. saturation zones) near the extreme ends of the Sigmoid curve. As the training proceeds, redundant hidden nodes' activations are pushed towards their extreme value corresponding to a low entropy state with maximum information, while some relevant nodes remain active in the linear zone. As training progresses, more nodes get into saturation zones. The early creation of such nodes may impair generalization performance. To prevent the network from being driven into saturation before it can really learn, an entropy cycle is proposed in this paper to dampen the creation of such inactive nodes in the early stage of training. At the end of training, these inactive nodes can then be eliminated without affecting the performance of the original network. The concept has been successfully applied for pruning in two classification problems. The experiments indicate that redundant nodes are pruned resulting in optimal network topologies.",2000,Knowledge and Information Systems volume 2 issue 1 pp 53-72,cross entropy;extreme value theory;network topology;information theory;pattern recognition;artificial intelligence;machine learning;statistics;computer science;mathematics;
A temporal logic for supporting historical databases,Shichao Zhang (National University of Singapore);,2617908352,"Tackling data with gap-interval time is an important issue faced by the temporal database community. While a number of interval logics have been developed, less work has been reported on gap-interval time. To represent and handle data with time, a clause ‘when’ is generally added into each conventional operator so as to incorporate time dimension in temporal databases, which clause ‘when’ is really a temporal logical sentence. Unfortunately, though several temporal database models have dealt with data with gap-interval time, they still put interval calculus methods on gap-intervals. Certainly, it is inadequate to tackle data with gap-interval time using interval calculus methods in historical databases. Consequently, what temporal expressions are valid in the clause ‘when’ for tackling data with gap-interval time? Further, what temporal operations and relations can be used in the clause ‘when’? To solve these problems, a formal tool for supporting data with gap-interval time must be explored. For this reason, a gap-interval-based logic for historical databases is established in this paper. In particular, we discuss how to determine the temporal relationships after an event explodes. This can be used to describe the temporal forms of tuples splitting in historical databases.",2000,Knowledge and Information Systems volume 2 issue 1 pp 97-114,interval temporal logic;temporal database;database theory;temporal logic;planning;data mining;database;artificial intelligence;algorithm;computer science;
Extended Conceptual Retrieval,Radboud Winkels (University of Amsterdam);Doeko Bosscher;Alexander Boer (University of Amsterdam);Rinke J. Hoekstra (VU University Amsterdam);,"1996130453,2660393043,2148868092,2144494049",-,2000,Knowledge and Information Systems,cognitive models of information retrieval;standard boolean model;divergence from randomness model;human computer information retrieval;adversarial information retrieval;concept search;okapi bm25;deontic logic;visual word;query expansion;vector space model;string searching algorithm;data retrieval;query language;relevance;information system;question answering;document retrieval;natural language processing;information retrieval;database;computer science;
A Comprehensive Survey of Evolutionary-Based Multiobjective Optimization Techniques,Carlos A. Coello Coello (CINVESTAV);,2040834078,"This paper presents a critical review of the most important evolutionary-based multiobjective optimization techniques developed over the years, emphasizing the importance of analyzing their Operations Research roots as a way to motivate the development of new approaches that exploit the search capabilities of evolutionary algorithms. Each technique is briefly described with its advantages and disadvantages, its degree of applicability and some of its known applications. Finally, the future trends in this discipline and some of the open areas of research are also addressed.",1999,Knowledge and Information Systems volume 1 issue 3 pp 269-308,engineering optimization;test functions for optimization;evolutionary programming;multi objective optimization;metaheuristic;evolutionary computation;evolutionary algorithm;management science;artificial intelligence;machine learning;mathematical optimization;computer science;
Using Unbalanced Trees for Indexing Multidimensional Objects,Charu C. Aggarwal (IBM);Joel L. Wolf (IBM);Philip S. Yu (IBM);Marina Epelman (Massachusetts Institute of Technology);,"2146335907,2127587015,2125104194,2032839960","In this paper we introduce a new multidimensional index structure called the S-tree. Such indexes are appropriate for a large variety of pictorial databases such as cartography, satellite and medical images. The S-tree discussed in this paper is similar in flavor to the standard R-tree, but accepts mild imbalance in the resulting tree in return for a significantly reduced area, overlap and perimeter in the resulting minimum bounding rectangles. In fact, the S-tree is defined in terms of a parameter which governs the degree to which this trade-off is allowed. We develop an efficient packing algorithm based on this parameter. We then analyze the S-tree analytically, giving theoretical bounds on the degree of imbalance of the tree. We also analyze the S-tree experimentally. The S-tree does well in two dimensions, and even better in three dimensions. Indeed, the S-tree can be expected to do better still as the dimensionality increases. While the S-tree is extremely effective for static databases, we outline the extension to dynamic databases as well.",1999,Knowledge and Information Systems volume 1 issue 3 pp 309-336,r tree;theoretical computer science;combinatorics;data mining;database;artificial intelligence;machine learning;statistics;computer science;mathematics;
Searching the Web with Queries,Zhixiang Chen (University of Texas–Pan American);Xiannong Meng (University of Texas–Pan American);Richard H. Fowler (University of Texas–Pan American);,"2223943589,2097154047,2132592458","In this paper we study the problem of searching the Web with online learning algorithms. We consider that Web documents can be represented by vectors of n boolean attributes. A search engine is viewed as a learner, and a user is viewed as a teacher. We investigate the number of queries a search engine needs from the user to search for a collection of Web documents. We design several efficient learning algorithms to search for any collection of documents represented by a disjunction (or a conjunction) of relevant attributes with the help of membership queries or equivalence queries.",1999,Knowledge and Information Systems volume 1 issue 3 pp 369-375,queries per second;search analytics;web search query;web query classification;search engine;vector space;semantic search;metasearch engine;world wide web;information retrieval;data mining;database;computer science;
Handling of Alternatives and Events in Temporal Databases,Nandlal L. Sarda (Indian Institute of Technology Bombay);P. V. Siva Prasada Reddy (Indian Institute of Technology Bombay);,"1984189984,2479254607","Planning for the future is an important activity both at the individual and organizational levels. Planning consists of defining alternative actions to handle various events in the future. The alternatives arise becau]se of different possible outcomes of events. A plan consists of a sequence of actions to be carried out for each possible outcome. In the context of database modeling, the actions are operations on a database. A database management system should enable its users to define events and alternatives, and also allow them to interact with the database under different alternatives (possibly to evaluate different plans). The existing temporal data models treat the future analogous to the past or present; they provide for one future path (in the sense that facts valid at some future time can be stored), but do not provide support for alternatives in the future. In this paper, we present a model for incorporating events and alternatives by extending the temporal data model to support branching time. The extended model permits definitions of events, their interdependencies and associated actions. The events that affect an object are modeled by a tree, permitting an object to have different states at the same valid time but under different alternatives. The branching time paradigm is obtained by superimposing a linear valid time on the event tree. We extend the temporal relational algebra and the Temporal SQL2 to support a branching time data model. The paper also briefly deals with the uncertainties associated with future planning as well as probabilities of possible event outcomes. Finally, we sketch an implementation strategy for the branching time data model.",1999,Knowledge and Information Systems volume 1 issue 3 pp 337-368,temporal database;query language;systems modeling;data mining;database;simulation;computer science;
Feature Selection Using the Domain Relationship with Genetic Algorithms,Nidapan Chaikla (Asian Institute of Technology);Yulu Qi (Asian Institute of Technology);,"2643752089,2673609917","Considering the importance of the domain relationship in eliminating noisy features in feature selection, we present an alternate approach to designing a multi-objective fitness function using multiple correlation for the genetic algorithm (GA), which is used as a search tool in the problem. Multiple correlation is a simple statistical technique that uses the multiple correlation coefficients to measure the relationship between a dependent variable and a set of independent variables within the domain space. Simulation studies were conducted on both real-world and controlled data sets to assess the performance of the proposed fitness function. The comparison between the traditional fitness function and our proposed function is also reported. The results show that the proposed fitness function can perform more satisfactorily than the traditional one in all cases considered, including different data types, multi-class and multi-dimensional data.",1999,Knowledge and Information Systems volume 1 issue 3 pp 377-390,fitness proportionate selection;fitness approximation;multiple correlation;fitness function;genetic algorithm;feature selection;pattern recognition;machine learning;statistics;computer science;mathematics;
Agent-Based Systems for Intelligent Manufacturing: A State-of-the-Art Survey,Weiming Shen (University of Calgary);Douglas H. Norrie (University of Calgary);,"2700651817,2010126967","Agent technology has been considered as an important approach for developing distributed intelligent manufacturing systems. A number of researchers have attempted to apply agent technology to manufacturing enterprise integration, supply chain management, manufacturing planning, scheduling and control, materials handling, and holonic manufacturing systems. This paper gives a brief survey of some related projects in this area, and discusses some key issues in developing agent-based manufacturing systems such as agent technology for enterprise integration and supply chain management, agent encapsulation, system architectures, dynamic system reconfiguration, learning, design and manufacturability assessments, distributed dynamic scheduling, integration of planning and scheduling, concurrent scheduling and execution, factory control structures, potential tools and standards for developing agent-based manufacturing systems. An extensive annotated bibliography is provided.",1999,Knowledge and Information Systems volume 1 issue 2 pp 129-156,integrated computer aided manufacturing;manufacturing execution system;process development execution system;computer integrated manufacturing;software agent;intelligent agent;artificial intelligence;computer science;
On modeling and verification of temporal constraints in production workflows,Oliveria Marjanovic (University of Queensland);Maria E. Orlowska (University of Queensland);,"2575990311,2123081123","The dynamic nature of events, in particular business processes, is a natural and accepted feature of today’s business environment. Therefore, workflow systems, if they are to successfully model portions of the real world, need to acknowledge the temporal aspect of business processes. This is particularly true for processes where any deviation from the prescribed model is either very expensive, dangerous or even illegal. Such processes include legal processes, airline maintenance or hazardous material handling. However, time modeling in workflows is still an open research problem. This paper proposes a framework for time modeling in production workflows. Relevant temporal constraints are presented, and rules for their verification are defined. Furthermore, to enable visualization of some temporal constraints, a concept of “duration space” is introduced. The duration algorithm which calculates the shortest/longest workflow instance is presented. It is a generalization of two categories of algorithms: the shortest-path partitioning algorithm and the Critical Path Method (CPM). Based on the duration algorithm, the verification algorithm is designed to check the consistency of introduced temporal constraints.",1999,Knowledge and Information Systems volume 1 issue 2 pp 157-192,verification;data mining;database;real time computing;machine learning;simulation;computer science;
HierarchyScan: A Hierarchical Algorithm for Similarity Search in Databases Consisting of Long Sequences,Chung-Sheng Li (IBM);Philip S. Yu (IBM);Vittorio Castelli (IBM);,"2111614675,2125104194,2132335929","In this paper, a hierarchical algorithm, HierarchyScan, is proposed to efficiently locate one-dimensional subsequences within a collection of sequences with arbitrary length. The proposed algorithm performs correlation between the stored sequences and the template pattern in the transformed domain to identify subsequences in a scale- and phase-independent fashion. This is in contrast to those approaches based on the computation of Euclidean distance in the transformed domain. In the proposed hierarchical algorithm, the transformed domain representation of each original sequence is divided into multiple groups of coefficients. The matching is performed hierarchically from the group with the greatest filtering capability to the group with the lowest filtering capability. Only those subsequences whose maximum correlation value is higher than a predefined threshold will be selected for additional screening. This approach is compared to the sequential scanning and an order-of-magnitude speedup is observed.",1999,Knowledge and Information Systems volume 1 issue 2 pp 229-256,template matching;correlation;time series;combinatorics;pattern recognition;machine learning;statistics;mathematics;
IMC: A Method for Interval Calculus in Matrix,Shichao Zhang (National University of Singapore);Chengqi Zhang (Deakin University);,"2617908352,2166080598","Time representation is important in many applications, such as temporal databases, planning, and multi-agents. Since Allen’s work on binary interval relations (called interval algebra), many researchers have further investigated temporal information processing based on interval calculus. However, there are still some limitations, e.g. constraint satisfaction is a NP-hard problem in interval calculus. For this reason, we propose a new interpretation for interval relationships and their calculus in this paper, which establishes a new method to transform interval calculus into matrix calculus. Our experiments show that this method propagates temporal relations faster than interval algebra.",1999,Knowledge and Information Systems volume 1 issue 2 pp 257-268,interval temporal logic;temporal database;constraint satisfaction;np hard;temporal logic;planning;discrete mathematics;combinatorics;artificial intelligence;algorithm;computer science;mathematics;
Making Database Schema Hierarchical for Visual Access to Databases,Ping-Kuen Chen (National Central University);Gwo-Dong Chen (National Central University);,"2492142020,2684098563","Making the database schema hierarchical can help a casual user retrieve information from a complex database. The hierarchical database schema provides further insight into database content and focuses on meaningful data by a top-down method. The user can proceed with a hierarchical visual query, which ultimately simplifies the query, reduces the syntax error rate and conserves the query time. In this paper, we present a hierarchical graph which makes the database schema hierarchical, naturally integrates the browsing and querying and, consequently, allows the user to proceed with an incremental query on the hierarchical database schema. Also proven herein are the existence, uniformity, and consistency of the hierarchical graph to verify that the graph can be used to query the database. This paper also discusses the semantics of high-level nodes and conducts an experiment to evaluate users’ performance. Finally, we describe how one can use the hierarchical graph to unify the tasks of making the schema hierarchical, creating concept hierarchies, and integrating the databases.",1999,Knowledge and Information Systems volume 1 issue 2 pp 193-227,semi structured model;schema migration;online aggregation;star schema;database tuning;graph database;database model;view;database schema;conceptual schema;query optimization;network model;information retrieval;data mining;database;computer science;
Data Preparation for Mining World Wide Web Browsing Patterns,Robert Cooley (University of Minnesota);Bamshad Mobasher (University of Minnesota);Jaideep Srivastava (University of Minnesota);,"2117479945,1892801027,2192802387","The World Wide Web (WWW) continues to grow at an astounding rate in both the sheer volume of traffic and the size and complexity of Web sites. The complexity of tasks such as Web site design, Web server design, and of simply navigating through a Web site have increased along with this growth. An important input to these design tasks is the analysis of how a Web site is being used. Usage analysis includes straightforward statistics, such as page access frequency, as well as more sophisticated forms of analysis, such as finding the common traversal paths through a Web site. Web Usage Mining is the application of data mining techniques to usage logs of large Web data repositories in order to produce results that can be used in the design tasks mentioned above. However, there are several preprocessing tasks that must be performed prior to applying data mining algorithms to the data collected from server logs. This paper presents several data preparation techniques in order to identify unique users and user sessions. Also, a method to divide user sessions into semantically meaningful transactions is defined and successfully tested against two other methods. Transactions identified by the proposed methods are used to discover association rules from real world data using the WEBMINER system [15].",1999,Knowledge and Information Systems volume 1 issue 1 pp 5-32,web 2 0;web modeling;social semantic web;web analytics;data web;web standards;web based simulation;site map;web mapping;web development;web design;web navigation;web server;web service;web page;association rule learning;web intelligence;web mining;world wide web;data mining;database;computer science;
"Data Mining via Discretization, Generalization and Rough Set Feature Selection",Xiaohua Hu (Drexel University);Nick Cercone (University of Waterloo);,"2207916209,76047928","We present a data mining method which integrates discretization, generalization and rough set feature selection. Our method reduces the data horizontally and vertically. In the first phase, discretization and generalization are integrated. Numeric attributes are discretized into a few intervals. The primitive values of symbolic attributes are replaced by high level concepts and some obvious superfluous or irrelevant symbolic attributes are also eliminated. The horizontal reduction is done by merging identical tuples after substituting an attribute value by its higher level value in a pre- defined concept hierarchy for symbolic attributes, or the discretization of continuous (or numeric) attributes. This phase greatly decreases the number of tuples we consider further in the database(s). In the second phase, a novel context- sensitive feature merit measure is used to rank features, a subset of relevant attributes is chosen, based on rough set theory and the merit values of the features. A reduced table is obtained by removing those attributes which are not in the relevant attributes subset and the data set is further reduced vertically without changing the interdependence relationships between the classes and the attributes. Finally, the tuples in the reduced relation are transformed into different knowledge rules based on different knowledge discovery algorithms. Based on these principles, a prototype knowledge discovery system DBROUGH-II has been constructed by integrating discretization, generalization, rough set feature selection and a variety of data mining algorithms. Tests on a telecommunication customer data warehouse demonstrates that different kinds of knowledge rules, such as characteristic rules, discriminant rules, maximal generalized classification rules, and data evolution regularities, can be discovered efficiently and effectively.",1999,Knowledge and Information Systems volume 1 issue 1 pp 33-60,discretization of continuous features;superkey;discretization;rough set;generalization;data warehouse;feature selection;data mining;pattern recognition;machine learning;computer science;mathematics;
Towards Automated Case Knowledge Discovery in the M2 Case-based Reasoning System,David W. Patterson (Ulster University);Sarabjot S. Anand (Ulster University);Werner Dubitzky (Ulster University);John G. Hughes (Ulster University);,"2148759883,2278760076,258661121,2130559414","In this paper we present the M2 Case-Based Reasoning (CBR) system. The M2 system addresses a number of issues that present methodologies for CBR systems have shied away from. We discuss techniques for removing the knowledge acquisition bottlenecks when acquiring case knowledge. Here, case knowledge refers to the complementary knowledge structures case (more specific in nature) and adaptation rules (more general). We address the use of negative cases for updating the case knowledge as well as for refining the similarity measures. In particular we discuss in detail, showing experimental results, the use of Data Mining within the M2 system to build the case base from a database containing operational data, and discover adaptation rules. A methodology to monitor the competence of the CBR system and to utilise negative cases for updating the CBR system to enhance its competence is also discussed. The M2 CBR system also employs Rough Set and Fuzzy Set theories to further enhance its capabilities within real-world applications as well as providing a richer and truer model of human reasoning.",1999,Knowledge and Information Systems volume 1 issue 1 pp 61-82,case based reasoning;rough set;model based reasoning;knowledge management;data mining;database;artificial intelligence;machine learning;computer science;
Comparative Evaluation of Two Neural Network Based Techniques for the Classification of Microcalcifications in Digital Mammograms,Brijesh K. Verma (Griffith University);,2162951420,This paper investigates two neural network based techniques for the classification of microcalcifications in digital mammograms. Both techniques extract suspicious areas containing microcalcifications from digital mammograms and classify them into two categories: whether they contain benign or malignant clusters. The centroids and radii provided by expert radiologists are being used to locate and extract suspicious areas. Two neural networks based on iterative and non-iterative training methods are used to classify them into benign or malignant. The proposed techniques have been implemented in C++ on the SP2 supercomputer. The database from the Department of Radiology at the University of Nijmegen has been used for the experiments. The comparative results are very interesting and promising. Some of them are included in this paper.,1999,Knowledge and Information Systems volume 1 issue 1 pp 107-117,biological classification;artificial neural network;data mining;artificial intelligence;machine learning;computer science;
Learning from Batched Data: Model Combination Versus Data Combination,Kai Ming Ting (Deakin University);Boon Toh Low (The Chinese University of Hong Kong);Ian H. Witten (University of Waikato);,"2121496889,2110714070,2163446563","Combining models learned from multiple batches of data provide an alternative to the common practice of learning one model from all the available data (i.e. the data combination approach). This paper empirically examines the base-line behavior of the model combination approach in this multiple-data-batches scenario. We find that model combination can lead to better performance even if the disjoint batches of data are drawn randomly from a larger sample, and relate the relative performance of the two approaches to the learning curve of the classifier used. In the beginning of the curve, model combination has higher bias and variance than data combination and thus a higher error rate. As training data increases, model combination has either a lower error rate than or a comparable performance to data combination because the former achieves larger variance reduction. We also show that this result is not sensitive to the methods of model combination employed. Another interesting result is that we empirically show that the near-asymptotic performance of a single model in some classification tasks can be significantly improved by combining multiple models (derived from the same algorithm) in the multiple-data-batches scenario.",1999,Knowledge and Information Systems volume 1 issue 1 pp 83-106,learning curve;data model;econometrics;data mining;artificial intelligence;machine learning;statistics;computer science;
Managing Null Entries in Pairwise Comparisons,Waldemar W. Koczkodaj (Laurentian University);Michael W. Herman (Laurentian University);Marian Orlowski (Queensland University of Technology);,"2051004478,2100440747,2233818682","This paper shows how to manage null entries in pairwise comparisons matrices. Although assessments can be imprecise, since subjective criteria are involved, the classical pairwise comparisons theory expects all of them to be available. In practice, some experts may not be able (or available) to provide all assessments. Therefore managing null entries is a necessary extension of the pairwise comparisons method. It is shown that certain null entries can be recovered on the basis of the transitivity property which each pairwise comparisons matrix is expected to satisfy.",1999,Knowledge and Information Systems volume 1 issue 1 pp 119-125,potentially all pairwise rankings of all possible alternatives;pairwise comparison;econometrics;data mining;statistics;mathematics;
Imprecise Reliability of General Structures,Lev V. Utkin (Ludwig Maximilian University of Munich);Sergey V. Gurov;,"2310847206,1690143206","This paper discusses the important aspects of the reliability of systems with an imprecise general model of the structure function. It is assumed that the information about reliability behavior of components is restricted by the mean levels of component performance. In this case the classical reliability theory cannot provide a way for analyzing the reliability of systems. The theory of imprecise probabilities may be a basis in developing a general reliability theory which allows us to solve such problems. The basic tool for computing new reliability measures is the natural extension which can be regarded as a linear optimization problem. However, the linear programming computations will become impracticable when the number of components in the system is large. Therefore, the main aim of this paper is to obtain explicit expressions for computing the system reliability measures. We analyze the reliability of general structures and typical systems. The numerical examples illustrate the usefulness of the presented approach to reliability analyzing.",1999,Knowledge and Information Systems volume 1 issue 4 pp 459-480,imprecise probability;kolmogorov structure function;reliability;linear programming;mathematical optimization;statistics;mathematics;
An Axiom Foundation for Uncertain Reasonings in Rule-Based Expert Systems: NT-Algebra,Xudong Luo (The Chinese University of Hong Kong);Chengqi Zhang (Deakin University);,"2146718932,2166080598","This paper identifies an axiom foundation for uncertain reasonings in rule-based expert systems: a near topological algebra (NT-algebra for short), which holds some basic notions hidden behind the uncertain reasoning models in rule-based expert systems. According to the basic means of topological connection in an inference network, an NT-algebraic structure has five basic operators, i.e. AND, OR, NOT, Sequential combination and Parallel combination, which obey some axioms. An NT-algebraic structure is defined on a near-degree space introduced by the authors, which is a special topological space. The continuities of real functions, of fuzzy functions and functions in other senses can be uniformly considered in the framework of a near-degree space. This paper also proves that EMYCIN’s and PROSPECTOR’s uncertain reasoning models correspond to good NT-algebras. Moreover, the existence of any finite NT-algebraic structure is constructively proved. Compared to other related research efforts, the NT-algebra as an axiom foundation has the following characteristics: (1) various cases of assessments for uncertainties of evidence and rules are put into a unified algebraic structure; and (2) major emphasis has been placed on the basic laws of the propagation for them in an inference network, especially the continuity of propagation operations and the relationships between propagation operations.",1999,Knowledge and Information Systems volume 1 issue 4 pp 415-433,topological algebra;fuzzy set;uncertainty;expert system;pure mathematics;discrete mathematics;data mining;artificial intelligence;machine learning;computer science;mathematics;
Run Placement Policies for Concurrent Mergesorts Using Parallel Prefetching,Kun-Lung Wu (IBM);Philip S. Yu (IBM);James Z. Teng (IBM);,"2095612124,2125104194,2139556000","We study the performance of various run placement policies on disks for the merge phase of concurrent mergesorts using parallel prefetching. The initial sorted runs (input) of a merge and its final sorted run (output) are stored on multiple disks but each run resides only on a single disk. In this paper, we examine through detailed simulations three different run placement policies and the impact of buffer thrashing. The results show that, with buffer thrashing avoidance, the best performance can be achieved by a run placement policy that uses a proper subset of the disks dedicated for writing the output runs while the rest of the disks are used for prefetching the input runs in parallel. However, the proper number of write disks is workload dependent, and if not carefully chosen, it can adversely affect the system performance. In practice, a reasonably good performance can be achieved by a run placement policy that does not place the output run of a merge on any of the disks that store its own input runs but allows the output run to share the same disk with some of the input runs of other merges.",1999,Knowledge and Information Systems volume 1 issue 4 pp 435-457,run queue;parallel computing;distributed computing;real time computing;computer science;
Exploration of Ordinal Data Using Association Rules,Oliver Büchter (Daimler AG);Rüdiger Wirth (Daimler AG);,"2423211209,2144893781","The discovery of association rules is a very efficient data mining technique that is especially suitable for large amounts of categorical data. This paper shows how the discovery of association rules can be of benefit for numeric data as well. Based on a review of previous approaches we introduce Q2, a faster algorithm for the discovery of multi-dimensional association rules over ordinal data. We experimentally compare the new algorithm with the previous approach, obtaining performance improvements of more than an order of magnitude on supermarket data. In addition, a new absolute measure for the interestingness of quantitative association rules is introduced. It is based on the view that quantitative association rules have to be interpreted with respect to their Boolean generalizations. This measure has two major benefits compared to the previously used relative interestingness measure; first, it speeds up rule extraction and evaluation and second, it is easier to interpret for a user. Finally we introduce a rule browser which supports the exploration of ordinal data with quantitative association rules.",1999,Knowledge and Information Systems volume 1 issue 4 pp 393-414,ordinal data;association rule learning;data mining;pattern recognition;machine learning;computer science;mathematics;
Efficient Join Processing Using Partial Precomputation,Kian-Lee Tan (National University of Singapore);Cheng Hian Goh (National University of Singapore);Mong Li Lee (National University of Singapore);Beng Chin Ooi (National University of Singapore);,"2142476784,2153596714,2159408573,2037466936","In this paper, we generalize conventional join indexes to a cluster-based join index, in which objects are grouped into clusters based on proximity. Each record of our join index represents a pair of clusters in which the join condition is satisfied by some members of the cluster. This strategy is especially useful for spatial and high-dimensional databases because of their typically large data volume and complex operations. Our approach leverages on the structure of R-trees by exploiting the internal nodes of an R-tree in effectively determining the precomputed clusters which can be used in our join index. By varying the size of the cluster, we are able to fine-tune the join index to achieve a balance between update cost and retrieval cost to suit individual applications. Different implementations of the join index are examined to determine how the join index can be efficiently maintained. To this end, we also conduct a number of experiments on intersection join and window queries, and the results confirm that semi-precomputation of join results is a robust and cost effective approach to join processing.",1999,Knowledge and Information Systems volume 1 issue 4 pp 481-514,block nested loop;recursive join;sort merge join;hash join;join dependency;distributed computing;data mining;database;computer science;
