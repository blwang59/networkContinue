"Data Science for Food, Energy and Water: A Workshop Report",Naoki Abe (IBM)Yiqun Xie (University of Minnesota)Shashi Shekhar (University of Minnesota)Chid Apte (IBM)Vipin Kumar (University of Minnesota)Mitch Tuinstra (Purdue University)Ranga Raju Vatsavai (North Carolina State University),"1996852325,2541619136,2134885186,2194853952,2161062602,2008380015,2121870003","At the 22nd ACM SIGKDD conference on Knowledge and Data Discovery (KDD), a workshop on Data Science for Food, Energy andWater (DSFEW) was held to foster an interdisciplinary community intersecting data science and societally important domains of food, energy and water. The workshop included keynotes, panel discussion, presentations and posters, and introduced the emerging area of DSFEW to ACM SIGKDD audience, and triggered interdisciplinary idea-sharing in DSFEW research. The workshop website is sites.google.com/site/2016dsfew.",2017,Knowledge Discovery and Data Mining,Fields of study: data sciencedata miningcomputer science
User Identity Linkage across Online Social Networks: A Review,Kai Shu (Arizona State University)Suhang Wang (Arizona State University)Jiliang Tang (Michigan State University)Reza Zafarani (Syracuse University)Huan Liu (Arizona State University),"2108363897,2122735199,2147392410,2055981231,2122391114","The increasing popularity and diversity of social media sites has encouraged more and more people to participate on multiple online social networks to enjoy their services. Each user may create a user identity, which can includes profile, content, or network information, to represent his or her unique public figure in every social network. Thus, a fundamental question arises -- can we link user identities across online social networks? User identity linkage across online social networks is an emerging task in social media and has attracted increasing attention in recent years. Advancements in user identity linkage could potentially impact various domains such as recommendation and link prediction. Due to the unique characteristics of social network data, this problem faces tremendous challenges. To tackle these challenges, recent approaches generally consist of (1) extracting features and (2) constructing predictive models from a variety of perspectives. In this paper, we review key achievements of user identity linkage across online social networks including stateof- the-art algorithms, evaluation metrics, and representative datasets. We also discuss related research areas, open problems, and future research directions for user identity linkage across online social networks.",2017,Knowledge Discovery and Data Mining,Fields of study: world wide webdata miningcomputer science
Handling hybrid and missing data in constraint-based causal discovery to study the etiology of ADHD,Elena Sokolova (Radboud University Nijmegen)Daniel von Rhein (Radboud University Nijmegen)Jilly Naaijen (Radboud University Nijmegen)Perry Groot (Radboud University Nijmegen)Tom Claassen (Radboud University Nijmegen)Jan Buitelaar (Radboud University Nijmegen)Tom Heskes (Radboud University Nijmegen),"2586383887,2083848930,2440733295,2106856679,2034687773,1081053655,2017622874","Causal discovery is an increasingly important method for data analysis in the field of medical research. In this paper, we consider two challenges in causal discovery that occur very often when working with medical data: a mixture of discrete and continuous variables and a substantial amount of missing values. To the best of our knowledge, there are no methods that can handle both challenges at the same time. In this paper, we develop a new method that can handle these challenges based on the assumption that data are missing at random and that continuous variables obey a non-paranormal distribution. We demonstrate the validity of our approach for causal discovery on simulated data as well as on two real-world data sets from a monetary incentive delay task and a reversal learning task. Our results help in the understanding of the etiology of attention-deficit/hyperactivity disorder (ADHD).",2017,Knowledge Discovery and Data Mining,Fields of study: missing datadata sciencedata miningartificial intelligencestatisticscomputer science
Subjective interestingness of subgraph patterns,Matthijs van Leeuwen (Katholieke Universiteit Leuven)Tijl De Bie (University of Bristol)Eirini Spyropoulou (University of Bristol)Cédric Mesnage (University of Bristol),"2143928993,2080198120,1981173521,1988021698","The utility of a dense subgraph in gaining a better understanding of a graph has been formalised in numerous ways, each striking a different balance between approximating actual interestingness and computational efficiency. A difficulty in making this trade-off is that, while computational cost of an algorithm is relatively well-defined, a pattern's interestingness is fundamentally subjective. This means that this latter aspect is often treated only informally or neglected, and instead some form of density is used as a proxy. We resolve this difficulty by formalising what makes a dense subgraph pattern interesting to a given user. Unsurprisingly, the resulting measure is dependent on the prior beliefs of the user about the graph. For concreteness, in this paper we consider two cases: one case where the user only has a belief about the overall density of the graph, and another case where the user has prior beliefs about the degrees of the vertices. Furthermore, we illustrate how the resulting interestingness measure is different from previous proposals. We also propose effective exact and approximate algorithms for mining the most interesting dense subgraph according to the proposed measure. Usefully, the proposed interestingness measure and approach lend themselves well to iterative dense subgraph discovery. Contrary to most existing approaches, our method naturally allows subsequently found patterns to be overlapping. The empirical evaluation highlights the properties of the new interestingness measure given different prior belief sets, and our approach's ability to find interesting subgraphs that other methods are unable to find.",2016,Knowledge Discovery and Data Mining,Fields of study: principle of maximum entropyknowledge extractiondata sciencedata miningmachine learningstatisticscomputer sciencemathematics
A Closed-Loop Approach in Data-Driven Resource Allocation to Improve Network User Experience,"Yanan Bao (University of California, Davis)Huasen Wu (University of California, Davis)Xin Liu (University of California, Davis)","2103719961,2114102195,2616096743",-,2016,Knowledge Discovery and Data Mining,Fields of study: resource allocationresource allocationknowledge management
Transferring Knowledge between Cities: A Perspective of Multimodal Data and A Case Study in Air Qual,Ying Wei (Hong Kong University of Science and Technology)Yu Zheng (Microsoft)Qiang Yang (Hong Kong University of Science and Technology),"2235263654,2145115012,2109031554",-,2016,Knowledge Discovery and Data Mining,Fields of study: management scienceknowledge managementdata mining
Stochastic Optimization Techniques for Quantification Performance Measures,Harikrishna Narasimhan (Harvard University)Shuai Li (University of Insubria)Purushottam Kar (Indian Institute of Technology Kanpur)Sanjay Chawla (Qatar Airways)Fabrizio Sebastiani (Qatar Airways),"2637028757,2708734412,2136781283,2596304911,2596389720","The estimation of class prevalence, i.e., the fraction of the population that belongs to a certain class, is a very useful tool in data analytics and learning, and finds applications in many domains, such as sentiment analysis, epidemiology, etc. For example, in sentiment analysis, the objective is often not to estimate whether a specific text conveys positive or negative sentiment, but rather estimate the overall distribution of positive and negative sentiment during an event window. A popular way of performing the above task, often dubbed quantification, is to use supervised learning to train a prevalence estimator from labelled data. In this paper we propose the first online stochastic algorithms for directly optimizing (i) performance measures for quantification, and (ii) hybrid performance measures that seek to balance quantification and classification performance. We prove rigorous bounds for our algorithms which demonstrate that they exhibit optimal convergence. Our algorithms present a significant advancement in the theory of multivariate optimization. We also report extensive experiments on benchmark and real data sets which demonstrate that our methods significantly outperform existing optimization techniques used for the quantification problem.",2016,Knowledge Discovery and Data Mining,Fields of study: econometricsdata miningmachine learningstatisticscomputer science
Fast Unsupervised Online Drift Detection,Denis Dos Reis (University of São Paulo)Gustavo Batista (University of São Paulo)Peter Flach (University of Bristol)Stan Matwin (Dalhousie University),"2366579606,2165222361,1814273096,2631100416",-,2016,Knowledge Discovery and Data Mining,-
Pseudo-Document-based Topic Modeling of Short Texts without Auxiliary Information,Yuan Zuo (Beihang University)Junjie Wu (Beihang University)Hao Lin (Beihang University)Hui Xiong (Rutgers University),"2271787629,2149366604,2293401935,2153710278",-,2016,Knowledge Discovery and Data Mining,Fields of study: data sciencenatural language processinginformation retrievalcomputer science
Recurrent Temporal Point Process,Nan Du (Georgia Institute of Technology)Hanjun Dai (Georgia Institute of Technology)Rakshit Trivedi (Georgia Institute of Technology)Utkarsh Upadhyay (Max Planck Society)Manuel Gomez-Rodriguez (Max Planck Society)Le Song (Georgia Institute of Technology),"2681008474,2646439162,2150403271,2353884032,2279633593,2113868374",-,2016,Knowledge Discovery and Data Mining,-
Generalized Hierarchical Sparse Model for Arbitrary-Order Interactive Antigenic Sites Identification,Lei Han (Rutgers University)Yu Zhang (Hong Kong University of Science and Technology)Xiu-Feng Wan (Mississippi State University)Tong Zhang (Rutgers University),"2531318857,2648094648,2718011926,2510858842",-,2016,Knowledge Discovery and Data Mining,Fields of study: bioinformaticspattern recognitionmachine learning
Optimal Linear Aggregate Query Processing under Approximate Differential Privacy,Ganzhao Yuan (South China University of Technology)Yin Yang (Khalifa University)Zhenjie Zhang (National University of Singapore)Zhifeng Hao (South China University of Technology),"2118188494,2280962535,2161417483,2684555521",-,2016,Knowledge Discovery and Data Mining,Fields of study: theoretical computer sciencedata miningdatabasecomputer science
Burstiness Scale: a highly parsimonious model forcharacterizing random series of events,Rodrigo A S Alves (Centro Federal de Educação Tecnológica de Minas Gerais)Renato Assunção (Universidade Federal de Minas Gerais)Pedro O S Vaz de Melo (Universidade Federal de Minas Gerais),"2683066055,2001700347,2162838672","The problem to accurately and parsimoniously characterize random series of events (RSEs) present in the Web, such as e-mail conversations or Twitter hashtags, is not trivial. Reports found in the literature reveal two apparent conflicting visions of how RSEs should be modeled. From one side, the Poissonian processes, of which consecutive events follow each other at a relatively regular time and should not be correlated. On the other side, the self-exciting processes, which are able to generate bursts of correlated events and periods of inactivities. The existence of many and sometimes conflicting approaches to model RSEs is a consequence of the unpredictability of the aggregated dynamics of our individual and routine activities, which sometimes show simple patterns, but sometimes results in irregular rising and falling trends. In this paper we propose a highly parsimonious way to characterize general RSEs, namely the Burstiness Scale (BuSca) model. BuSca views each RSE as a mix of two independent process: a Poissonian and a self-exciting one. Here we describe a fast method to extract the two parameters of BuSca that, together, gives the burstyness scale, which represents how much of the RSE is due to bursty and viral effects. We validated our method in eight diverse and large datasets containing real random series of events seen in Twitter, Yelp, e-mail conversations, Digg, and online forums. Results showed that, even using only two parameters, BuSca is able to accurately describe RSEs seen in these diverse systems, what can leverage many applications.",2016,Knowledge Discovery and Data Mining,Fields of study: data miningsimulationstatistics
XGBoost: A Scalable Tree Boosting System,Tianqi Chen (University of Washington)Carlos Guestrin (University of Washington),"2126135973,1988556028","Tree boosting is a highly effective and widely used machine learning method. In this paper, we describe a scalable end-to-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems.",2016,Knowledge Discovery and Data Mining,Fields of study: incremental decision treeid3 algorithmdata miningpattern recognitionmachine learningcomputer science
"""Why Should I Trust You?"": Explaining the Predictions of Any Classifier",Marco Tulio Ribeiro (University of Washington)Sameer Singh (University of Washington)Carlos Guestrin (University of Washington),"2141784236,2279876130,1988556028","Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally varound the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.",2016,Knowledge Discovery and Data Mining,Fields of study: data miningartificial intelligencemachine learningcomputer science
node2vec: Scalable Feature Learning for Networks,Aditya Grover (Stanford University)Jure Leskovec (Stanford University),"2008277149,1878631932","Prediction tasks over nodes and edges in networks require careful effort in engineering features used by learning algorithms. Recent research in the broader field of representation learning has led to significant progress in automating prediction by learning the features themselves. However, present feature learning approaches are not expressive enough to capture the diversity of connectivity patterns observed in networks. Here we propose node2vec, an algorithmic framework for learning continuous feature representations for nodes in networks. In node2vec, we learn a mapping of nodes to a low-dimensional space of features that maximizes the likelihood of preserving network neighborhoods of nodes. We define a flexible notion of a node's network neighborhood and design a biased random walk procedure, which efficiently explores diverse neighborhoods. Our algorithm generalizes prior work which is based on rigid notions of network neighborhoods, and we argue that the added flexibility in exploring neighborhoods is the key to learning richer representations. We demonstrate the efficacy of node2vec over existing state-of-the-art techniques on multi-label classification and link prediction in several real-world networks from diverse domains. Taken together, our work represents a new way for efficiently learning state-of-the-art task-independent representations in complex networks.",2016,Knowledge Discovery and Data Mining,Fields of study: evolving networkscompetitive learningfeature learningsemi supervised learningtheoretical computer sciencecombinatoricsartificial intelligencemachine learningcomputer sciencemathematics
Joint Community and Structural Hole Spanner Detection via Harmonic Modularity,Lifang He (Shenzhen University)Chun Ta Lu (University of Illinois at Chicago)Jiaqi Ma (Tsinghua University)Jianping Cao (National University of Defense Technology)Linlin Shen (Shenzhen University)Philip S. Yu (University of Illinois at Chicago),"2555188826,2224372854,2510249200,2144024520,2665312263,2125104194","Detecting communities (or modular structures) and structural hole spanners, the nodes bridging different communities in a network, are two essential tasks in the realm of network analytics. Due to the topological nature of communities and structural hole spanners, these two tasks are naturally tangled with each other, while there has been little synergy between them. In this paper, we propose a novel harmonic modularity method to tackle both tasks simultaneously. Specifically, we apply a harmonic function to measure the smoothness of community structure and to obtain the community indicator. We then investigate the sparsity level of the interactions between communities, with particular emphasis on the nodes connecting to multiple communities, to discriminate the indicator of SH spanners and assist the community guidance. Extensive experiments on real-world networks demonstrate that our proposed method outperforms several state-of-the-art methods in the community detection task and also in the SH spanner identification task (even the methods that require the supervised community information). Furthermore, by removing the SH spanners spotted by our method, we show that the quality of other community detection methods can be further improved.",2016,Knowledge Discovery and Data Mining,Fields of study: modularityharmonic functionmodularitysocial networkmachine learningsimulation
Smart Reply: Automated Response Suggestion for Email,Anjuli Kannan (Google)Karol Kurach (Google)Sujith Ravi (Google)Tobias Kaufmann (Google)Andrew Tomkins (Google)Balint Miklos (Google)Greg Corrado (Google)Laszlo Lukacs (Google)Marina Ganea (Google)Peter Young (Google)Vivek Ramavajjala (Google),"2128062926,2421278887,2590734359,2515993056,2535415812,2342928363,1994222016,2435589108,2514249824,2511757444,2507702640","In this paper we propose and investigate a novel end-to-end method for automatically generating short email responses, called Smart Reply. It generates semantically diverse suggestions that can be used as complete email responses with just one tap on mobile. The system is currently used in Inbox by Gmail and is responsible for assisting with 10% of all mobile responses. It is designed to work at very high throughput and process hundreds of millions of messages daily. The system exploits state-of-the-art, large-scale deep learning. We describe the architecture of the system as well as the challenges that we faced while building it, like response diversity and scalability. We also introduce a new method for semantic clustering of user-generated content that requires only a modest amount of explicitly labeled data.",2016,Knowledge Discovery and Data Mining,Fields of study: html emaillong short term memorycluster analysisdeep learningsemanticsinternet privacyworld wide webdata miningartificial intelligencemachine learningcomputer science
The Evolving Meaning of Information Security,Whitfield Diffie (Stanford University),2307656538,"When you are developing security systems, new penetration techniques seem to appear as responses to new security measures but in general the flow is the other way around: security exists and evolves because of the evolution of threats. Beginning with the rise of radio in the 20th Century attacks on communication networks have shown two forms: those that go for the big kill --- such as the breaking of Enigma --- and those that assemble small seemingly innocuous leaks of information into a comprehensive understanding of the target's behavior. We will analyze the way in which these trends interact with others to create a situation in which what is possible in security and even the meaning of security in communication networks needs reexamination.",2016,Knowledge Discovery and Data Mining,Fields of study: security through obscurityinformation security standardsassetcloud computing securitysecurity testingcovert channelcommunications securitysecurity engineeringinformation securityinternet privacycomputer securitydata miningcomputer science
Assessing Human Error Against a Benchmark of Perfection,Ashton Anderson (Microsoft)Jon M. Kleinberg (Cornell University)Sendhil Mullainathan (Harvard University),"2155788867,2261367123,109652375","An increasing number of domains are providing us with detailed trace data on human decisions in settings where we can evaluate the quality of these decisions via an algorithm. Motivated by this development, an emerging line of work has begun to consider whether we can characterize and predict the kinds of decisions where people are likely to make errors. To investigate what a general framework for human error prediction might look like, we focus on a model system with a rich history in the behavioral sciences: the decisions made by chess players as they select moves in a game. We carry out our analysis at a large scale, employing datasets with several million recorded games, and using chess tablebases to acquire a form of ground truth for a subset of chess positions that have been completely solved by computers but remain challenging even for the best players in the world. We organize our analysis around three categories of features that we argue are present in most settings where the analysis of human error is applicable: the skill of the decision-maker, the time available to make the decision, and the inherent difficulty of the decision. We identify rich structure in all three of these categories of features, and find strong evidence that in our domain, features describing the inherent difficulty of an instance are significantly more powerful than features based on skill or time.",2016,Knowledge Discovery and Data Mining,Fields of study: human errorartificial intelligencemachine learningsimulationstatisticscomputer science
Compressing Convolutional Neural Networks in the Frequency Domain,Wenlin Chen (Washington University in St. Louis)James T. Wilson (University of Edinburgh)Stephen Tyree (Nvidia)Kilian Q. Weinberger (Cornell University)Yixin Chen (Washington University in St. Louis),"2231133452,2515705533,2655516276,2003907699,2295009377","Convolutional neural networks (CNN) are increasingly used in many areas of computer vision. They are particularly attractive because of their ability to ""absorb"" great quantities of labeled data through millions of parameters. However, as model sizes increase, so do the storage and memory requirements of the classifiers, hindering many applications such as image and speech recognition on mobile phones and other devices. In this paper, we present a novel net- work architecture, Frequency-Sensitive Hashed Nets (FreshNets), which exploits inherent redundancy in both convolutional layers and fully-connected layers of a deep learning model, leading to dramatic savings in memory and storage consumption. Based on the key observation that the weights of learned convolutional filters are typically smooth and low-frequency, we first convert filter weights to the frequency domain with a discrete cosine transform (DCT) and use a low-cost hash function to randomly group frequency parameters into hash buckets. All parameters assigned the same hash bucket share a single value learned with standard back-propagation. To further reduce model size, we allocate fewer hash buckets to high-frequency components, which are generally less important. We evaluate FreshNets on eight data sets, and show that it leads to better compressed performance than several relevant baselines.",2016,Knowledge Discovery and Data Mining,Fields of study: hash functionconvolutional neural networktheoretical computer sciencespeech recognitiondata miningmachine learningcomputer science
Label Noise Reduction in Entity Typing by Heterogeneous Partial-Label Embedding,Xiang Ren (University of Illinois at Urbana–Champaign)Wenqi He (University of Illinois at Urbana–Champaign)Meng Qu (University of Illinois at Urbana–Champaign)Clare R. Voss (United States Army Research Laboratory)Heng Ji (Rensselaer Polytechnic Institute)Jiawei Han (University of Illinois at Urbana–Champaign),"2129405715,2518651166,2508960786,2118725505,2282310307,2121939561","Current systems of fine-grained entity typing use distant supervision in conjunction with existing knowledge bases to assign categories (type labels) to entity mentions. However, the type labels so obtained from knowledge bases are often noisy ( i.e. , incorrect for the entity mention's local context). We define a new task, Label Noise Reduction in Entity Typing (LNR), to be the automatic identification of correct type labels (type-paths) for training examples , given the set of candidate type labels obtained by distant supervision with a given type hierarchy. The unknown type labels for individual entity mentions and the semantic similarity between entity types pose unique challenges for solving the LNR task. We propose a general framework, called PLE , to jointly embed entity mentions, text features and entity types into the same low-dimensional space where, in that space, objects whose types are semantically close have similar representations . Then we estimate the type-path for each training example in a top-down manner using the learned embeddings. We formulate a global objective for learning the embeddings from text corpora and knowledge bases, which adopts a novel margin-based loss that is robust to noisy labels and faithfully models type correlation derived from knowledge bases. Our experiments on three public typing datasets demonstrate the effectiveness and robustness of PLE, with an average of 25% improvement in accuracy compared to next best method.",2016,Knowledge Discovery and Data Mining,Fields of study: entity linkingknowledge basedata miningpattern recognitionartificial intelligencemachine learningcomputer science
Dynamic and Robust Wildfire Risk Prediction System: An Unsupervised Approach,Mahsa Salehi (IBM)Laura Irina Rusu (IBM)Timothy M. Lynar (IBM)Anna Phan (IBM),"2408564922,2096641372,2593345984,2485260314","Ability to predict the risk of damaging events (e.g. wildfires) is crucial in helping emergency services in their decision making processes, to mitigate and reduce the impact of such events. Today, wildfire rating systems have been in operation extensively in many countries around the world to estimate the danger of wildfires. In this paper we propose a data-driven approach to predict wildfire risk using weather data. We show how we address the inherent challenge arising due to the temporal dynamicity of weather data. Weather observations naturally change in time, with finer-scale variation (e.g. stationary day or night) or large variations (nonstationary day or night), and this determines a temporal variation of the predicted wildfire danger. We show how our dynamic wildfire danger prediction model addresses the aforementioned challenge using context-based anomaly detection techniques. We call our predictive model a Context-Based Fire Risk (CBFR) model. The advantage of our model is that it maintains multiple historical models for different temporal variations (e.g. day versus night), and uses ensemble learning techniques to predict wildfire risk with high accuracy. In addition, it is completely unsupervised and does not rely on expert knowledge, which makes it flexible and easily applied to any region of interest. Our CBFR model is also scalable and can potentially be parallelised to speed up computation. We have considered multiple wildfire locations in the Blue Mountains, Australia as a case study, and compared the results of our system with the existing well-established Australian wildfire rating system. The experimental results show that our predictive model has a substantially higher accuracy in predicting wildfire risk, which makes it an effective model to supplement the operational Australian wildfire rating system.",2016,Knowledge Discovery and Data Mining,Fields of study: data stream miningunsupervised learningmachine learningsimulationcomputer science
Matrix Computations and Optimization in Apache Spark,"Reza Bosagh Zadeh (Stanford University)Xiangrui MengAlexander Ulanov (HP Labs)Burak YavuzLi Pu (Twitter)Shivaram Venkataraman (University of California, Berkeley)Evan R. Sparks (University of California, Berkeley)Aaron StapleMatei Zaharia (Massachusetts Institute of Technology)","2021036281,2522147838,2110026627,2484040650,2553136135,2232375382,2095953017,2481868097,2009645378","We describe matrix computations available in the cluster programming framework, Apache Spark. Out of the box, Spark provides abstractions and implementations for distributed matrices and optimization routines using these matrices. When translating single-node algorithms to run on a distributed cluster, we observe that often a simple idea is enough: separating matrix operations from vector operations and shipping the matrix operations to be ran on the cluster, while keeping vector operations local to the driver. In the case of the Singular Value Decomposition, by taking this idea to an extreme, we are able to exploit the computational power of a cluster, while running code written decades ago for a single core. Another example is our Spark port of the popular TFOCS optimization package, originally built for MATLAB, which allows for solving Linear programs as well as a variety of other convex programs. We conclude with a comprehensive set of benchmarks for hardware accelerated matrix computations from the JVM, which is interesting in its own right, as many cluster programming frameworks use the JVM. The contributions described in this paper are already merged into Apache Spark and available on Spark installations by default, and commercially supported by a slew of companies which provide further services.",2016,Knowledge Discovery and Data Mining,Fields of study: scalasparkpackagelinear algebratheoretical computer scienceparallel computingoperating systemdistributed computingreal time computingmachine learningprogramming languagealgorithmcomputer science
CatchTartan: Representing and Summarizing Dynamic Multicontextual Behaviors,Meng Jiang (University of Illinois at Urbana–Champaign)Christos Faloutsos (Carnegie Mellon University)Jiawei Han (University of Illinois at Urbana–Champaign),"2115305989,2198983026,2121939561","Representing and summarizing human behaviors with rich contexts facilitates behavioral sciences and user-oriented services. Traditional behavioral modeling represents a behavior as a tuple in which each element is one contextual factor of one type, and the tensor-based summaries look for high-order dense blocks by clustering the values (including timestamps) in each dimension. However, the human behaviors are multicontextual and dynamic: (1) each behavior takes place within multiple contexts in a few dimensions, which requires the representation to enable non-value and set-values for each dimension; (2) many behavior collections, such as tweets or papers, evolve over time. In this paper, we represent the behavioral data as a two-level matrix (temporal-behaviors by dimensional-values) and propose a novel representation for behavioral summary called Tartan that includes a set of dimensions, the values in each dimension, a list of consecutive time slices and the behaviors in each slice. We further develop a propagation method CatchTartan to catch the dynamic multicontextual patterns from the temporal multidimensional data in a principled and scalable way: it determines the meaningfulness of updating every element in the Tartan by minimizing the encoding cost in a compression manner. CatchTartan outperforms the baselines on both the accuracy and speed. We apply CatchTartan to four Twitter datasets up to 10 million tweets and the DBLP data, providing comprehensive summaries for the events, human life and scientific development.",2016,Knowledge Discovery and Data Mining,Fields of study: minimum description lengthdata sciencetheoretical computer sciencedata miningmachine learningstatisticscomputer science
DopeLearning: A Computational Approach to Rap Lyrics Generation,Eric Malmi (Aalto University)Pyry Takala (Aalto University)Hannu Toivonen (University of Helsinki)Tapani Raiko (Aalto University)Aristides Gionis (Aalto University),"2289295535,2078188445,2250270171,344142627,737311942","Writing rap lyrics requires both creativity to construct a meaningful, interesting story and lyrical skills to produce complex rhyme patterns, which form the cornerstone of good flow. We present a rap lyrics generation method that captures both of these aspects. First, we develop a prediction model to identify the next line of existing lyrics from a set of candidate next lines. This model is based on two machine-learning techniques: the RankSVM algorithm and a deep neural network model with a novel structure. Results show that the prediction model can identify the true next line among 299 randomly selected lines with an accuracy of 17%, i.e., over 50 times more likely than by random. Second, we employ the prediction model to combine lines from existing songs, producing lyrics with rhyme and a meaning. An evaluation of the produced lyrics shows that in terms of quantitative rhyme density, the method outperforms the best human rappers by 21%. The rap lyrics generator has been deployed as an online tool called DeepBeat, and the performance of the tool has been assessed by analyzing its usage logs. This analysis shows that machine-learned rankings correlate with user preferences.",2016,Knowledge Discovery and Data Mining,Fields of study: modigliani risk adjusted performancedeep learningspeech recognitionmachine learningcomputer science
GMove: Group-Level Mobility Modeling Using Geo-Tagged Social Media,Chao Zhang (University of Illinois at Urbana–Champaign)Keyang Zhang (University of Illinois at Urbana–Champaign)Quan Yuan (University of Illinois at Urbana–Champaign)Luming Zhang (Hefei University of Technology)Tim Hanratty (United States Army Research Laboratory)Jiawei Han (University of Illinois at Urbana–Champaign),"2618902948,2510995246,2683532865,2138764651,2311313130,2121939561","Understanding human mobility is of great importance to various applications, such as urban planning, traffic scheduling, and location prediction. While there has been fruitful research on modeling human mobility using tracking data (e.g., GPS traces), the recent growth of geo-tagged social media (GeoSM) brings new opportunities to this task because of its sheer size and multi-dimensional nature. Nevertheless, how to obtain quality mobility models from the highly sparse and complex GeoSM data remains a challenge that cannot be readily addressed by existing techniques. We propose GMove, a group-level mobility modeling method using GeoSM data. Our insight is that the GeoSM data usually contains multiple user groups, where the users within the same group share significant movement regularity. Meanwhile, user grouping and mobility modeling are two intertwined tasks: (1) better user grouping offers better within-group data consistency and thus leads to more reliable mobility models; and (2) better mobility models serve as useful guidance that helps infer the group a user belongs to. GMove thus alternates between user grouping and mobility modeling, and generates an ensemble of Hidden Markov Models (HMMs) to characterize group-level movement regularity. Furthermore, to reduce text sparsity of GeoSM data, GMove also features a text augmenter. The augmenter computes keyword correlations by examining their spatiotemporal distributions. With such correlations as auxiliary knowledge, it performs sampling-based augmentation to alleviate text sparsity and produce high-quality HMMs. Our extensive experiments on two real-life data sets demonstrate that GMove can effectively generate meaningful group-level mobility models. Moreover, with context-aware location prediction as an example application, we find that GMove significantly outperforms baseline mobility models in terms of prediction accuracy.",2016,Knowledge Discovery and Data Mining,Fields of study: social mediamobility modelworld wide webdata miningmachine learningsimulationstatisticscomputer science
Structural Deep Network Embedding,Daixin Wang (Tsinghua University)Peng Cui (Tsinghua University)Wenwu Zhu (Tsinghua University),"2099175083,2113115369,2111511002","Network embedding is an important method to learn low-dimensional representations of vertexes in networks, aiming to capture and preserve the network structure. Almost all the existing network embedding methods adopt shallow models. However, since the underlying network structure is complex, shallow models cannot capture the highly non-linear network structure, resulting in sub-optimal network representations. Therefore, how to find a method that is able to effectively capture the highly non-linear network structure and preserve the global and local structure is an open yet important problem. To solve this problem, in this paper we propose a Structural Deep Network Embedding method, namely SDNE . More specifically, we first propose a semi-supervised deep model, which has multiple layers of non-linear functions, thereby being able to capture the highly non-linear network structure. Then we propose to exploit the first-order and second-order proximity jointly to preserve the network structure. The second-order proximity is used by the unsupervised component to capture the global network structure. While the first-order proximity is used as the supervised information in the supervised component to preserve the local network structure. By jointly optimizing them in the semi-supervised deep model, our method can preserve both the local and global network structure and is robust to sparse networks. Empirically, we conduct the experiments on five real-world networks, including a language network, a citation network and three social networks. The results show that compared to the baselines, our method can reconstruct the original network significantly better and achieves substantial gains in three applications, i.e. multi-label classification, link prediction and visualization.",2016,Knowledge Discovery and Data Mining,Fields of study: dynamic network analysisnetwork formationnetwork simulationnetwork analysisdeep belief networkdeep learningtheoretical computer sciencedata miningmachine learningcomputer science
From Truth Discovery to Trustworthy Opinion Discovery: An Uncertainty-Aware Quantitative Modeling Approach,"Mengting Wan (University of California, San Diego)Xiangyu Chen (University of Illinois at Urbana–Champaign)Lance M. Kaplan (United States Army Research Laboratory)Jiawei Han (University of Illinois at Urbana–Champaign)Jing Gao (University at Buffalo)Bo Zhao (LinkedIn)","2665663483,2630097597,2119398415,2121939561,2096731881,2674375462","In this era of information explosion, conflicts are often encountered when information is provided by multiple sources. Traditional truth discovery task aims to identify the truth the most trustworthy information, from conflicting sources in different scenarios. In this kind of tasks, truth is regarded as a fixed value or a set of fixed values. However, in a number of real-world cases, objective truth existence cannot be ensured and we can only identify single or multiple reliable facts from opinions. Different from traditional truth discovery task, we address this uncertainty and introduce the concept of trustworthy opinion of an entity, treat it as a random variable , and use its distribution to describe consistency or controversy, which is particularly difficult for data which can be numerically measured, i.e. quantitative information. In this study, we focus on the quantitative opinion, propose an uncertainty-aware approach called Kernel Density Estimation from Multiple Sources ( KDEm ) to estimate its probability distribution, and summarize trustworthy information based on this distribution. Experiments indicate that KDEm not only has outstanding performance on the classical numeric truth discovery task, but also shows good performance on multi-modality detection and anomaly detection in the uncertain-opinion setting.",2016,Knowledge Discovery and Data Mining,Fields of study: kernel density estimationdata sciencedata miningartificial intelligencestatisticscomputer sciencemathematics
Interpretable Decision Sets: A Joint Framework for Description and Prediction,Himabindu Lakkaraju (Stanford University)Stephen H. Bach (Stanford University)Jure Leskovec (Stanford University),"2237122046,2712713707,1878631932","One of the most important obstacles to deploying predictive models is the fact that humans do not understand and trust them. Knowing which variables are important in a model's prediction and how they are combined can be very powerful in helping people understand and trust automatic decision making systems. Here we propose interpretable decision sets, a framework for building predictive models that are highly accurate, yet also highly interpretable. Decision sets are sets of independent if-then rules. Because each rule can be applied independently, decision sets are simple, concise, and easily interpretable. We formalize decision set learning through an objective function that simultaneously optimizes accuracy and interpretability of the rules. In particular, our approach learns short, accurate, and non-overlapping rules that cover the whole feature space and pay attention to small but important classes. Moreover, we prove that our objective is a non-monotone submodular function, which we efficiently optimize to find a near-optimal set of rules. Experiments show that interpretable decision sets are as accurate at classification as state-of-the-art machine learning techniques. They are also three times smaller on average than rule-based models learned by other methods. Finally, results of a user study show that people are able to answer multiple-choice questions about the decision boundaries of interpretable decision sets and write descriptions of classes based on them faster and more accurately than with other rule-based models that were designed for interpretability. Overall, our framework provides a new approach to interpretable machine learning that balances accuracy, interpretability, and computational efficiency.",2016,Knowledge Discovery and Data Mining,Fields of study: decision rulebiological classificationdata miningpattern recognitionmachine learningmathematics
Mining Reliable Information from Passively and Actively Crowdsourced Data,Jing Gao (University at Buffalo)Qi Li (University at Buffalo)Bo Zhao (LinkedIn)Wei Fan (Baidu)Jiawei Han (University of Illinois at Urbana–Champaign),"2096731881,2261907930,2674375462,2422054197,2121939561","Recent years have witnessed an astonishing growth of crowd-contributed data, which has become a powerful information source that covers almost every aspect of our lives. This big treasure trove of information has fundamentally changed the ways in which we learn about our world. Crowdsourcing has attracted considerable attentions with various approaches developed to utilize these enormous crowdsourced data from different perspectives. From the data collection perspective, crowdsourced data can be divided into two types: ""passively"" crowdsourced data and ""actively"" crowdsourced data; from task perspective, crowdsourcing research includes information aggregation, budget allocation, worker incentive mechanism, etc. To answer the need of a systematic introduction of the field and comparison of the techniques, we will present an organized picture on crowdsourcing methods in this tutorial. The covered topics will be interested for both advanced researchers and beginners in this field.",2016,Knowledge Discovery and Data Mining,Fields of study: crowdsourcingdata scienceworld wide webdata miningcomputer science
Positive-Unlabeled Learning in Streaming Networks,Shiyu Chang (University of Illinois at Urbana–Champaign)Yang Zhang (University of Illinois at Urbana–Champaign)Jiliang Tang (Michigan State University)Dawei Yin (Yahoo!)Yi Chang (Yahoo!)Mark A. Hasegawa-Johnson (University of Illinois at Urbana–Champaign)Thomas S. Huang (University of Illinois at Urbana–Champaign),"2098291119,2677264140,2147392410,2170531144,2168000538,2234370517,2149631809","Data of many problems in real-world systems such as link prediction and one-class recommendation share common characteristics. First, data are in the form of positive unlabeled (PU) measurements (e.g. Twitter ""following"", Facebook ""like"", etc.) that do not provide negative information, which can be naturally represented as networks. Second, in the era of big data, such data are generated temporally-ordered, continuously and rapidly, which determines its streaming nature. These common characteristics allow us to unify many problems into a novel framework -- PU learning in streaming networks. In this paper, a principled probabilistic approach SPU is proposed to leverage the characteristics of the streaming PU inputs. In particular, SPU captures temporal dynamics and provides real-time adaptations and predictions by identifying the potential negative signals concealed in unlabeled data. Our empirical results on various real-world datasets demonstrate the effectiveness of the proposed framework over other state-of-the-art methods in both link prediction and recommendation.",2016,Knowledge Discovery and Data Mining,Fields of study: pu learningdynamic network analysisworld wide webdata miningmachine learningcomputer science
ABRA: Approximating Betweenness Centrality in Static and Dynamic Graphs with Rademacher Averages,Matteo Riondato (Brown University)Eli Upfal (Brown University),"1555209364,2685185700","We present ABRA, a suite of algorithms to compute and maintain probabilistically-guaranteed, high-quality, approximations of the betweenness centrality of all nodes (or edges) on both static and fully dynamic graphs. Our algorithms use progressive random sampling and their analysis rely on Rademacher averages and pseudodimension, fundamental concepts from statistical learning theory. To our knowledge, this is the first application of these concepts to the field of graph analysis. Our experimental results show that ABRA is much faster than exact methods, and vastly outperforms, in both runtime and number of samples, state-of-the-art algorithms with the same quality guarantees.",2016,Knowledge Discovery and Data Mining,Fields of study: random walk closeness centralityrademacher complexitycentralitybetweenness centralitysamplingcombinatoricsmachine learningstatisticsmathematics
FRAUDAR: Bounding Graph Fraud in the Face of Camouflage,Bryan Hooi (Carnegie Mellon University)Hyun Ah Song (Carnegie Mellon University)Alex Beutel (Carnegie Mellon University)Neil Shah (Carnegie Mellon University)Kijung Shin (Carnegie Mellon University)Christos Faloutsos (Carnegie Mellon University),"1755863881,2158947101,2045447989,2397954333,2226806500,2198983026","Given a bipartite graph of users and the products that they review, or followers and followees, how can we detect fake reviews or follows? Existing fraud detection methods (spectral, etc.) try to identify dense subgraphs of nodes that are sparsely connected to the remaining graph. Fraudsters can evade these methods using camouflage, by adding reviews or follows with honest targets so that they look ""normal"". Even worse, some fraudsters use hijacked accounts from honest users, and then the camouflage is indeed organic. Our focus is to spot fraudsters in the presence of camouflage or hijacked accounts. We propose FRAUDAR, an algorithm that (a) is camouflage-resistant, (b) provides upper bounds on the effectiveness of fraudsters, and (c) is effective in real-world data. Experimental results under various attacks show that FRAUDAR outperforms the top competitor in accuracy of detecting both camouflaged and non-camouflaged fraud. Additionally, in real-world experiments with a Twitter follower-followee graph of 1.47 billion edges, FRAUDAR successfully detected a subgraph of more than 4000 detected accounts, of which a majority had tweets showing that they used follower-buying services.",2016,Knowledge Discovery and Data Mining,Fields of study: internet privacycomputer securitydata miningcomputer science
Predicting Matchups and Preferences in Context,Shuo Chen (Cornell University)Thorsten Joachims (Cornell University),"2295814016,245171893","We present a general probabilistic framework for predicting the outcome of pairwise matchups (e.g. two-player sport matches) and pairwise preferences (e.g. product preferences), both of which have widespread applications ranging from matchmaking in computer games to recommendation in e-commerce. Unlike existing models for these tasks, our model not only learns representations of the items in a more expressive latent vector space, but also models how context modifies matchup and preference outcomes. For example, the context ""weather"" may alter the winning probability in a tennis match, or the fact that the user is on a mobile device may alter his preferences among restaurants. More generally, the model is capable of handling any symmetric game/comparison problem that can be described by vectorized player/item and game/context features. We provide a comprehensive evaluation of its predictive performance with real datasets from both domains to show its ability to predict preference and game outcomes more accurately than existing models. Furthermore, we demonstrate on synthetic datasets the expressiveness of the model when compared against theoretical limits.",2016,Knowledge Discovery and Data Mining,Fields of study: rankingpairwise comparisongamesfeature learningdata miningmachine learningsimulationstatisticscomputer science
Finding Gangs in War from Signed Networks,Lingyang Chu (Simon Fraser University)Zhefeng Wang (University of Science and Technology of China)Jian Pei (Simon Fraser University)Jiannan Wang (Simon Fraser University)Zijin Zhao (Simon Fraser University)Enhong Chen (University of Science and Technology of China),"2157117012,2227256930,2126330539,2103786759,2509178450,2136372366","Given a signed network where edges are weighted in real number, and positive weights indicate cohesion between vertices and negative weights indicate opposition, we are interested in finding k -Oppositive Cohesive Groups ( k -OCG). Each k -OCG is a group of k subgraphs such that (1) the edges within each subgraph are dense and cohesive; and (2) the edges crossing different subgraphs are dense and oppositive. Finding k -OCGs is challenging since the subgraphs are often small, there are multiple k -OCGs in a large signed network, and many existing dense subgraph extraction methods cannot handle edges of two signs. We model k -OCG finding task as a quadratic optimization problem. However, the classical Proximal Gradient method is very costly since it has to use the entire adjacency matrix, which is huge on large networks. Thus, we develop FOCG, an algorithm that is two orders of magnitudes faster than the Proximal Gradient method. The main idea is to only search in small subgraphs and thus avoids using a major portion of the adjacency matrix. Our experimental results on synthetic and real data sets as well as a case study clearly demonstrate the effectiveness and efficiency of our method.",2016,Knowledge Discovery and Data Mining,Fields of study: cluster analysistheoretical computer sciencediscrete mathematicscombinatoricsmachine learningcomputer sciencemathematics
Asymmetric Transitivity Preserving Graph Embedding,Mingdong Ou (Tsinghua University)Peng Cui (Tsinghua University)Jian Pei (Simon Fraser University)Ziwei Zhang (Tsinghua University)Wenwu Zhu (Tsinghua University),"2157830877,2113115369,2126330539,2510900624,2111511002","Graph embedding algorithms embed a graph into a vector space where the structure and the inherent properties of the graph are preserved. The existing graph embedding methods cannot preserve the asymmetric transitivity well, which is a critical property of directed graphs. Asymmetric transitivity depicts the correlation among directed edges, that is, if there is a directed path from u to v, then there is likely a directed edge from u to v. Asymmetric transitivity can help in capturing structures of graphs and recovering from partially observed graphs. To tackle this challenge, we propose the idea of preserving asymmetric transitivity by approximating high-order proximity which are based on asymmetric transitivity. In particular, we develop a novel graph embedding algorithm, High-Order Proximity preserved Embedding ( HOPE for short), which is scalable to preserve high-order proximities of large scale graphs and capable of capturing the asymmetric transitivity. More specifically, we first derive a general formulation that cover multiple popular high-order proximity measurements, then propose a scalable embedding algorithm to approximate the high-order proximity measurements based on their general formulation. Moreover, we provide a theoretical upper bound on the RMSE (Root Mean Squared Error) of the approximation. Our empirical experiments on a synthetic dataset and three real-world datasets demonstrate that HOPE can approximate the high-order proximities significantly better than the state-of-art algorithms and outperform the state-of-art algorithms in tasks of reconstruction, link prediction and vertex recommendation.",2016,Knowledge Discovery and Data Mining,Fields of study: planar straight line graphvoltage graphcomplement graphasymmetric graphcomparability graphbook embeddingfeedback arc setnull graphtopological graph theorygraph propertyline graphgraph embeddinggraphplanar graphdirected graphdiscrete mathematicscombinatoricstopologymathematics
Multi-layer Representation Learning for Medical Concepts,Edward Choi (Georgia Institute of Technology)Mohammad Taha Bahadori (Georgia Institute of Technology)Elizabeth SearlesCatherine CoffeyMichael ThompsonJames BostJavier Tejedor-SojoJimeng Sun (Georgia Institute of Technology),"2524090778,2032867848,2510366178,2554429167,2682301811,2590039928,2477983982,2110385854","Proper representations of medical concepts such as diagnosis, medication, procedure codes and visits from Electronic Health Records (EHR) has broad applications in healthcare analytics. Patient EHR data consists of a sequence of visits over time, where each visit includes multiple medical concepts, e.g., diagnosis, procedure, and medication codes. This hierarchical structure provides two types of relational information, namely sequential order of visits and co-occurrence of the codes within a visit. In this work, we propose Med2Vec , which not only learns the representations for both medical codes and visits from large EHR datasets with over million visits, but also allows us to interpret the learned representations confirmed positively by clinical experts. In the experiments, Med2Vec shows significant improvement in prediction accuracy in clinical applications compared to baselines such as Skip-gram, GloVe, and stacked autoencoder, while providing clinically meaningful interpretation.",2016,Knowledge Discovery and Data Mining,Fields of study: artificial neural networkdata scienceknowledge managementdata miningmachine learningcomputer science
Robust Influence Maximization,Wei Chen (Microsoft)Tian Lin (Tsinghua University)Zihan Tan (Tsinghua University)Mingfei Zhao (Tsinghua University)Xuren Zhou (Hong Kong University of Science and Technology),"2527738285,2122568511,2113456608,2512255978,2511313199","In this paper, we address the important issue of uncertainty in the edge influence probability estimates for the well studied influence maximization problem --- the task of finding k seed nodes in a social network to maximize the influence spread. We propose the problem of robust influence maximization, which maximizes the worst-case ratio between the influence spread of the chosen seed set and the optimal seed set, given the uncertainty of the parameter input. We design an algorithm that solves this problem with a solution-dependent bound. We further study uniform sampling and adaptive sampling methods to effectively reduce the uncertainty on parameters and improve the robustness of the influence maximization task. Our empirical results show that parameter uncertainty may greatly affect influence maximization performance and prior studies that learned influence probabilities could lead to poor performance in robust influence maximization due to relatively large uncertainty in parameter estimates, and information cascade based adaptive sampling method may be an effective way to improve the robustness of influence maximization.",2016,Knowledge Discovery and Data Mining,Fields of study: social networkrobust optimizationsocial sciencemachine learningmathematical optimizationstatisticscomputer sciencemathematics
Computational Social Science: Exciting Progress and Future Challenges,Duncan Watts (Microsoft),2138805444,"The past 15 years have witnessed a remarkable increase in both the scale and scope of social and behavioral data available to researchers, leading some to herald the emergence of a new field: ""computational social science."" Against these exciting developments stands a stubborn fact: that in spite of many thousands of published papers, there has been surprisingly little progress on the ""big"" questions that motivated the field in the first place?questions concerning systemic risk in financial systems, problem solving in complex organizations, and the dynamics of epidemics or social movements, among others. In this talk I highlight some examples of research that would not have been possible just a handful of years ago and that illustrate the promise of CSS. At the same time, they illustrate its limitations. I then conclude with some thoughts on how CSS can bridge the gap between its current state and its potential.",2016,Knowledge Discovery and Data Mining,Fields of study: management sciencecomputer science
Squish: Near-Optimal Compression for Archival of Relational Datasets,Yihan Gao (University of Illinois at Urbana–Champaign)Aditya G. Parameswaran (University of Illinois at Urbana–Champaign),"2171035091,2077695977","Relational datasets are being generated at an alarmingly rapid rate across organizations and industries. Compressing these datasets could significantly reduce storage and archival costs. Traditional compression algorithms, e.g., gzip, are suboptimal for compressing relational datasets since they ignore the table structure and relationships between attributes. We study compression algorithms that leverage the relational structure to compress datasets to a much greater extent. We develop Squish, a system that uses a combination of Bayesian Networks and Arithmetic Coding to capture multiple kinds of dependencies among attributes and achieve near-entropy compression rate. Squish also supports user-defined attributes: users can instantiate new data types by simply implementing five functions for a new class interface. We prove the asymptotic optimality of our compression algorithm and conduct experiments to show the effectiveness of our system: Squish achieves a reduction of over 50% in storage size relative to systems developed in prior work on a variety of real datasets.",2016,Knowledge Discovery and Data Mining,Fields of study: lossless compressionbayesian networkarithmetic codinglossy compressiondata compressiontheoretical computer sciencedata miningdatabasemachine learningstatisticscomputer science
Recurrent Marked Temporal Point Processes: Embedding Event History to Vector,Nan Du (Georgia Institute of Technology)Hanjun Dai (Georgia Institute of Technology)Rakshit Trivedi (Georgia Institute of Technology)Utkarsh Upadhyay (Max Planck Society)Manuel Gomez-Rodriguez (Max Planck Society)Le Song (Georgia Institute of Technology),"2681008474,2646439162,2150403271,2353884032,2279633593,2113868374","Large volumes of event data are becoming increasingly available in a wide variety of applications, such as healthcare analytics, smart cities and social network analysis. The precise time interval or the exact distance between two events carries a great deal of information about the dynamics of the underlying systems. These characteristics make such data fundamentally different from independently and identically distributed data and time-series data where time and space are treated as indexes rather than random variables. Marked temporal point processes are the mathematical framework for modeling event data with covariates. However, typical point process models often make strong assumptions about the generative processes of the event data, which may or may not reflect the reality, and the specifically fixed parametric assumptions also have restricted the expressive power of the respective processes. Can we obtain a more expressive model of marked temporal point processes? How can we learn such a model from massive data? In this paper, we propose the Recurrent Marked Temporal Point Process (RMTPP) to simultaneously model the event timings and the markers. The key idea of our approach is to view the intensity function of a temporal point process as a nonlinear function of the history, and use a recurrent neural network to automatically learn a representation of influences from the event history. We develop an efficient stochastic gradient algorithm for learning the model parameters which can readily scale up to millions of events. Using both synthetic and real world datasets, we show that, in the case where the true models have parametric specifications, RMTPP can learn the dynamics of such models without the need to know the actual parametric forms; and in the case where the true models are unknown, RMTPP can also learn the dynamics and achieve better predictive performance than other parametric alternatives based on particular prior assumptions.",2016,Knowledge Discovery and Data Mining,Fields of study: recurrent neural networkstochastic processeconometricsdata miningmachine learningstatisticscomputer sciencemathematics
Unbounded Human Learning: Optimal Scheduling for Spaced Repetition,Siddharth Reddy (Cornell University)Igor Labutov (Cornell University)Siddhartha Banerjee (Cornell University)Thorsten Joachims (Cornell University),"2286135830,1028054495,2146436969,245171893","In the study of human learning, there is broad evidence that our ability to retain information improves with repeated exposure and decays with delay since last exposure. This plays a crucial role in the design of educational software, leading to a trade-off between teaching new material and reviewing what has already been taught. A common way to balance this trade-off is spaced repetition , which uses periodic review of content to improve long-term retention. Though spaced repetition is widely used in practice, e.g., in electronic flashcard software, there is little formal understanding of the design of these systems. Our paper addresses this gap in three ways. First, we mine log data from spaced repetition software to establish the functional dependence of retention on reinforcement and delay. Second, we use this memory model to develop a stochastic model for spaced repetition systems. We propose a queueing network model of the Leitner system for reviewing flashcards, along with a heuristic approximation that admits a tractable optimization problem for review scheduling. Finally, we empirically evaluate our queueing model through a Mechanical Turk experiment, verifying a key qualitative prediction of our model: the existence of a sharp phase transition in learning outcomes upon increasing the rate of new item introductions.",2016,Knowledge Discovery and Data Mining,Fields of study: memoryreal time computingartificial intelligencemachine learningsimulationstatisticscomputer science
Point-of-Interest Recommendations: Learning Potential Check-ins from Friends,Huayu Li (University of North Carolina System)Yong Ge (University of Arizona)Richang Hong (Hefei University of Technology)Hengshu Zhu (Baidu),"2304865086,2695934969,2632078846,2098414524","The emergence of Location-based Social Network (LBSN) services provides a wonderful opportunity to build personalized Point-of-Interest (POI) recommender systems. Although a personalized POI recommender system can significantly facilitate users' outdoor activities, it faces many challenging problems, such as the hardness to model user's POI decision making process and the difficulty to address data sparsity and user/location cold-start problem. To cope with these challenges, we define three types of friends (i.e., social friends, location friends, and neighboring friends) in LBSN, and develop a two-step framework to leverage the information of friends to improve POI recommendation accuracy and address cold-start problem. Specifically, we first propose to learn a set of potential locations that each individual's friends have checked-in before and this individual is most interested in. Then we incorporate three types of check-ins (i.e., observed check-ins, potential check-ins and other unobserved check-ins) into matrix factorization model using two different loss functions (i.e., the square error based loss and the ranking error based loss). To evaluate the proposed model, we conduct extensive experiments with many state-of-the-art baseline methods and evaluation metrics on two real-world data sets. The experimental results demonstrate the effectiveness of our methods.",2016,Knowledge Discovery and Data Mining,Fields of study: point of interestmatrix decompositionworld wide webdata miningmachine learning
"People, Computers, and The Hot Mess of Real Data","Joseph M. Hellerstein (University of California, Berkeley)",2063640528,"In practice, end-to-end data analysis is rarely a cleanly engineered process. Acquiring data can be tricky. Data assessment, wrangling and feature extraction are time-consuming and subjective. Models and algorithms used to derive data products are highly contextualized by time-varying properties of data sources, code and application needs. All of these issues would ideally benefit from an organizational view, but are often driven by individual users. Viewed holistically, both agile analytics and the establishment of analytic pipelines involve interactions between people, computation and infrastructure. In this talk I'll share some anecdotes from our research, user studies, and field experience with companies (Trifacta, Captricity), as well as an emerging open-source project (Ground).",2016,Knowledge Discovery and Data Mining,Fields of study: data sciencedata miningdatabaseartificial intelligencemachine learningcomputer science
Aircraft Trajectory Prediction Made Easy with Predictive Analytics,"Samet Ayhan (University of Maryland, College Park)Hanan Samet (University of Maryland, College Park)","2479110344,89934235","At the heart of Air Traffic Management (ATM) lies the Decision Support Systems (DST) that rely upon accurate trajectory prediction to determine how the airspace will look like in the future to make better decisions and advisories. Dealing with airspace that is prone to congestion due to environmental factors still remains the challenge especially when a deterministic approach is used in the trajectory prediction process. In this paper, we describe a novel stochastic trajectory prediction approach for ATM that can be used for more efficient and realistic flight planning and to assist airspace flow management, potentially resulting in higher safety, capacity, and efficiency commensurate with fuel savings thereby reducing emissions for a better environment. Our approach considers airspace as a 3D grid network, where each grid point is a location of a weather observation. We hypothetically build cubes around these grid points, so the entire airspace can be considered as a set of cubes. Each cube is defined by its centroid, the original grid point, and associated weather parameters that remain homogeneous within the cube during a period of time. Then, we align raw trajectories to a set of cube centroids which are basically fixed 3D positions independent of trajectory data. This creates a new form of trajectories which are 4D joint cubes, where each cube is a segment that is associated with not only spatio-temporal attributes but also with weather parameters. Next, we exploit machine learning techniques to train inference models from historical data and apply a stochastic model, a Hidden Markov Model (HMM), to predict trajectories taking environmental uncertainties into account. During the process, we apply time series clustering to generate input observations from an excessive set of weather parameters to feed into the Viterbi algorithm. Our experiments use a real trajectory dataset with pertaining weather observations and demonstrate the effectiveness of our approach to the trajectory prediction process for ATM.",2016,Knowledge Discovery and Data Mining,Fields of study: predictive analyticstime serieshidden markov modeldata miningmachine learningsimulationcomputer science
Deep Visual-Semantic Hashing for Cross-Modal Retrieval,Yue Cao (Tsinghua University)Mingsheng Long (Tsinghua University)Jianmin Wang (Tsinghua University)Qiang Yang (Hong Kong University of Science and Technology)Philip S. Yu (University of Illinois at Chicago),"2503224341,2152250954,2310637432,2109031554,2125104194","Due to the storage and retrieval efficiency, hashing has been widely applied to approximate nearest neighbor search for large-scale multimedia retrieval. Cross-modal hashing, which enables efficient retrieval of images in response to text queries or vice versa, has received increasing attention recently. Most existing work on cross-modal hashing does not capture the spatial dependency of images and temporal dynamics of text sentences for learning powerful feature representations and cross-modal embeddings that mitigate the heterogeneity of different modalities. This paper presents a new Deep Visual-Semantic Hashing (DVSH) model that generates compact hash codes of images and sentences in an end-to-end deep learning architecture, which capture the intrinsic cross-modal correspondences between visual data and natural language. DVSH is a hybrid deep architecture that constitutes a visual-semantic fusion network for learning joint embedding space of images and text sentences, and two modality-specific hashing networks for learning hash functions to generate compact binary codes. Our architecture effectively unifies joint multimodal embedding and cross-modal hashing, which is based on a novel combination of Convolutional Neural Networks over images, Recurrent Neural Networks over sentences, and a structured max-margin objective that integrates all things together to enable learning of similarity-preserving and high-quality hash codes. Extensive empirical evidence shows that our DVSH approach yields state of the art results in cross-modal retrieval experiments on image-sentences datasets, i.e. standard IAPR TC-12 and large-scale Microsoft COCO.",2016,Knowledge Discovery and Data Mining,Fields of study: feature hashinglocality preserving hashingdynamic perfect hashingextendible hashinguniversal hashinglocality sensitive hashinghash functionhash tabletheoretical computer sciencedata miningpattern recognitionmachine learningcomputer science
Come-and-Go Patterns of Group Evolution: A Dynamic Model,Tianyang Zhang (Tsinghua University)Peng Cui (Tsinghua University)Christos Faloutsos (Carnegie Mellon University)Yunfei Lu (Tsinghua University)Hao Ye (Tencent)Wenwu Zhu (Tsinghua University)Shiqiang Yang (Tsinghua University),"2294458475,2113115369,2198983026,2511457056,2710084553,2111511002,2127183023","How do social groups, such as Facebook groups and Wechat groups, dynamically evolve over time? How do people join the social groups, uniformly or with burst? What is the pattern of people quitting from groups? Is there a simple universal model to depict the come-and-go patterns of various groups? In this paper, we examine temporal evolution patterns of more than 100 thousands social groups with more than 10 million users. We surprisingly find that the evolution patterns of real social groups goes far beyond the classic dynamic models like SI and SIR. For example, we observe both diffusion and non-diffusion mechanism in the group joining process, and power-law decay in group quitting process, rather than exponential decay as expected in SIR model. Therefore we propose a new model comeNgo , a concise yet flexible dynamic model for group evolution. Our model has the following advantages: (a) unification power: it generalizes earlier theoretical models and different joining and quitting mechanisms we find from observation. (b) succinctness and interpretability: it contains only six parameters with clear physical meanings. (c) accuracy: it can capture various kinds of group evolution patterns preciously and the goodness of fit increase by 58% over baseline. (d) usefulness: it can be used in multiple application scenarios such as forecasting and pattern discovery.",2016,Knowledge Discovery and Data Mining,Fields of study: artificial intelligencesimulationstatistics
A Multiple Test Correction for Streams and Cascades of Statistical Hypothesis Tests,Geoffrey I. Webb (Monash University)François Petitjean (Monash University),"2126304162,2709762332","Statistical hypothesis testing is a popular and powerful tool for inferring knowledge from data. For every such test performed, there is always a non-zero probability of making a false discovery, i.e.~rejecting a null hypothesis in error. Familywise error rate (FWER) is the probability of making at least one false discovery during an inference process. The expected FWER grows exponentially with the number of hypothesis tests that are performed, almost guaranteeing that an error will be committed if the number of tests is big enough and the risk is not managed; a problem known as the multiple testing problem. State-of-the-art methods for controlling FWER in multiple comparison settings require that the set of hypotheses be predetermined. This greatly hinders statistical testing for many modern applications of statistical inference, such as model selection, because neither the set of hypotheses that will be tested, nor even the number of hypotheses, can be known in advance. This paper introduces Subfamilywise Multiple Testing, a multiple-testing correction that can be used in applications for which there are repeated pools of null hypotheses from each of which a single null hypothesis is to be rejected and neither the specific hypotheses nor their number are known until the final rejection decision is completed. To demonstrate the importance and relevance of this work to current machine learning problems, we further refine the theory to the problem of model selection and show how to use Subfamilywise Multiple Testing for learning graphical models. We assess its ability to discover graphical models on more than 7,000 datasets, studying the ability of Subfamilywise Multiple Testing to outperform the state of the art on data with varying size and dimensionality, as well as with varying density and power of the present correlations. Subfamilywise Multiple Testing provides a significant improvement in statistical efficiency, often requiring only half as much data to discover the same model, while strictly controlling FWER.",2016,Knowledge Discovery and Data Mining,Fields of study: testing hypotheses suggested by the datap repper comparison error ratefalse discovery ratemodel selectionmultiple comparisons problemstatistical hypothesis testingstatistical powereconometricsmachine learningstatisticsmathematics
"Beyond Sigmoids: The NetTide Model for Social Network Growth, and Its Applications",Chengxi Zang (Tsinghua University)Peng Cui (Tsinghua University)Christos Faloutsos (Carnegie Mellon University),"2514402881,2113115369,2198983026","What is the growth pattern of social networks, like Facebook and WeChat? Does it truly exhibit exponential early growth, as predicted by textbook models like the Bass model, SI, or the Branching Process? How about the count of links, over time, for which there are few published models? We examine the growth of several real networks, including one of the world's largest online social network, ``WeChat'', with 300 million nodes and 4.75 billion links by 2013; and we observe power law growth for both nodes and links, a fact that completely breaks the sigmoid models (like SI, and Bass). In its place, we propose NETTIDE, along with differential equations for the growth of the count of nodes, as well as links. Our model accurately fits the growth patterns of real graphs; it is general , encompassing as special cases all the known, traditional models (including Bass, SI, log-logistic growth); while still remaining parsimonious , requiring only a handful of parameters. Moreover, our NETTIDE for link growth is the first one of its kind, accurately fitting real data, and naturally leading to the densification phenomenon. We validate our model with four real, time-evolving social networks, where NETTIDE gives good fitting accuracy, and, more importantly, applied on the WeChat data, our NETTIDE forecasted more than 730 days into the future, with 3% error.",2016,Knowledge Discovery and Data Mining,Fields of study: social networkeconometricsdata miningartificial intelligencemachine learningsimulationstatisticscomputer sciencemathematics
Healthcare Data Mining with Matrix Models,Fei Wang (Cornell University)Ping Zhang (IBM)Joel Dudley (Icahn School of Medicine at Mount Sinai),"2614650233,2621458975,1987535789","In the last decade, advances in high-throughput technologies, growth of clinical data warehouses, and rapid accumulation of biomedical knowledge provided unprecedented opportunities and challenges to researchers in biomedical informatics. One distinct solution, to efficiently conduct big data analytics for biomedical problems, is the application of matrix computation and factorization methods such as non-negative matrix factorization, joint matrix factorization, tensor factorization. Compared to probabilistic and information theoretic approaches, matrix-based methods are fast, easy to understand and implement. In this tutorial, we provide a review of recent advances in algorithms and methods using matrix and their potential applications in biomedical informatics. We survey various related articles from data mining venues as well as from biomedical informatics venues to share with the audience key problems and trends in matrix computation research, with different novel applications such as drug repositioning, personalized medicine, and electronic phenotyping.",2016,Knowledge Discovery and Data Mining,Fields of study: data sciencetheoretical computer sciencedata miningcomputer science
City-Scale Map Creation and Updating using GPS Collections,Chen Chen 0018 (Stanford University)Cewu Lu (Stanford University)Qixing Huang (Toyota Technological Institute at Chicago)Qiang Yang 0001 (Hong Kong University of Science and Technology)Dimitrios Gunopulos (National and Kapodistrian University of Athens)Leonidas J. Guibas (Stanford University),"2646844496,2120761920,2166170891,2109031554,2712250546,356043702","Applications such as autonomous driving or real-time route recommendations require up-to-date and accurate digital maps. However, manually creating and updating such maps is too costly to meet the rising demands. As large collections of GPS trajectories become widely available, constructing and updating maps using such trajectory collections can greatly reduce the cost of such maps. Unfortunately, due to GPS noise and varying trajectory sampling rates, inferring maps from GPS trajectories can be very challenging. In this paper, we present a framework to create up-to-date maps with rich knowledge from GPS trajectory collections. Starting from an unstructured GPS point cloud, we discover road segments using novel graph-based clustering techniques with prior knowledge on road design. Based on road segments, we develop a scale- and orientation-invariant traj-SIFT feature to localize and recognize junctions using a supervised learning framework. Maps with rich knowledge are created based on discovered road segments and junctions. Compared to state-of-the-art methods, our approach can efficiently construct high-quality maps at city scales from large collections of GPS trajectories.",2016,Knowledge Discovery and Data Mining,Fields of study: computer visiondata mining
Detecting Devastating Diseases in Search Logs,John Paparrizos (Columbia University)Ryen W. White (Microsoft)Eric Horvitz (Microsoft),"2412068506,2096583854,1970391018","Web search queries can offer a unique population-scale window onto streams of evidence that are useful for detecting the emergence of health conditions. We explore the promise of harnessing behavioral signals in search logs to provide advance warning about the presence of devastating diseases such as pancreatic cancer. Pancreatic cancer is often diagnosed too late to be treated effectively as the cancer has usually metastasized by the time of diagnosis. Symptoms of the early stages of the illness are often subtle and nonspecific. We identify searchers who issue credible, first-person diagnostic queries for pancreatic cancer and we learn models from prior search histories that predict which searchers will later input such queries. We show that we can infer the likelihood of seeing the rise of diagnostic queries months before they appear and characterize the tradeoff between predictivity and false positive rate. The findings highlight the potential of harnessing search logs for the early detection of pancreatic cancer and more generally for harnessing search systems to reduce health risks for individuals.",2016,Knowledge Discovery and Data Mining,Fields of study: data scienceworld wide webdata mining
Parallel Dual Coordinate Descent Method for Large-scale Linear Classification in Multi-core Environments,Wei-Lin Chiang (National Taiwan University)Mu-Chu Lee (National Taiwan University)Chih-Jen Lin (National Taiwan University),"2250130882,2246527660,2168176072","Dual coordinate descent method is one of the most effective approaches for large-scale linear classification. However, its sequential design makes the parallelization difficult. In this work, we target at the parallelization in a multi-core environment. After pointing out difficulties faced in some existing approaches, we propose a new framework to parallelize the dual coordinate descent method. The key idea is to make the majority of all operations (gradient calculation here) parallelizable. The proposed framework is shown to be theoretically sound. Further, we demonstrate through experiments that the new framework is robust and efficient in a multi-core environment.",2016,Knowledge Discovery and Data Mining,Fields of study: coordinate descentlinear classifiertheoretical computer sciencedistributed computingmachine learningcomputer science
Developing a Data-Driven Player Ranking in Soccer Using Predictive Model Weights,Joel Brooks (Massachusetts Institute of Technology)Matthew Kerr (Massachusetts Institute of Technology)John V. Guttag (Massachusetts Institute of Technology),"2518096622,2431617671,120651226","Quantitative evaluation of the ability of soccer players to contribute to team offensive performance is typically based on goals scored, assists made, and shots taken. In this paper, we describe a novel player ranking system based entirely on the value of passes completed. This value is derived based on the relationship of pass locations in a possession and shot opportunities generated. This relationship is learned by applying a supervised machine learning model to pass locations in event data from the 2012-2013 La Liga season. Interestingly, though this metric is based entirely on passes, the derived player rankings are largely consistent with general perceptions of offensive ability, e.g., Messi and Ronaldo are near the top. Additionally, when used to rank midfielders, it separates the more offensively-minded players from others.",2016,Knowledge Discovery and Data Mining,Fields of study: multimediamachine learningsimulationcomputer science
When Social Influence Meets Item Inference,Hui-Ju Hung (Pennsylvania State University)Hong-Han Shuai (Academia Sinica)De-Nian Yang (Academia Sinica)Liang-Hao Huang (Academia Sinica)Wang-Chien Lee (Pennsylvania State University)Jian Pei (Simon Fraser University)Ming-Syan Chen (National Taiwan University),"2145006012,2184003779,2096343151,2146630906,2143778659,2126330539,2122365371","Research issues and data mining techniques for product recommendation and viral marketing have been widely studied. Existing works on seed selection in social networks do not take into account the effect of product recommendations in e-commerce stores. In this paper, we investigate the seed selection problem for viral marketing that considers both effects of social influence and item inference (for product recommendation). We develop a new model, Social Item Graph (SIG) , that captures both effects in the form of hyperedges. Accordingly, we formulate a seed selection problem, called Social Item Maximization Problem (SIMP) , and prove the hardness of SIMP. We design an efficient algorithm with performance guarantee, called Hyperedge-Aware Greedy (HAG), for SIMP and develop a new index structure, called SIG-index, to accelerate the computation of diffusion process in HAG. Moreover, to construct realistic SIG models for SIMP, we develop a statistical inference based framework to learn the weights of hyperedges from data. Finally, we perform a comprehensive evaluation on our proposals with various baselines. Experimental result validates our ideas and demonstrates the effectiveness and efficiency of the proposed model and algorithms over baselines.",2016,Knowledge Discovery and Data Mining,Fields of study: viral marketingworld wide webdata miningartificial intelligencemachine learningstatisticscomputer sciencemathematics
Bid-aware Gradient Descent for Unbiased Learning with Censored Data in Display Advertising,Weinan Zhang (Shanghai Jiao Tong University)Tianxiong ZhouJun Wang (University College London)Jian Xu,"2527611484,2510556814,2557836567,2705616049","In real-time display advertising, ad slots are sold per impression via an auction mechanism. For an advertiser, the campaign information is incomplete --- the user responses (e.g, clicks or conversions) and the market price of each ad impression are observed only if the advertiser's bid had won the corresponding ad auction. The predictions, such as bid landscape forecasting, click-through rate (CTR) estimation, and bid optimisation, are all operated in the pre-bid stage with full-volume bid request data. However, the training data is gathered in the post-bid stage with a strong bias towards the winning impressions. A common solution for learning over such censored data is to reweight data instances to correct the discrepancy between training and prediction. However, little study has been done on how to obtain the weights independent of previous bidding strategies and consequently integrate them into the final CTR prediction and bid generation steps. In this paper, we formulate CTR estimation and bid optimisation under such censored auction data. Derived from a survival model, we show that historic bid information is naturally incorporated to produce Bid-aware Gradient Descents (BGD) which controls both the importance and the direction of the gradient to achieve unbiased learning. The empirical study based on two large-scale real-world datasets demonstrates remarkable performance gains from our solution. The learning framework has been deployed on Yahoo!'s real-time bidding platform and provided 2.97% AUC lift for CTR estimation and 9.30% eCPC drop for bid optimisation in an online A/B test.",2016,Knowledge Discovery and Data Mining,Fields of study: unique bid auctionreal time biddingcensoringdata miningmachine learningstatistics
Images Don't Lie: Transferring Deep Visual Semantic Features to Large-Scale Multimodal Learning to Rank,Corey LynchKamelia Aryafar (Drexel University)Josh Attenberg (Polytechnic Institute of New York University),"2158118044,286824370,2065581641","Search is at the heart of modern e-commerce. As a result, the task of ranking search results automatically (learning to rank) is a multibillion dollar machine learning problem. Traditional models optimize over a few hand-constructed features based on the item's text. In this paper, we introduce a multimodal learning to rank model that combines these traditional features with visual semantic features transferred from a deep convolutional neural network. In a large scale experiment using data from the online marketplace Etsy, we verify that moving to a multimodal representation significantly improves ranking quality. We show how image features can capture fine-grained style information not available in a text-only representation. In addition, we show concrete examples of how image information can successfully disentangle pairs of highly different items that are ranked similarly by a text-only model.",2016,Knowledge Discovery and Data Mining,Fields of study: deep learninglearning to rankcomputer visionpattern recognitionmachine learningcomputer science
Collaborative Knowledge Base Embedding for Recommender Systems,Fuzheng Zhang (Microsoft)Nicholas Jing Yuan (Microsoft)Defu Lian (University of Electronic Science and Technology of China)Xing Xie (Microsoft)Wei-Ying Ma (Microsoft),"2110384818,2096490164,2110195189,2125800575,2134693834","Among different recommendation techniques, collaborative filtering usually suffer from limited performance due to the sparsity of user-item interactions. To address the issues, auxiliary information is usually used to boost the performance. Due to the rapid collection of information on the web, the knowledge base provides heterogeneous information including both structured and unstructured data with different semantics, which can be consumed by various applications. In this paper, we investigate how to leverage the heterogeneous information in a knowledge base to improve the quality of recommender systems. First, by exploiting the knowledge base, we design three components to extract items' semantic representations from structural content, textual content and visual content, respectively. To be specific, we adopt a heterogeneous network embedding method, termed as TransR, to extract items' structural representations by considering the heterogeneity of both nodes and relationships. We apply stacked denoising auto-encoders and stacked convolutional auto-encoders, which are two types of deep learning based embedding techniques, to extract items' textual representations and visual representations, respectively. Finally, we propose our final integrated framework, which is termed as Collaborative Knowledge Base Embedding (CKE), to jointly learn the latent representations in collaborative filtering as well as items' semantic representations from the knowledge base. To evaluate the performance of each embedding component as well as the whole system, we conduct extensive experiments with two real-world datasets from different scenarios. The results reveal that our approaches outperform several widely adopted state-of-the-art recommendation methods.",2016,Knowledge Discovery and Data Mining,Fields of study: collaborative filteringrecommender systemknowledge baseworld wide webdata miningmachine learningcomputer science
Transfer Knowledge between Cities,Ying Wei (Hong Kong University of Science and Technology)Yu Zheng (Microsoft)Qiang Yang (Hong Kong University of Science and Technology),"2235263654,2145115012,2109031554","The rapid urbanization has motivated extensive research on urban computing. It is critical for urban computing tasks to unlock the power of the diversity of data modalities generated by different sources in urban spaces, such as vehicles and humans. However, we are more likely to encounter the label scarcity problem and the data insufficiency problem when solving an urban computing task in a city where services and infrastructures are not ready or just built. In this paper, we propose a FLexible multimOdal tRAnsfer Learning (FLORAL) method to transfer knowledge from a city where there exist sufficient multimodal data and labels, to this kind of cities to fully alleviate the two problems. FLORAL learns semantically related dictionaries for multiple modalities from a source domain, and simultaneously transfers the dictionaries and labelled instances from the source into a target domain. We evaluate the proposed method with a case study of air quality prediction.",2016,Knowledge Discovery and Data Mining,Fields of study: transfer of learningdata miningartificial intelligencemachine learningsimulationcomputer science
Data-Driven Metric Development for Online Controlled Experiments: Seven Lessons Learned,Alex Deng (Microsoft)Xiaolin Shi (Yahoo!),"2172042952,2529829703","Online controlled experiments, also called A/B testing, have been established as the mantra for data-driven decision making in many web-facing companies. In recent years, there are emerging research works focusing on building the platform and scaling it up, best practices and lessons learned to obtain trustworthy results, and experiment design techniques and various issues related to statistical inference and testing. However, despite playing a central role in online controlled experiments, there is little published work on treating metric development itself as a data-driven process. In this paper, we focus on the topic of how to develop meaningful and useful metrics for online services in their online experiments, and show how data-driven techniques and criteria can be applied in metric development process. In particular, we emphasize two fundamental qualities for the goal metrics (or Overall Evaluation Criteria) of any online service: directionality and sensitivity. We share lessons on why these two qualities are critical, how to measure these two qualities of metrics of interest, how to develop metrics with clear directionality and high sensitivity by using approaches based on user behavior models and data-driven calibration, and how to choose the right goal metrics for the entire online services.",2016,Knowledge Discovery and Data Mining,Fields of study: management sciencedata miningsimulationcomputer science
Understanding Behaviors that Lead to Purchasing: A Case Study of Pinterest,Caroline Lo (Stanford University)Dan FrankowskiJure Leskovec (Stanford University),"2511782389,2639739921,1878631932","Online e-commerce applications are becoming a primary vehicle for people to find, compare, and ultimately purchase products. One of the fundamental questions that arises in e-commerce is to characterize, understand, and model user long-term purchasing intent, which is important as it allows for personalized and context relevant e-commerce services. In this paper we study user activity and purchasing behavior with the goal of building models of time-varying user purchasing intent. We analyze the purchasing behavior of nearly three million Pinterest users to determine short-term and long-term signals in user behavior that indicate higher purchase intent. We find that users with long-term purchasing intent tend to save and clickthrough on more content. However, as users approach the time of purchase their activity becomes more topically focused and actions shift from saves to searches. We further find that purchase signals in online behavior can exist weeks before a purchase is made and can also be traced across different purchase categories. Finally, we synthesize these insights in predictive models of user purchasing intent. Taken together, our work identifies a set of general principles and signals that can be used to model user purchasing intent across many content discovery applications.",2016,Knowledge Discovery and Data Mining,Fields of study: purchase order
MAP: Frequency-Based Maximization of Airline Profits based on an Ensemble Forecasting Approach,"Bo An (Nanyang Technological University)Haipeng Chen (Nanyang Technological University)Noseong Park (University of Maryland, College Park)V.S. Subrahmanian (University of Maryland, College Park)","2169236860,2710629891,2304434517,2261167843","Though there are numerous traditional models to predict market share and demand along airline routes, the prediction of existing models is not precise enough and, to the best of our knowledge, there is no use of data-mining based forecasting techniques to improve airline profitability. We propose the MAP (Maximizing Airline Profits) architecture designed to help airlines and make two key contributions in airline market share and route demand prediction and prediction-based airline profit optimization. Compared with past methods to forecast market share and demand along airline routes, we introduce a novel Ensemble Forecasting (MAP-EF) approach considering two new classes of features: (i) features derived from clusters of similar routes, and (ii) features based on equilibrium pricing. We show that MAP-EF achieves much better Pearson Correlation Coefficients (over 0.95 vs. 0.82 for market share, 0.98 vs. 0.77 for demand) and R2-values compared with three state-of-the-art works for forecasting market share and demand, while showing much lower variance. Using the results of MAP-EF, we develop MAP-Bilevel Branch and Bound (MAP-BBB) and MAP-Greedy (MAP-G) algorithms to optimally allocate flight frequencies over multiple routes, to maximize an airline's profit. Experimental results show that airlines can increase profits by a significant margin. All experiments were conducted with data aggregated from four sources: US Bureau of Transportation Statistics (BTS), US Bureau of Economic Analysis (BEA), the National Transportation Safety Board (NTSB), and the US Census Bureau (CB).",2016,Knowledge Discovery and Data Mining,Fields of study: regression
Robust Extreme Multi-label Learning,"Chang Xu (Peking University)Dacheng Tao (University of Technology, Sydney)Chao Xu (Peking University)","2115321199,2104129307,2210879162","Tail labels in the multi-label learning problem undermine the low-rank assumption. Nevertheless, this problem has rarely been investigated. In addition to using the low-rank structure to depict label correlations, this paper explores and exploits an additional sparse component to handle tail labels behaving as outliers, in order to make the classical low-rank principle in multi-label learning valid. The divide-and-conquer optimization technique is employed to increase the scalability of the proposed algorithm while theoretically guaranteeing its performance. A theoretical analysis of the generalizability of the proposed algorithm suggests that it can be improved by the low-rank and sparse decomposition given tail labels. Experimental results on real-world data demonstrate the significance of investigating tail labels and the effectiveness of the proposed algorithm.",2016,Knowledge Discovery and Data Mining,Fields of study: data miningartificial intelligencemachine learningstatisticscomputer science
A Multi-Task Learning Formulation for Survival Analysis,Yan Li (Wayne State University)Jie Wang (University of Michigan)Jieping Ye (University of Michigan)Chandan K. Reddy (Wayne State University),"2607418379,2471893859,2305258894,2100435683","Predicting the occurrence of a particular event of interest at future time points is the primary goal of survival analysis. The presence of incomplete observations due to time limitations or loss of data traces is known as censoring which brings unique challenges in this domain and differentiates survival analysis from other standard regression methods. The popularly used survival analysis methods such as Cox proportional hazard model and parametric survival regression suffer from some strict assumptions and hypotheses that are not realistic in most of the real-world applications. To overcome the weaknesses of these two types of methods, in this paper, we reformulate the survival analysis problem as a multi-task learning problem and propose a new multi-task learning based formulation to predict the survival time by estimating the survival status at each time interval during the study duration. We propose an indicator matrix to enable the multi-task learning algorithm to handle censored instances and incorporate some of the important characteristics of survival problems such as non-negative non-increasing list structure into our model through max-heap projection. We employ the L2,1-norm penalty which enables the model to learn a shared representation across related tasks and hence select important features and alleviate over-fitting in high-dimensional feature spaces; thus, reducing the prediction error of each task. To efficiently handle the two non-smooth constraints, in this paper, we propose an optimization method which employs Alternating Direction Method of Multipliers (ADMM) algorithm to solve the proposed multi-task learning problem. We demonstrate the performance of the proposed method using real-world microarray gene expression high-dimensional benchmark datasets and show that our method outperforms state-of-the-art methods.",2016,Knowledge Discovery and Data Mining,Fields of study: multi task learningclustering high dimensional dataregularizationsurvival analysiseconometricsdata miningmachine learningstatisticscomputer science
Scalable Pattern Matching over Compressed Graphs via Dedensification,Antonio Maccioni (Roma Tre University)Daniel J. Abadi (Yale University),"2096345549,2170206103","One of the most common operations on graph databases is graph pattern matching ( e.g. , graph isomorphism and more general types of ""subgraph pattern matching""). In fact, in some graph query languages every single query is expressed as a graph matching operation. Consequently, there has been a significant amount of research effort in optimizing graph matching operations in graph database systems. As graph databases have scaled in recent years, so too has recent work on scaling graph matching operations. However, the performance of recent proposals for scaling graph pattern matching is limited by the presence of high-degree nodes. These high-degree nodes result in an explosion of intermediate result sizes during query execution, and therefore significant performance bottlenecks. In this paper we present a dedensification technique that losslessly compresses the neighborhood around high-degree nodes. Furthermore, we introduce a query processing technique that enables direct operation of graph query processing operations over the compressed data, without ever having to decompress the data. For pattern matching operations, we show how this technique can be implemented as a layer above existing graph database systems, so that the end-user can benefit from this technique without requiring modifications to the core graph database engine code. Our technique reduces the size of the intermediate result sets during query processing, and thereby improves query performance.",2016,Knowledge Discovery and Data Mining,Fields of study: folded cube graphfactor critical graphdistance hereditary graphsimplex graphstrength of a graphvoltage graphcomplement graphgraph bandwidthgraph powergraph operationsnull graphclique widthgraph propertysparqlgraph databasegraphrdfpower lawtheoretical computer sciencedata miningdatabasecomputer science
Smart Broadcasting: Do You Want to be Seen?,Mohammad Reza Karimi (Sharif University of Technology)Erfan Tavakoli (Sharif University of Technology)Mehrdad Farajtabar (Georgia Institute of Technology)Le Song (Georgia Institute of Technology)Manuel Gomez Rodriguez (Max Planck Society),"2515129431,2518046445,1198110563,2113868374,2421855118","Many users in online social networks are constantly trying to gain attention from their followers by broadcasting posts to them. These broadcasters are likely to gain greater attention if their posts can remain visible for a longer period of time among their followers' most recent feeds. Then when to post? In this paper, we study the problem of smart broadcasting using the framework of temporal point processes, where we model users feeds and posts as discrete events occurring in continuous time. Based on such continuous-time model, then choosing a broadcasting strategy for a user becomes a problem of designing the conditional intensity of her posting events. We derive a novel formula which links this conditional intensity with the visibility of the user in her followers' feeds. Furthermore, by exploiting this formula, we develop an efficient convex optimization framework for the when-to-post problem. Our method can find broadcasting strategies that reach a desired visibility level with provable guarantees. We experimented with data gathered from Twitter, and show that our framework can consistently make broadcasters' post more visible than alternatives.",2016,Knowledge Discovery and Data Mining,Fields of study: visibilitysocial networkworld wide webtelecommunicationssocial sciencedistributed computingmachine learningsimulationstatisticscomputer science
Computational Drug Repositioning Using Continuous Self-Controlled Case Series,Zhaobin Kuang (University of Wisconsin-Madison)James A. Thomson (Morgridge Institute for Research)Michael Caldwell (Marshfield Clinic)Peggy L. Peissig (Marshfield Clinic)Ron M. Stewart (Morgridge Institute for Research)David Page (University of Wisconsin-Madison),"2513022391,2141025565,2098265439,2028114619,2173824337,1975712082",-,2016,Knowledge Discovery and Data Mining,-
Gemello: Creating a Detailed Energy Breakdown from Just the Monthly Electricity Bill,Nipun Batra (Indraprastha Institute of Information Technology)Amarjeet Singh (Indraprastha Institute of Information Technology)Kamin Whitehouse (University of Virginia),"2116968475,2146959451,2001774235","The first step to saving energy in the home is often to create an energy breakdown: the amount of energy used by each individual appliance in the home. Unfortunately, current techniques that produce an energy breakdown are not scalable: they require hardware to be installed in each and every home. In this paper, we propose a more scalable solution called Gemello that estimates the energy breakdown for one home by matching it with similar homes for which the breakdown is already known. This matching requires only the monthly energy bill and household characteristics such as square footage of the home and the size of the household. We evaluate this approach using 57 homes and results indicate that the accuracy of Gemello is comparable to or better than existing techniques that use sensing infrastructure in each home. The information required by Gemello is often publicly available and, as such, it can be immediately applied to many homes around the world.",2016,Knowledge Discovery and Data Mining,Fields of study: feedbacktelecommunicationssimulationcomputer science
Online Optimization Methods for the Quantification Problem,Purushottam Kar (Indian Institute of Technology Kanpur)Shuai Li (University of Insubria)Harikrishna Narasimhan (Harvard University)Sanjay Chawla (Qatar Computing Research Institute)Fabrizio Sebastiani (Qatar Computing Research Institute),"2136781283,2708734412,2637028757,2201421368,2130902957","The estimation of class prevalence, i.e., of the fraction of a population that belongs to a certain class, is an important task in data analytics, and finds applications in many domains such as the social sciences, market research, epidemiology, and others. For example, in sentiment analysis the goal is often not to estimate whether a specific text conveys a positive or a negative sentiment, but rather to estimate the overall distribution of positive and negative sentiments, e.g., in a certain time frame. A popular way of performing the above task, often dubbed quantification , is to use supervised learning in order to train a prevalence estimator from labeled data. In the literature there are several performance metrics for measuring the success of such prevalence estimators. In this paper we propose the first online stochastic algorithms for directly optimizing these quantification-specific performance measures. We also provide algorithms that optimize hybrid performance measures that seek to balance quantification and classification performance. Our algorithms present a significant advancement in the theory of multivariate optimization; we show, via a rigorous theoretical analysis, that they exhibit optimal convergence. We also report extensive experiments on benchmark and real data sets which demonstrate that our methods significantly outperform existing optimization techniques used for these performance measures.",2016,Knowledge Discovery and Data Mining,Fields of study: biological classificationstochastic optimizationeconometricsdata miningmachine learningstatisticscomputer sciencemathematics
Scalable Fast Rank-1 Dictionary Learning for fMRI Big Data Analysis,"Xiang Li (University of Georgia)Milad Makkie (University of Georgia)Binbin Lin (University of Michigan)Mojtaba Sedigh Fazli (University of Georgia)Ian Davidson (University of California, Davis)Jieping Ye (University of Michigan)Tianming Liu (University of Georgia)Shannon Quinn (University of Georgia)","2068388382,2480698404,2643646471,2630584540,2560595684,2305258894,2161381501,2517111332","It has been shown from various functional neuroimaging studies that sparsity-regularized dictionary learning could achieve superior performance in decomposing comprehensive and neuroscientifically meaningful functional networks from massive fMRI signals. However, the computational cost for solving the dictionary learning problem has been known to be very demanding, especially when dealing with large-scale data sets. Thus in this work, we propose a novel distributed rank-1 dictionary learning (D-r1DL) model and apply it for fMRI big data analysis. The model estimates one rank-1 basis vector with sparsity constraint on its loading coefficient from the input data at each learning step through alternating least squares updates. By iteratively learning the rank-1 basis and deflating the input data at each step, the model is then capable of decomposing the whole set of functional networks. We implement and parallelize the rank-1 dictionary learning algorithm using Spark engine and deployed the resilient distributed dataset (RDDs) abstracts for the data distribution and operations. Experimental results from applying the model on the Human Connectome Project (HCP) data show that the proposed D-r1DL model is efficient and scalable towards fMRI big data analytics, thus enabling data-driven neuroscientific discovery from massive fMRI big data in the future.",2016,Knowledge Discovery and Data Mining,Fields of study: k svdneural codingtheoretical computer sciencedata miningmachine learningcomputer science
Contextual Intent Tracking for Personal Assistants,Yu Sun (University of Melbourne)Nicholas Jing Yuan (Microsoft)Yingzi Wang (University of Science and Technology of China)Xing Xie (Microsoft)Kieran McDonald (Microsoft)Rui Zhang (University of Melbourne),"2617049789,2096490164,2274817761,2125800575,2508691199,2690388134","A new paradigm of recommendation is emerging in intelligent personal assistants such as Apple's Siri, Google Now, and Microsoft Cortana, which recommends ""the right information at the right time"" and proactively helps you ""get things done"". This type of recommendation requires precisely tracking users' contemporaneous intent, i.e., what type of information (e.g., weather, stock prices) users currently intend to know, and what tasks (e.g., playing music, getting taxis) they intend to do. Users' intent is closely related to context, which includes both external environments such as time and location, and users' internal activities that can be sensed by personal assistants. The relationship between context and intent exhibits complicated co-occurring and sequential correlation, and contextual signals are also heterogeneous and sparse, which makes modeling the context intent relationship a challenging task. To solve the intent tracking problem, we propose the Kalman filter regularized PARAFAC2 (KP2) nowcasting model, which compactly represents the structure and co-movement of context and intent. The KP2 model utilizes collaborative capabilities among users, and learns for each user a personalized dynamic system that enables efficient nowcasting of users' intent. Extensive experiments using real-world data sets from a commercial personal assistant show that the KP2 model significantly outperforms various methods, and provides inspiring implications for deploying large-scale proactive recommendation systems in personal assistants.",2016,Knowledge Discovery and Data Mining,Fields of study: multi task learningmultimediaworld wide webdata miningmachine learningsimulationcomputer science
DeepIntent: Learning Attentions for Online Advertising with Recurrent Neural Networks,Shuangfei Zhai (Binghamton University)Keng-hao Chang (Microsoft)Ruofei Zhang (Microsoft)Zhongfei Mark Zhang (Microsoft),"2178509154,2603165909,2681549092,2427460458","In this paper, we investigate the use of recurrent neural networks (RNNs) in the context of search-based online advertising. We use RNNs to map both queries and ads to real valued vectors, with which the relevance of a given (query, ad) pair can be easily computed. On top of the RNN, we propose a novel attention network, which learns to assign attention scores to different word locations according to their intent importance (hence the name DeepIntent). The vector output of a sequence is thus computed by a weighted sum of the hidden states of the RNN at each word according their attention scores. We perform end-to-end training of both the RNN and attention network under the guidance of user click logs, which are sampled from a commercial search engine. We show that in most cases the attention network improves the quality of learned vector representations, evaluated by AUC on a manually labeled dataset. Moreover, we highlight the effectiveness of the learned attention scores from two aspects: query rewriting and a modified BM25 metric. We show that using the learned attention scores, one is able to produce sub-queries that are of better qualities than those of the state-of-the-art methods. Also, by modifying the term frequency with the attention scores in a standard BM25 formula, one is able to improve its performance evaluated by AUC.",2016,Knowledge Discovery and Data Mining,Fields of study: okapi bm25deep learningonline advertisingspeech recognitiondata miningmachine learningcomputer science
Learning Cumulatively to Become More Knowledgeable,Geli Fei (University of Illinois at Chicago)Shuai Wang (University of Illinois at Chicago)Bing Liu (University of Illinois at Chicago),"2133167487,2441217776,2244698799","In classic supervised learning, a learning algorithm takes a fixed training data of several classes to build a classifier. In this paper, we propose to study a new problem, i.e., building a learning system that learns cumulatively. As time goes by, the system sees and learns more and more classes of data and becomes more and more knowledgeable. We believe that this is similar to human learning. We humans learn continuously, retaining the learned knowledge, identifying and learning new things, and updating the existing knowledge with new experiences. Over time, we cumulate more and more knowledge. A learning system should be able to do the same. As algorithmic learning matures, it is time to tackle this cumulative machine learning (or simply cumulative learning ) problem, which is a kind of lifelong machine learning problem. It presents two major challenges. First, the system must be able to detect data from unseen classes in the test set. Classic supervised learning, however, assumes all classes in testing are known or seen at the training time. Second, the system needs to be able to selectively update its models whenever a new class of data arrives without re-training the whole system using the entire past and present training data. This paper proposes a novel approach and system to tackle these challenges. Experimental results on two datasets with learning from 2 classes to up to 100 classes show that the proposed approach is highly promising in terms of both classification accuracy and computational efficiency.",2016,Knowledge Discovery and Data Mining,Fields of study: online machine learningstabilitypreference learninginductive transfermulti task learningrobot learningsynchronous learninggeneralization errorcompetitive learningactive learningerror driven learningalgorithmic learning theorylearning classifier systemsemi supervised learningcomputational learning theoryinstance based learningunsupervised learningdata miningartificial intelligencemachine learningcomputer science
Repeat Buyer Prediction for E-Commerce,"Guimei Liu (Agency for Science, Technology and Research)Tam T. Nguyen (Agency for Science, Technology and Research)Gang ZhaoWei Zha (Agency for Science, Technology and Research)Jianbo Yang (General Electric)Jianneng Cao (Agency for Science, Technology and Research)Min Wu (Agency for Science, Technology and Research)Peilin Zhao (Agency for Science, Technology and Research)Wei Chen","2641724321,2537889826,2667920047,2532738178,2538785804,2630008670,2723782748,2096910461,2675036247","A large number of new buyers are often acquired by merchants during promotions. However, many of the attracted buyers are one-time deal hunters, and the promotions may have little long-lasting impact on sales. It is important for merchants to identify who can be converted to regular loyal buyers and then target them to reduce promotion cost and increase the return on investment (ROI). At International Joint Conferences on Artificial Intelligence (IJCAI) 2015, Alibaba hosted an international competition for repeat buyer prediction based on the sales data of the ``Double 11"" shopping event in 2014 at Tmall.com. We won the first place at stage 1 of the competition out of 753 teams. In this paper, we present our winning solution, which consists of comprehensive feature engineering and model training. We created profiles for users, merchants, brands, categories, items and their interactions via extensive feature engineering. These profiles are not only useful for this particular prediction task, but can also be used for other important tasks in e-commerce, such as customer segmentation, product recommendation, and customer base augmentation for brands. Feature engineering is often the most important factor for the success of a prediction task, but not much work can be found in the literature on feature engineering for prediction tasks in e-commerce. Our work provides some useful hints and insights for data science practitioners in e-commerce.",2016,Knowledge Discovery and Data Mining,Fields of study: featuree commercedata miningmachine learningcomputer science
Towards Confidence in the Truth: A Bootstrapping based Truth Discovery Approach,Houping Xiao (State University of New York System)Jing Gao (State University of New York System)Qi Li (State University of New York System)Fenglong Ma (State University of New York System)Lu Su (State University of New York System)Yunlong Feng (Katholieke Universiteit Leuven)Aidong Zhang (State University of New York System),"2303863390,2096731881,2261907930,2227076362,2148733542,2705548786,2228514421","The demand for automatic extraction of true information (i.e., truths) from conflicting multi-source data has soared recently. A variety of truth discovery methods have witnessed great successes via jointly estimating source reliability and truths. All existing truth discovery methods focus on providing a point estimator for each object's truth, but in many real-world applications, confidence interval estimation of truths is more desirable, since confidence interval contains richer information. To address this challenge, in this paper, we propose a novel truth discovery method ( ETCIBoot ) to construct confidence interval estimates as well as identify truths, where the bootstrapping techniques are nicely integrated into the truth discovery procedure. Due to the properties of bootstrapping, the estimators obtained by ETCIBoot are more accurate and robust compared with the state-of-the-art truth discovery approaches. Theoretically, we prove the asymptotical consistency of the confidence interval obtained by ETCIBoot . Experimentally, we demonstrate that ETCIBoot is not only effective in constructing confidence intervals but also able to obtain better truth estimates.",2016,Knowledge Discovery and Data Mining,Fields of study: bootstrappingconfidence intervaleconometricsdata miningstatisticscomputer sciencemathematics
Efficient Shift-Invariant Dictionary Learning,Guoqing Zheng (Carnegie Mellon University)Yiming Yang (Carnegie Mellon University)Jaime G. Carbonell (Carnegie Mellon University),"2619846774,2159253281,2100444261","Shift-invariant dictionary learning (SIDL) refers to the problem of discovering a set of latent basis vectors (the dictionary) that captures informative local patterns at different locations of the input sequences, and a sparse coding for each sequence as a linear combination of the latent basis elements. It differs from conventional dictionary learning and sparse coding where the latent basis has the same dimension as the input vectors, where the focus is on global patterns instead of shift-invariant local patterns. Unsupervised discovery of shift-invariant dictionary and the corresponding sparse coding has been an open challenge as the number of candidate local patterns is extremely large, and the number of possible linear combinations of such local patterns is even more so. In this paper we propose a new framework for unsupervised discovery of both the shift-invariant basis and the sparse coding of input data, with efficient algorithms for tractable optimization. Empirical evaluations on multiple time series data sets demonstrate the effectiveness and efficiency of the proposed method.",2016,Knowledge Discovery and Data Mining,Fields of study: k svdneural codingsparse approximationtime seriesspeech recognitionpattern recognitionmachine learningstatisticscomputer science
Extracting Optimal Performance from Dynamic Time Warping,"Abdullah Mueen (University of New Mexico)Eamonn J. Keogh (University of California, Riverside)","2083987245,2170070822","Dynamic Time Warping (DTW) is a distance measure that compares two time series after optimally aligning them. DTW is being used for decades in thousands of academic and industrial projects despite the very expensive computational complexity, O(n 2 ). These applications include data mining, image processing, signal processing, robotics and computer graphics among many others. In spite of all this research effort, there are many myths and misunderstanding about DTW in the literature, for example ""it is too slow to be useful"" or ""the warping window size does not matter much."" In this tutorial, we correct these misunderstandings and we summarize the research efforts in optimizing both the efficiency and effectiveness of both the basic DTW algorithm, and of the higher-level algorithms that exploit DTW such as similarity search, clustering and classification. We will discuss variants of DTW such as constrained DTW, multidimensional DTW and asynchronous DTW, and optimization techniques such as lower bounding, early abandoning, run-length encoding, bounded approximation and hardware optimization. We will discuss a multitude of application areas including physiological monitoring, social media mining, activity recognition and animal sound processing. The optimization techniques are generalizable to other domains on various data types and problems.",2016,Knowledge Discovery and Data Mining,Fields of study: dynamic time warpingpruningapproximationtime seriestheoretical computer sciencespeech recognitiondata miningmachine learningstatisticscomputer science
Learning to Learn and Compositionality with Deep Recurrent Neural Networks: Learning to Learn and Compositionality,Nando de Freitas (Google),2112824674,"Deep neural network representations play an important role in computer vision, speech, computational linguistics, robotics, reinforcement learning and many other data-rich domains. In this talk I will show that learning-to-learn and compositionality are key ingredients for dealing with knowledge transfer so as to solve a wide range of tasks, for dealing with small-data regimes, and for continual learning. I will demonstrate this with several examples from my research team: learning to learn by gradient descent by gradient descent, neural programmers and interpreters, and learning communication.",2016,Knowledge Discovery and Data Mining,Fields of study: principle of compositionalitytransfer of learningrobot learningsynchronous learningrecurrent neural networkcompetitive learningerror driven learningactive learningdeep learninginstance based learningartificial neural networknatural language processingartificial intelligencemachine learningcomputer science
Lexis: An Optimization Framework for Discovering the Hierarchical Structure of Sequential Data,Payam Siyari (Georgia Institute of Technology)Bistra Dilkina (Georgia Institute of Technology)Constantine Dovrolis (Georgia Institute of Technology),"2303974943,2252452800,1945437555","Data represented as strings abounds in biology, linguistics, document mining, web search and many other fields. Such data often have a hierarchical structure, either because they were artificially designed and composed in a hierarchical manner or because there is an underlying evolutionary process that creates repeatedly more complex strings from simpler substrings. We propose a framework, referred to as Lexis, that produces an optimized hierarchical representation of a given set of ""target"" strings. The resulting hierarchy, ""Lexis-DAG"", shows how to construct each target through the concatenation of intermediate substrings, minimizing the total number of such concatenations or DAG edges. The Lexis optimization problem is related to the smallest grammar problem. After we prove its NP-hardness for two cost formulations, we propose an efficient greedy algorithm for the construction of Lexis-DAGs. We also consider the problem of identifying the set of intermediate nodes (substrings) that collectively form the ""core"" of a Lexis-DAG, which is important in the analysis of Lexis-DAGs. We show that the Lexis framework can be applied in diverse applications such as optimized synthesis of DNA fragments in genomic libraries, hierarchical structure discovery in protein sequences, dictionary-based text compression, and feature extraction from a set of documents.",2016,Knowledge Discovery and Data Mining,Fields of study: centralitydna synthesiscompressiondirected acyclic graphfeature extractiondata miningpattern recognitionmachine learningcomputer sciencemathematics
Parallel Lasso Screening for Big Data Optimization,Qingyang Li (Arizona State University)Shuang Qiu (University of Michigan)Shuiwang Ji (Washington State University)Paul M. Thompson (University of Southern California)Jieping Ye (University of Michigan)Jie Wang (University of Michigan),"2222994316,2660622005,2149659377,2114138198,2305258894,2471893859","Lasso regression is a widely used technique in data mining for model selection and feature extraction. In many applications, it remains challenging to apply the regression model to large-scale problems that have massive data samples with high-dimensional features. One popular and promising strategy is to solve the Lasso problem in parallel. Parallel solvers run multiple cores in parallel on a shared memory system to speedup the computation, while the practical usage is limited by the huge dimension in the feature space. Screening is a promising method to solve the problem of high dimensionality by discarding the inactive features and removing them from optimization. However, when integrating screening methods with parallel solvers, most of solvers cannot guarantee the convergence on the reduced feature matrix. In this paper, we propose a novel parallel framework by parallelizing screening methods and integrating it with our proposed parallel solver. We propose two parallel screening algorithms: Parallel Strong Rule (PSR) and Parallel Dual Polytope Projection (PDPP). For the parallel solver, we proposed an Asynchronous Grouped Coordinate Descent method (AGCD) to optimize the regression problem in parallel on the reduced feature matrix. AGCD is based on a grouped selection strategy to select the coordinate that has the maximum descent for the objective function in a group of candidates. Empirical studies on the real-world datasets demonstrate that the proposed parallel framework has a superior performance compared to the state-of-the-art parallel solvers.",2016,Knowledge Discovery and Data Mining,Fields of study: coordinate descenttheoretical computer sciencemachine learningmathematical optimizationcomputer science
Deploying Analytics with the Portable Format for Analytics (PFA),Jim Pivarski (University of Chicago)Collin BennettRobert L. Grossman (University of Chicago),"2549702503,2131176913,2095279874","We introduce a new language for deploying analytic models into products, services and operational systems called the Portable Format for Analytics (PFA). PFA is an example of what is sometimes called a model interchange format, a language for describing analytic models that is independent of specific tools, applications or systems. Model interchange formats allow one application (the model producer) to export models and another application (the model consumer or scoring engine) to import models. The core idea behind PFA is to support the safe execution of statistical functions, mathematical functions, and machine learning algorithms and their compositions within a safe execution environment. With this approach, the common analytic models used in data science can be implemented, as well as the data transformations and data aggregations required for pre- and post-processing data. PFA compliant scoring engines can be extended by adding new user defined functions described in PFA. We describe the design of PFA. A Data Mining Group (DMG) Working Group is developing the PFA standard. The current version is 0.8.1 and contains many of the commonly used statistical and machine learning models, including regression, clustering, support vector machines, neural networks, etc. We also describe two implementations of Hadrian, one in Scala and one in Python. We discuss four case studies that use PFA and Hadrian to specify analytic models, including two that are deployed in operations at client sites.",2016,Knowledge Discovery and Data Mining,Fields of study: predictive model markup languageworld wide webdata miningdatabasemachine learningcomputer science
Identifying Earmarks in Congressional Bills,Ellery Wulczyn (Stanford University)Madian Khabsa (Microsoft)Vrushank Vora (University of Chicago)Matthew Heston (Northwestern University)Joe Walsh (University of Chicago)Christopher Berry (University of Chicago)Rayid Ghani (University of Chicago),"2701281806,296516693,2614720474,2647221767,2680879499,2169003226,2655769349","Earmarks are legislative provisions that direct federal funds to specific projects, circumventing the competitive grant-making process of federal agencies. Identifying and cataloging earmarks is a tedious, time-consuming process carried out by experts from public interest groups. In this paper, we present a machine learning system for automatically extracting earmarks from congressional bills and reports. We first describe a table-parsing algorithm for extracting budget allocations from appropriations tables in congressional bills. We then use machine learning classifiers to identify budget allocations as earmarked objects with an out of sample ROC AUC score of 0.89. Using this system, we construct the first publicly available database of earmarks dating back to 1995. Our machine learning approach adds transparency, accuracy, and speed to the congressional appropriations process.",2016,Knowledge Discovery and Data Mining,Fields of study: information extractiondata sciencedata miningmachine learningcomputer science
Designing Policy Recommendations to Reduce Home Abandonment in Mexico,"Klaus Ackermann (Monash University)Eduardo Blancas Reyes (University of Chicago)Sue He (University of Virginia)Thomas Anderson Keller (University of California, San Diego)Paul van der Boor (University of Chicago)Romana Khan (Northwestern University)Rayid Ghani (University of Chicago)José Carlos González","2504984702,2507525880,2618421523,2514371254,2581893250,2107509328,2655769349,2689512287","Infonavit, the largest provider of mortgages in Mexico, assists working families to obtain low-interest rate housing solutions. An increasingly prevalent problem is home abandonment: when a homeowner decides to leave their property and forego their investment. A major causal factor of this outcome is a mismatch between the homeowner's needs, in terms of access to services and employment, and the location characteristics of the home. This paper describes our collaboration with Infonavit to reduce home abandonment at two levels: develop policy recommendations for targeted improvements in location characteristics, and develop a decision-support tool to assist the homeowner in the home location decision. Using 20 years of mortgage history data combined with surveys, census, and location information, we develop a model to predict the probability of home abandonment based on both individual and location characteristics. The model is used to develop a tool that provides Infonavit the ability to give advice to Mexican workers when they apply for a loan, evaluate and improve the locations of new housing developments, and provide data-driven recommendations to the federal government to influence local development initiatives and infrastructure investments. The result is improving economic outcomes for the citizens of Mexico by pre-emptively identifying at-risk home mortgages, thereby allowing them to be altered or remedied before they result in abandonment.",2016,Knowledge Discovery and Data Mining,Fields of study: common goodactuarial sciencemachine learningcomputer science
AnyDBC: An Efficient Anytime Density-based Clustering Algorithm for Very Large Complex Datasets,Son T. Mai (Aarhus University)Ira Assent (Aarhus University)Martin Storgaard (Aarhus University),"2650155944,145164693,2705453835","The density-based clustering algorithm DBSCAN is a state-of-the-art data clustering technique with numerous applications in many fields. However, its O(n 2 ) time complexity still remains a severe weakness. In this paper, we propose a novel anytime approach to cope with this problem by reducing both the range query and the label propagation time of DBSCAN. Our algorithm, called AnyDBC, compresses the data into smaller density-connected subsets called primitive clusters and labels objects based on connected components of these primitive clusters for reducing the label propagation time. Moreover, instead of passively performing the range query for all objects like existing techniques, AnyDBC iteratively and actively learns the current cluster structure of the data and selects a few most promising objects for refining clusters at each iteration. Thus, in the end, it performs substantially fewer range queries compared to DBSCAN while still guaranteeing the exact final result of DBSCAN. Experiments show speedup factors of orders of magnitude compared to DBSCAN and its fastest variants on very large real and synthetic complex datasets.",2016,Knowledge Discovery and Data Mining,Fields of study: subcluoptics algorithmdetermining the number of clusters in a data setdbscanactive learningclustering high dimensional datacluster analysisdata miningdatabasemachine learningcomputer science
From Prediction to Action: A Closed-Loop Approach for Data-Guided Network Resource Allocation,"Yanan Bao (University of California, Davis)Huasen Wu (University of California, Davis)Xin Liu (University of California, Davis)","2103719961,2114102195,2616096743","Machine learning methods have been widely used in modeling and predicting network user experience. In this paper, moving beyond user experience prediction, we propose a closed-loop approach that uses data-generated prediction models to explicitly guide resource allocation for user experience improvement. The closed-loop approach leverages and verifies the causal relation that often exists between certain feature values (e.g., bandwidth) and user experience in computer networks. The approach consists of three components: we train a neural network classifier to predict user experience, utilize the trained neural network classifier as the objective function to allocate network resource, and then evaluate user experience with allocated resource to (in)validate and adjust the original model. Specifically, we propose a dual decomposition algorithm to solve the neural network-based resource optimization problem, which is complex and non-convex. We further develop an iterative mechanism for classifier optimization. Numerical results show that the dual algorithm reduces the expected number of unsatisfied users by up to 2x compared with the baseline, and the optimized classifier further improves the performance by 50%.",2016,Knowledge Discovery and Data Mining,Fields of study: biological classificationdata miningmachine learningsimulationcomputer science
Infinite Ensemble for Image Clustering,Hongfu Liu (Northeastern University)Ming Shao (Northeastern University)Sheng Li (Northeastern University)Yun Fu (Northeastern University),"2108071053,2106967882,2618462548,2123131494","Image clustering has been a critical preprocessing step for vision tasks, e.g., visual concept discovery, content-based image retrieval. Conventional image clustering methods use handcraft visual descriptors as basic features via K-means, or build the graph within spectral clustering. Recently, representation learning with deep structure shows appealing performance in unsupervised feature pre-treatment. However, few studies have discussed how to deploy deep representation learning to image clustering problems, especially the unified framework which integrates both representation learning and ensemble clustering for efficient image clustering still remains void. In addition, even though it is widely recognized that with the increasing number of basic partitions, ensemble clustering gets better performance and lower variances, the best number of basic partitions for a given data set is a pending problem. In light of this, we propose the Infinite Ensemble Clustering (IEC), which incorporates the power of deep representation and ensemble clustering in a one-step framework to fuse infinite basic partitions. Generally speaking, a set of basic partitions is firstly generated from the image data, then by converting the basic partitions to the 1-of- K codings, we link the marginalized auto-encoder to the infinite ensemble clustering with i.i.d. basic partitions, which can be approached by the closed-form solutions, finally we follow the layer-wise training procedure and feed the concatenated deep features to K-means for final clustering. Extensive experiments on diverse vision data sets with different levels of visual descriptors demonstrate both the time efficiency and superior performance of IEC compared to the state-of-the-art ensemble clustering and deep clustering methods.",2016,Knowledge Discovery and Data Mining,Fields of study: flame clusteringk medians clusteringbrown clusteringcanopy clustering algorithmdbscancorrelation clusteringconstrained clusteringdata stream clusteringcure data clustering algorithmfuzzy clusteringk means clusteringclustering high dimensional datacluster analysisconsensus clusteringbiclusteringconceptual clusteringensemble learningdata miningpattern recognitionmachine learningstatisticscomputer sciencemathematics
Ranking Causal Anomalies via Temporal and Dynamical Analysis on Vanishing Correlations,"Wei Cheng (NEC)Kai Zhang (NEC)Haifeng Chen (NEC)Guofei Jiang (NEC)Zhengzhang Chen (NEC)Wei Wang (University of California, Los Angeles)","2620045292,2600447970,2571838709,2168090285,2132666618,2315689540","Modern world has witnessed a dramatic increase in our ability to collect, transmit and distribute real-time monitoring and surveillance data from large-scale information systems and cyber-physical systems. Detecting system anomalies thus attracts significant amount of interest in many fields such as security, fault management, and industrial optimization. Recently, invariant network has shown to be a powerful way in characterizing complex system behaviours. In the invariant network, a node represents a system component and an edge indicates a stable, significant interaction between two components. Structures and evolutions of the invariance network, in particular the vanishing correlations, can shed important light on locating causal anomalies and performing diagnosis. However, existing approaches to detect causal anomalies with the invariant network often use the percentage of vanishing correlations to rank possible casual components, which have several limitations: 1) fault propagation in the network is ignored; 2) the root casual anomalies may not always be the nodes with a high-percentage of vanishing correlations; 3) temporal patterns of vanishing correlations are not exploited for robust detection. To address these limitations, in this paper we propose a network diffusion based framework to identify significant causal anomalies and rank them. Our approach can effectively model fault propagation over the entire invariant network, and can perform joint inference on both the structural, and the time-evolving broken invariance patterns. As a result, it can locate high-confidence anomalies that are truly responsible for the vanishing correlations, and can compensate for unstructured measurement noise in the system. Extensive experiments on synthetic datasets, bank information system datasets, and coal plant cyber-physical system datasets demonstrate the effectiveness of our approach.",2016,Knowledge Discovery and Data Mining,Fields of study: non negative matrix factorizationdata miningmachine learningstatisticscomputer sciencemathematics
Deep Crossing: Web-Scale Modeling without Manually Crafted Combinatorial Features,Ying Shan (Microsoft)T. Ryan Hoens (Microsoft)Jian Jiao (Microsoft)Haijing Wang (Microsoft)Dong Yu (Microsoft)Jc Mao (Microsoft),"2634058586,2525304567,2514994191,2517320129,2685486628,2508991298","Manually crafted combinatorial features have been the ""secret sauce"" behind many successful models. For web-scale applications, however, the variety and volume of features make these manually crafted features expensive to create, maintain, and deploy. This paper proposes the Deep Crossing model which is a deep neural network that automatically combines features to produce superior models. The input of Deep Crossing is a set of individual features that can be either dense or sparse. The important crossing features are discovered implicitly by the networks, which are comprised of an embedding and stacking layer, as well as a cascade of Residual Units. Deep Crossing is implemented with a modeling tool called the Computational Network Tool Kit (CNTK), powered by a multi-GPU platform. It was able to build, from scratch, two web-scale models for a major paid search engine, and achieve superior results with only a sub-set of the features used in the production models. This demonstrates the potential of using Deep Crossing as a general modeling paradigm to improve existing products, as well as to speed up the development of new models with a fraction of the investment in feature engineering and acquisition of deep domain knowledge.",2016,Knowledge Discovery and Data Mining,Fields of study: deep learningdata miningmachine learningsimulationcomputer science
Accelerated Stochastic Block Coordinate Descent with Optimal Sampling,Aston Zhang (University of Illinois at Urbana–Champaign)Quanquan Gu (University of Virginia),"2102121030,2167348148","We study the composite minimization problem where the objective function is the sum of two convex functions: one is the sum of a finite number of strongly convex and smooth functions, and the other is a general convex function that is non-differentiable. Specifically, we consider the case where the non-differentiable function is block separable and admits a simple proximal mapping for each block. This type of composite optimization is common in many data mining and machine learning problems, and can be solved by block coordinate descent algorithms. We propose an accelerated stochastic block coordinate descent (ASBCD) algorithm, which incorporates the incrementally averaged partial derivative into the stochastic partial derivative and exploits optimal sampling. We prove that ASBCD attains a linear rate of convergence. In contrast to uniform sampling, we reveal that the optimal non-uniform sampling can be employed to achieve a lower iteration complexity. Experimental results on different large-scale real data sets support our theory.",2016,Knowledge Discovery and Data Mining,Fields of study: random coordinate descentcoordinate descentstochastic gradient descentsamplingdiscrete mathematicscombinatoricsmathematical optimizationstatisticscomputer sciencemathematics
Goal-Directed Inductive Matrix Completion,"Si Si (University of Texas at Austin)Kai-Yang Chiang (University of Texas at Austin)Cho-Jui Hsieh (University of California, Davis)Nikhil Rao (Technicolor)Inderjit S. Dhillon (University of Texas at Austin)","2099379656,2129170864,2148022289,2128988510,2033403132","Matrix completion (MC) with additional information has found wide applicability in several machine learning applications. Among algorithms for solving such problems, Inductive Matrix Completion(IMC) has drawn a considerable amount of attention, not only for its well established theoretical guarantees but also for its superior performance in various real-world applications. However, IMC based methods usually place very strong constraints on the quality of the features(side information) to ensure accurate recovery, which might not be met in practice. In this paper, we propose Goal-directed Inductive Matrix Completion(GIMC) to learn a nonlinear mapping of the features so that they satisfy the required properties. A key distinction between GIMC and IMC is that the feature mapping is learnt in a supervised manner, deviating from the traditional approach of unsupervised feature learning followed by model training. We establish the superiority of our method on several popular machine learning applications including multi-label learning, multi-class classification, and semi-supervised clustering.",2016,Knowledge Discovery and Data Mining,Fields of study: data miningartificial intelligencemachine learningalgorithmcomputer sciencemathematics
Robust Influence Maximization,Xinran He (University of Southern California)David Kempe (University of Southern California),"2096901250,2137221145","Uncertainty about models and data is ubiquitous in the computational social sciences, and it creates a need for robust social network algorithms, which can simultaneously provide guarantees across a spectrum of models and parameter settings. We begin an investigation into this broad domain by studying robust algorithms for the Influence Maximization problem, in which the goal is to identify a set of k nodes in a social network whose joint influence on the network is maximized. We define a Robust Influence Maximization framework wherein an algorithm is presented with a set of influence functions, typically derived from different influence models or different parameter settings for the same model. The different parameter settings could be derived from observed cascades on different topics, under different conditions, or at different times. The algorithm's goal is to identify a set of k nodes who are simultaneously influential for all influence functions, compared to the (function-specific) optimum solutions. We show strong approximation hardness results for this problem unless the algorithm gets to select at least a logarithmic factor more seeds than the optimum solution. However, when enough extra seeds may be selected, we show that techniques of Krause et al. can be used to approximate the optimum robust influence to within a factor of 1-1/e. We evaluate this bicriteria approximation algorithm against natural heuristics on several real-world data sets. Our experiments indicate that the worst-case hardness does not necessarily translate into bad performance on real-world data sets; all algorithms perform fairly well.",2016,Knowledge Discovery and Data Mining,Fields of study: noiseuncertaintyrobust optimizationeconometricsmachine learningmathematical optimizationstatisticsmathematics
Hierarchical Incomplete Multi-source Feature Learning for Spatiotemporal Event Forecasting,"Liang Zhao (Virginia Tech)Jieping Ye (University of Michigan)Feng Chen (University at Albany, SUNY)Chang Tien Lu (Virginia Tech)Naren Ramakrishnan (Virginia Tech)","2619584304,2305258894,2601749234,2112878203,2199255697","Forecasting significant societal events is an interesting and challenging problem as it taking into consideration multiple aspects of a society, including its economics, politics, and culture. Traditional forecasting methods based on a single data source find it hard to cover all these aspects comprehensively, thus limiting model performance. Multi source event forecasting has proven promising but still suffers from several challenges, including 1) geographical hierarchies in multi-source data features, 2) missing values, and 3) characterization of structured feature sparsity. This paper proposes a novel feature learning model that concurrently addresses all the above challenges. Specifically, given multi-source data from different geographical levels, we design a new forecasting model by characterizing the lower-level features' dependence on higher-level features. To handle the correlations amidst structured feature sets and deal with missing values among the coupled features, we propose a novel feature learning model based on an $N$th-order strong hierarchy and fused-overlapping group Lasso. An efficient algorithm is developed to optimize model parameters and ensure global optima. Extensive experiments on 10 datasets in different domains demonstrate the effectiveness and efficiency of the proposed model.",2016,Knowledge Discovery and Data Mining,Fields of study: feature selectiondata sciencedata miningmachine learningstatisticscomputer science
Causal Clustering for 1-Factor Measurement Models,Erich Kummerfeld (University of Pittsburgh)Joseph Ramsey (Carnegie Mellon University),"2512442103,2113768324","Many scientific research programs aim to learn the causal structure of real world phenomena. This learning problem is made more difficult when the target of study cannot be directly observed. One strategy commonly used by social scientists is to create measurable ``indicator'' variables that covary with the latent variables of interest. Before leveraging the indicator variables to learn about the latent variables, however, one needs a measurement model of the causal relations between the indicators and their corresponding latents. These measurement models are a special class of Bayesian networks. This paper addresses the problem of reliably inferring measurement models from measured indicators, without prior knowledge of the causal relations or the number of latent variables. We present a provably correct novel algorithm, FindOneFactorClusters (FOFC), for solving this inference problem. Compared to other state of the art algorithms, FOFC is faster, scales to larger sets of indicators, and is more reliable at small sample sizes. We also present the first correctness proofs for this problem that do not assume linearity or acyclicity among the latent variables.",2016,Knowledge Discovery and Data Mining,Fields of study: latent class modellatent variablefactor analysiseconometricsdata miningmachine learningstatisticscomputer sciencemathematics
A Truth Discovery Approach with Theoretical Guarantee,Houping Xiao (State University of New York System)Jing Gao (State University of New York System)Zhaoran Wang (Princeton University)Shiyu Wang (University of Illinois at Urbana–Champaign)Lu Su (State University of New York System)Han Liu (Princeton University),"2303863390,2096731881,2115173548,2659842982,2148733542,2614350313","In the information age, people can easily collect information about the same set of entities from multiple sources, among which conflicts are inevitable. This leads to an important task, truth discovery , i.e., to identify true facts (truths) via iteratively updating truths and source reliability. However, the convergence to the truths is never discussed in existing work, and thus there is no theoretical guarantee in the results of these truth discovery approaches. In contrast, in this paper we propose a truth discovery approach with theoretical guarantee. We propose a randomized gaussian mixture model (RGMM) to represent multi-source data, where truths are model parameters. We incorporate source bias which captures its reliability degree into RGMM formulation. The truth discovery task is then modeled as seeking the maximum likelihood estimate (MLE) of the truths. Based on expectation-maximization (EM) techniques, we propose population-based (i.e., on the limit of infinite data) and sample-based (i.e., on a finite set of samples) solutions for the MLE. Theoretically, we prove that both solutions are contractive to an e-ball around the MLE, under certain conditions. Experimentally, we evaluate our method on both simulated and real-world datasets. Experimental results show that our method achieves high accuracy in identifying truths with convergence guarantee.",2016,Knowledge Discovery and Data Mining,Fields of study: mixture modeleconometricsdata miningmachine learningstatisticscomputer sciencemathematics
A Subsequence Interleaving Model for Sequential Pattern Mining,Jaroslav M. Fowkes (University of Edinburgh)Charles A. Sutton (University of Edinburgh),"2362208703,2113665458","Recent sequential pattern mining methods have used the minimum description length (MDL) principle to define an encoding scheme which describes an algorithm for mining the most compressing patterns in a database. We present a novel subsequence interleaving model based on a probabilistic model of the sequence database, which allows us to search for the most compressing set of patterns without designing a specific encoding scheme. Our proposed algorithm is able to efficiently mine the most relevant sequential patterns and rank them using an associated measure of interestingness. The efficient inference in our model is a direct result of our use of a structural expectation-maximization framework, in which the expectation-step takes the form of a submodular optimization problem subject to a coverage constraint. We show on both synthetic and real world datasets that our model mines a set of sequential patterns with low spuriousness and redundancy, high interpretability and usefulness in real-world applications. Furthermore, we demonstrate that the quality of the patterns from our approach is comparable to, if not better than, existing state of the art sequential pattern mining algorithms.",2016,Knowledge Discovery and Data Mining,Fields of study: generative modelsequential pattern miningexploratory data analysisdata miningpattern recognitionmachine learningstatisticscomputer sciencemathematics
EMBERS AutoGSR: Automated Coding of Civil Unrest Events,Parang Saraf (Virginia Tech)Naren Ramakrishnan (Virginia Tech),"2160730744,2199255697","We describe the EMBERS AutoGSR system that conducts automated coding of civil unrest events from news articles published in multiple languages. The nuts and bolts of the AutoGSR system constitute an ecosystem of filtering, ranking, and recommendation models to determine if an article reports a civil unrest event and, if so, proceed to identify and encode specific characteristics of the civil unrest event such as the when, where, who, and why of the protest. AutoGSR is a deployed system for the past 6 months continually processing data 24x7 in languages such as Spanish, Portuguese, English and encoding civil unrest events in 10 countries of Latin America: Argentina, Brazil, Chile, Colombia, Ecuador, El Salvador, Mexico, Paraguay, Uruguay, and Venezuela. We demonstrate the superiority of AutoGSR over both manual approaches and other state-of-the-art encoding systems for civil unrest.",2016,Knowledge Discovery and Data Mining,Fields of study: text miningcomputer science
Large-Scale Item Categorization in e-Commerce Using Multiple Recurrent Neural Networks,Jung-Woo HaHyuna PyoJeonghee Kim,"2633226502,2417879881,2629591549","Precise item categorization is a key issue in e-commerce domains. However, it still remains a challenging problem due to data size, category skewness, and noisy metadata. Here, we demonstrate a successful report on a deep learning-based item categorization method, i.e., deep categorization network (DeepCN), in an e-commerce website. DeepCN is an end-to-end model using multiple recurrent neural networks (RNNs) dedicated to metadata attributes for generating features from text metadata and fully connected layers for classifying item categories from the generated features. The categorization errors are propagated back through the fully connected layers to the RNNs for weight update in the learning process. This deep learning-based approach allows diverse attributes to be integrated into a common representation, thus overcoming sparsity and scalability problems. We evaluate DeepCN on large-scale real-world data including more than 94 million items with approximately 4,100 leaf categories from a Korean e-commerce website. Experiment results show our method improves the categorization accuracy compared to the model using single RNN as well as a standard classification model using unigram-based bag-of-words. Furthermore, we investigate how much the model parameters and the used attributes influence categorization performances.",2016,Knowledge Discovery and Data Mining,Fields of study: boosting methods for object categorizationrecurrent neural networkcategorizationdeep learningdata miningpattern recognitionmachine learningcomputer science
Days on Market: Measuring Liquidity in Real Estate Markets,Hengshu Zhu (Baidu)Hui Xiong (Rutgers–Newark)Fangshuang Tang (University of Science and Technology of China)Qi Liu (University of Science and Technology of China)Yong Ge (University of Arizona)Enhong Chen (University of Science and Technology of China)Yanjie Fu (Missouri University of Science and Technology),"2098414524,2153710278,2223294387,2420624292,2695934969,2136372366,2168873515","Days on Market (DOM) refers to the number of days a property is on the active market, which is an important measurement of market liquidity in real estate industry. Indeed, at the micro level, DOM is not only a special concern of house sellers, but also a useful indicator for potential buyers to evaluate the popularity of a house. At the macro level, DOM is an important indicator of real estate market status. However, it is very challenging to measure DOM, since there are a variety of factors which can impact on the DOM of a property. To this end, in this paper, we aim to measure real estate liquidity by examining multiple factors in a holistic manner. A special goal is to predict the DOM of a given property listing. Specifically, we first extract key features from multiple types of heterogeneous real estate-related data, such as house profiles and geo-social information of residential communities. Then, based on these features, we develop a multi-task learning based regression approach for predicting the DOM of real estates. This approach can effectively learn district-aware models for different property listings by considering multiple factors. Finally, we conduct extensive experiments on real-world real estate data collected in Beijing and develop a prototype system for practical use. The experimental results clearly validate the effectiveness of the proposed approach for measuring liquidity in real estate markets.",2016,Knowledge Discovery and Data Mining,Fields of study: cost approachprice on applicationmulti task learningreal estatemachine learningcomputer science
Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,Balaji Krishnapuram (IBM)Mohak Shah (Bosch)Alex Smola (Amazon.com)Charu Aggarwal (IBM)Dou Shen (Baidu)Rajeev Rastogi (Amazon.com),"2582962261,2277080105,2593672690,2146335907,2136428695,2163156181","It is our great pleasure to welcome you to the 2016 ACM Conference on Knowledge Discovery and Data Mining -- KDD'16. We hope that the content and the professional network at KDD'16 will help you succeed professionally by enabling you to: identify technology trends early; make new/creative contributions; increase your productivity by using newer/better tools, processes or ways of organizing teams; identify new job opportunities; and hire new team members. We are living in an exciting time for our profession. On the one hand, we are witnessing the industrialization of data science, and the emergence of the industrial assembly line processes characterized by the division of labor, integrated processes/pipelines of work, standards, automation, and repeatability. Data science practitioners are organizing themselves in more sophisticated ways, embedding themselves in larger teams in many industry verticals, improving their productivity substantially, and achieving a much larger scale of social impact. On the other hand we are also witnessing astonishing progress from research in algorithms and systems -- for example the field of deep neural networks has revolutionized speech recognition, NLP, computer vision, image recognition, etc. By facilitating interaction between practitioners at large companies & startups on the one hand, and the algorithm development researchers including leading academics on the other, KDD'16 fosters technological and entrepreneurial innovation in the area of data science. This year's conference continues its tradition of being the premier forum for presentation of results in the field of data mining, both in the form of cutting edge research, and in the form of insights from the development and deployment of real world applications. Further, the conference continues with its tradition of a strong tutorial and workshop program on leading edge issues of data mining. The mission of this conference has broadened in recent years even as we placed a significant amount of focus on both the research and applied aspects of data mining. As an example of this broadened focus, this year we have introduced a strong hands-on tutorial program nduring the conference in which participants will learn how to use practical tools for data mining. KDD'16 also gives researchers and practitioners a unique opportunity to form professional networks, and to share their perspectives with others interested in the various aspects of data mining. For example, we have introduced office hours for budding entrepreneurs from our community to meet leading Venture Capitalists investing in this area. We hope that KDD 2016 conference will serve as a meeting ground for researchers, practitioners, funding agencies, and investors to help create new algorithms and commercial products. The call for papers attracted a significant number of submissions from countries all over the world. In particular, the research track attracted 784 submissions and the applied data science track attracted 331 submissions. Papers were accepted either as full papers or as posters. The overall acceptance rate either as full papers or posters was less than 20%. For full papers in the research track, the acceptance rate was lower than 10%. This is consistent with the fact that the KDD Conference is a premier conference in data mining and the acceptance rates historically tend to be low. It is noteworthy that the applied data science track received a larger number of submissions compared to previous years. We view this as an encouraging sign that research in data mining is increasingly becoming relevant to industrial applications. All papers were reviewed by at least three program committee members and then discussed by the PC members in a discussion moderated by a meta-reviewer. Borderline papers were thoroughly reviewed by the program chairs before final decisions were made.",2016,Knowledge Discovery and Data Mining,Fields of study: operations researchdata miningcomputer science
Anomaly Detection Using Program Control Flow Graph Mining From Execution Logs,Animesh Nandi (IBM)Atri Mandal (IBM)Shubham Atreja (Indian Institute of Technology Kanpur)Gargi Banerjee Dasgupta (IBM)Subhrajit Bhattacharya (IBM),"2232379423,2558632365,2557992055,2720295200,2195416166","We focus on the problem of detecting anomalous run-time behavior of distributed applications from their execution logs. Specifically we mine templates and template sequences from logs to form a control flow graph (cfg) spanning distributed components. This cfg represents the baseline healthy system state and is used to flag deviations from the expected behavior of runtime logs. The novelty in our work stems from the new techniques employed to: (1) overcome the instrumentation requirements or application specific assumptions made in prior log mining approaches, (2) improve the accuracy of mined templates and the cfg in the presence of long parameters and high amount of interleaving respectively, and (3) improve by orders of magnitude the scalability of the cfg mining process in terms of volume of log data that can be processed per day. We evaluate our approach using (a) synthetic log traces and (b) multiple real-world log datasets collected at different layers of application stack. Results demonstrate that our template mining, cfg mining, and anomaly detection algorithms have high accuracy. The distributed implementation of our pipeline is highly scalable and has more than 500 GB/day of log data processing capability even on a 10 low-end VM based (Spark + Hadoop) cluster. We also demonstrate the efficacy of our end-to-end system using a case study with the Openstack VM provisioning system.",2016,Knowledge Discovery and Data Mining,Fields of study: sparktemplatedata miningdatabasereal time computingcomputer science
Revisiting Random Binning Features: Fast Convergence and Strong Parallelizability,Lingfei Wu (College of William & Mary)Ian En-Hsu Yen (University of Texas at Austin)Jie Chen (IBM)Rui Yan (Baidu),"2701551955,2162095421,2718189575,2109109241","Kernel method has been developed as one of the standard approaches for nonlinear learning, which however, does not scale to large data set due to its quadratic complexity in the number of samples. A number of kernel approximation methods have thus been proposed in the recent years, among which the random features method gains much popularity due to its simplicity and direct reduction of nonlinear problem to a linear one. Different random feature functions have since been proposed to approximate a variety of kernel functions. Among them the Random Binning (RB) feature, proposed in the first random-feature paper [21], has drawn much less attention than the Random Fourier (RF) feature proposed also in [21]. In this work, we observe that the RB features, with right choice of optimization solver, could be orders-of-magnitude more efficient than other random features and kernel approximation methods under the same requirement of accuracy. We thus propose the first analysis of RB from the perspective of optimization, which by interpreting RB as a Randomized Block Coordinate Descent in the infinite-dimensional space, gives a faster convergence rate compared to that of other random features. In particular, we show that by drawing R random grids with at least κ number of non-empty bins per grid in expectation, RB method achieves a convergence rate of O (1/κ R )), which not only sharpens its O (1/√ R ) rate from Monte Carlo analysis, but also shows a κ times speedup over other random features under the same analysis framework. In addition, we demonstrate another advantage of RB in the L1-regularized setting, where unlike other random features, a RB-based Coordinate Descent solver can be parallelized with guaranteed speedup proportional to κ. Our extensive experiments demonstrate the superior performance of the RB features over other random features and kernel approximation methods.",2016,Knowledge Discovery and Data Mining,Fields of study: random functionmachine learningmathematical optimizationstatisticsmathematics
Taxi Driving Behavior Analysis in Latent Vehicle-to-Vehicle Networks: A Social Influence Perspective,Tong Xu (University of Science and Technology of China)Hengshu Zhu (Baidu)Xiangyu Zhao (University of Science and Technology of China)Qi Liu (University of Science and Technology of China)Hao Zhong (Rutgers University)Enhong Chen (University of Science and Technology of China)Hui Xiong (Rutgers University),"2291800725,2098414524,2498438471,2420624292,2523559495,2136372366,2153710278","With recent advances in mobile and sensor technologies, a large amount of efforts have been made on developing intelligent applications for taxi drivers, which provide beneficial guide and opportunity to improve the profit and work efficiency. However, limited scopes focus on the latent social interaction within cab drivers, and corresponding social propagation scheme to share driving behaviors has been largely ignored. To that end, in this paper, we propose a comprehensive study to reveal how the social propagation affects for better prediction of cab drivers' future behaviors. To be specific, we first investigate the correlation between drivers' skills and their mutual interactions in the latent vehicle-to-vehicle network, which intuitively indicates the effects of social influences. Along this line, by leveraging the classic social influence theory , we develop a two-stage framework for quantitatively revealing the latent driving pattern propagation within taxi drivers. Comprehensive experiments on a real-word data set collected from the New York City clearly validate the effectiveness of our proposed framework on predicting future taxi driving behaviors, which also support the hypothesis that social factors indeed improve the predictability of driving behaviors.",2016,Knowledge Discovery and Data Mining,Fields of study: social influencesimulation
Continuous Experience-aware Language Model,Subhabrata Mukherjee (Max Planck Society)Stephan Günnemann (Technische Universität München)Gerhard Weikum (Max Planck Society),"2301124665,316694267,514836396","Online review communities are dynamic as users join and leave, adopt new vocabulary, and adapt to evolving trends. Recent work has shown that recommender systems benefit from explicit consideration of user experience. However, prior work assumes a fixed number of discrete experience levels, whereas in reality users gain experience and mature continuously over time. This paper presents a new model that captures the continuous evolution of user experience, and the resulting language model in reviews and other posts. Our model is unsupervised and combines principles of Geometric Brownian Motion, Brownian Motion, and Latent Dirichlet Allocation to trace a smooth temporal progression of user experience and language model respectively. We develop practical algorithms for estimating the model parameters from data and for inference with our model (e.g., to recommend items). Extensive experiments with five real-world datasets show that our model not only fits data better than discrete-model baselines, but also outperforms state-of-the-art methods for predicting item ratings.",2016,Knowledge Discovery and Data Mining,Fields of study: topic modelbrownian motionuser experience designdata sciencedata miningmachine learningsimulationstatisticscomputer science
Topic Modeling of Short Texts: A Pseudo-Document View,Yuan Zuo (Beihang University)Junjie Wu (Beihang University)Hui Zhang (Beihang University)Hao Lin (Beihang University)Fei Wang (Beihang University)Ke Xu (Beihang University)Hui Xiong (Rutgers University),"2271787629,2149366604,2634170829,2293401935,2618217994,2600070012,2153710278","Recent years have witnessed the unprecedented growth of online social media, which empower short texts as the prevalent format for information of Internet. Given the nature of sparsity, however, short text topic modeling remains a critical yet much-watched challenge in both academy and industry. Rich research efforts have been put on building different types of probabilistic topic models for short texts, among which the self aggregation methods without using auxiliary information become an emerging solution for providing informative cross-text word co-occurrences. However, models along this line are still rarely seen, and the representative one Self-Aggregation Topic Model (SATM) is prone to overfitting and computationally expensive. In light of this, in this paper, we propose a novel probabilistic model called Pseudo-document-based Topic Model (PTM) for short text topic modeling. PTM introduces the concept of pseudo document to implicitly aggregate short texts against data sparsity. By modeling the topic distributions of latent pseudo documents rather than short texts, PTM is expected to gain excellent performance in both accuracy and efficiency. A Sparsity-enhanced PTM (SPTM for short) is also proposed by applying Spike and Slab prior, with the purpose of eliminating undesired correlations between pseudo documents and latent topics. Extensive experiments on various real-world data sets with state-of-the-art baselines demonstrate the high quality of topics learned by PTM and its robustness with reduced training samples. It is also interesting to show that i ) SPTM gains a clear edge over PTM when the number of pseudo documents is relatively small, and ii ) the constraint that a short text belongs to only one pseudo document is critically important for the success of PTM. We finally take an in-depth semantic analysis to unveil directly the fabulous function of pseudo documents in finding cross-text word co-occurrences for topic modeling.",2016,Knowledge Discovery and Data Mining,Fields of study: topic modellatent dirichlet allocationdata sciencespeech recognitiondata miningmachine learningcomputer science
The Million Domain Challenge: Broadcast Email Prioritization by Cross-domain Recommendation,Beidou Wang (Zhejiang University)Martin Ester (Simon Fraser University)Yikang Liao (Zhejiang University)Jiajun Bu (Zhejiang University)Yu Zhu (Zhejiang University)Ziyu Guan (Northwest University)Deng Cai (Zhejiang University),"2147723110,2067196623,2507901320,2169876372,2689214791,2163043385,2141500565","With email overload becoming a billion-level drag on the economy, personalized email prioritization is of urgent need to help predict the importance level of an email. Despite lots of previous effort on the topic, broadcast email, an important type of emails with its unique challenges and intriguing opportunities, has been overlooked. The most salient opportunity lies in that effective collaborative filtering can be exploited due to thousands of receivers of a typical broadcast email. However, every broadcast email is completely cold and it is very costly to obtain users' preference feedback. Fortunately, there exist up to million-level broadcast mailing lists in a real life email system. Similar mailing lists can provide useful extra information for broadcast email prioritization in a target mailing list. How to mine such useful extra information is a challenging problem that has never been touched. In this work, we propose the first broadcast email prioritization framework considering large numbers of mailing lists by formulating this problem as a cross domain recommendation problem. An optimization framework is proposed to select the optimal set of source domains considering multiple criteria including overlap of users, feedback pattern similarity and coverage of users. Our method is thoroughly evaluated on a real world industrial dataset from Samsung Electronics and is proved highly effective and outperforms all the baselines.",2016,Knowledge Discovery and Data Mining,Fields of study: html emailemail address harvestingcollaborative filteringinternet privacyworld wide webdata miningmachine learningcomputer science
Dynamic Clustering of Streaming Short Documents,Shangsong Liang (University College London)Emine Yilmaz (University College London)Evangelos Kanoulas (University of Amsterdam),"2131113258,2342836604,2627690640","Clustering technology has found numerous applications in mining textual data. It was shown to enhance the performance of retrieval systems in various different ways, such as identifying different query aspects in search result diversification, improving smoothing in the context of language modeling, matching queries with documents in a latent topic space in ad-hoc retrieval, summarizing documents etc. The vast majority of clustering methods have been developed under the assumption of a static corpus of long (and hence textually rich) documents. Little attention has been given to streaming corpora of short text, which is the predominant type of data in Web 2.0 applications, such as social media, forums, and blogs. In this paper, we consider the problem of dynamically clustering a streaming corpus of short documents. The short length of documents makes the inference of the latent topic distribution challenging, while the temporal dynamics of streams allow topic distributions to change over time. To tackle these two challenges we propose a new dynamic clustering topic model - DCT - that enables tracking the time-varying distributions of topics over documents and words over topics. DCT models temporal dynamics by a short-term or long-term dependency model over sequential data, and overcomes the difficulty of handling short text by assigning a single topic to each short document and using the distributions inferred at a certain point in time as priors for the next inference, allowing the aggregation of information. At the same time, taking a Bayesian approach allows evidence obtained from new streaming documents to change the topic distribution. Our experimental results demonstrate that the proposed clustering algorithm outperforms state-of-the-art dynamic and non-dynamic clustering topic models in terms of perplexity and when integrated in a cluster-based query likelihood model it also outperforms state-of-the-art models in terms of retrieval quality.",2016,Knowledge Discovery and Data Mining,Fields of study: dynamic topic modeldata stream clusteringtopic modelclustering high dimensional datacluster analysisdocument clusteringdata scienceinformation retrievaldata miningmachine learningcomputer science
User Identity Linkage by Latent User Space Modelling,Xin Mu (Nanjing University)Feida Zhu (Singapore Management University)Ee-Peng Lim (Singapore Management University)Jing XiaoJianzong WangZhi-Hua Zhou (Nanjing University),"2512568377,2160602068,2130308643,2514138882,2600564016,2286237009","User identity linkage across social platforms is an important problem of great research challenge and practical value. In real applications, the task often assumes an extra degree of difficulty by requiring linkage across multiple platforms. While pair-wise user linkage between two platforms, which has been the focus of most existing solutions, provides reasonably convincing linkage, the result depends by nature on the order of platform pairs in execution with no theoretical guarantee on its stability. In this paper, we explore a new concept of ``Latent User Space'' to more naturally model the relationship between the underlying real users and their observed projections onto the varied social platforms, such that the more similar the real users, the closer their profiles in the latent user space. We propose two effective algorithms, a batch model(ULink) and an online model(ULink-On), based on latent user space modelling. Two simple yet effective optimization methods are used for optimizing objective function: the first one based on the constrained concave-convex procedure(CCCP) and the second on accelerated proximal gradient. To our best knowledge, this is the first work to propose a unified framework to address the following two important aspects of the multi-platform user identity linkage problem --- (I) the platform multiplicity and (II) online data generation. We present experimental evaluations on real-world data sets for not only traditional pairwise-platform linkage but also multi-platform linkage. The results demonstrate the superiority of our proposed method over the state-of-the-art ones.",2016,Knowledge Discovery and Data Mining,Fields of study: user modelingsocial networkworld wide webdata miningmachine learningsimulationcomputer science
Communication Efficient Distributed Kernel Principal Component Analysis,Maria-Florina Balcan (Carnegie Mellon University)Yingyu Liang (Princeton University)Le Song (Georgia Institute of Technology)David P. Woodruff (IBM)Bo Xie 0002 (Georgia Institute of Technology),"2169342471,2124668569,2113868374,2142501412,2102785714","Kernel Principal Component Analysis (KPCA) is a key machine learning algorithm for extracting nonlinear features from data. In the presence of a large volume of high dimensional data collected in a distributed fashion, it becomes very costly to communicate all of this data to a single data center and then perform kernel PCA. Can we perform kernel PCA on the entire dataset in a distributed and communication efficient fashion while maintaining provable and strong guarantees in solution quality? In this paper, we give an affirmative answer to the question by developing a communication efficient algorithm to perform kernel PCA in the distributed setting. The algorithm is a clever combination of subspace embedding and adaptive sampling techniques, and we show that the algorithm can take as input an arbitrary configuration of distributed datasets, and compute a set of global kernel principal components with relative error guarantees independent of the dimension of the feature space or the total number of data points. In particular, computing k principal components with relative error e over s workers has communication cost O( spk /e+ sk 2 /e 3 ) words, where p is the average number of nonzero entries in each data point. Furthermore, we experimented the algorithm with large-scale real world datasets and showed that the algorithm produces a high quality kernel PCA solution while using significantly less communication than alternative approaches.",2016,Knowledge Discovery and Data Mining,Fields of study: kernel embedding of distributionstree kernelvariable kernel density estimationpolynomial kernelstring kernelradial basis function kernelkernel principal component analysisprincipal component regressionkernel methodprincipal component analysisdata miningpattern recognitionmachine learningstatisticscomputer sciencemathematics
Catch Me If You Can: Detecting Pickpocket Suspects from Large-Scale Transit Records,Bowen Du (Beihang University)Chuanren Liu (Drexel University)Wenjun Zhou (University of Tennessee)Zhenshan Hou (Beihang University)Hui Xiong (Rutgers–Newark),"2697129405,2169554947,2097769770,2668497643,2153710278","Massive data collected by automated fare collection (AFC) systems provide opportunities for studying both personal traveling behaviors and collective mobility patterns in the urban area. Existing studies on the AFC data have primarily focused on identifying passengers' movement patterns. In this paper, however, we creatively leveraged such data for identifying thieves in the public transit systems. Indeed, stopping pickpockets in the public transit systems has been critical for improving passenger satisfaction and public safety. However, it is challenging to tell thieves from regular passengers in practice. To this end, we developed a suspect detection and surveillance system, which can identify pick-pocket suspects based on their daily transit records. Specifically, we first extracted a number of features from each passenger's daily activities in the transit systems. Then, we took a two-step approach that exploits the strengths of unsupervised outlier detection and supervised classification models to identify thieves, who exhibit abnormal traveling behaviors. Experimental results demonstrated the effective- ness of our method. We also developed a prototype system with a user-friendly interface for the security personnel.",2016,Knowledge Discovery and Data Mining,Fields of study: anomaly detectioncomputer securitymachine learningsimulationcomputer science
Improving Survey Aggregation with Sparsely Represented Signals,"Tianlin Shi (Stanford University)Forest Agostinelli (University of California, Irvine)Matthew Staib (Massachusetts Institute of Technology)David P. Wipf (Microsoft)Thomas Moscibroda (Microsoft)","2326692705,2592431085,2721338023,1985957493,139673579","In this paper, we develop a new aggregation technique to reduce the cost of surveying. Our method aims to jointly estimate a vector of target quantities such as public opinion or voter intent across time and maintain good estimates when using only a fraction of the data. Inspired by the James-Stein estimator, we resolve this challenge by shrinking the estimates to a global mean which is assumed to have a sparse representation in some known basis. This assumption has lead to two different methods for estimating the global mean: orthogonal matching pursuit and deep learning. Both of which significantly reduce the number of samples needed to achieve good estimates of the true means of the data and, in the case of presidential elections, can estimate the outcome of the 2012 United States elections while saving hundreds of thousands of samples and maintaining accuracy.",2016,Knowledge Discovery and Data Mining,Fields of study: multi task learningcompressed sensingjames stein estimatordeep learningeconometricsdata miningmachine learningstatisticscomputer science
Compressing Graphs and Indexes with Recursive Graph Bisection,Laxman Dhulipala (Carnegie Mellon University)Igor Kabiljo (Facebook)Brian Karrer (Facebook)Giuseppe Ottaviano (Facebook)Sergey Pupyrev (Facebook)Alon Shalita (Facebook),"2227580508,2292206520,2633680482,2153528897,2515314286,2345469155","Graph reordering is a powerful technique to increase the locality of the representations of graphs, which can be helpful in several applications. We study how the technique can be used to improve compression of graphs and inverted indexes. We extend the recent theoretical model of Chierichetti et al. (KDD 2009) for graph compression, and show how it can be employed for compression-friendly reordering of social networks and web graphs and for assigning document identifiers in inverted indexes. We design and implement a novel theoretically sound reordering algorithm that is based on recursive graph bisection. Our experiments show a significant improvement of the compression rate of graph and indexes over existing heuristics. The new method is relatively simple and allows efficient parallel and distributed implementations, which is demonstrated on graphs with billions of vertices and hundreds of billions of edges.",2016,Knowledge Discovery and Data Mining,Fields of study: implicit graphvoltage graphindifference graphcomplement graphgraph bandwidth1 planar graphgraph operationscomparability graphblock graphgraph productclique widthmodular decompositionpathwidthchordal graphgraphcompressionsocial networkdistributed algorithmapproximation algorithmtheoretical computer sciencecombinatoricsdata miningmachine learningmathematical optimizationalgorithmcomputer science
Accelerating Online CP Decompositions for Higher Order Tensors,"Shuo Zhou (University of Melbourne)Nguyen Xuan Vinh (University of Melbourne)James Bailey (University of Melbourne)Yunzhe Jia (University of Melbourne)Ian Davidson (University of California, Davis)","2478464571,1982975784,2131557737,2538254647,2560595684","Tensors are a natural representation for multidimensional data. In recent years, CANDECOMP/PARAFAC (CP) decomposition, one of the most popular tools for analyzing multi-way data, has been extensively studied and widely applied. However, today's datasets are often dynamically changing over time. Tracking the CP decomposition for such dynamic tensors is a crucial but challenging task, due to the large scale of the tensor and the velocity of new data arriving. Traditional techniques, such as Alternating Least Squares (ALS), cannot be directly applied to this problem because of their poor scalability in terms of time and memory. Additionally, existing online approaches have only partially addressed this problem and can only be deployed on third-order tensors. To fill this gap, we propose an efficient online algorithm that can incrementally track the CP decompositions of dynamic tensors with an arbitrary number of dimensions. In terms of effectiveness, our algorithm demonstrates comparable results with the most accurate algorithm, ALS, whilst being computationally much more efficient. Specifically, on small and moderate datasets, our approach is tens to hundreds of times faster than ALS, while for large-scale datasets, the speedup can be more than 3,000 times. Compared to other state-of-the-art online approaches, our method shows not only significantly better decomposition quality, but also better performance in terms of stability, efficiency and scalability.",2016,Knowledge Discovery and Data Mining,Fields of study: theoretical computer sciencemachine learningmathematical optimizationcomputer science
Analyzing Volleyball Match Data from the 2014 World Championships Using Machine Learning Techniques,Jan Van Haaren (Katholieke Universiteit Leuven)Horesh Ben Shitrit (École Polytechnique Fédérale de Lausanne)Jesse Davis (Katholieke Universiteit Leuven)Pascal Fua (École Polytechnique Fédérale de Lausanne),"2036917247,2146000585,2144802550,2461306359","This paper proposes a relational-learning based approach for discovering strategies in volleyball matches based on optical tracking data. In contrast to most existing methods, our approach permits discovering patterns that account for both spatial (that is, partial configurations of the players on the court) and temporal (that is, the order of events and positions) aspects of the game. We analyze both the men's and women's final match from the 2014 FIVB Volleyball World Championships, and are able to identify several interesting and relevant strategies from the matches.",2016,Knowledge Discovery and Data Mining,Fields of study: spatial analysismultimediasimulationstatistics
Optimally Discriminative Choice Sets in Discrete Choice Models: Application to Data-Driven Test Design,Igor Labutov (Cornell University)Frans Schalekamp (Cornell University)Kelvin Luu (University of Washington)Hod Lipson (Columbia University)Christoph Studer (Cornell University),"1028054495,1583385990,2395179958,2510944775,2153121925","Difficult multiple-choice (MC) questions can be made easy by providing a set of answer options of which most are obviously wrong. In the education literature, a plethora of instructional guides exist for crafting a suitable set of wrong choices (distractors) that enable the assessment of the students' understanding. The art of MC question design thus hinges on the question-maker's experience and knowledge of the potential misconceptions. In contrast, we advocate a data-driven approach, where correct and incorrect options are assembled directly from the students' own past submissions. Large-scale online classroom settings, such as massively open online courses (MOOCs), provide an opportunity to design optimal and adaptive multiple-choice questions that are maximally informative about the students' level of understanding of the material. In this work, we (i) develop a multinomial-logit discrete choice model for the setting of MC testing, (ii) derive an optimization objective for selecting optimally discriminative option sets, (iii) propose an algorithm for finding a globally-optimal solution, and (iv) demonstrate the effectiveness of our approach via synthetic experiments and a user study. We finally showcase an application of our approach to crowd-sourcing tests from technical online forums.",2016,Knowledge Discovery and Data Mining,Fields of study: crowdsourcingeducational assessmentadaptive learningmultimediadata miningmachine learningsimulationstatisticscomputer science
"Dynamics of Large Multi-View Social Networks: Synergy, Cannibalization and Cross-View Interplay",Yu Shi (University of Illinois at Urbana–Champaign)Myunghwan Kim (LinkedIn)Shaunak Chatterjee (LinkedIn)Mitul Tiwari (LinkedIn)Souvik Ghosh (LinkedIn)Rómer Rosales (LinkedIn),"2664581530,2140237911,2198941248,2296043577,2097018069,2117656073","Most social networking services support multiple types of relationships between users, such as getting connected, sending messages, and consuming feed updates. These users and relationships can be naturally represented as a dynamic multi-view network, which is a set of weighted graphs with shared common nodes but having their own respective edges. Different network views, representing structural relationship and interaction types, could have very distinctive properties individually and these properties may change due to interplay across views. Therefore, it is of interest to study how multiple views interact and affect network dynamics and, in addition, explore possible applications to social networking. In this paper, we propose approaches to capture and analyze multi-view network dynamics from various aspects. Through our proposed descriptors, we observe the synergy and cannibalization between different user groups and network views from LinkedIn dataset. We then develop models that consider the synergy and cannibalization per new relationship, and show the outperforming predictive capability of our models compared to baseline models. Finally, the proposed models allow us to understand the interplay among different views where they dynamically change over time.",2016,Knowledge Discovery and Data Mining,Fields of study: network dynamicssocial networkdata miningsimulation
The Legislative Influence Detector: Finding Text Reuse in State Legislation,"Matthew Burgess (University of Michigan)Eugenia Giraudy (University of California, Berkeley)Julian Katz-Samuels (University of Michigan)Joe Walsh (University of Chicago)Derek WillisLauren Haynes (University of Chicago)Rayid Ghani (University of Chicago)","2277759540,2505760661,2715831950,2680879499,2656276355,2281046338,2655769349","State legislatures introduce at least 45,000 bills each year. However, we lack a clear understanding of who is actually writing those bills. As legislators often lack the time and staff to draft each bill, they frequently copy text written by other states or interest groups. However, existing approaches to detect text reuse are slow, biased, and incomplete. Journalists or researchers who want to know where a particular bill originated must perform a largely manual search. Watchdog organizations even hire armies of volunteers to monitor legislation for matches. Given the time-consuming nature of the analysis, journalists and researchers tend to limit their analysis to a subset of topics (e.g. abortion or gun control) or a few interest groups. This paper presents the Legislative Influence Detector (LID). LID uses the Smith-Waterman local alignment algorithm to detect sequences of text that occur in model legislation and state bills. As it is computationally too expensive to run this algorithm on a large corpus of data, we use a search engine built using Elasticsearch to limit the number of comparisons. We show how system has found 45,405 instances of bill-to-bill text reuse and 14,137 instances of model-legislation-to-bill text reuse. System reduces the time it takes to manually find text reuse from days to seconds.",2016,Knowledge Discovery and Data Mining,Fields of study: common gooddata miningcomputer science
Identifying Police Officers at Risk of Adverse Events,Samuel Carton (University of Michigan)Jennifer Helsby (University of Chicago)Kenneth Joseph (Carnegie Mellon University)Ayesha Mahmud (Princeton University)Youngsoo Park (University of Arizona)Joe Walsh (University of Chicago)Crystal CodyCpt Estella PattersonLauren Haynes (University of Chicago)Rayid Ghani (University of Chicago),"2224892635,2052523325,2107547116,2481417253,2443439548,2680879499,2674174511,2511718359,2281046338,2655769349","Adverse events between police and the public, such as deadly shootings or instances of racial profiling, can cause serious or deadly harm, damage police legitimacy, and result in costly litigation. Evidence suggests these events can be prevented by targeting interventions based on an Early Intervention System (EIS) that flags police officers who are at a high risk for involvement in such adverse events. Today's EIS are not data-driven and typically rely on simple thresholds based entirely on expert intuition. In this paper, we describe our work with the Charlotte-Mecklenburg Police Department (CMPD) to develop a machine learning model to predict which officers are at risk for an adverse event. Our approach significantly outperforms CMPD's existing EIS, increasing true positives by ~12% and decreasing false positives by ~32%. Our work also sheds light on features related to officer characteristics, situational factors, and neighborhood factors that are predictive of adverse events. This work provides a starting point for police departments to take a comprehensive, data-driven approach to improve policing and reduce harm to both officers and members of the public.",2016,Knowledge Discovery and Data Mining,Fields of study: computer securitycomputer science
Lifelong Machine Learning and Computer Reading the Web,Zhiyuan Chen (Google)Estevam R. Hruschka (Federal University of São Carlos)Bing Liu (University of Illinois at Chicago),"2143860226,2137503162,2244698799","This tutorial introduces Lifelong Machine Learning (LML) and Machine Reading. The core idea of LML is to learn continuously and accumulate the learned knowledge, and to use the knowledge to help future learning, which is perhaps the hallmark of human learning and human intelligence. By us- ing prior knowledge seamlessly and effortlessly, we humans can learn without a lot of training data, but current machine learning algorithms tend to need a huge amount of training data. LML aims to mimic this human capability. Machine Reading is a research area with the goal of building systems to read natural language text. Among different approaches employed in Machine Reading, this tutorial focuses on projects and approaches that use the idea of LML. Most current machine learning (ML) algorithms learn in isolation. They are designed to address a specific problem using a single dataset. That is, given a dataset, an ML algorithm is executed on the dataset to build a model. Although this type of isolated learning is very useful, it does not have the ability to accumulate past knowledge and to make use of the knowledge for future learning, which we believe are critical for the future of machine learning and data mining. LML aims to design and develop computational systems and algorithms with this capability, i.e., to learn as humans do in a lifelong manner. In this tutorial, we introduce this important problem and the existing LML techniques and discuss opportunities and challenges of big data for lifelong machine learning. We also want to motivate researchers and practitioners to actively explore LML as the big data provides us a golden opportunity to learn a large volume of diverse knowledge, to connect different pieces of it, and to use it to raise data mining and machine learning to a new level.",2016,Knowledge Discovery and Data Mining,Fields of study: inductive transfermulti task learningtransfer of learningrobot learningactive learningactive learningerror driven learningalgorithmic learning theoryinstance based learningdata sciencedata miningartificial intelligencemachine learningcomputer science
Collaborative Multi-View Denoising,Lei Zhang (Chinese Academy of Sciences)Shupeng Wang (Chinese Academy of Sciences)Xiaoyu Zhang (Chinese Academy of Sciences)Yong Wang (Chinese Academy of Sciences)Binbin Li (Chinese Academy of Sciences)Dinggang Shen (University of North Carolina at Chapel Hill)Shuiwang Ji (Washington State University),"2719765012,2164944214,2147062167,2708288867,2711491591,2150708589,2149659377","In multi-view learning applications, like multimedia analysis and information retrieval, we often encounter the corrupted view problem in which the data are corrupted by two different types of noises, i.e., the intra- and inter-view noises. The noises may affect these applications that commonly acquire complementary representations from different views. Therefore, how to denoise corrupted views from multi-view data is of great importance for applications that integrate and analyze representations from different views. However, the heterogeneity among multi-view representations brings a significant challenge on denoising corrupted views. To address this challenge, we propose a general framework to jointly denoise corrupted views in this paper. Specifically, aiming at capturing the semantic complementarity and distributional similarity among different views, a novel Heterogeneous Linear Metric Learning (HLML) model with low-rank regularization, leave-one-out validation, and pseudo-metric constraints is proposed. Our method linearly maps multi-view data to a high-dimensional feature-homogeneous space that embeds the complementary information from different views. Furthermore, to remove the intra- and inter-view noises, we present a new Multi-view Semi-supervised Collaborative Denoising (MSCD) method with elementary transformation constraints and gradient energy competition to establish the complementary relationship among the heterogeneous representations. Experimental results demonstrate that our proposed methods are effective and efficient.",2016,Knowledge Discovery and Data Mining,Fields of study: heterogeneitynoise reductioncomputer visiondata miningmachine learningcomputer sciencemathematics
Targeted Topic Modeling for Focused Analysis,Shuai Wang (University of Illinois at Chicago)Zhiyuan Chen (University of Illinois at Chicago)Geli Fei (University of Illinois at Chicago)Bing Liu (University of Illinois at Chicago)Sherry Emery (University of Illinois at Chicago),"2441217776,2143860226,2133167487,2244698799,2305915050","One of the overarching tasks of document analysis is to find what topics people talk about. One of the main techniques for this purpose is topic modeling. So far many models have been proposed. However, the existing models typically perform full analysis on the whole data to find all topics. This is certainly useful, but in practice we found that the user almost always also wants to perform more detailed analyses on some specific aspects, which we refer to as targets (or targeted aspects). Current full-analysis models are not suitable for such analyses as their generated topics are often too coarse and may not even be on target. For example, given a set of tweets about e-cigarette, one may want to find out what topics under discussion are specifically related to children. Likewise, given a collection of online reviews about a camera, a consumer or camera manufacturer may be interested in finding out all topics about the camera's screen, the targeted aspect. As we will see in our experiments, current full topic models are ineffective for such targeted analyses. This paper studies this problem and proposes a novel targeted topic model (TTM) to enable focused analyses on any specific aspect of interest. Our experimental results demonstrate the effectiveness of the TTM.",2016,Knowledge Discovery and Data Mining,Fields of study: topic modeldata sciencemultimediabioinformaticsmachine learningcomputer science
Scalable Betweenness Centrality Maximization via Sampling,Ahmad Mahmoody (Brown University)Charalampos E. Tsourakakis (Harvard University)Eli Upfal (Brown University),"2090690368,750472553,2685185700","Betweenness centrality (BWC) is a fundamental centrality measure in social network analysis. Given a large-scale network, how can we find the most central nodes? This question is of great importance to many key applications that rely on BWC, including community detection and understanding graph vulnerability. Despite the large amount of work on scalable approximation algorithm design for BWC, estimating BWC on large-scale networks remains a computational challenge. In this paper, we study the Centrality Maximization problem (CMP): given a graph G = ( V,E ) and a positive integer k , find a set S * ⊆ V that maximizes BWC subject to the cardinality constraint |S*| ≤ k . We present an efficient randomized algorithm that provides a (1 -- 1/ e -- e)-approximation with high probability, where e > 0. Our results improve the current state-of-the-art result [40]. Furthermore, we provide the first theoretical evidence for the validity of a crucial assumption in betweenness centrality estimation, namely that in real-world networks O(| V | 2 ) shortest paths pass through the top-k central nodes, where k is a constant. This also explains why our algorithm runs in near linear time on real-world networks. We also show that our algorithm and analysis can be applied to a wider range of centrality measures, by providing a general analytical framework. On the experimental side, we perform an extensive experimental analysis of our method on real-world networks, demonstrate its accuracy and scalability, and study different properties of central nodes. Then, we compare the sampling method used by the state-of-the-art algorithm with our method. Furthermore, we perform a study of BWC in time evolving networks, and see how the centrality of the central nodes in the graphs changes over time. Finally, we compare the performance of the stochastic Kronecker model [28] to real data, and observe that it generates a similar growth pattern.",2016,Knowledge Discovery and Data Mining,Fields of study: alpha centralitynetwork controllabilitycentralitynetwork sciencebetweenness centralitynetwork theorysocial networksamplingtheoretical computer sciencecombinatoricssocial sciencemachine learningmathematics
Engagement Capacity and Engaging Team Formation for Reach Maximization of Online Social Media Platforms,Alexander G. Nikolaev (University at Buffalo)Shounak Gore (University at Buffalo)Venu Govindaraju (University at Buffalo),"2152231056,2510115460,2102146905","The challenges of assessing the ""health"" of online social media platforms and strategically growing them are recognized by many practitioners and researchers. For those platforms that primarily rely on user-generated content, the reach -- the degree of participation referring to the percentage and involvement of users -- is a key indicator of success. This paper lays a theoretical foundation for measuring engagement as a driver of reach that achieves growth via positive externality effects. The paper takes a game theoretic approach to quantifying engagement, viewing a platform's social capital as a cooperatively created value and finding a fair distribution of this value among the contributors. It introduces engagement capacity , a measure of the ability of users and user groups to engage peers, and formulates the Engaging Team Formation Problem (EngTFP) to identify the sets of users that ""make a platform go"". We show how engagement capacity can be useful in characterizing forum user behavior and in the reach maximization efforts. We also stress how engagement analysis differs from influence measurement. Computational investigations with Twitter and Health Forum data reveal the properties of engagement capacity and the utility of EngTFP.",2016,Knowledge Discovery and Data Mining,Fields of study: social networksimulation
Singapore in Motion: Insights on Public Transport Service Level Through Farecard and Mobile Data Analytics,"Hasan Poonawala (IBM)Vinay Kolar (Cisco Systems, Inc.)Sebastien Blandin (IBM)Laura Wynter (IBM)Sambit Sahu (IBM)","2521892632,2720556456,2117376187,1990593394,2169578810","Given the changing dynamics of mobility patterns and rapid growth of cities, transport agencies seek to respond more rapidly to needs of the public with the goal of offering an effective and competitive public transport system. A more data-centric approach for transport planning is part of the evolution of this process. In particular, the vast penetration of mobile phones provides an opportunity to monitor and derive insights on transport usage. Real time and historical analyses of such data can give a detailed understanding of mobility patterns of people and also suggest improvements to current transit systems. On its own, however, mobile geolocation data has a number of limitations. We thus propose a joint telco-and-farecard-based learning approach to understanding urban mobility. The approach enhances telecommunications data by leveraging it jointly with other sources of real-time data. The approach is illustrated on the First- and last-mile problem as well as route choice estimation within a densely-connected train network.",2016,Knowledge Discovery and Data Mining,Fields of study: big datadata miningsimulationcomputer science
Online Feature Selection: A Limited-Memory Substitution Algorithm and Its Asynchronous Parallel Variation,Haichuan Yang (University of Rochester)Ryohei Fujimaki (NEC)Yukitaka Kusumura (NEC)Ji Liu (University of Rochester),"2113438510,1124837368,2304482293,2301968909","This paper considers the feature selection scenario where only a few features are accessible at any time point. For example, features are generated sequentially and visible one by one. Therefore, one has to make an online decision to identify key features after all features are only scanned once or twice. The optimization based approach is a powerful tool for the online feature selection. However, most existing optimization based algorithms explicitly or implicitly adopt L 1 norm regularization to identify important features, and suffer two main disadvantages: 1) the penalty term for L 1 norm term is hard to choose; and 2) the memory usage is hard to control or predict. To overcome these two drawbacks, this paper proposes a limited-memory and model parameter free online feature selection algorithm, namely online substitution (OS) algorithm. To improve the selection efficiency, an asynchronous parallel extension for OS (Asy-OS) is proposed. Convergence guarantees are provided for both algorithms. Empirical study suggests that the performance of OS and Asy-OS is comparable to the benchmark algorithm Grafting, but requires much less memory cost and can be easily extended to the parallel implementation.",2016,Knowledge Discovery and Data Mining,Fields of study: feature selectiondata miningpattern recognitionmachine learningcomputer science
FINAL: Fast Attributed Network Alignment,Si Zhang (Arizona State University)Hanghang Tong (Arizona State University),"2515619255,2667261544","Multiple networks naturally appear in numerous high-impact applications. Network alignment (i.e., finding the node correspondence across different networks) is often the very first step for many data mining tasks. Most, if not all, of the existing alignment methods are solely based on the topology of the underlying networks. Nonetheless, many real networks often have rich attribute information on nodes and/or edges. In this paper, we propose a family of algorithms FINAL to align attributed networks. The key idea is to leverage the node/edge attribute information to guide (topology-based) alignment process. We formulate this problem from an optimization perspective based on the alignment consistency principle, and develop effective and scalable algorithms to solve it. Our experiments on real networks show that (1) by leveraging the attribute information, our algorithms can significantly improve the alignment accuracy (i.e., up to a 30% improvement over the existing methods); (2) compared with the exact solution, our proposed fast alignment algorithm leads to a more than 10 times speed-up, while preserving a 95% accuracy; and (3) our on-query alignment method scales linearly, with an around 90% ranking accuracy compared with our exact full alignment method and a near real-time response time.",2016,Knowledge Discovery and Data Mining,Fields of study: theoretical computer sciencedata miningmachine learningcomputer science
Convolutional Neural Networks for Steady Flow Approximation,Xiaoxiao Guo (University of Michigan)Wei Li (Autodesk)Francesco Iorio (Autodesk),"2682870269,2634777495,2104335678","In aerodynamics related design, analysis and optimization problems, flow fields are simulated using computational fluid dynamics (CFD) solvers. However, CFD simulation is usually a computationally expensive, memory demanding and time consuming iterative process. These drawbacks of CFD limit opportunities for design space exploration and forbid interactive design. We propose a general and flexible approximation model for real-time prediction of non-uniform steady laminar flow in a 2D or 3D domain based on convolutional neural networks (CNNs). We explored alternatives for the geometry representation and the network architecture of CNNs. We show that convolutional neural networks can estimate the velocity field two orders of magnitude faster than a GPU-accelerated CFD solver and four orders of magnitude faster than a CPU-based CFD solver at a cost of a low error rate. This approach can provide immediate feedback for real-time design iterations at the early stage of design. Compared with existing approximation models in the aerodynamics domain, CNNs enable an efficient estimation for the entire velocity field. Furthermore, designers and engineers can directly apply the CNN approximation model in their design space exploration algorithms without training extra lower-dimensional surrogate models.",2016,Knowledge Discovery and Data Mining,Fields of study: computational fluid dynamicsconvolutional neural networktheoretical computer sciencemachine learningsimulationcomputer science
A Non-parametric Approach to Detect Epileptogenic Lesions using Restricted Boltzmann Machines,Yijun Zhao (Tufts University)Bilal Ahmed (Tufts University)Thomas Thesen (New York University)Karen E. Blackmon (New York University)Jennifer G. Dy (Northeastern University)Carla E. Brodley (Northeastern University)Ruben Kuzniekcy (New York University)Orrin Devinsky (New York University),"2135745075,2470180175,1973210777,2641482471,2239241780,1994240001,2517002241,277843997","Visual detection of lesional areas on a cortical surface is critical in rendering a successful surgical operation for Treatment Resistant Epilepsy (TRE) patients. Unfortunately, 45% of Focal Cortical Dysplasia (FCD, the most common kind of TRE) patients have no visual abnormalities in their brains' 3D-MRI images. We collaborate with doctors from NYU Langone's Comprehensive Epilepsy Center and apply machine learning methodologies to identify the resective zones for these { MRI-negative } FCD patients. Our task is particularly challenging because MRI images can only provide a limited number of features. Furthermore, data from different patients often exhibit inter-patient variabilities due to age, gender, left/right handedness, etc. In this paper, we introduce a new approach which combines the restricted Boltzmann machines and a Bayesian non-parametric mixture model to address these issues. We demonstrate the efficacy of our model by applying it to a retrospective dataset of MRI-negative FCD patients who are seizure free after surgery.",2016,Knowledge Discovery and Data Mining,Fields of study: restricted boltzmann machinemixture modelartificial intelligencemachine learningsimulationcomputer science
Annealed Sparsity via Adaptive and Dynamic Shrinking,Kai Zhang (NEC)Shandian Zhe (Purdue University)Chaoran Cheng (New Jersey Institute of Technology)Zhi Wei (New Jersey Institute of Technology)Zhengzhang Chen (NEC)Haifeng Chen (NEC)Guofei Jiang (NEC)Yuan Qi (Purdue University)Jieping Ye (University of Michigan),"2600447970,1995322055,2363752216,2126219825,2132666618,2571838709,2168090285,2115766665,2305258894","Sparse learning has received tremendous amount of interest in high-dimensional data analysis due to its model interpretability and the low-computational cost. Among the various techniques, adaptive l1-regularization is an effective framework to improve the convergence behaviour of the LASSO, by using varying strength of regularization across different features. In the meantime, the adaptive structure makes it very powerful in modelling grouped sparsity patterns as well, being particularly useful in high-dimensional multi-task problems. However, choosing an appropriate, global regularization weight is still an open problem. In this paper, inspired by the annealing technique in material science, we propose to achieve ""annealed sparsity"" by designing a dynamic shrinking scheme that simultaneously optimizes the regularization weights and model coefficients in sparse (multi-task) learning. The dynamic structures of our algorithm are twofold. Feature-wise (spatially), the regularization weights are updated interactively with model coefficients, allowing us to improve the global regularization structure. Iteration-wise (temporally), such interaction is coupled with gradually boosted l1-regularization by adjusting an equality norm-constraint, achieving an annealing effect to further improve model selection. This renders interesting shrinking behaviour in the whole solution path. Our method competes favorably with state-of-the-art methods in sparse (multi-task) learning. We also apply it in expression quantitative trait loci analysis (eQTL), which gives useful biological insights in human cancer (melanoma) study.",2016,Knowledge Discovery and Data Mining,Fields of study: multi task learninglassocompressed sensingannealingfeature selectionpattern recognitionmachine learningmathematical optimizationstatisticscomputer sciencemathematics
Scalable Partial Least Squares Regression on Grammar-Compressed Data Matrices,Yasuo Tabei (National Presto Industries)Hiroto Saigo (Kyushu University)Yoshihiro Yamanishi (Kyushu University)Simon J. Puglisi (University of Helsinki),"2094849311,1180009366,2029139975,324716293","With massive high-dimensional data now commonplace in research and industry, there is a strong and growing demand for more scalable computational techniques for data analysis and knowledge discovery. Key to turning these data into knowledge is the ability to learn statistical models with high interpretability. Current methods for learning statistical models either produce models that are not interpretable or have prohibitive computational costs when applied to massive data. In this paper we address this need by presenting a scalable algorithm for partial least squares regression (PLS), which we call compression-based PLS (cPLS), to learn predictive linear models with a high interpretability from massive high-dimensional data. We propose a novel grammar-compressed representation of data matrices that supports fast row and column access while the data matrix is in a compressed form. The original data matrix is grammar-compressed and then the linear model in PLS is learned on the compressed data matrix, which results in a significant reduction in working space, greatly improving scalability. We experimentally test cPLS on its ability to learn linear models for classification, regression and feature extraction with various massive high-dimensional data, and show that cPLS performs superiorly in terms of prediction accuracy, computational efficiency, and interpretability.",2016,Knowledge Discovery and Data Mining,Fields of study: partial least squares regressionbig datadata miningpattern recognitionmachine learningstatisticscomputer science
An Engagement-Based Customer Lifetime Value System for E-commerce,Ali VanderveldAddhyan PandeyAngela Han (Google)Rajesh Parekh (Facebook),"2598640635,2507911175,2652940682,2528283674","A comprehensive understanding of individual customer value is crucial to any successful customer relationship management strategy. It is also the key to building products for long-term value returns. Modeling customer lifetime value (CLTV) can be fraught with technical difficulties, however, due to both the noisy nature of user-level behavior and the potentially large customer base. Here we describe a new CLTV system that solves these problems. This was built at Groupon, a large global e-commerce company, where confronting the unique challenges of local commerce means quickly iterating on new products and the optimal inventory to appeal to a wide and diverse audience. Given current purchaser frequency we need a faster way to determine the health of individual customers, and given finite resources we need to know where to focus our energy. Our CLTV system predicts future value on an individual user basis with a random forest model which includes features that account for nearly all aspects of each customer's relationship with our platform. This feature set includes those quantifying engagement via email and our mobile app, which give us the ability to predict changes in value far more quickly than models based solely on purchase behavior. We further model different customer types, such as one-time buyers and power users, separately so as to allow for different feature weights and to enhance the interpretability of our results. Additionally, we developed an economical scoring framework wherein we re-score a user when any trigger events occur and apply a decay function otherwise, to enable frequent scoring of a large customer base with a complex model. This system is deployed, predicting the value of hundreds of millions of users on a daily cadence, and is actively being used across our products and business initiatives.",2016,Knowledge Discovery and Data Mining,Fields of study: customer reference programcustomer intelligencevoice of the customercustomer to customercustomer lifetime valuecustomer retentionrandom foreste commercedata miningmachine learningcomputer science
Crime Rate Inference with Big Data,Hongjian Wang (Pennsylvania State University)Daniel Kifer (Pennsylvania State University)Corina Graif (Pennsylvania State University)Zhenhui Li (Pennsylvania State University),"2669795438,2049563562,341083243,2098136913","Crime is one of the most important social problems in the country, affecting public safety, children development, and adult socioeconomic status. Understanding what factors cause higher crime is critical for policy makers in their efforts to reduce crime and increase citizens' life quality. We tackle a fundamental problem in our paper: crime rate inference at the neighborhood level. Traditional approaches have used demographics and geographical influences to estimate crime rates in a region. With the fast development of positioning technology and prevalence of mobile devices, a large amount of modern urban data have been collected and such big data can provide new perspectives for understanding crime. In this paper, we used large-scale Point-Of-Interest data and taxi flow data in the city of Chicago, IL in the USA. We observed significantly improved performance in crime rate inference compared to using traditional features. Such an improvement is consistent over multiple years. We also show that these new features are significant in the feature importance analysis.",2016,Knowledge Discovery and Data Mining,Fields of study: big datadata sciencecomputer securitydata miningcomputer science
"Collective Sensemaking via Social Sensors: Extracting, Profiling, Analyzing, and Predicting Real-world Events",Yuheng Hu (University of Illinois at Chicago)Yu-Ru Lin (University of Pittsburgh)Jiebo Luo (University of Rochester),"2655196453,2155397203,2059910451","Social media platforms like Twitter and Facebook have emerged as some of the most important platforms for people to discover, report, share, and communicate with others about various public events, be they of global or local interest (some high profile examples include the U.S Presidential debates, the Boston bombings, the hurricane Sandy, etc). The burst of social media reaction can be seen as a valuable real-time reflection of events as they happen, and can be used for a variety of applications such as computational journalism. Until now, such analysis has been mostly done manually or through primitive tools. Scalable and automated approaches are needed given the massive amounts of both event and reaction information. These approaches must also be able to conduct in-depth analysis of complex interactions between an event and its audience. Supporting such automation and examination however poses several computational challenges. In recent years, research communities have witnessed a growing interest in tackling these challenges. Furthermore, much recent research has begun to focus on solving more complex event analytics tasks such as post-event effect quantification and event progress prediction. This tutorial aims to review and examine current state of the research progress on this emerging topic.",2016,Knowledge Discovery and Data Mining,Fields of study: complex event processingsocial mediadata sciencedata miningsimulationcomputer science
Portfolio Selections in P2P Lending: A Multi-Objective Perspective,Hongke Zhao (University of Science and Technology of China)Qi Liu (University of Science and Technology of China)Guifeng Wang (University of Science and Technology of China)Yong Ge (University of Arizona)Enhong Chen (University of Science and Technology of China),"2223484430,2420624292,2508017931,2695934969,2136372366","P2P lending is an emerging wealth-management service for individuals, which allows lenders to directly bid and invest on the loans created by borrowers. In these platforms, lenders often pursue multiple objectives (e.g., non-default probability, fully-funded probability and winning-bid probability ) when they select loans to invest. How to automatically assess loans from these objectives and help lenders select loan portfolios is a very important but challenging problem. To that end, in this paper, we present a holistic study on portfolio selections in P2P lending. Specifically, we first propose to adapt gradient boosting decision tree, which combines both static features and dynamic features , to assess loans from multiple objectives. Then, we propose two strategies, i.e., weighted objective optimization strategy and multi-objective optimization strategy , to select portfolios for lenders. For each lender, the first strategy attempts to provide one optimal portfolio while the second strategy attempts to provide a Pareto-optimal portfolio set. Further, we design two algorithms, namely DPA and EVA , which can efficiently resolve the optimizations in these two strategies, respectively. Finally, extensive experiments on a large-scale real-world data set demonstrate the effectiveness of our solutions.",2016,Knowledge Discovery and Data Mining,Fields of study: multi objective optimizationactuarial science
Semi-Markov Switching Vector Autoregressive Model-Based Anomaly Detection in Aviation Systems,Igor Melnyk (University of Minnesota)Arindam Banerjee (University of Minnesota)Bryan L. Matthews (Ames Research Center)Nikunj C. Oza (Ames Research Center),"2132303832,2037585042,2149808489,2047114418","In this work we consider the problem of anomaly detection in heterogeneous, multivariate, variable-length time series datasets. Our focus is on the aviation safety domain, where data objects are flights and time series are sensor readings and pilot switches. In this context the goal is to detect anomalous flight segments, due to mechanical, environmental, or human factors in order to identifying operationally significant events and highlight potential safety risks. For this purpose, we propose a framework which represents each flight using a semi-Markov switching vector autoregressive (SMS-VAR) model. Detection of anomalies is then based on measuring dissimilarities between the model's prediction and data observation. The framework is scalable, due to the inherent parallel nature of most computations, and can be used to perform online anomaly detection. Extensive experimental results on simulated and real datasets illustrate that the framework can detect various types of anomalies along with the key parameters involved.",2016,Knowledge Discovery and Data Mining,Fields of study: graphical modeltime seriesanomaly detectiondata miningmachine learningsimulationcomputer science
Predict Risk of Relapse for Patients with Multiple Stages of Treatment of Depression,Zhi Nie (Arizona State University)Pinghua Gong (University of Michigan)Jieping Ye (University of Michigan),"2703094151,2323553286,2305258894","Depression is a serious mood disorder afflicting millions of people around the globe. Medications of different types and with different effects on neural activity have been developed for its treatments during the past few decades. Due to the heterogeneity of the disorder, many patients cannot achieve symptomatic remission from a single clinical trial. Instead they need multiple clinical trials to achieve remission, resulting in a multiple stage treatment pattern. Furthermore those who indeed achieve symptom remission are still faced with substantial risk of relapse. One promising approach to predicting the risk of relapse is censored regression. Traditional censored regression typically applies only to situations in which the exact time of event of interest is known. However, follow-up studies that track the patients' relapse status can only provide an interval of time during which relapse occurs. The exact time of relapse is usually unknown. In this paper, we present a censored regression approach with a truncated $l_1$ loss function that can handle the uncertainty of relapse time. Based on this general loss function, we develop a gradient boosting algorithm and a stochastic dual coordinate ascent algorithm when the hypothesis in the loss function is represented as (1) an ensemble of decision trees and (2) a linear combination of covariates, respectively. As an extension of our linear model, a multi-stage linear approach is further proposed to harness the data collected from multiple stages of treatment. We evaluate the proposed algorithms using a real-world clinical trial dataset. Results show that our methods outperform the well-known Cox proportional hazard model. In addition, the risk factors identified by our multi-stage linear model not only corroborate findings from recent research but also yield some new insights into how to develop effective measures for prevention of relapse among patients after their initial remission from the acute treatment stage.",2016,Knowledge Discovery and Data Mining,Fields of study: censored regression modelsurvival analysiseconometricsstatistics
Meta Structure: Computing Relevance in Large Heterogeneous Information Networks,Zhipeng Huang (University of Hong Kong)Yudian Zheng (University of Hong Kong)Reynold Cheng (University of Hong Kong)Yizhou Sun (Northeastern University)Nikos Mamoulis (University of Hong Kong)Xiang Li (University of Hong Kong),"2511877038,2105493843,2138267588,2131539564,18851973,2644215135","A heterogeneous information network (HIN) is a graph model in which objects and edges are annotated with types. Large and complex databases, such as YAGO and DBLP, can be modeled as HINs. A fundamental problem in HINs is the computation of closeness, or relevance, between two HIN objects. Relevance measures can be used in various applications, including entity resolution, recommendation, and information retrieval. Several studies have investigated the use of HIN information for relevance computation, however, most of them only utilize simple structure, such as path, to measure the similarity between objects. In this paper, we propose to use meta structure, which is a directed acyclic graph of object types with edge types connecting in between, to measure the proximity between objects. The strength of meta structure is that it can describe complex relationship between two HIN objects (e.g., two papers in DBLP share the same authors and topics). We develop three relevance measures based on meta structure. Due to the computational complexity of these measures, we further design an algorithm with data structures proposed to support their evaluation. Our extensive experiments on YAGO and DBLP show that meta structure-based relevance is more effective than state-of-the-art approaches, and can be efficiently computed.",2016,Knowledge Discovery and Data Mining,Fields of study: relevancetheoretical computer sciencedata miningmachine learningcomputer science
Graphons and Machine Learning: Modeling and Estimation of Sparse Massive Networks,Jennifer T. Chayes (Microsoft),2021736417,"There are numerous examples of sparse massive networks, in particular the Internet, WWW and online social networks. How do we model and learn these networks? In contrast to conventional learning problems, where we have many independent samples, it is often the case for these networks that we can get only one independent sample. How do we use a single snapshot today to learn a model for the network, and therefore be able to predict a similar, but larger network in the future? In the case of relatively small or moderately sized networks, it's appropriate to model the network parametrically, and attempt to learn these parameters. For massive networks, a non-parametric representation is more appropriate. In this talk, we first review the theory of graphons, developed over the last decade to describe limits of dense graphs, and the more the recent theory describing sparse graphs of unbounded average degree, including power-law graphs. We then show how to use these graphons as non-parametric models for sparse networks. Finally, we show how to get consistent estimators of these non-parametric models, and moreover how to do this in a way that protects the privacy of individuals on the network.",2016,Knowledge Discovery and Data Mining,Fields of study: evolving networkscomplex networkdata sciencedata miningmachine learningcomputer science
Probabilistic Robust Route Recovery with Spatio-Temporal Dynamics,Hao Wu (Fudan University)Jiangyun Mao (Fudan University)Weiwei Sun (Fudan University)Baihua Zheng (Singapore Management University)Hanyuan Zhang (Fudan University)Ziyang Chen (Fudan University)Wei Wang (Fudan University),"2649278900,2507701226,2142659109,2200653933,2512529277,2516867379,2296847996","Vehicle trajectories are one of the most important data in location-based services. The quality of trajectories directly affects the services. However, in the real applications, trajectory data are not always sampled densely. In this paper, we study the problem of recovering the entire route between two distant consecutive locations in a trajectory. Most existing works solve the problem without using those informative historical data or solve it in an empirical way. We claim that a data-driven and probabilistic approach is actually more suitable as long as data sparsity can be well handled. We propose a novel route recovery system in a fully probabilistic way which incorporates both temporal and spatial dynamics and addresses all the data sparsity problem introduced by the probabilistic method. It outperforms the existing works with a high accuracy (over 80%) and shows a strong robustness even when the length of routes to be recovered is very long (about 30 road segments) or the data is very sparse.",2016,Knowledge Discovery and Data Mining,Fields of study: location based servicetrajectorydata miningmachine learningsimulationcomputer science
"Extreme Multi-label Loss Functions for Recommendation, Tagging, Ranking & Other Missing Label Applications",Himanshu Jain (Indian Institute of Technology Delhi)Yashoteja Prabhu (Indian Institute of Technology Delhi)Manik Varma (Microsoft),"2423330096,1988058231,2130043413","The choice of the loss function is critical in extreme multi-label learning where the objective is to annotate each data point with the most relevant subset of labels from an extremely large label set. Unfortunately, existing loss functions, such as the Hamming loss, are unsuitable for learning, model selection, hyperparameter tuning and performance evaluation. This paper addresses the issue by developing propensity scored losses which: (a) prioritize predicting the few relevant labels over the large number of irrelevant ones; (b) do not erroneously treat missing labels as irrelevant but instead provide unbiased estimates of the true loss function even when ground truth labels go missing under arbitrary probabilistic label noise models; and (c) promote the accurate prediction of infrequently occurring, hard to predict, but rewarding tail labels. Another contribution is the development of algorithms which efficiently scale to extremely large datasets with up to 9 million labels, 70 million points and 2 million dimensions and which give significant improvements over the state-of-the-art. This paper's results also apply to tagging, recommendation and ranking which are the motivating applications for extreme multi-label learning. They generalize previous attempts at deriving unbiased losses under the restrictive assumption that labels go missing uniformly at random from the ground truth. Furthermore, they provide a sound theoretical justification for popular label weighting heuristics used to recommend rare items. Finally, they demonstrate that the proposed contributions align with real world applications by achieving superior clickthrough rates on sponsored search advertising in Bing.",2016,Knowledge Discovery and Data Mining,Fields of study: rankinginformation retrievaldata miningmachine learningstatisticscomputer science
CNTK: Microsoft's Open-Source Deep-Learning Toolkit,Frank Seide (Microsoft)Amit Agarwal (Microsoft),"2081329872,2488072657","This tutorial will introduce the Computational Network Toolkit, or CNTK, Microsoft's cutting-edge open-source deep-learning toolkit for Windows and Linux. CNTK is a powerful computation-graph based deep-learning toolkit for training and evaluating deep neural networks. Microsoft product groups use CNTK, for example to create the Cortana speech models and web ranking. CNTK supports feed-forward, convolutional, and recurrent networks for speech, image, and text workloads, also in combination. Popular network types are supported either natively (convolution) or can be described as a CNTK configuration (LSTM, sequence-to-sequence). CNTK scales to multiple GPU servers and is designed around efficiency. The tutorial will give an overview of CNTK's general architecture and describe the specific methods and algorithms used for automatic differentiation, recurrent-loop inference and execution, memory sharing, on-the-fly randomization of large corpora, and multi-server parallelization. We will then show how typical uses looks like for relevant tasks like image recognition, sequence-to-sequence modeling, and speech recognition.",2016,Knowledge Discovery and Data Mining,Fields of study: deep learningtheoretical computer sciencedata miningmachine learningcomputer science
Absolute Fused Lasso and Its Application to Genome-Wide Association Studies,Tao Yang (Arizona State University)Jun Liu (SAS Institute)Pinghua Gong (University of Michigan)Ruiwen Zhang (SAS Institute)Xiaotong Shen (University of Minnesota)Jieping Ye (University of Michigan),"2688435411,2719093250,2323553286,2692486559,2125069230,2305258894","In many real-world applications, the samples/features acquired are in spatial or temporal order. In such cases, the magnitudes of adjacent samples/features are typically close to each other. Meanwhile, in the high-dimensional scenario, identifying the most relevant samples/features is also desired. In this paper, we consider a regularized model which can simultaneously identify important features and group similar features together. The model is based on a penalty called Absolute Fused Lasso (AFL). The AFL penalty encourages sparsity in the coefficients as well as their successive differences of absolute values' i.e., local constancy of the coefficient components in absolute values. Due to the non-convexity of AFL, it is challenging to develop efficient algorithms to solve the optimization problem. To this end, we employ the Difference of Convex functions (DC) programming to optimize the proposed non-convex problem. At each DC iteration, we adopt the proximal algorithm to solve a convex regularized sub-problem. One of the major contributions of this paper is to develop a highly efficient algorithm to compute the proximal operator. Empirical studies on both synthetic and real-world data sets from Genome-Wide Association Studies demonstrate the efficiency and effectiveness of the proposed approach in simultaneous identifying important features and grouping similar features.",2016,Knowledge Discovery and Data Mining,Fields of study: genome wide association studymachine learningmathematical optimizationstatisticsmathematics
Fast Unsupervised Online Drift Detection Using Incremental Kolmogorov-Smirnov Test,Denis dos Reis (University of São Paulo)Peter Flach (University of Bristol)Stan Matwin (Polish Academy of Sciences)Gustavo Enrique de Almeida Prado Alves Batista (Spanish National Research Council),"2366579606,1814273096,2195580174,2165222361","Data stream research has grown rapidly over the last decade. Two major features distinguish data stream from batch learning: stream data are generated on the fly, possibly in a fast and variable rate; and the underlying data distribution can be non-stationary, leading to a phenomenon known as concept drift. Therefore, most of the research on data stream classification focuses on proposing efficient models that can adapt to concept drifts and maintain a stable performance over time. However, specifically for the classification task, the majority of such methods rely on the instantaneous availability of true labels for all already classified instances. This is a strong assumption that is rarely fulfilled in practical applications. Hence there is a clear need for efficient methods that can detect concept drifts in an unsupervised way. One possibility is the well-known Kolmogorov-Smirnov test, a statistical hypothesis test that checks whether two samples differ. This work has two main contributions. The first one is the Incremental Kolmogorov-Smirnov algorithm that allows performing the Kolmogorov-Smirnov hypothesis test instantly using two samples that change over time, where the change is an insertion and/or removal of an observation. Our algorithm employs a randomized tree and is able to perform the insertion and removal operations in O (log N ) with high probability and calculate the Kolmogorov-Smirnov test in O (1), where N is the number of sample observations. This is a significant speed-up compared to the O ( N log N ) cost of the non-incremental implementation. The second contribution is the use of the Incremental Kolmogorov-Smirnov test to detect concept drifts without true labels. Classification algorithms adapted to use the test rely on a limited portion of those labels just to update the classification model after a concept drift is detected.",2016,Knowledge Discovery and Data Mining,Fields of study: treapcartesian treeconcept driftkolmogorov smirnov testdata miningmachine learningstatisticscomputer science
Firebird: Predicting Fire Risk and Prioritizing Fire Inspections in Atlanta,"Michael Madaio (Carnegie Mellon University)Shang-Tse Chen (Georgia Institute of Technology)Oliver L. Haimson (University of California, Irvine)Wenwen Zhang (Georgia Institute of Technology)Xiang Cheng (Emory University)Matthew Hinds-AldrichDuen Horng Chau (Georgia Institute of Technology)Bistra Dilkina (Georgia Institute of Technology)","2504639629,2576491193,48930185,2309462951,2635027973,1498291670,2024561599,2252452800","The Atlanta Fire Rescue Department (AFRD), like many municipal fire departments, actively works to reduce fire risk by inspecting commercial properties for potential hazards and fire code violations. However, AFRD's fire inspection practices relied on tradition and intuition, with no existing data-driven process for prioritizing fire inspections or identifying new properties requiring inspection. In collaboration with AFRD, we developed the Firebird framework to help municipal fire departments identify and prioritize commercial property fire inspections, using machine learning, geocoding, and information visualization. Firebird computes fire risk scores for over 5,000 buildings in the city, with true positive rates of up to 71% in predicting fires. It has identified 6,096 new potential commercial properties to inspect, based on AFRD's criteria for inspection. Furthermore, through an interactive map, Firebird integrates and visualizes fire incidents, property information and risk scores to help AFRD make informed decisions about fire inspections. Firebird has already begun to make positive impact at both local and national levels. It is improving AFRD's inspection processes and Atlanta residents' safety, and was highlighted by National Fire Protection Association (NFPA) as a best practice for using data to inform fire inspections.",2016,Knowledge Discovery and Data Mining,Fields of study: flame simfire protectionpredictive analyticsinteractive visualizationcomputer securitysimulationcomputer science
Predicting Disk Replacement towards Reliable Data Centers,Mirela Madalina Botezatu (IBM)Ioana Giurgiu (IBM)Jasmina Bogojeska (IBM)Dorothea Wiesmann (IBM),"2229656828,2203074430,317490117,2048997920","Disks are among the most frequently failing components in today's IT environments. Despite a set of defense mechanisms such as RAID, the availability and reliability of the system are still often impacted severely. In this paper, we present a highly accurate SMART-based analysis pipeline that can correctly predict the necessity of a disk replacement even 10-15 days in advance. Our method has been built and evaluated on more than 30000 disks from two major manufacturers, monitored over 17 months. Our approach employs statistical techniques to automatically detect which SMART parameters correlate with disk replacement and uses them to predict the replacement of a disk with even 98% accuracy.",2016,Knowledge Discovery and Data Mining,Fields of study: time seriesbiological classificationembedded systemreal time computingstatistics
Towards Conversational Recommender Systems,Konstantina Christakopoulou (University of Minnesota)Filip Radlinski (Microsoft)Katja Hofmann (Microsoft),"2223173307,2072292845,2722514380","People often ask others for restaurant recommendations as a way to discover new dining experiences. This makes restaurant recommendation an exciting scenario for recommender systems and has led to substantial research in this area. However, most such systems behave very differently from a human when asked for a recommendation. The goal of this paper is to begin to reduce this gap. In particular, humans can quickly establish preferences when asked to make a recommendation for someone they do not know. We address this cold-start recommendation problem in an online learning setting. We develop a preference elicitation framework to identify which questions to ask a new user to quickly learn their preferences. Taking advantage of latent structure in the recommendation space using a probabilistic latent factor model, our experiments with both synthetic and real world data compare different types of feedback and question selection strategies. We find that our framework can make very effective use of online user feedback, improving personalized recommendations over a static model by 25% after asking only 2 questions. Our results demonstrate dramatic benefits of starting from offline embeddings, and highlight the benefit of bandit-based explore-exploit strategies in this setting.",2016,Knowledge Discovery and Data Mining,Fields of study: cold startrecommender systemknowledge managementmultimediaworld wide webdata miningmachine learningcomputer science
Approximate Personalized PageRank on Dynamic Graphs,Hongyang Zhang (Stanford University)Peter Lofgren (Stanford University)Ashish Goel (Stanford University),"2494713920,2503222904,2112067655","We propose and analyze two algorithms for maintaining approximate Personalized PageRank (PPR) vectors on a dynamic graph, where edges are added or deleted. Our algorithms are natural dynamic versions of two known local variations of power iteration. One, Forward Push, propagates probability mass forwards along edges from a source node, while the other, Reverse Push, propagates local changes backwards along edges from a target. In both variations, we maintain an invariant between two vectors, and when an edge is updated, our algorithm first modifies the vectors to restore the invariant, then performs any needed local push operations to restore accuracy. For Reverse Push, we prove that for an arbitrary directed graph in a random edge model, or for an arbitrary undirected graph, given a uniformly random target node t , the cost to maintain a PPR vector to t of additive error e as k edges are updated is O ( k + d/e, where d is the average degree of the graph. This is O (1) work per update, plus the cost of computing a reverse vector once on a static graph. For Forward Push, we show that on an arbitrary undirected graph, given a uniformly random start node s , the cost to maintain a PPR vector from s of degree-normalized error e as k edges are updated is O ( k + 1/e, which is again O (1) per update plus the cost of computing a PPR vector once on a static graph.",2016,Knowledge Discovery and Data Mining,Fields of study: simplex graphstrength of a graphvoltage graphcomplement graphmultiple edgesgraph powerbutterfly graphfeedback arc setmixed graphnull graphtranspose graphpathrandom geometric graphcycle graphline graphmultigraphdegreedirected graphrandom walktheoretical computer sciencecombinatoricsdistributed computingmathematical optimizationstatisticsmathematics
Overcoming Key Weaknesses of Distance-based Neighbourhood Methods using a Data Dependent Dissimilarity Measure,"Kai Ming Ting (Federation University Australia)Ye Zhu (Monash University, Clayton campus)Mark James Carman (Monash University)Yue Zhu (Nanjing University)Zhi-Hua Zhou (Nanjing University)","2644558103,2597410478,2123487663,2152538905,2286237009","This paper introduces the first generic version of data dependent dissimilarity and shows that it provides a better closest match than distance measures for three existing algorithms in clustering, anomaly detection and multi-label classification. For each algorithm, we show that by simply replacing the distance measure with the data dependent dissimilarity measure, it overcomes a key weakness of the otherwise unchanged algorithm.",2016,Knowledge Discovery and Data Mining,Fields of study: data miningpattern recognitionmachine learningmathematics
Scalable Data Analytics Using R: Single Machines to Hadoop Spark Clusters,John-Mark Agosta (Microsoft)Debraj GuhaThakurta (Microsoft)Robert Horton (Microsoft)Mario Inchiosa (Microsoft)Srini Kumar (Microsoft)Mengyue Zhao (Microsoft),"2522191117,2521559845,2516413223,2509016422,2514624990,2516893623","R is one of the most popular languages in the data science, statistical and machine learning (ML) community. However, when it comes to scalable data analysis and ML using R, many data scientists are blocked or hindered by (a) its limitations of available functions to handle large datasets efficiently, and (b) knowledge about the appropriate computing environments to scale R scripts from desktop exploratory analysis to elastic and distributed cloud services. In this tutorial we will discuss solutions that demonstrate the use of distributed compute environments and end to end solutions for R. We will present the topics through presentations and worked-out examples with sample code. In addition, we will provide a public code repository that attendees will be able to access and adapt to their own practice. We believe this tutorial will be of strong interest to a large and growing community of data scientists and developers using R for data analysis and modeling.",2016,Knowledge Discovery and Data Mining,Fields of study: sparkpredictive analyticssqllearning curvescalabilityvisualizationstatistical modelcomputational statisticsdata sciencetheoretical computer sciencedata miningmachine learningcomputer science
Algorithmic Bias: From Discrimination Discovery to Fairness-aware Data Mining,Sara HajianFrancesco Bonchi (Institute for Scientific Interchange)Carlos Castillo (Yahoo!),"2617388898,2176652147,2125169605","Algorithms and decision making based on Big Data have become pervasive in all aspects of our daily lives lives (offline and online), as they have become essential tools in personal finance, health care, hiring, housing, education, and policies. It is therefore of societal and ethical importance to ask whether these algorithms can be discriminative on grounds such as gender, ethnicity, or health status. It turns out that the answer is positive: for instance, recent studies in the context of online advertising show that ads for high-income jobs are presented to men much more often than to women [Datta et al., 2015]; and ads for arrest records are significantly more likely to show up on searches for distinctively black names [Sweeney, 2013]. This algorithmic bias exists even when there is no discrimination intention in the developer of the algorithm. Sometimes it may be inherent to the data sources used (software making decisions based on data can reflect, or even amplify, the results of historical discrimination), but even when the sensitive attributes have been suppressed from the input, a well trained machine learning algorithm may still discriminate on the basis of such sensitive attributes because of correlations existing in the data. These considerations call for the development of data mining systems which are discrimination-conscious by-design. This is a novel and challenging research area for the data mining community. The aim of this tutorial is to survey algorithmic bias, presenting its most common variants, with an emphasis on the algorithmic techniques and key ideas developed to derive efficient solutions. The tutorial covers two main complementary approaches: algorithms for discrimination discovery and discrimination prevention by means of fairness-aware data mining. We conclude by summarizing promising paths for future research.",2016,Knowledge Discovery and Data Mining,Fields of study: data sciencedata miningartificial intelligencemachine learningstatisticscomputer science
FUSE: Full Spectral Clustering,Wei Ye (Ludwig Maximilian University of Munich)Sebastian Goebl (Ludwig Maximilian University of Munich)Claudia Plant (University of Vienna)Christian Böhm (Ludwig Maximilian University of Munich),"2299720385,119557107,2590664453,2486446532","Multi-scale data which contains structures at different scales of size and density is a big challenge for spectral clustering. Even given a suitable locally scaled affinity matrix, the first k eigenvectors of such a matrix still cannot separate clusters well. Thus, in this paper, we exploit the fusion of the cluster-separation information from all eigenvectors to achieve a better clustering result. Our method FU ll S pectral Clust E ring (FUSE) is based on Power Iteration (PI) and Independent Component Analysis (ICA). PI is used to fuse all eigenvectors to one pseudo-eigenvector which inherits all the cluster-separation information. To conquer the cluster-collision problem, we utilize PI to generate p ( p > k ) pseudo-eigenvectors. Since these pseudo-eigenvectors are redundant and the cluster-separation information is contaminated with noise, ICA is adopted to rotate the pseudo-eigenvectors to make them pairwise statistically independent. To let ICA overcome local optima and speed up the search process, we develop a self-adaptive and self-learning greedy search method. Finally, we select k rotated pseudo-eigenvectors (independent components) which have more cluster-separation information measured by kurtosis for clustering. Various synthetic and real-world data verifies the effectiveness and efficiency of our FUSE method.",2016,Knowledge Discovery and Data Mining,Fields of study: givens rotationspectral clusteringpower iterationpattern recognitionmachine learningmathematical optimizationstatisticsmathematics
Kam1n0: MapReduce-based Assembly Clone Search for Reverse Engineering,Steven H.H. Ding (McGill University)Benjamin C.M. Fung (McGill University)Philippe Charland (Defence Research and Development Canada),"2286376525,2015549731,2619933766","Assembly code analysis is one of the critical processes for detecting and proving software plagiarism and software patent infringements when the source code is unavailable. It is also a common practice to discover exploits and vulnerabilities in existing software. However, it is a manually intensive and time-consuming process even for experienced reverse engineers. An effective and efficient assembly code clone search engine can greatly reduce the effort of this process, since it can identify the cloned parts that have been previously analyzed. The assembly code clone search problem belongs to the field of software engineering. However, it strongly depends on practical nearest neighbor search techniques in data mining and databases. By closely collaborating with reverse engineers and Defence Research and Development Canada (DRDC), we study the concerns and challenges that make existing assembly code clone approaches not practically applicable from the perspective of data mining. We propose a new variant of LSH scheme and incorporate it with graph matching to address these challenges. We implement an integrated assembly clone search engine called Kam1n0. It is the first clone search engine that can efficiently identify the given query assembly function's subgraph clones from a large assembly code repository. Kam1n0 is built upon the Apache Spark computation framework and Cassandra-like key-value distributed storage. A deployed demo system is publicly available. Extensive experimental results suggest that Kam1n0 is accurate, efficient, and scalable for handling large volume of assembly code.",2016,Knowledge Discovery and Data Mining,Fields of study: world wide webbioinformaticsdata miningdatabasecomputer science
Inferring Network Effects from Observational Data,David T. Arbour (University of Massachusetts Amherst)Dan Garant (University of Massachusetts Amherst)David D. Jensen (University of Massachusetts Amherst),"2223056767,2388112039,2124028641","We present Relational Covariate Adjustment (RCA), a general method for estimating causal effects in relational data. Relational Covariate Adjustment is implemented through two high-level operations: identification of an adjustment set and relational regression adjustment. The former is achieved through an extension of Pearl's back-door criterion to relational domains. We demonstrate how this extended definition can be used to estimate causal effects in the presence of network interference and confounding. RCA is agnostic to functional form, and it can easily model both discrete and continuous treatments as well as estimate the effects of a wider array of network interventions than existing experimental approaches. We show that RCA can yield robust estimates of causal effects using common regression models without extensive parameter tuning. Through a series of simulation experiments on a variety of synthetic and real-world network structures, we show that causal effects estimated on observational data with RCA are nearly as accurate as those estimated from well-designed network experiments",2016,Knowledge Discovery and Data Mining,Fields of study: causalityeconometricsdata miningmachine learningstatisticscomputer sciencemathematics
Bayesian Inference of Arrival Rate and Substitution Behavior from Sales Transaction Data with Stockouts,Benjamin Letham (Massachusetts Institute of Technology)Lydia M. Letham (Massachusetts Institute of Technology)Cynthia Rudin (Duke University),"1983818389,2510404420,2600280817","When an item goes out of stock, sales transaction data no longer reflect the original customer demand, since some customers leave with no purchase while others substitute alternative products for the one that was out of stock. Here we develop a Bayesian hierarchical model for inferring the underlying customer arrival rate and choice model from sales transaction data and the corresponding stock levels. The model uses a nonhomogeneous Poisson process to allow the arrival rate to vary throughout the day, and allows for a variety of choice models. Model parameters are inferred using a stochastic gradient MCMC algorithm that can scale to large transaction databases. We fit the model to data from a local bakery and show that it is able to make accurate out-of-sample predictions, and to provide actionable insight into lost cookie sales.",2016,Knowledge Discovery and Data Mining,Fields of study: markov chain monte carlobayesian probabilityeconometricsstatisticsmathematics
Batch Model for Batched Timestamps Data Analysis with Application to the SSA Disability Program,Qingqi Yue (National Institutes of Health)Ao Yuan (National Institutes of Health)Xuan Che (National Institutes of Health)Minh HuynhChunxiao Zhou (National Institutes of Health),"2531142739,2702211679,2647843527,2517598830,2682842892","The Office of Disability Adjudication and Review (ODAR) is responsible for holding hearings, issuing decisions, and reviewing appeals as part of the Social Security Administration's disability determining process. In order to control and process cases, the ODAR has established a Case Processing and Management System (CPMS) to record management information since December 2003. The CPMS provides a detailed case status history for each case. Due to the large number of appeal requests and limited resources, the number of pending claims at ODAR was over one million cases by March 31, 2015. Our National Institutes of Health (NIH) team collaborated with SSA and developed a Case Status Change Model (CSCM) project to meet the ODAR's urgent need of reducing backlogs and improve hearings and appeals process. One of the key issues in our CSCM project is to estimate the expected service time and its variation for each case status code. The challenge is that the system's recorded job departure times may not be the true job finished times. As the CPMS timestamps data of case status codes showed apparent batch patterns, we proposed a batch model and applied the constrained least squares method to estimate the mean service times and the variances. We also proposed a batch search algorithm to determine the optimal batch partition, as no batch partition was given in the real data. Simulation studies were conducted to evaluate the performance of the proposed methods. Finally, we applied the method to analyze a real CPMS data from ODAR/SSA.",2016,Knowledge Discovery and Data Mining,Fields of study: operations researchdata miningstatisticscomputer science
Robust Large-Scale Machine Learning in the Cloud,Steffen Rendle (Google)Dennis Fetterly (Google)Eugene J. Shekita (Google)Bor-yiing Su (Google),"2516383678,2515256586,2309335715,2278618929","The convergence behavior of many distributed machine learning (ML) algorithms can be sensitive to the number of machines being used or to changes in the computing environment. As a result, scaling to a large number of machines can be challenging. In this paper, we describe a new scalable coordinate descent (SCD) algorithm for generalized linear models whose convergence behavior is always the same, regardless of how much SCD is scaled out and regardless of the computing environment. This makes SCD highly robust and enables it to scale to massive datasets on low-cost commodity servers. Experimental results on a real advertising dataset in Google are used to demonstrate SCD's cost effectiveness and scalability. Using Google's internal cloud, we show that SCD can provide near linear scaling using thousands of cores for 1 trillion training examples on a petabyte of compressed data. This represents 10,000x more training examples than the 'large-scale' Netflix prize dataset. We also show that SCD can learn a model for 20 billion training examples in two hours for about $10.",2016,Knowledge Discovery and Data Mining,Fields of study: coordinate descentlinear regressiondata miningmachine learningsimulationcomputer science
Scalable Time-Decaying Adaptive Prediction Algorithm,Yinyan Tan (Huawei)Zhe Fan (Huawei)Guilin Li (Huawei)Fangshan Wang (Huawei)Zhengbing Li (Huawei)Shikai Liu (Huawei)Qiuling Pan (Huawei)Eric P. Xing (Carnegie Mellon University)Qirong Ho (Carnegie Mellon University),"2628843760,2530753879,2507703984,2646546277,2116727413,2510477028,2515210992,351197510,2716009146","Online learning is used in a wide range of real applications, e.g., predicting ad click-through rates (CTR) and personalized recommendations. Based on the analysis of users' behaviors in Video-On-Demand (VoD) recommender systems,we discover that the most recent users' actions can better reflect users' current intentions and preferences. Under this observation, we thereby propose a novel time-decaying online learning algorithm derived from the state-of-the-art FTRL-proximal algorithm, called Time-Decaying Adaptive Prediction (TDAP) algorithm. To scale Big Data, we further parallelize our algorithm following the data parallel scheme under both BSP and SSP consistency model. We experimentally evaluate our TDAP algorithm on real IPTV VoD datasets using two state-of-the-art distributed computing platforms, TDAP achieves good accuracy: it improves at least 5.6% in terms of prediction accuracy, compared to FTRL-proximal algorithm; and TDAP scales well: it runs 4 times faster when the number of machines increases from 2 to 10.",2016,Knowledge Discovery and Data Mining,Fields of study: theoretical computer sciencedata miningmachine learningsimulationcomputer science
Unified Point-of-Interest Recommendation with Temporal Interval Assessment,Yanchi Liu (Rutgers–Newark)Chuanren Liu (Drexel University)Bin Liu (Rutgers–Newark)Meng Qu (Rutgers–Newark)Hui Xiong (Rutgers–Newark),"2159798580,2169554947,2428181972,2223048303,2153710278","Point-of-interest (POI) recommendation, which helps mobile users explore new places, has become an important location-based service. Existing approaches for POI recommendation have been mainly focused on exploiting the information about user preferences, social influence, and geographical influence. However, these approaches cannot handle the scenario where users are expecting to have POI recommendation for a specific time period. To this end, in this paper, we propose a unified recommender system, named the 'Where and When to gO' (WWO) recommender system, to integrate the user interests and their evolving sequential preferences with temporal interval assessment. As a result, the WWO system can make recommendations dynamically for a specific time period and the traditional POI recommender system can be treated as the special case of the WWO system by setting this time period long enough. Specifically, to quantify users' sequential preferences, we consider the distributions of the temporal intervals between dependent POIs in the historical check-in sequences. Then, to estimate the distributions with only sparse observations, we develop the low-rank graph construction model, which identifies a set of bi-weighted graph bases so as to learn the static user preferences and the dynamic sequential preferences in a coherent way. Finally, we evaluate the proposed approach using real-world data sets from several location-based social networks (LBSNs). The experimental results show that our method outperforms the state-of-the-art approaches for POI recommendation in terms of various metrics, such as F-measure and NDCG, with a significant margin.",2016,Knowledge Discovery and Data Mining,Fields of study: world wide webinformation retrievaldata miningcomputer science
Subjectively Interesting Component Analysis: Data Projections that Contrast with Prior Expectations,Bo Kang (Ghent University)Jefrey Lijffijt (Ghent University)Raúl Santos-Rodríguez (University of Bristol)Tijl De Bie (Ghent University),"2497045425,2509450284,2109855641,2522659473","Methods that find insightful low-dimensional projections are essential to effectively explore high-dimensional data. Principal Component Analysis is used pervasively to find low-dimensional projections, not only because it is straightforward to use, but it is also often effective, because the variance in data is often dominated by relevant structure. However, even if the projections highlight real structure in the data, not all structure is interesting to every user. If a user is already aware of, or not interested in the dominant structure, Principal Component Analysis is less effective for finding interesting components. We introduce a new method called Subjectively Interesting Component Analysis (SICA), designed to find data projections that are subjectively interesting , i.e, projections that truly surprise the end-user. It is rooted in information theory and employs an explicit model of a user's prior expectations about the data. The corresponding optimization problem is a simple eigenvalue problem, and the result is a trade-off between explained variance and novelty. We present five case studies on synthetic data, images, time-series, and spatial data, to illustrate how SICA enables users to find (subjectively) interesting projections.",2016,Knowledge Discovery and Data Mining,Fields of study: dimensionality reductioninformation theorydata miningpattern recognitionmachine learningstatisticscomputer sciencemathematics
Big Data Needs Big Dreamers: Lessons from Successful Big Data Investors,Evangelos Simoudis (IBM)Mark GorenbergTim GuleriMatt OckoGreg Sands,"4129615,2179178191,2508595895,2515246776,2652982569",-,2016,Knowledge Discovery and Data Mining,Fields of study: computer science
Boosted Decision Tree Regression Adjustment for Variance Reduction in Online Controlled Experiments,Alexey Poyarkov (Yandex)Alexey Drutsa (Yandex)Andrey Khalyavin (Yandex)Gleb Gusev (Yandex)Pavel Serdyukov (Yandex),"2563653331,2229408502,2565024728,2005728791,2130450538","Nowadays, the development of most leading web services is controlled by online experiments that qualify and quantify the steady stream of their updates achieving more than a thousand concurrent experiments per day. Despite the increasing need for running more experiments, these services are limited in their user traffic. This situation leads to the problem of finding a new or improving existing key performance metric with a higher sensitivity and lower variance. We focus on the problem of variance reduction for engagement metrics of user loyalty that are widely used in A/B testing of web services. We develop a general framework that is based on evaluation of the mean difference between the actual and the approximated values of the key performance metric (instead of the mean of this metric). On the one hand, it allows us to incorporate the state-of-the-art techniques widely used in randomized experiments of clinical and social research, but limitedly used in online evaluation. On the other hand, we propose a new class of methods based on advanced machine learning algorithms, including ensembles of decision trees, that, to the best of our knowledge, have not been applied earlier to the problem of variance reduction. We validate the variance reduction approaches on a very large set of real large-scale A/B experiments run at Yandex for different engagement metrics of user loyalty. Our best approach demonstrates $63\%$ average variance reduction (which is equivalent to 63% saved user traffic) and detects the treatment effect in $2$ times more A/B experiments.",2016,Knowledge Discovery and Data Mining,Fields of study: variance reductionpredictiondata miningmachine learningsimulationstatisticscomputer science
PTE: Enumerating Trillion Triangles On Distributed Systems,Ha-Myung Park (KAIST)Sung-Hyon Myaeng (KAIST)U. Kang (Seoul National University),"2229610034,2062572430,2708234210","How can we enumerate triangles from an enormous graph with billions of vertices and edges? Triangle enumeration is an important task for graph data analysis with many applications including identifying suspicious users in social networks, detecting web spams, finding communities, etc. However, recent networks are so large that most of the previous algorithms fail to process them. Recently, several MapReduce algorithms have been proposed to address such large networks; however, they suffer from the massive shuffled data resulting in a very long processing time. In this paper, we propose PTE (Pre-partitioned Triangle Enumeration), a new distributed algorithm for enumerating triangles in enormous graphs by resolving the structural inefficiency of the previous MapReduce algorithms. PTE enumerates trillions of triangles in a billion scale graph by decreasing three factors: the amount of shuffled data, total work, and network read. Experimental results show that PTE provides up to 47 times faster performance than recent distributed algorithms on real world graphs, and succeeds in enumerating more than 3 trillion triangles on the ClueWeb12 graph with 6.3 billion vertices and 72 billion edges, which any previous triangle computation algorithm fail to process.",2016,Knowledge Discovery and Data Mining,Fields of study: network analysisbig datadistributed algorithmtheoretical computer sciencecombinatoricsdata miningartificial intelligencemachine learningalgorithmcomputer science
Predicting Socio-Economic Indicators using News Events,Sunandan Chakraborty (New York University)Ashwin Venkataraman (New York University)Srikanth Jagabathula (New York University)Lakshminarayanan Subramanian (New York University),"2168126670,2114574273,233778557,2102680016","Many socio-economic indicators are sensitive to real-world events. Proper characterization of the events can help to identify the relevant events that drive fluctuations in these indicators. In this paper, we propose a novel generative model of real-world events and employ it to extract events from a large corpus of news articles. We introduce the notion of an event class, which is an abstract grouping of similarly themed events. These event classes are manifested in news articles in the form of event triggers which are specific words that describe the actions or incidents reported in any article. We use the extracted events to predict fluctuations in different socio-economic indicators. Specifically, we focus on food prices and predict the price of 12 different crops based on real-world events that potentially influence food price volatility, such as transport strikes, festivals etc. Our experiments demonstrate that incorporating event information in the prediction tasks reduces the root mean square error (RMSE) of prediction by 22% compared to the standard ARIMA model. We also predict sudden increases in the food prices (i.e. spikes) using events as features, and achieve an average 5-10% increase in accuracy compared to baseline models, including an LDA topic-model based predictive model.",2016,Knowledge Discovery and Data Mining,Fields of study: news analyticsdata sciencedata mining
Mining Subgroups with Exceptional Transition Behavior,Florian Lemmerich (Leibniz Association)Martin Becker (University of Würzburg)Philipp Singer (Leibniz Association)Denis Helic (Graz University of Technology)Andreas Hotho (University of Würzburg)Markus Strohmaier (Leibniz Association),"156522191,2343475332,2167599249,315966907,20543882,142799918","We present a new method for detecting interpretable subgroups with exceptional transition behavior in sequential data. Identifying such patterns has many potential applications, e.g., for studying human mobility or analyzing the behavior of internet users. To tackle this task, we employ exceptional model mining, which is a general approach for identifying interpretable data subsets that exhibit unusual interactions between a set of target attributes with respect to a certain model class. Although exceptional model mining provides a well-suited framework for our problem, previously investigated model classes cannot capture transition behavior. To that end, we introduce first-order Markov chains as a novel model class for exceptional model mining and present a new interestingness measure that quantifies the exceptionality of transition subgroups. The measure compares the distance between the Markov transition matrix of a subgroup and the respective matrix of the entire data with the distance of random dataset samples. In addition, our method can be adapted to find subgroups that match or contradict given transition hypotheses. We demonstrate that our method is consistently able to recover subgroups with exceptional transition models from synthetic data and illustrate its potential in two application examples. Our work is relevant for researchers and practitioners interested in detecting exceptional transition behavior in sequential data.",2016,Knowledge Discovery and Data Mining,Fields of study: markov chaincombinatoricsdata miningartificial intelligencestatisticsmathematics
Keeping it Short and Simple: Summarising Complex Event Sequences with Multivariate Patterns,Roel Bertens (Utrecht University)Jilles Vreeken (Max Planck Society)Arno Siebes (Utrecht University),"2116895963,1971070670,1988376837","We study how to obtain concise descriptions of discrete multivariate sequential data. In particular, how to do so in terms of rich multivariate sequential patterns that can capture potentially highly interesting (cor)relations between sequences. To this end we allow our pattern language to span over the domains (alphabets) of all sequences, allow patterns to overlap temporally, as well as allow for gaps in their occurrences. We formalise our goal by the Minimum Description Length principle, by which our objective is to discover the set of patterns that provides the most succinct description of the data. To discover high-quality pattern sets directly from data, we introduce Ditto, a highly efficient algorithm that approximates the ideal result very well. Experiments show that Ditto correctly discovers the patterns planted in synthetic data. Moreover, it scales favourably with the length of the data, the number of attributes, the alphabet sizes. On real data, ranging from sensor networks to annotated text, Ditto discovers easily interpretable summaries that provide clear insight in both the univariate and multivariate structure.",2016,Knowledge Discovery and Data Mining,Fields of study: data miningpattern recognitionmachine learningstatisticsalgorithmmathematics
Lossless Separation of Web Pages into Layout Code and Data,Adi Omari (Technion – Israel Institute of Technology)Benny Kimelfeld (Technion – Israel Institute of Technology)Eran Yahav (Technion – Israel Institute of Technology)Sharon Shoham (Tel Aviv University),"2480507036,2676503258,2194672974,2144636783","A modern web page is often served by running layout code on data, producing an HTML document that enhances the data with front/back matters and layout/style operations. In this paper, we consider the opposite task: separating a given web page into a data component and a layout program. This separation has various important applications: page encoding may be significantly more compact (reducing web traffic), data representation is normalized across web designs (facilitating wrapping, retrieval and extraction), and repetitions are diminished (expediting site updates and redesign). We present a framework for defining the separation task, and devise an algorithm for synthesizing layout code from a web page while distilling its data in a lossless manner. The main idea is to synthesize layout code hierarchically for parts of the page, and use a combined program-data representation cost to decide whether to align intermediate programs. When intermediate programs are aligned, they are transformed into a single program, possibly with loops and conditionals. At the same time, differences between the aligned programs are captured by the data component such that executing the layout code on the data results in the original page. We have implemented our approach and conducted a thorough experimental study of its effectiveness. Our experiments show that our approach features state of the art (and higher) performance in both size compression and record extraction.",2016,Knowledge Discovery and Data Mining,Fields of study: comprehensive layoutlossless compressiontheoretical computer sciencedata miningdatabasemachine learningcomputer science
Diversified Temporal Subgraph Pattern Mining,Yi Yang (Fudan University)Da Yan (The Chinese University of Hong Kong)Huanhuan Wu (The Chinese University of Hong Kong)James Cheng (The Chinese University of Hong Kong)Shuigeng Zhou (Fudan University)John C.S. Lui (The Chinese University of Hong Kong),"2662026395,2146437384,2165269316,2304873892,2157277864,2045404162","Many graphs in real-world applications, such as telecommunications networks, social-interaction graphs and co-authorship graphs, contain temporal information. However, existing graph mining algorithms fail to exploit these temporal information and the resulting subgraph patterns do not contain any temporal attribute. In this paper, we study the problem of mining a set of diversified temporal subgraph patterns from a temporal graph, where each subgraph is associated with the time interval that the pattern spans. This problem motivates important applications such as finding social trends in social networks, or detecting temporal hotspots in telecommunications networks. We propose a divide-and-conquer algorithm along with effective pruning techniques, and our approach runs 2 to 3 orders of magnitude faster than a baseline algorithm and obtains high-quality temporal subgraph patterns in real temporal graphs.",2016,Knowledge Discovery and Data Mining,Fields of study: degeneracydistance hereditary graphinduced subgraph isomorphism problemsubgraph isomorphism problemcombinatoricsdata miningmachine learningmathematics
Reconstructing an Epidemic Over Time,Polina Rozenshtein (Aalto University)Aristides Gionis (Aalto University)B. Aditya Prakash (Virginia Tech)Jilles Vreeken (Saarland University),"285511381,737311942,2124002246,1971070670","We consider the problem of reconstructing an epidemic over time, or, more general, reconstructing the propagation of an activity in a network. Our input consists of a temporal network , which contains information about when two nodes interacted, and a sample of nodes that have been reported as infected. The goal is to recover the flow of the spread, including discovering the starting nodes, and identifying other likely-infected nodes that are not reported. The problem we consider has multiple applications, from public health to social media and viral marketing purposes. Previous work explicitly factor-in many unrealistic assumptions: it is assumed that (a) the underlying network does not change;(b) we have access to perfect noise-free data; or (c) we know the exact propagation model. In contrast, we avoid these simplifications: we take into account the temporal network, we require only a small sample of reported infections, and we do not make any restrictive assumptions about the propagation model. We develop CulT, a scalable and effective algorithm to reconstruct epidemics that is also suited for online settings. CulT works by formulating the problem as that of a temporal Steiner-tree computation, for which we design a fast algorithm leveraging the specific problem structure. We demonstrate the efficacy of the proposed approach through extensive experiments on diverse datasets.",2016,Knowledge Discovery and Data Mining,Fields of study: approximation algorithmtheoretical computer sciencedata miningartificial intelligencemachine learningstatisticscomputer sciencemathematics
Bayesian Optimization and Embedded Learning Systems,Jeff Schneider (Carnegie Mellon University),2165347984,"An important property of embedded learning systems is the ever-changing environment they create for all algorithms operating in the system. Optimizing the performance of those algorithms becomes a perpetual on-line activity rather than a one-off task. I will review some of these challenges in autonomous vehicles. I will discuss Bayesian optimization methods and their application in robotics and scientific applications, focusing on scaling up the dimensionality and managing multi-fidelity evaluations. I will finish with lessons learned and thoughts on future directions as these methods move into embedded systems.",2016,Knowledge Discovery and Data Mining,Fields of study: roboticsartificial intelligencemachine learningsimulationcomputer science
A Text Clustering Algorithm Using an Online Clustering Scheme for Initialization,Jianhua Yin (Tsinghua University)Jianyong Wang (Tsinghua University),"2145412373,2105625159","In this paper, we propose a text clustering algorithm using an online clustering scheme for initialization called FGSDMM+. FGSDMM+ assumes that there are at most K max clusters in the corpus, and regards these K max potential clusters as one large potential cluster at the beginning. During initialization, FGSDMM+ processes the documents one by one in an online clustering scheme. The first document will choose the potential cluster, and FGSDMM+ will create a new cluster to store this document. Later documents will choose one of the non-empty clusters or the potential cluster with probabilities derived from the Dirichlet multinomial mixture model. Each time a document chooses the potential cluster, FGSDMM+ will create a new cluster to store that document and decrease the probability of later documents choosing the potential cluster. After initialization, FGSDMM+ will run a collapsed Gibbs sampling algorithm several times to obtain the final clustering result. Our extensive experimental study shows that FGSDMM+ can achieve better performance than three other clustering methods on both short and long text datasets.",2016,Knowledge Discovery and Data Mining,Fields of study: k medians clusteringflame clusteringcanopy clustering algorithmcomplete linkage clusteringcorrelation clusteringconstrained clusteringdata stream clusteringcure data clustering algorithmsingle linkage clusteringfuzzy clusteringclustering high dimensional datagibbs samplingcluster analysisdocument clusteringdata miningpattern recognitionmachine learningcomputer science
Regime Shifts in Streams: Real-time Forecasting of Co-evolving Time Sequences,Yasuko Matsubara (Kumamoto University)Yasushi Sakurai (Kumamoto University),"2106416586,1989929707","Given a large, online stream of multiple co-evolving event sequences, such as sensor data and Web-click logs, that contains various types of non-linear dynamic evolving patterns of different durations, how can we efficiently and effectively capture important patterns? How do we go about forecasting long-term future events? In this paper, we present REGIMECAST, an efficient and effective method for forecasting co-evolving data streams. REGIMECAST is designed as an adaptive non-linear dynamical system, which is inspired by the concept of ""regime shifts"" in natural dynamical systems. Our method has the following properties: (a) Effective: it operates on large data streams, captures important patterns and performs long-term forecasting; (b) Adaptive: it automatically and incrementally recognizes the latent trends and dynamic evolution patterns (i.e., regimes) that are unknown in advance; (c) Scalable: it is fast and the computation cost does not depend on the length of data streams; (d) Any-time: it provides a response at any time and generates long-range future events. Extensive experiments on real datasets demonstrate that REGIMECAST does indeed make long-range forecasts, and it outperforms state-of-the-art competitors as regards accuracy and speed.",2016,Knowledge Discovery and Data Mining,Fields of study: time seriesdata miningreal time computingsimulationstatisticscomputer science
MANTRA: A Scalable Approach to Mining Temporally Anomalous Sub-trajectories,Prithu Banerjee (University of British Columbia)Pranali Yawalkar (Indian Institute of Technology Madras)Sayan Ranu (Indian Institute of Technology Madras),"2185325292,2703137274,2096541091","In this paper, we study the problem of mining temporally anomalous sub-trajectory patterns from an input trajectory in a scalable manner. Given the prevailing road conditions, a sub-trajectory is temporally anomalous if its travel time deviates significantly from the expected time. Mining these patterns requires us to delve into the sub-trajectory space, which is not scalable for real-time analytics. To overcome this scalability challenge, we design a technique called MANTRA . We study the properties unique to anomalous sub-trajectories and utilize them in MANTRA to iteratively refine the search space into a disjoint set of sub-trajectory islands . The expensive enumeration of all possible sub-trajectories is performed only on the islands to compute the answer set of maximal anomalous sub-trajectories . Extensive experiments on both real and synthetic datasets establish MANTRA as more than 3 orders of magnitude faster than baseline techniques. Moreover, through trajectory classification and segmentation, we demonstrate that the proposed model conforms to human intuition.",2016,Knowledge Discovery and Data Mining,Fields of study: global positioning systemdata miningartificial intelligencesimulationcomputer science
Fast Component Pursuit for Large-Scale Inverse Covariance Estimation,Lei Han (Rutgers University)Yu Zhang (Hong Kong University of Science and Technology)Tong Zhang (Rutgers University),"2531318857,2648094648,2510858842","The maximum likelihood estimation (MLE) for the Gaussian graphical model, which is also known as the inverse covariance estimation problem, has gained increasing interest recently. Most existing works assume that inverse covariance estimators contain sparse structure and then construct models with the l 1 regularization. In this paper, different from existing works, we study the inverse covariance estimation problem from another perspective by efficiently modeling the low-rank structure in the inverse covariance, which is assumed to be a combination of a low-rank part and a diagonal matrix. One motivation for this assumption is that the low-rank structure is common in many applications including the climate and financial analysis, and another one is that such assumption can reduce the computational complexity when computing its inverse. Specifically, we propose an efficient COmponent Pursuit (COP) method to obtain the low-rank part, where each component can be sparse. For optimization, the COP method greedily learns a rank-one component in each iteration by maximizing the log-likelihood. Moreover, the COP algorithm enjoys several appealing properties including the existence of an efficient solution in each iteration and the theoretical guarantee on the convergence of this greedy approach. Experiments on large-scale synthetic and real-world datasets including thousands of millions variables show that the COP method is faster than the state-of-the-art techniques for the inverse covariance estimation problem when achieving comparable log-likelihood on test data.",2016,Knowledge Discovery and Data Mining,Fields of study: covariance intersectioncovariance functioncovariancegreedy algorithmestimation of covariance matriceseconometricsmachine learningmathematical optimizationstatisticscomputer sciencemathematics
TRIÈST: Counting Local and Global Triangles in Fully-Dynamic Streams with Fixed Memory Size,Lorenzo De Stefani (Brown University)Alessandro Epasto (Google)Matteo Riondato (Brown University)Eli Upfal (Brown University),"2487262625,2303398780,1555209364,2685185700","We present TRIEST, a suite of one-pass streaming algorithms to compute unbiased, low-variance, high-quality approximations of the global and local (i.e., incident to each vertex) number of triangles in a fully-dynamic graph represented as an adversarial stream of edge insertions and deletions. Our algorithms use reservoir sampling and its variants to exploit the user-specified memory space at all times. This is in contrast with previous approaches, which require hard-to-choose parameters (e.g., a fixed sampling probability) and offer no guarantees on the amount of memory they use. We analyze the variance of the estimations and show novel concentration bounds for these quantities. Our experimental results on very large graphs demonstrate that TRIEST outperforms state-of-the-art approaches in accuracy and exhibits a small update time.",2016,Knowledge Discovery and Data Mining,Fields of study: graph enumerationcycle countreservoir samplingprobabilistic analysis of algorithmssocial networkdata stream miningtheoretical computer sciencediscrete mathematicscombinatoricsmathematical optimizationstatisticsalgorithmcomputer sciencemathematics
Online Context-Aware Recommendation with Time Varying Multi-Armed Bandit,Chunqiu Zeng (Florida International University)Qing Wang (Florida International University)Shekoofeh Mokhtari (Florida International University)Tao Li (Florida International University),"2155150411,2564420227,2515803648,2472069284","Contextual multi-armed bandit problems have gained increasing popularity and attention in recent years due to their capability of leveraging contextual information to deliver online personalized recommendation services (e.g., online advertising and news article selection). To predict the reward of each arm given a particular context, existing relevant research studies for contextual multi-armed bandit problems often assume the existence of a fixed yet unknown reward mapping function. However, this assumption rarely holds in practice, since real-world problems often involve underlying processes that are dynamically evolving over time. In this paper, we study the time varying contextual multi-armed problem where the reward mapping function changes over time. In particular, we propose a dynamical context drift model based on particle learning. In the proposed model, the drift on the reward mapping function is explicitly modeled as a set of random walk particles, where good fitted particles are selected to learn the mapping dynamically. Taking advantage of the fully adaptive inference strategy of particle learning, our model is able to effectively capture the context change and learn the latent parameters. In addition, those learnt parameters can be naturally integrated into existing multi-arm selection strategies such as LinUCB and Thompson sampling . Empirical studies on two real-world applications, including online personalized advertising and news recommendation, demonstrate the effectiveness of our proposed approach. The experimental results also show that our algorithm can dynamically track the changing reward over time and consequently improve the click-through rate.",2016,Knowledge Discovery and Data Mining,Fields of study: personalizationrecommender systemdata miningartificial intelligencemachine learningsimulationcomputer science
Identifying Decision Makers from Professional Social Networks,Shipeng Yu (LinkedIn)Evangelia Christakopoulou (University of Minnesota)Abhishek Gupta (LinkedIn),"2723179103,77291933,2101578767","Sales professionals help organizations win clients for products and services. Generating new clients starts with identifying the right decision makers at the target organization. For the past decade, online professional networks have collected tremendous amount of data on people's identity, their network and behavior data of buyers and sellers building relationships with each other for a variety of use-cases. Sales professionals are increasingly relying on these networks to research, identify and reach out to potential prospects, but it is often hard to find the right people effectively and efficiently. In this paper we present LDMS, the LinkedIn Decision Maker Score, to quantify the ability of making a sales decision for each of the 400M+ LinkedIn members. It is the key data-driven technology underlying Sales Navigator, a proprietary LinkedIn product that is designed for sales professionals. We will specifically discuss the modeling challenges of LDMS, and present two graph-based approaches to tackle this problem by leveraging the professional network data at LinkedIn. Both approaches are able to leverage both the graph information and the contextual information on the vertices, deal with small amount of labels on the graph, and handle heterogeneous graphs among different types of vertices. We will show some offline evaluations of LDMS on historical data, and also discuss its online usage in multiple applications in live production systems as well as future use cases within the LinkedIn ecosystem.",2016,Knowledge Discovery and Data Mining,Fields of study: management sciencedata miningmachine learning
Predictors without Borders: Behavioral Modeling of Product Adoption in Three Developing Countries,Muhammad Raza Khan (University of Washington)Joshua E. Blumenstock (University of Washington),"2529948456,2501829080","Billions of people around the world live without access to banks or other formal financial institutions. In the past several years, many mobile operators have launched ""Mobile Money"" platforms that deliver basic financial services over the mobile phone network. While many believe that these services can improve the lives of the poor, in many countries adoption of Mobile Money still remains anemic. In this paper, we develop a predictive model of Mobile Money adoption that uses billions of mobile phone communications records to understand the behavioral determinants of adoption. We describe a novel approach to feature engineering that uses a Deterministic Finite Automaton to construct thousands of behavioral metrics of phone use from a concise set of recursive rules. These features provide the foundation for a predictive model that is tested on mobile phone operators logs from Ghana, Pakistan, and Zambia, three very different developing-country contexts. The results highlight the key correlates of Mobile Money use in each country, as well as the potential for such methods to predict and drive adoption. More generally, our analysis provides insight into the extent to which homogenized supervised learning methods can generalize across geographic contexts. We find that without careful tuning, a model that performs very well in one country frequently does not generalize to another.",2016,Knowledge Discovery and Data Mining,Fields of study: gradient boostingmobile paymentfeaturesupervised learningdata miningmachine learningsimulationcomputer science
Latent Space Model for Road Networks to Predict Time-Varying Traffic,Dingxiong Deng (University of Southern California)Cyrus Shahabi (University of Southern California)Ugur Demiryurek (University of Southern California)Linhong Zhu (Information Sciences Institute)Rose Yu (University of Southern California)Yan Liu (University of Southern California),"2109638660,240820708,319304351,2114458094,2107161032,2240541904","Real-time traffic prediction from high-fidelity spatiotemporal traffic sensor datasets is an important problem for intelligent transportation systems and sustainability. However, it is challenging due to the complex topological dependencies and high dynamism associated with changing road conditions. In this paper, we propose a Latent Space Model for Road Networks (LSM-RN) to address these challenges holistically. In particular, given a series of road network snapshots, we learn the attributes of vertices in latent spaces which capture both topological and temporal properties. As these latent attributes are time-dependent, they can estimate how traffic patterns form and evolve. In addition, we present an incremental online algorithm which sequentially and adaptively learns the latent attributes from the temporal graph changes. Our framework enables real-time traffic prediction by 1) exploiting real-time sensor readings to adjust/update the existing latent spaces, and 2) training as data arrives and making predictions on-the-fly. By conducting extensive experiments with a large volume of real-world traffic sensor data, we demonstrate the superiority of our framework for real-time traffic prediction on large road networks over competitors as well as baseline graph-based LSM's.",2016,Knowledge Discovery and Data Mining,Fields of study: data miningmachine learningsimulation
Sampling of Attributed Networks from Hierarchical Generative Models,Pablo Robles (Purdue University)Sebastian Moreno (Adolfo Ibáñez University)Jennifer Neville (Purdue University),"2480430435,2104332529,2124572662","Network sampling is a widely used procedure in social network analysis where a random network is sampled from a generative network model (GNM). Recently proposed GNMs, allow generation of networks with more realistic structural characteristics than earlier ones. This facilitates tasks such as hypothesis testing and sensitivity analysis. However, sampling of networks with correlated vertex attributes remains a challenging problem. While the recent work of \cite{Pfeiffer:14} has provided a promising approach for attributed-network sampling, the approach was developed for use with relatively simple GNMs and does not work well with more complex hierarchical GNMs (which can model the range of characteristics and variation observed in real world networks more accurately). In contrast to simple GNMs where the probability mass is spread throughout the space of edges more evenly, hierarchical GNMs concentrate the mass to smaller regions of the space to reflect dependencies among edges in the network---this produces more realistic network characteristics, but also makes it more difficult to identify candidate networks from the sampling space. In this paper, we propose a novel sampling method, CSAG, to sample from hierarchical GNMs and generate networks with correlated attributes. CSAG constrains every step of the sampling process to consider the structure of the GNM---in order to bias the search to regions of the space with higher likelihood. We implemented CSAG using mixed Kronecker Product Graph Models and evaluated our approach on three real-world datasets. The results show that CSAG jointly models the correlation and structure of the networks better than the state of the art. Specifically, CSAG maintains the variability of the underlying GNM while providing a ≥ 5X reduction in attribute correlation error.",2016,Knowledge Discovery and Data Mining,Fields of study: complex networkdata miningmachine learningstatisticscomputer sciencemathematics
Graph Wavelets via Sparse Cuts,"Arlei Lopes da Silva (University of California, Santa Barbara)Xuan-Hong Dang (University of California, Santa Barbara)Prithwish Basu (BBN Technologies)Ambuj Singh (University of California, Santa Barbara)Ananthram Swami (United States Army Research Laboratory)","2166555088,1964285635,2111459030,2099219664,2059211748","Modeling information that resides on vertices of large graphs is a key problem in several real-life applications, ranging from social networks to the Internet-of-things. Signal Processing on Graphs and, in particular, graph wavelets can exploit the intrinsic smoothness of these datasets in order to represent them in a compact and accurate manner. However, how to discover wavelet bases that capture the geometry of the data with respect to the signal as well as the graph structure remains an open problem. In this paper, we study the problem of computing graph wavelet bases via sparse cuts in order to produce low-dimensional encodings of data-driven bases. This problem is connected to known hard problems in graph theory (e.g. multiway cuts) and thus requires an efficient heuristic. We formulate the basis discovery task as a relaxation of a vector optimization problem, which leads to an elegant solution as a regularized eigenvalue computation. Moreover, we propose several strategies in order to scale our algorithm to large graphs. Experimental results show that the proposed algorithm can effectively encode both the graph structure and signal, producing compressed and accurate representations for vertex values in a wide range of datasets (e.g. sensor and gene networks) and significantly outperforming the best baseline.",2016,Knowledge Discovery and Data Mining,Fields of study: graph cuts in computer visiondistance hereditary graphstrength of a graphvoltage graphcomplement graphgraph bandwidthgraph powercomparability graphlattice graphlevel structurenull graphclique widthgraph propertymodular decompositiongraphspectral theorywaveletcombinatoricsmachine learningmathematical optimizationmathematics
Multi-Task Feature Interaction Learning,Kaixiang Lin (Michigan State University)Jianpeng Xu (Michigan State University)Inci M. Baytas (Michigan State University)Shuiwang Ji (Washington State University)Jiayu Zhou (Michigan State University),"2644838670,2228111156,2520407735,2149659377,2629643112","One major limitation of linear models is the lack of capability to capture predictive information from interactions between features. While introducing high-order feature interaction terms can overcome this limitation, this approach tremendously increases the model complexity and imposes significant challenges in the learning against overfitting. In this paper, we proposed a novel Multi-Task feature Interaction Learning~(MTIL) framework to exploit the task relatedness from high-order feature interactions, which provides better generalization performance by inductive transfer among tasks via shared representations of feature interactions. We formulate two concrete approaches under this framework and provide efficient algorithms: the shared interaction approach and the embedded interaction approach. The former assumes tasks share the same set of interactions, and the latter assumes feature interactions from multiple tasks come from a shared subspace. We have provided efficient algorithms for solving the two approaches. Extensive empirical studies on both synthetic and real datasets have demonstrated the effectiveness of the proposed framework.",2016,Knowledge Discovery and Data Mining,Fields of study: multi task learningdata miningpattern recognitionmachine learningcomputer science
Domain Adaptation in the Absence of Source Domain Data,Boris Chidlovskii (Xerox)Stephane Clinchant (Xerox)Gabriela Csurka (Xerox),"36378520,288365595,2124675631","The overwhelming majority of existing domain adaptation methods makes an assumption of freely available source domain data. An equal access to both source and target data makes it possible to measure the discrepancy between their distributions and to build representations common to both target and source domains. In reality, such a simplifying assumption rarely holds, since source data are routinely a subject of legal and contractual constraints between data owners and data customers. When source domain data can not be accessed, decision making procedures are often available for adaptation nevertheless. These procedures are often presented in the form of classification, identification, ranking etc. rules trained on source data and made ready for a direct deployment and later reuse. In other cases, the owner of a source data is allowed to share a few representative examples such as class means. In this paper we address the domain adaptation problem in real world applications, where the reuse of source domain data is limited to classification rules or a few representative examples. We extend the recent techniques of feature corruption and their marginalization , both in supervised and unsupervised settings. We test and compare them on private and publicly available source datasets and show that significant performance gains can be achieved despite the absence of source data and shortage of labeled target data.",2016,Knowledge Discovery and Data Mining,Fields of study: social exclusionbiological classificationdata sciencedata miningmachine learningstatisticscomputer science
Generalized Hierarchical Sparse Model for Arbitrary-Order Interactive Antigenic Sites Identification in Flu Virus Data,Lei Han (Rutgers University)Yu Zhang (Hong Kong University of Science and Technology)Xiu-Feng Wan (Mississippi State University)Tong Zhang (Rutgers University),"2531318857,2648094648,2718011926,2510858842","Recent statistical evidence has shown that a regression model by incorporating the interactions among the original covariates (features) can significantly improve the interpretability for biological data. One major challenge is the exponentially expanded feature space when adding high-order feature interactions to the model. To tackle the huge dimensionality, Hierarchical Sparse Models (HSM) are developed by enforcing sparsity under heredity structures in the interactions among the covariates. However, existing methods only consider pairwise interactions, making the discovery of important high-order interactions a non-trivial open problem. In this paper, we propose a Generalized Hierarchical Sparse Model (GHSM) as a generalization of the HSM models to learn arbitrary-order interactions. The GHSM applies the l 1 penalty to all the model coefficients under a constraint that given any covariate, if none of its associated k th-order interactions contribute to the regression model, then neither do its associated higher-order interactions. The resulting objective function is non-convex with a challenge lying in the coupled variables appearing in the arbitrary-order hierarchical constraints and we devise an efficient optimization algorithm to directly solve it. Specifically, we decouple the variables in the constraints via both the GIST and ADMM methods into three subproblems, each of which is proved to admit an efficiently analytical solution. We evaluate the GHSM method in both synthetic problem and the antigenic sites identification problem for the flu virus data, where we expand the feature space up to the 5th-order interactions. Empirical results demonstrate the effectiveness and efficiency of the proposed method and the learned high-order interactions have meaningful synergistic covariate patterns in the virus antigenicity.",2016,Knowledge Discovery and Data Mining,Fields of study: econometricsdata miningmachine learningstatisticsmathematics
EMBERS at 4 years: Experiences operating an Open Source Indicators Forecasting System,"Sathappan Muthiah (Virginia Tech)Patrick Butler (Virginia Tech)Rupinder Paul Khandpur (Virginia Tech)Parang Saraf (Virginia Tech)Nathan Self (Virginia Tech)Alla Rozovskaya (Virginia Tech)Liang Zhao (Virginia Tech)Jose Cadena (Virginia Tech)Chang-Tien Lu (Virginia Tech)Anil Vullikanti (Virginia Tech)Achla Marathe (Virginia Tech)Kristen Maria Summers (IBM)Graham Katz (CACI)Andy Doyle (CACI)Jaime Arredondo (University of California, San Diego)Dipak K. Gupta (San Diego State University)David Mares (University of California, San Diego)Naren Ramakrishnan (Virginia Tech)","1509108369,2293049679,2054069009,2160730744,2093704914,2518199447,2619584304,2153027628,2112878203,393896382,2111312453,2175894886,2104271048,2238455168,2135686031,2142889206,2224701853,2199255697","EMBERS is an anticipatory intelligence system forecasting population-level events in multiple countries of Latin America. A deployed system from 2012, EMBERS has been generating alerts 24x7 by ingesting a broad range of data sources including news, blogs, tweets, machine coded events,currency rates, and food prices. In this paper, we describe our experiences operating EMBERS continuously for nearly 4 years, with specific attention to the discoveries it has enabled, correct as well as missed forecasts, lessons learnt from participating in a forecasting tournament, and our perspectives on the limits of forecasting including ethical considerations.",2016,Knowledge Discovery and Data Mining,Fields of study: operations researchworld wide webtelecommunicationsmanagementcomputer securitydata miningartificial intelligencesimulation
From Online Behaviors to Offline Retailing,Ping Luo (Chinese Academy of Sciences)Su Yan (Chinese Academy of Sciences)Zhiqiang Liu (Baidu)Zhiyong Shen (Baidu)Shengwen Yang (Baidu)Qing He (Chinese Academy of Sciences),"2291210646,2599389837,2514157585,2096063433,2652284171,2167314737","To combat the ease of online shopping in pajamas, offline mall owners focus increasingly on driving satisfaction and improving retention by identifying customers' preferences. However, most of these studies are based on customers' offline consuming history only. Benefiting from the internet, we can also get customers' online behaviors, such as the search logs, web browsing logs, online shopping logs, and so on. Might these seemingly irrelevant information from two different modalities (i.e. online and offline) be somehow interrelated? How can we make use of the online behaviors and offline actions jointly to promote recommendation for offline retailing? In this study, we formulate this task as a cross-modality recommendation problem, and present its solution via a proposed probabilistic graphical model, called Online-to-Offline Topic Modeling (O2OTM). Specifically, this method explicitly models the relationships between online and offline topics so that the likelihood of both online and offline behaviors is maximized. Then, the recommendation is made only based on the pairs of online and offline topics, denoted by (t,l), with high values of lift , such that the existence of the online topic $t$ greatly increases the response on the corresponding offline topic $l$ compared with the average response for the population without the online topic t. Furthermore, we evaluate this solution in both live and retrospect experiments. The real-world deployment of this model for the anniversary promotion campaign of a famous shopping mall in Beijing shows that our approach increases the occurred customer purchases per promotion message by 29.75\% compared with the baseline. Also, our model finds some interesting interpretable relationships between the online search topics and offline brand topics.",2016,Knowledge Discovery and Data Mining,Fields of study: topic modelworld wide webdata miningmachine learningcomputer science
Audience Expansion for Online Social Network Advertising,Haishan Liu (LinkedIn)David Pardoe (LinkedIn)Kun Liu (LinkedIn)Manoj Thakur (LinkedIn)Frank Cao (LinkedIn)Chongzhe Li (LinkedIn),"2720207618,2512113778,2636583453,2531597640,2531042109,2531212888","Online social network advertising platforms, such as that provided by LinkedIn, generally allow marketers to specify targeting options so that their ads appear to a desired demographic. Audience Expansion is a technique developed at LinkedIn to simplify targeting and identify new audiences with similar attributes to the original target audience. We developed two methods to achieve Audience Expansion: campaign-agnostic expansion and campaign-aware expansion. In this paper, we describe the details of these methods, present in-depth analysis of their trade-offs, and demonstrate a hybrid strategy that possesses the combined strength of both methods. Through large scale online experiments, we show the effectiveness of the proposed approach, and as a result, the benefits it brings to the whole marketplace including both LinkedIn and advertisers. The achieved benefits can be characterized as: 1) simplified targeting process and increased reach for advertisers, and 2) better utilization of LinkedIn's ads inventory and higher and more efficient market participation.",2016,Knowledge Discovery and Data Mining,Fields of study: online advertisingcomputer science
Efficient Processing of Network Proximity Queries via Chebyshev Acceleration,Mustafa Coskun (Case Western Reserve University)Ananth Grama (Purdue University)Mehmet Koyuturk (Case Western Reserve University),"2280576955,245679212,128763826","Network proximity is at the heart of a large class of network analytics and information retrieval techniques, including node/ edge rankings, network alignment, and randomwalk based proximity queries, among many others. Owing to its importance, significant effort has been devoted to accelerating iterative processes underlying network proximity computations. These techniques rely on numerical properties of power iterations, as well as structural properties of the networks to reduce the run time of iterative algorithms. In this paper, we present an alternate approach to acceleration of network proximity queries using Chebyshev polynomials. We show that our approach, called CHOPPER, yields asymptotically faster convergence in theory, and significantly reduced convergence times in practice. We also show that other existing acceleration techniques can be used in conjunction with Chopper to further reduce runtime. Using a number of large real-world networks, and top- k proximity queries as the benchmark problem, we show that CHOPPER outperforms existing methods for wide ranges of parameter values. CHOPPER is implemented in Matlab and is freely available at http://compbio.case.edu/chopper/.",2016,Knowledge Discovery and Data Mining,Fields of study: chebyshev polynomialstheoretical computer sciencedata miningreal time computingmachine learningmathematical optimizationstatisticscomputer science
Safe Pattern Pruning: An Efficient Approach for Predictive Pattern Mining,Kazuya Nakagawa (Nagoya Institute of Technology)Shinya Suzumura (Nagoya Institute of Technology)Masayuki Karasuyama (Nagoya Institute of Technology)Koji Tsuda (University of Tokyo)Ichiro Takeuchi (Nagoya Institute of Technology),"2280734645,2090618807,1256560114,1858955830,2619145315","In this paper we study predictive pattern mining problems where the goal is to construct a predictive model based on a subset of predictive patterns in the database. Our main contribution is to introduce a novel method called safe pattern pruning (SPP ) for a class of predictive pattern mining problems. The SPP method allows us to efficiently find a superset of all the predictive patterns in the database that are needed for the optimal predictive model. The advantage of the SPP method over existing boosting-type method is that the former can find the superset by a single search over the database, while the latter requires multiple searches. The SPP method is inspired by recent development of safe feature screening. In order to extend the idea of safe feature screening into predictive pattern mining, we derive a novel pruning rule called safe pattern pruning (SPP) rule that can be used for searching over the tree defined among patterns in the database. The SPP rule has a property that, if a node corresponding to a pattern in the database is pruned out by the SPP rule, then it is guaranteed that all the patterns corresponding to its descendant nodes are never needed for the optimal predictive model. We apply the SPP method to graph mining and item-set mining problems, and demonstrate its computational advantage.",2016,Knowledge Discovery and Data Mining,Fields of study: convex optimizationdata miningpattern recognitionmachine learningcomputer sciencemathematics
Efficient Frequent Directions Algorithm for Sparse Matrices,Mina Ghashami (University of Utah)Edo Liberty (Yahoo!)Jeff M. Phillips (University of Utah),"2010898942,1215165747,2162012049","This paper describes Sparse Frequent Directions, a variant of Frequent Directions for sketching sparse matrices. It resembles the original algorithm in many ways: both receive the rows of an input matrix A n x d one by one in the streaming setting and compute a small sketch B ∈ R l x d . Both share the same strong (provably optimal) asymptotic guarantees with respect to the space-accuracy tradeoff in the streaming setting. However, unlike Frequent Directions which runs in O ( ndl ) time regardless of the sparsity of the input matrix A , Sparse Frequent Directions runs in O (nnz( A ) l + nl 2 ) time. Our analysis loosens the dependence on computing the Singular Value Decomposition (SVD) as a black box within the Frequent Directions algorithm. Our bounds require recent results on the properties of fast approximate SVD computations. Finally, we empirically demonstrate that these asymptotic improvements are practical and significant on real and synthetic data.",2016,Knowledge Discovery and Data Mining,Fields of study: sparse matrixtheoretical computer sciencecombinatoricsmathematical optimizationalgorithmcomputer sciencemathematics
"Leveraging Propagation for Data Mining: Models, Algorithms and Applications",B. Aditya Prakash (Virginia Tech)Naren Ramakrishnan (Virginia Tech),"2124002246,2199255697","Can we infer if a user is sick from her tweet? How do opinions get formed in online forums? Which people should we immunize to prevent an epidemic as fast as possible? How do we quickly zoom out of a graph? Graphs---also known as networks---are powerful tools for modeling processes and situations of interest in real life domains of social systems, cyber-security, epidemiology, and biology. They are ubiquitous, from online social networks, gene-regulatory networks, to router graphs. This tutorial will cover recent and state-of-the-art research on how propagation-like processes can help big-data mining specifically involving large networks and time-series, algorithms behind network problems, and their practical applications in various diverse settings. Topics include diffusion and virus propagation in networks, anomaly and outbreak detection, event prediction and connections with work in public health, the web and online media, social sciences, humanities, and cyber-security.",2016,Knowledge Discovery and Data Mining,Fields of study: social mediadiffusionpublic healthdata sciencedata miningmachine learningsimulationcomputer science
Rebalancing Bike Sharing Systems: A Multi-source Data Smart Optimization,Junming Liu (Rutgers–Newark)Leilei Sun (Dalian University of Technology)Weiwei Chen (Rutgers–Newark)Hui Xiong (Rutgers–Newark),"2226988312,2098357117,2669133569,2153710278","Bike sharing systems, aiming at providing the missing links in public transportation systems, are becoming popular in urban cities. A key to success for a bike sharing systems is the effectiveness of rebalancing operations, that is, the efforts of restoring the number of bikes in each station to its target value by routing vehicles through pick-up and drop-off operations. There are two major issues for this bike rebalancing problem: the determination of station inventory target level and the large scale multiple capacitated vehicle routing optimization with outlier stations. The key challenges include demand prediction accuracy for inventory target level determination, and an effective optimizer for vehicle routing with hundreds of stations. To this end, in this paper, we develop a Meteorology Similarity Weighted K-Nearest-Neighbor (MSWK) regressor to predict the station pick-up demand based on large-scale historic trip records. Based on further analysis on the station network constructed by station-station connections and the trip duration, we propose an inter station bike transition (ISBT) model to predict the station drop-off demand. Then, we provide a mixed integer nonlinear programming (MINLP) formulation of multiple capacitated bike routing problem with the objective of minimizing total travel distance. To solve it, we propose an Adaptive Capacity Constrained K-centers Clustering (AdaCCKC) algorithm to separate outlier stations (the demands of these stations are very large and make the optimization infeasible) and group the rest stations into clusters within which one vehicle is scheduled to redistribute bikes between stations. In this way, the large scale multiple vehicle routing problem is reduced to inner cluster one vehicle routing problem with guaranteed feasible solutions. Finally, the extensive experimental results on the NYC Citi Bike system show the advantages of our approach for bike demand prediction and large-scale bike rebalancing optimization.",2016,Knowledge Discovery and Data Mining,Fields of study: cluster analysismachine learningsimulationcomputer science
Improving the Sensitivity of Online Controlled Experiments: Case Studies at Netflix,Huizhi Xie (Netflix)Juliette Aurisset (Netflix),"2521220520,2225021794","Controlled experiments are widely regarded as the most scientific way to establish a true causal relationship between product changes and their impact on business metrics. Many technology companies rely on such experiments as their main data-driven decision-making tool. The sensitivity of a controlled experiment refers to its ability to detect differences in business metrics due to product changes. At Netflix, with tens of millions of users, increasing the sensitivity of controlled experiments is critical as failure to detect a small effect, either positive or negative, can have a substantial revenue impact. This paper focuses on methods to increase sensitivity by reducing the sampling variance of business metrics. We define Netflix business metrics and share context around the critical need for improved sensitivity. We review popular variance reduction techniques that are broadly applicable to any type of controlled experiment and metric. We describe an innovative implementation of stratified sampling at Netflix where users are assigned to experiments in real time and discuss some surprising challenges with the implementation. We conduct case studies to compare these variance reduction techniques on a few Netflix datasets. Based on the empirical results, we recommend to use post-assignment variance reduction techniques such as post stratification and CUPED instead of at-assignment variance reduction techniques such as stratified sampling in large-scale controlled experiments.",2016,Knowledge Discovery and Data Mining,Fields of study: randomized experimentvariance reductionsensitivityeconometricsdata miningsimulationstatistics
IoT Big Data Stream Mining,Gianmarco De Francisci Morales (Qatar Computing Research Institute)Albert Bifet (Université Paris-Saclay)Latifur Khan (University of Texas at Dallas)Joao Gama (University of Porto)Wei Fan (Baidu),"2153118160,307521372,2155983610,2113857198,2422054197","The challenge of deriving insights from the Internet of Things (IoT) has been recognized as one of the most exciting and key opportunities for both academia and industry. Advanced analysis of big data streams from sensors and devices is bound to become a key area of data mining research as the number of applications requiring such processing increases. Dealing with the evolution over time of such data streams, i.e., with concepts that drift or change completely, is one of the core issues in IoT stream mining. This tutorial is a gentle introduction to mining IoT big data streams. The first part introduces data stream learners for classification, regression, clustering, and frequent pattern mining. The second part deals with scalability issues inherent in IoT applications, and discusses how to mine data streams on distributed engines such as Spark, Flink, Storm, and Samza.",2016,Knowledge Discovery and Data Mining,Fields of study: internet of thingsdata stream miningbig datadata scienceworld wide webdata miningcomputer science
A Real Linear and Parallel Multiple Longest Common Subsequences (MLCS) Algorithm,Yanni Li (Xidian University)Hui Li (Xidian University)Tihua Duan (Shanghai Finance University)Sheng WangZhi Wang (Xidian University)Yang Cheng (Xidian University),"2668838690,2617926336,2512054100,2623250925,2512744947,2632713803","Information in various applications is often expressed as character sequences over a finite alphabet ( e.g. , DNA or protein sequences). In Big Data era, the lengths and sizes of these sequences are growing explosively, leading to grand challenges for the classical NP-hard problem, namely searching for the Multiple Longest Common Subsequences ( MLCS ) from multiple sequences. In this paper, we first unveil the fact that the state-of-the-art MLCS algorithms are unable to be applied to long and large-scale sequences alignments. To overcome their defects and tackle the longer and large-scale or even big sequences alignments, based on the proposed novel problem-solving model and various strategies, e.g. , parallel topological sorting, optimal calculating, reuse of intermediate results, subsection calculation and serialization, etc., we present a novel parallel MLCS algorithm. Exhaustive experiments on the datasets of both synthetic and real-world biological sequences demonstrate that both the time and space of the proposed algorithm are only linear in the number of dominants from aligned sequences, and the proposed algorithm significantly outperforms the state-of-the-art MLCS algorithms, being applicable to longer and large-scale sequences alignments.",2016,Knowledge Discovery and Data Mining,Fields of study: topological sortingtheoretical computer sciencecombinatoricsalgorithmcomputer sciencemathematics
Distributing the Stochastic Gradient Sampler for Large-Scale LDA,Yuan Yang (Beihang University)Jianfei Chen (Tsinghua University)Jun Zhu (Tsinghua University),"2435984540,2694226565,2305755055","Learning large-scale Latent Dirichlet Allocation (LDA) models is beneficial for many applications that involve large collections of documents.Recent work has been focusing on developing distributed algorithms in the batch setting, while leaving stochastic methods behind, which can effectively explore statistical redundancy in big data and thereby are complementary to distributed computing.The distributed stochastic gradient Langevin dynamics (DSGLD) represents one attempt to combine stochastic sampling and distributed computing, but it suffers from drawbacks such as excessive communications and sensitivity to partitioning of datasets across nodes. DSGLD is typically limited to learn small models that have about 10 3 topics and $10^3$ vocabulary size. In this paper, we present embarrassingly parallel SGLD (EPSGLD), a novel distributed stochastic gradient sampling method for topic models. Our sampler is built upon a divide-and-conquer architecture which enables us to produce robust and asymptotically exact samples with less communication overhead than DSGLD. We further propose several techniques to reduce the overhead in I/O and memory usage. Experiments on Wikipedia and ClueWeb12 documents demonstrate that, EPSGLD can scale up to large models with 10 10 parameters (i.e., 10 5 topics, 10 5 vocabulary size), four orders of magnitude larger than DSGLD, and converge faster.",2016,Knowledge Discovery and Data Mining,Fields of study: theoretical computer sciencedata miningmachine learningstatisticscomputer science
Just One More: Modeling Binge Watching Behavior,William Trouleau (École Polytechnique Fédérale de Lausanne)Azin Ashkan (Technicolor)Weicong Ding (Technicolor)Brian Eriksson (Technicolor),"2513603576,2075598693,2674195122,2147262100","Easy accessibility can often lead to over-consumption, as seen in food and alcohol habits. On video on-demand (VOD) services, this has recently been referred to as binge watching, where potentially entire seasons of TV shows are consumed in a single viewing session. While a user viewership model may reveal this binging behavior, creating an accurate model has several challenges, including censored data, deviations in the population, and the need to consider external influences on consumption habits. In this paper, we introduce a novel statistical mixture model that incorporates these factors and presents a first of its kind characterization of viewer consumption behavior using a real-world dataset that includes playback data from a VOD service. From our modeling, we tackle various predictive tasks to infer the consumption decisions of a user in a viewing session, including estimating the number of episodes they watch and classifying if they continue watching another episode. Using these insights, we then identify binge watching sessions based on deviation from normal viewing behavior. We observe different types of binging behavior, that binge watchers often view certain content out-of-order, and that binge watching is not a consistent behavior among our users. These insights and our findings have application in VOD revenue generation, consumer health applications, and customer retention analysis.",2016,Knowledge Discovery and Data Mining,Fields of study: mixture modelmultimediamachine learningsimulationcomputer science
How to Get Them a Dream Job?: Entity-Aware Features for Personalized Job Search Ranking,Jia Li (University of Illinois at Chicago)Dhruv Arya (LinkedIn)Viet Ha-Thuc (LinkedIn)Shakti Sinha (LinkedIn),"2710729114,2231240302,2310324677,2155323900","This paper proposes an approach to applying standardized entity data to improve job search quality and to make search results more personalized. Specifically, we explore three types of entity-aware features and incorporate them into the job search ranking function. The first is query-job matching features which extract and standardize entities mentioned in queries and documents, then semantically match them based on these entities. The second type, searcher-job expertise homophily, aims to capture the fact that job searchers tend to be interested in the jobs requiring similar expertise as theirs. To measure the similarity, we use standardized skills in job descriptions and searchers' profiles as well as skills that we infer searchers might have but not explicitly list in their profiles. Third, we propose a concept of entity-faceted historical click-through-rates (CTRs) to capture job document quality. Faceting jobs by their standardized companies, titles, locations, etc., and computing historical CTRs at the facet level instead of individual job level alleviate sparseness issue in historical action data. This is particularly important in job search where job lifetime is typically short. Both offline and online experiments confirm the effectiveness of the features. In offline experiment, using the entity-aware features gives improvements of +20%, +12.1% and +8.3% on Precision@1, MRR and NDCG@25, respectively. Online A/B test shows that a new model with these features is +11.3% and +5.3% better than the baseline in terms of click-through-rate and apply rate.",2016,Knowledge Discovery and Data Mining,Fields of study: job analysispersonalizationworld wide webinformation retrievaldata miningmachine learningcomputer science
Scalable Learning of Graphical Models,Francois Petitjean (Monash University)Geoffrey I. Webb (Monash University),"2709762332,2126304162","From understanding the structure of data, to classification and topic modeling, graphical models are core tools in machine learning and data mining. They combine probability and graph theories to form a compact representation of probability distributions. In the last decade, as data stores became larger and higher-dimensional, traditional algorithms for learning graphical models from data, with their lack of scalability, became less and less usable, thus directly decreasing the potential benefits of this core technology. To scale graphical modeling techniques to the size and dimensionality of most modern data stores, data science researchers and practitioners now have to meld the most recent advances in numerous specialized fields including graph theory, statistics, pattern mining and graphical modeling. This tutorial covers the core building blocks that are necessary to build and use scalable graphical modeling technologies on large and high-dimensional data.",2016,Knowledge Discovery and Data Mining,Fields of study: clustering high dimensional datagraphical modeldata sciencetheoretical computer sciencedata miningmachine learningstatisticscomputer science
Ranking Universities Based on Career Outcomes of Graduates,Navneet KapurNikita I. Lytkin (LinkedIn)Bee-Chung Chen (LinkedIn)Deepak Agarwal (LinkedIn)Igor Perisic (LinkedIn),"2592458651,2530163099,2152441490,2116605949,2234655699","Every year, millions of new students enter higher educational programs. Publicly available rankings of academic programs play a key role in prospective students' decisions regarding which universities to apply to and enroll in. While surveys indicate that majority of freshmen enter college to get good jobs after graduation, established methodologies for ranking universities rely on indirect indicators of career outcomes such as reputational assessments of the universities among academic peers, acceptance and graduation rates, learning environment, and availability of research funding. In addition, many of these methodologies rely on arbitrary choices of weighting factors for the different ranking indicators, and suffer from lack of analyses of statistical stability. In this paper, we addresses these challenges holistically by developing a novel methodology for ranking and recommending universities for different professions on the basis of career outcomes of professionals who graduated from those schools. Our methodology incorporates a number of techniques for achieving statistical stability, and represents a step towards personalized educational recommendations based on interests and ambitions of individuals. We have applied this methodology on LinkedIn's Economic Graph data of over 400 million professional from around the world. The resulting university rankings have been made available to the public and demonstrate that there are valuable insights to be gleaned from professional career data on LinkedIn.",2016,Knowledge Discovery and Data Mining,Fields of study: statistics
Privacy-preserving Class Ratio Estimation,Arun Shankar Iyer (Indian Institute of Technology Bombay)J. Saketha Nath (Indian Institute of Technology Bombay)Sunita Sarawagi (Indian Institute of Technology Bombay),"2128888966,2137193185,156875573","In this paper we present learning models for the class ratio estimation problem, which takes as input an unlabeled set of instances and predicts the proportions of instances in the set belonging to the different classes. This problem has applications in social and commercial data analysis. Existing models for class-ratio estimation however require instance-level supervision. Whereas in domains like politics, and demography, set-level supervision is more common. We present a new method for directly estimating class-ratios using set-level supervision. Another serious limitation in applying these techniques to sensitive domains like health is data privacy. We propose a novel label privacy-preserving mechanism that is well-suited for supervised class ratio estimation and has guarantees for achieving efficient differential privacy, provided the per-class counts are large enough. We derive learning bounds for the estimation with and without privacy constraints, which lead to important insights for the data-publisher. Extensive empirical evaluation shows that our model is more accurate than existing methods and that the proposed privacy mechanism and learning model are well-suited for each other.",2016,Knowledge Discovery and Data Mining,Fields of study: differential privacykernel methodlearning theoryeconometricsdata miningmachine learningstatisticscomputer sciencemathematics
Lightweight Monitoring of Distributed Streams,Arnon Lazerson (Technion – Israel Institute of Technology)Daniel Keren (University of Haifa)Assaf Schuster (Technion – Israel Institute of Technology),"2288078023,2061587065,2135728993","As data becomes dynamic, large, and distributed, there is increasing demand for what have become known as distributed stream algorithms . Since continuously collecting the data to a central server and processing it there incurs very high communication and computation complexities, it is advantageous to define local conditions at the nodes, such that -- as long as they are maintained -- some desirable global condition holds. A generic algorithm which proved very useful for reducing communication in distributed streaming environments is geometric monitoring (GM). Alas, applying GM to many important tasks is computationally very demanding, as it requires solving a notoriously difficult problem -- computing the distance between a point and a surface, which is often very time-consuming even in low dimensions. Thus, while useful for reducing communication, GM often suffers from exceedingly heavy computational burden at the nodes, which renders it very problematic to apply, especially for ``thin'', battery-operated sensors, which are prevalent in numerous applications, including the ``Internet of Things'' paradigm. Here we propose a very different approach, designated CB (for Convex/Concave Bounds). CB is based on directly bounding the monitored function by suitably chosen convex and concave functions, that naturally enable monitoring distributed streams. These functions can be checked on the fly, yielding far simpler local conditions than those applied by GM. CB's superiority over GM is demonstrated in reducing computational complexity, by several orders of magnitude in some cases. As an added bonus, CB also reduced communication overhead in all application scenarios we tested.",2016,Knowledge Discovery and Data Mining,Fields of study: distributed computingdata miningreal time computingmachine learningsimulationcomputer science
QUINT: On Query-Specific Optimal Networks,Liangyue Li (Arizona State University)Yuan Yao (Nanjing University)Jie Tang (Tsinghua University)Wei Fan (Baidu)Hanghang Tong (Arizona State University),"2144246580,2617797049,2158012360,2422054197,2676574370","Measuring node proximity on large scale networks is a fundamental building block in many application domains, ranging from computer vision, e-commerce, social networks, software engineering, disaster management to biology and epidemiology. The state of the art (e.g., random walk based methods) typically assumes the input network is given a priori, with the known network topology and the associated edge weights. A few recent works aim to further infer the optimal edge weights based on the side information. This paper generalizes the challenge in multiple dimensions, aiming to learn optimal networks for node proximity measures. First ( optimization scope ), our proposed formulation explores a much larger parameter space, so that it is able to simultaneously infer the optimal network topology and the associated edge weights. This is important as a noisy or missing edge could greatly mislead the network node proximity measures. Second ( optimization granularity ), while all the existing works assume one common optimal network, be it given as the input or learned by the algorithms, exists for all queries, our method performs optimization at a much finer granularity, essentially being able to infer an optimal network that is specific to a given query. Third ( optimization efficiency ), we carefully design our algorithms with a linear complexity wrt the neighborhood size of the user preference set. We perform extensive empirical evaluations on a diverse set of 10+ real networks, which show that the proposed algorithms (1) consistently outperform the existing methods on all six commonly used metrics; (2) empirically scale sub-linearly to billion-scale networks and (3) respond in a fraction of a second.",2016,Knowledge Discovery and Data Mining,Fields of study: data miningmachine learningmathematical optimizationstatisticscomputer sciencemathematics
GLMix: Generalized Linear Mixed Models For Large-Scale Response Prediction,XianXing Zhang (LinkedIn)Yitong Zhou (LinkedIn)Yiming Ma (LinkedIn)Bee-Chung Chen (LinkedIn)Liang Zhang (LinkedIn)Deepak Agarwal (LinkedIn),"2550559646,2294316712,2237551533,2152441490,2306956169,2116605949","Generalized linear model (GLM) is a widely used class of models for statistical inference and response prediction problems. For instance, in order to recommend relevant content to a user or optimize for revenue, many web companies use logistic regression models to predict the probability of the user's clicking on an item (e.g., ad, news article, job). In scenarios where the data is abundant, having a more fine-grained model at the user or item level would potentially lead to more accurate prediction, as the user's personal preferences on items and the item's specific attraction for users can be better captured. One common approach is to introduce ID-level regression coefficients in addition to the global regression coefficients in a GLM setting, and such models are called generalized linear mixed models (GLMix) in the statistical literature. However, for big data sets with a large number of ID-level coefficients, fitting a GLMix model can be computationally challenging. In this paper, we report how we successfully overcame the scalability bottleneck by applying parallelized block coordinate descent under the Bulk Synchronous Parallel (BSP) paradigm. We deployed the model in the LinkedIn job recommender system, and generated 20% to 40% more job applications for job seekers on LinkedIn.",2016,Knowledge Discovery and Data Mining,Fields of study: sparkstatistical modelbig datadata sciencedata miningmachine learningstatisticscomputer science
Online Asymmetric Active Learning with Imbalanced Data,Xiaoxuan Zhang (University of Iowa)Tianbao Yang (University of Iowa)Padmini Srinivasan (University of Iowa),"2508508812,2151859571,2237621063","This paper considers online learning with imbalanced streaming data under a query budget, where the act of querying for labels is constrained to a budget limit. We study different active querying strategies for classification. In particular, we propose an asymmetric active querying strategy that assigns different probabilities for query to examples predicted as positive and negative. To corroborate the proposed asymmetric query model, we provide a theoretical analysis on a weighted mistake bound. We conduct extensive evaluations of the proposed asymmetric active querying strategy in comparison with several baseline querying strategies and with previous online learning algorithms for imbalanced data. In particular, we perform two types of evaluations according to which examples appear as ``positive""/``negative''. In push evaluation only the positive predictions given to the user are taken into account; in push and query evaluation the decision to query is also considered for evaluation. The push and query evaluation strategy is particularly suited for a recommendation setting because the items selected for querying for labels may go to the end-user to enable customization and personalization. These would not be shown any differently to the end-user compared to recommended content (i.e., the examples predicated as positive). Additionally, given our interest in imbalanced data we measure F -score instead of accuracy that is traditionally considered by online classification algorithms. We also compare the querying strategies on five classification tasks from different domains, and show that the probabilistic query strategy achieves higher F -scores on both types of evaluation than deterministic strategy, especially when the budget is small, and the asymmetric query model further improves performance. When compared to the state-of-the-art cost-sensitive online learning algorithm under a budget, our online classification algorithm with asymmetric querying achieves a higher F -score on four of the five tasks, especially on the push evaluation.",2016,Knowledge Discovery and Data Mining,Fields of study: f1 scoreweb query classificationinformation retrievaldata miningmachine learningcomputer science
Skinny-dip: Clustering in a Sea of Noise,Samuel Maurus (Technische Universität München)Claudia Plant (University of Vienna),"1989291604,2590664453","Can we find heterogeneous clusters hidden in data sets with 80% noise? Although such settings occur in the real-world, we struggle to find methods from the abundance of clustering techniques that perform well with noise at this level. Indeed, perhaps this is enough of a departure from classical clustering to warrant its study as a separate problem. In this paper we present SkinnyDip which, based on Hartigan's elegant dip test of unimodality, represents an intriguing approach to clustering with an attractive set of properties. Specifically, SkinnyDip is highly noise-robust, practically parameter-free and completely deterministic. SkinnyDip never performs multivariate distance calculations, but rather employs insightful recursion based on ""dips"" into univariate projections of the data. It is able to detect a range of cluster shapes and densities, assuming only that each cluster admits a unimodal shape. Practically, its run-time grows linearly with the data. Finally, for high-dimensional data, continuity properties of the dip enable SkinnyDip to exploit multimodal projection pursuit in order to find an appropriate basis for clustering. Although not without its limitations, SkinnyDip compares favorably to a variety of clustering approaches on synthetic and real data, particularly in high-noise settings.",2016,Knowledge Discovery and Data Mining,Fields of study: flame clusteringbrown clusteringcanopy clustering algorithmcorrelation clusteringconstrained clusteringcure data clustering algorithmaffinity propagationfuzzy clusteringclustering high dimensional datacluster analysisconsensus clusteringdata miningmachine learningstatisticscomputer sciencemathematics
Email Volume Optimization at LinkedIn,Rupesh Gupta (LinkedIn)Guanfeng Liang (LinkedIn)Hsiao-Ping Tseng (LinkedIn)Ravi Kiran Holur Vijay (LinkedIn)Xiaoyu Chen (LinkedIn)Romer Rosales (LinkedIn),"2299393997,2709775982,2224987450,2528836707,2509170365,2117656073","Online social networking services distribute various types of messages to their members. Common types of messages include news, connection requests, membership notifications, promotions and event notifications. Such communication, if used judiciously, can provide an enormous value to members thereby keeping them engaged. However sending a message for every instance of news, connection request, or the like can result in an overwhelming number of messages in a member's mailbox. This may result in reduced effectiveness of communication if the messages are not sufficiently relevant to the member's interests. It may also result in a poor brand perception of the networking service. In this paper we discuss our strategy and experience with regard to the problem of email volume optimization at LinkedIn. In particular, we present a cost-benefit analysis of sending emails, the key factors to administer an effective volume optimization, our algorithm for volume optimization, the architecture of the supporting system and experimental results from online A/B tests.",2016,Knowledge Discovery and Data Mining,Fields of study: internet privacymultimediaworld wide webdata miningmachine learningcomputer science
FASCINATE: Fast Cross-Layer Dependency Inference on Multi-layered Networks,Chen Chen (Arizona State University)Hanghang Tong (Arizona State University)Lei Xie (City University of New York)Lei Ying (Arizona State University)Qing He (University at Buffalo),"2311808467,2667261544,2580828074,2686501891,2251990706","Multi-layered networks have recently emerged as a new network model, which naturally finds itself in many high-impact application domains, ranging from critical inter-dependent infrastructure networks, biological systems, organization-level collaborations, to cross-platform e-commerce, etc. Cross-layer dependency, which describes the dependencies or the associations between nodes across different layers/networks, often plays a central role in many data mining tasks on such multi-layered networks. Yet, it remains a daunting task to accurately know the cross-layer dependency a prior. In this paper, we address the problem of inferring the missing cross-layer dependencies on multi-layered networks. The key idea behind our method is to view it as a collective collaborative filtering problem. By formulating the problem into a regularized optimization model, we propose an effective algorithm to find the local optima with linear complexity. Furthermore, we derive an online algorithm to accommodate newly arrived nodes, whose complexity is just linear wrt the size of the neighborhood of the new node. We perform extensive empirical evaluations to demonstrate the effectiveness and the efficiency of the proposed methods.",2016,Knowledge Discovery and Data Mining,Fields of study: dependency theorydistributed computingdata miningmachine learningcomputer science
Text Mining in Clinical Domain: Dealing with Noise,Hoang Nguyen (Commonwealth Scientific and Industrial Research Organisation)Jon Patrick (University of Sydney),"2697665893,2630747136","Text mining in clinical domain is usually more difficult than general domains (e.g. newswire reports and scientific literature) because of the high level of noise in both the corpus and training data for machine learning (ML). A large number of unknown word, non-word and poor grammatical sentences made up the noise in the clinical corpus. Unknown words are usually complex medical vocabularies, misspellings, acronyms and abbreviations where unknown non-words are generally the clinical patterns including scores and measures. This noise produces obstacles in the initial lexical processing step as well as subsequent semantic analysis. Furthermore, the labelled data used to build ML models is very costly to obtain because it requires intensive clinical knowledge from the annotators. And even created by experts, the training examples usually contain errors and inconsistencies due to the variations in human annotators' attentiveness. Clinical domain also suffers from the nature of the imbalanced data distribution problem. These kinds of noise are very popular and potentially affect the overall information extraction performance but they were not carefully investigated in most presented health informatics systems. This paper introduces a general clinical data mining architecture which is potential of addressing all of these challenges using: automatic proof-reading process, trainable finite state pattern recogniser, iterative model development and active learning. The reportability classifier based on this architecture achieved 98.25% sensitivity and 96.14% specificity on an Australian cancer registry's held-out test set and up to 92% of training data provided for supervised ML was saved by active learning.",2016,Knowledge Discovery and Data Mining,Fields of study: active learningnatural language processingspeech recognitiondata miningmachine learningcomputer science
FLASH: Fast Bayesian Optimization for Data Analytic Pipelines,Yuyu Zhang (Georgia Institute of Technology)Mohammad Taha Bahadori (Georgia Institute of Technology)Hang Su (Georgia Institute of Technology)Jimeng Sun (Georgia Institute of Technology),"2525312675,2032867848,2627558300,2110385854","Modern data science relies on data analytic pipelines to organize interdependent computational steps. Such analytic pipelines often involve different algorithms across multiple steps, each with its own hyperparameters. To achieve the best performance, it is often critical to select optimal algorithms and to set appropriate hyperparameters, which requires large computational efforts. Bayesian optimization provides a principled way for searching optimal hyperparameters for a single algorithm. However, many challenges remain in solving pipeline optimization problems with high-dimensional and highly conditional search space. In this work, we propose Fast LineAr SearcH (FLASH), an efficient method for tuning analytic pipelines. FLASH is a two-layer Bayesian optimization framework, which firstly uses a parametric model to select promising algorithms, then computes a nonparametric model to fine-tune hyperparameters of the promising algorithms. FLASH also includes an effective caching algorithm which can further accelerate the search process. Extensive experiments on a number of benchmark datasets have demonstrated that FLASH significantly outperforms previous state-of-the-art methods in both search speed and accuracy. Using 50% of the time budget, FLASH achieves up to 20% improvement on test error rate compared to the baselines. FLASH also yields state-of-the-art performance on a real-world application for healthcare predictive modeling.",2016,Knowledge Discovery and Data Mining,Fields of study: data sciencedata miningmachine learningstatisticscomputer science
Learning Sparse Models at Scale,Ralf Herbrich (Amazon.com),2227627541,"Recently, learning deep models from dense data has received a lot of attention in tasks such as object recognition and signal processing. However, when dealing with non-sensory data about real-world entities, data is often sparse; for example people interaction with products in e-Commerce, people interacting with each other in social networks or word sequences in natural language. In this talk, I will share lessons learned over the past 10 years when learning predictive models based on sparse data: 1) how to scale the inference algorithms to distributed data setting, 2) how to automate the learning process by reducing the amount of hyper-parameters to zero, 3) how to deal with Zipf distributions when learning resource-constrained models, and 4) how to combine dense and sparse-learning algorithms. The talk will be drawing from many real-world experiences I gathered over the past decade in applications of the techniques in gaming, search, advertising and recommendations of systems developed at Microsoft, Facebook and Amazon.",2016,Knowledge Discovery and Data Mining,Fields of study: zipf s lawdata sciencedata miningmachine learningstatisticscomputer science
NetCycle: Collective Evolution Inference in Heterogeneous Information Networks,Yizhou Zhang (Fudan University)Yun Xiong (Fudan University)Xiangnan Kong (Worcester Polytechnic Institute)Yangyong Zhu (Fudan University),"2512584869,2638681803,2204127537,2661458478","Collective inference has attracted considerable attention in the last decade, where the response variables within a group of instances are correlated and should be inferred collectively, instead of independently. Previous works on collective inference mainly focus on exploiting the autocorrelation among instances in a static network during the inference process. There are also approaches on time series prediction, which mainly exploit the autocorrelation within an instance at different time points during the inference process. However, in many real-world applications, the response variables of related instances can co-evolve over time and their evolutions are not following a static correlation across time, but are following an internal life cycle. In this paper, we study the problem of collective evolution inference, where the goal is to predict the values of the response variables for a group of related instances at the end of their life cycles. This problem is extremely important for various applications, e.g., predicting fund-raising results in crowd-funding and predicting gene-expression levels in bioinformatics. This problem is also highly challenging because different instances in the network can co-evolve over time and they can be at different stages of their life cycles and thus have different evolving patterns. Moreover, the instances in collective evolution inference problems are usually connected through heterogeneous information networks, which involve complex relationships among the instances interconnected by multiple types of links. We propose an approach, called NetCycle, by incorporating information from both the correlation among related instances and their life cycles. We compared our approach with existing methods of collective inference and time series analysis on two real-world networks. The results demonstrate that our proposed approach can improve the inference performance by considering the autocorrelation through networks and the life cycles of the instances.",2016,Knowledge Discovery and Data Mining,Fields of study: data miningartificial intelligencemachine learningstatisticscomputer sciencemathematics
Streaming Analytics,Ashish Gupta (LinkedIn)Neera Agarwal,"2512098453,2688643513","Recently we have seen emergence and huge adoption of social media, internet of things for home, industrial internet of things, mobile applications and online transactions. These systems generate streaming data at very large scale. Building technologies and distributed systems that can capture, process and analyze this streaming data in real time is very important for gaining real time insights. Real-time analysis of streaming data can be used for applications as diverse as fraud detection, in-session targeting and recommendations, control systems for transportation systems and smarter cities, earthquake prediction and control of autonomous vehicles. This tutorial will provide overview of streaming systems and hands on tutorial on building streaming analytics systems using open source technologies.",2016,Knowledge Discovery and Data Mining,Fields of study: analyticsinternet of thingsstreamdata scienceworld wide webdata miningmachine learningcomputer science
Partial Label Learning via Feature-Aware Disambiguation,Min-Ling Zhang (Southeast University)Bin-Bin Zhou (Southeast University)Xu-Ying Liu (Southeast University),"2310402581,2517136083,2635414314","Partial label learning deals with the problem where each training example is represented by a feature vector while associated with a set of candidate labels, among which only one label is valid. To learn from such ambiguous labeling information, the key is to try to disambiguate the candidate label sets of partial label training examples. Existing disambiguation strategies work by either identifying the ground-truth label iteratively or treating each candidate label equally. Nonetheless, the disambiguation process is generally conducted by focusing on manipulating the label space, and thus ignores making full use of potentially useful information from the feature space. In this paper, a novel two-stage approach is proposed to learning from partial label examples based on feature-aware disambiguation. In the first stage, the manifold structure of feature space is utilized to generate normalized labeling confidences over candidate label set. In the second stage, the predictive model is learned by performing regularized multi-output regression over the generated labeling confidences. Extensive experiments on artificial as well as real-world partial label data sets clearly validate the superiority of the proposed feature-aware disambiguation approach.",2016,Knowledge Discovery and Data Mining,Fields of study: manifolddata miningpattern recognitionmachine learningmathematics
When Recommendation Goes Wrong: Anomalous Link Discovery in Recommendation Networks,Bryan Perozzi (Stony Brook University)Michael Schueppert (Google)Jack Saalweachter (Google)Mayur Thakur (Google),"1983756286,2227686788,2513242119,2235422528","We present a secondary ranking system to find and remove erroneous suggestions from a geospatial recommendation system. We discover such anomalous links by ""double checking"" the recommendation system's output to ensure that it is both structurally cohesive, and semantically consistent. Our approach is designed for the Google Related Places Graph, a geographic recommendation system which provides results for hundreds of millions of queries a day. We model the quality of a recommendation between two geographic entities as a function of their structure in the Related Places Graph, and their semantic relationship in the Google Knowledge Graph. To evaluate our approach, we perform a large scale human evaluation of such an anomalous link detection system. For the long tail of unpopular entities, our models can predict the recommendations users will consider poor with up to 42\% higher mean precision (29 raw points) than the live system. Results from our study reveal that structural and semantic features capture different facets of relatedness to human judges. We characterize our performance with a qualitative analysis detailing the categories of real-world anomalies our system is able to detect, and provide a discussion of additional applications of our method.",2016,Knowledge Discovery and Data Mining,Fields of study: anomaly detectionworld wide webinformation retrievaldata miningmachine learningcomputer science
How to Compete Online for News Audience: Modeling Words that Attract Clicks,Joon Hee Kim (KAIST)Amin Mantrach (Yahoo!)Alejandro Jaimes (Yahoo!)Alice H. Oh (KAIST),"2492617592,2027707398,2193615068,2647629047","Headlines are particularly important for online news outlets where there are many similar news stories competing for users' attention. Traditionally, journalists have followed rules-of-thumb and experience to master the art of crafting catchy headlines, but with the valuable resource of large-scale click-through data of online news articles, we can apply quantitative analysis and text mining techniques to acquire an in-depth understanding of headlines. In this paper, we conduct a large-scale analysis and modeling of 150K news articles published over a period of four months on the Yahoo home page. We define a simple method to measure click-value of individual words, and analyze how temporal trends and linguistic attributes affect click-through rate (CTR). We then propose a novel generative model, headline click-based topic model (HCTM), that extends latent Dirichlet allocation (LDA) to reveal the effect of topical context on the click-value of words in headlines. HCTM leverages clicks in aggregate on previously published headlines to identify words for headlines that will generate more clicks in the future. We show that by jointly taking topics and clicks into account we can detect changes in user interests within topics. We evaluate HCTM in two different experimental settings and compare its performance with ALDA (adapted LDA), LDA, and TextRank. The first task, full headline, is to retrieve full headline used for a news article given the body of news article. The second task, good headline , is to specifically identify words in the headline that have high click values for current news audience. For full headline task, our model performs on par with ALDA, a state-of-the art web-page summarization method that utilizes click-through information. For good headline task, which is of more practical importance to both individual journalists and online news outlets, our model significantly outperforms all other comparative methods.",2016,Knowledge Discovery and Data Mining,Fields of study: click through ratemultimediaworld wide webdata miningcomputer science
Question Independent Grading using Machine Learning: The Case of Computer Program Grading,Gursimran SinghShashank SrikantVarun Aggarwal,"2666593215,2226147892,2608931570","Learning supervised models to grade open-ended responses is an expensive process. A model has to be trained for every prompt/question separately, which in turn requires graded samples. In automatic programming evaluation specifically, the focus of this work, this issue is amplified. The models have to be trained not only for every question but also for every language the question is offered in. Moreover, the availability and time taken by experts to create a labeled set of programs for each question is a major bottleneck in scaling such a system. We address this issue by presenting a method to grade computer programs which requires no manually assigned labeled samples for grading responses to a new, unseen question. We extend our previous work [25] wherein we introduced a grammar of features to learn question specific models. In this work, we propose a method to transform those features into a set of features that maintain their structural relation with the labels across questions. Using these features we learn one supervised model, across questions for a given language, which can then be applied to an ungraded response to an unseen question. We show that our method rivals the performance of both, question specific models and the consensus among human experts while substantially outperforming extant ways of evaluating codes. We demonstrate the system single s value by deploying it to grade programs in a high stakes assessment. The learning from this work is transferable to other grading tasks such as math question grading and also provides a new variation to the supervised learning approach.",2016,Knowledge Discovery and Data Mining,Fields of study: featuresupervised learningnatural language processingdata miningartificial intelligencemachine learningcomputer science
Evaluating Mobile Apps with A/B and Quasi A/B Tests,Ya Xu (LinkedIn)Nanyu Chen (LinkedIn),"2310280520,2170586657","We have seen an explosive growth of mobile usage, particularly on mobile apps. It is more important than ever to be able to properly evaluate mobile app release. A/B testing is a standard framework to evaluate new ideas. We have seen much of its applications in the online world across the industry [9,10,12]. Running A/B tests on mobile apps turns out to be quite different, and much of it is attributed to the fact that we cannot ship code easily to mobile apps other than going through a lengthy build, review and release process. Mobile infrastructure and user behavior differences also contribute to how A/B tests are conducted differently on mobile apps, which will be discussed in details in this paper. In addition to measuring features individually in the new app version through randomized A/B tests, we have a unique opportunity to evaluate the mobile app as a whole using the quasi-experimental framework [21]. Not all features can be A/B tested due to infrastructure changes and wholistic product redesign. We propose and establish quasi-experimental techniques for measuring impact from mobile app release, with results shared from a recent major app launch at LinkedIn.",2016,Knowledge Discovery and Data Mining,Fields of study: mobile deep linkingmobile searchquasi experimentcausal inferencemobile technologyworld wide webcomputer securitydata miningsimulationcomputer science
Towards Optimal Cardinality Estimation of Unions and Intersections with Sketches,Daniel Ting (Facebook),2230432365,Estimating the cardinality of unions and intersections of sets is a problem of interest in OLAP. Large data applications often require the use of approximate methods based on small sketches of the data. We give new estimators for the cardinality of unions and intersection and show they approximate an optimal estimation procedure. These estimators enable the improved accuracy of the streaming MinCount sketch to be exploited in distributed settings. Both theoretical and empirical results demonstrate substantial improvements over existing methods.,2016,Knowledge Discovery and Data Mining,Fields of study: randomized algorithmdiscrete mathematicscombinatoricsmathematical optimizationcomputer sciencemathematics
Ranking Relevance in Yahoo Search,Dawei Yin (Yahoo!)Yuening Hu (Yahoo!)Jiliang Tang (Yahoo!)Tim Daly (Yahoo!)Mianwei Zhou (Yahoo!)Hua Ouyang (Yahoo!)Jianhui Chen (Yahoo!)Changsung Kang (Yahoo!)Hongbo Deng (Yahoo!)Chikashi Nobata (Yahoo!)Jean-Marc Langlois (Yahoo!)Yi Chang (Yahoo!),"2170531144,2134853736,2147392410,2478637130,2528719982,2489973799,2486916609,2158414383,2682826105,2295249286,2529055905,2168000538","Search engines play a crucial role in our daily lives. Relevance is the core problem of a commercial search engine. It has attracted thousands of researchers from both academia and industry and has been studied for decades. Relevance in a modern search engine has gone far beyond text matching, and now involves tremendous challenges. The semantic gap between queries and URLs is the main barrier for improving base relevance. Clicks help provide hints to improve relevance, but unfortunately for most tail queries, the click information is too sparse, noisy, or missing entirely. For comprehensive relevance, the recency and location sensitivity of results is also critical. In this paper, we give an overview of the solutions for relevance in the Yahoo search engine. We introduce three key techniques for base relevance -- ranking functions, semantic matching features and query rewriting. We also describe solutions for recency sensitive relevance and location sensitive relevance. This work builds upon 20 years of existing efforts on Yahoo search, summarizes the most recent advances and provides a series of practical relevance solutions. The performance reported is based on Yahoo's commercial search engine, where tens of billions of urls are indexed and served by the ranking system.",2016,Knowledge Discovery and Data Mining,Fields of study: rankingsearch enginedeep learninglearning to rankworld wide webinformation retrievaldata miningmachine learningcomputer science
Data-driven Automatic Treatment Regimen Development and Recommendation,Leilei Sun (Dalian University of Technology)Chuanren Liu (Drexel University)Chonghui Guo (Dalian University of Technology)Hui Xiong (Rutgers–Newark)Yanming Xie,"2098357117,2169554947,2719303095,2153710278,2642302339","The analysis of large-scale Electrical Medical Records (EMRs) has the potential to develop and optimize clinical treatment regimens. A treatment regimen usually includes a series of doctor orders containing rich temporal and heterogeneous information. However, in many existing studies, a doctor order is simplified as an event code and a treatment record is simplified as a code sequence. Thus, the information inherent in doctor orders is not fully used for in-depth analysis. In this paper, we aim at exploiting the rich information in doctor orders and developing data-driven approaches for improving clinical treatments. To this end, we first propose a novel method to measure the similarities between treatment records with consideration of sequential and multifaceted information in doctor orders. Then, we propose an efficient density-based clustering algorithm to summarize large-scale treatment records, and extract a semantic representation of each treatment cluster. Finally, we develop a unified framework to evaluate the discovered treatment regimens, and find the most effective treatment regimen for new patients. In the empirical study, we validate our methods with EMRs of 27,678 patients from 14 hospitals. The results show that: 1) Our method can successfully extract typical treatment regimens from large-scale treatment records. The extracted treatment regimens are intuitive and provide managerial implications for treatment regimen design and optimization. 2) By recommending the most effective treatment regimens, the total cure rate in our data improves from 19.89% to 21.28%, and the effective rate increases up to 98.29%.",2016,Knowledge Discovery and Data Mining,Fields of study: data mining
Accelerating the Race to Autonomous Cars,Danny Shapiro (Nvidia),2515414339,"Every automaker is working on driver assistance systems and self-driving cars. Conventional computer vision used for ADAS is reaching its threshold because it is impossible to write code for every possible scenario as a vehicle navigates. In order to develop a truly autonomous car, deep learning and artificial intelligence are required. With deep learning, the vehicle can be trained to have super human levels of perception, driving safer than anyone on the road. An end-to-end artificial intelligence platform based on supercomputers in the cloud and in the vehicle enables cars to get smarter and smarter. Coupled with an extensive software development kit with vision and AI libraries and software modules, automakers, tier 1s, and startups can build scalable systems from ADAS to full autonomy.",2016,Knowledge Discovery and Data Mining,Fields of study: scalabilitydeep learningembedded systemartificial intelligencemachine learningsimulationcomputer science
Joint Optimization of Multiple Performance Metrics in Online Video Advertising,Sahin Cem GeyikSergey FaleevJianqiang ShenSean O'DonnellSantanu Kolay (Yahoo!),"2704095071,2630348086,2507557475,2618978856,2062334970","The field of online advertising, in essence, deals with the problem of presenting ads to online users in the most appropriate contexts to achieve a multitude of advertiser goals. A vast amount of work in online advertising has been focused on optimizing banner display advertising campaigns where the main goal lies in direct response metrics, often as clicks or conversions. In this paper, we explore the newly popularized space of online video advertising, where brand recognition is the key focus. We propose a framework based on a feedback mechanism where we optimize multiple video specific performance indicators while making sure the delivery constraints (budget and user reach) of advertisers are satisfied. While our main focus is on improving metrics such as engagement (amount of view time), and viewability (whether a campaign is within eyesight of a user), we also discuss the possibilities of expanding to other metrics. We demonstrate the benefit of our framework via empirical results in multiple real-world advertising campaigns. To the best of our knowledge, this is the first paper that deals with the unique challenges arising from the nature of online video advertising.",2016,Knowledge Discovery and Data Mining,Fields of study: share of voicenative advertisinginformative advertisingadvertising campaignonline advertisingmultimediacomputer science
Business Applications of Predictive Modeling at Scale,Qiang Zhu (LinkedIn)Songtao Guo (LinkedIn)Paul Ogilvie (LinkedIn)Yan Liu (LinkedIn),"2685659162,2700533644,2530390680,2516943954","Predictive modeling is the art of building statistical models that forecast probabilities and trends of future events. It has broad applications in industry across different domains. Some popular examples include user intention predictions, lead scoring, churn analysis, etc. In this tutorial, we will focus on the best practice of predictive modeling in the big data era and its applications in industry, with motivating examples across a range of business tasks and relevance products. We will start with an overview of how predictive modeling helps power and drive various key business use cases. We will introduce the essential concepts and state of the art in building end-to-end predictive modeling solutions, and discuss the challenges, key technologies, and lessons learned from our practice, including case studies of LinkedIn feed relevance and a platform for email response prediction. Moreover, we will discuss some practical solutions of building predictive modeling platform to scale the modeling efforts for data scientists and analysts, along with an overview of popular tools and platforms used across the industry.",2016,Knowledge Discovery and Data Mining,Fields of study: predictive analyticsbusiness analyticsmanagement sciencedata sciencedata miningmachine learningcomputer science
A VC View of Investing in ML,Greg Papadopoulos,2643010249,"We are seeing a remarkable watershed in the application of data science across markets and industries. A trifecta of advances in algorithms, cheap cycles, and the capture of networked data from everywhere are no doubt the catalysts. The results for many are continuous improvements in efficiencies, and for some are a fundamental re-imagination and disruption of just about every industry. This talk will give examples we are seeing (and funding!) for the latter, and then focus on our views of the ecosystem of value-from-data infrastructure and end-application companies. A big question is whether the enormous collective advances in tools, techniques and education are in-fact converting would-be differentiated products into democratized features used everywhere. We'll follow the value and make our own predictions on future as ML as a business.",2016,Knowledge Discovery and Data Mining,Fields of study: investmentdata miningmachine learningcomputer science
Minimizing Legal Exposure of High-Tech Companies through Collaborative Filtering Methods,Bo Jin (Dalian University of Technology)Chao Che (Dalian University of Technology)Kuifei YuYue Qu (Dalian University of Technology)Li Guo (Dalian University of Technology)Cuili Yao (Dalian University of Technology)Ruiyun Yu (Northeastern University)Qiang Zhang (Dalian University of Technology),"2434583827,2632277346,2657291978,2515567699,2230458316,2499006483,2691302245,2684549118","Patent litigation not only covers legal and technical issues, it is also a key consideration for managers of high-technology (high-tech) companies when making strategic decisions. Patent litigation influences the market value of high-tech companies. However, this raises unique challenges. To this end, in this paper, we develop a novel recommendation framework to solve the problem of litigation risk prediction. We will introduce a specific type of patent-related litigation, that is, Section 337 investigations, which prohibit all acts of unfair competition, or any unfair trade practices, when exporting products to the United States. To build this recommendation framework, we collect and exploit a large amount of published information related to almost all Section 337 investigation cases. This study has two aims: (1) to predict the litigation risk in a specific industry category for high-tech companies and (2) to predict the litigation risk from competitors for high-tech companies. These aims can be achieved by mining historical investigation cases and related patents. Specifically, we propose two methods to meet the needs of both aims: a proximal slope one predictor and a time-aware predictor. Several factors are considered in the proposed methods, including the litigation risk if a company wants to enter a new market and the risk that a potential competitor would file a lawsuit against the new entrant. Comparative experiments using real-world data demonstrate that the proposed methods outperform several baselines with a significant margin.",2016,Knowledge Discovery and Data Mining,Fields of study: litigation risk analysiscollaborative filteringactuarial sciencedata miningmachine learningcomputer science
Structured Doubly Stochastic Matrix for Graph Based Clustering: Structured Doubly Stochastic Matrix,Xiaoqian Wang (University of Texas at Arlington)Feiping Nie (University of Texas at Arlington)Heng Huang (University of Texas at Arlington),"2149602155,2245267964,2137533801","As one of the most significant machine learning topics, clustering has been extensively employed in various kinds of area. Its prevalent application in scientific research as well as industrial practice has drawn high attention in this day and age. A multitude of clustering methods have been developed, among which the graph based clustering method using the affinity matrix has been laid great emphasis on. Recent research work used the doubly stochastic matrix to normalize the input affinity matrix and enhance the graph based clustering models. Although the doubly stochastic matrix can improve the clustering performance, the clustering structure in the doubly stochastic matrix is not clear as expected. Thus, post processing step is required to extract the final clustering results, which may not be optimal. To address this problem, in this paper, we propose a novel convex model to learn the structured doubly stochastic matrix by imposing low-rank constraint on the graph Laplacian matrix. Our new structured doubly stochastic matrix can explicitly uncover the clustering structure and encode the probabilities of pair-wise data points to be connected, such that the clustering results are enhanced. An efficient optimization algorithm is derived to solve our new objective. Also, we provide theoretical discussions that when the input differs, our method possesses interesting connections with K -means and spectral graph cut models respectively. We conduct experiments on both synthetic and benchmark datasets to validate the performance of our proposed method. The empirical results demonstrate that our model provides an approach to better solving the K -mean clustering problem. By using the cluster indicator provided by our model as initialization, K -means converges to a smaller objective function value with better clustering performance. Moreover, we compare the clustering performance of our model with spectral clustering and related double stochastic model. On all datasets, our method performs equally or better than the related methods.",2016,Knowledge Discovery and Data Mining,Fields of study: k medians clusteringflame clusteringbrown clusteringcanopy clustering algorithmdbscancorrelation clusteringconstrained clusteringdata stream clusteringcure data clustering algorithmdoubly stochastic matrixaffinity propagationspectral clusteringfuzzy clusteringlaplacian matrixk means clusteringclustering high dimensional datacluster analysisconsensus clusteringbiclusteringcombinatoricsdata miningmachine learningmathematical optimizationstatisticscomputer sciencemathematics
Towards Robust and Versatile Causal Discovery for Business Applications,Giorgos Borboudakis (University of Crete)Ioannis Tsamardinos (University of Crete),"2028001986,2619192764","Causal discovery algorithms can induce some of the causal relations from the data, commonly in the form of a causal network such as a causal Bayesian network. Arguably however, all such algorithms lack far behind what is necessary for a true business application. We develop an initial version of a new, general causal discovery algorithm called ETIO with many features suitable for business applications. These include (a) ability to accept prior causal knowledge (e.g., taking senior driving courses improves driving skills), (b) admitting the presence of latent confounding factors, (c) admitting the possibility of (a certain type of) selection bias in the data (e.g., clients sampled mostly from a given region), (d) ability to analyze data with missing-by-design (i.e., not planned to measure) values (e.g., if two companies merge and their databases measure different attributes), and (e) ability to analyze data from different interventions (e.g., prior and posterior to an advertisement campaign). ETIO is an instance of the logical approach to integrative causal discovery that has been relatively recently introduced and enables the solution of complex reverse-engineering problems in causal discovery. ETIO is compared against the state-of-the-art and is shown to be more effective in terms of speed, with only a slight degradation in terms of learning accuracy, while incorporating all the features above. The code is available on the mensxmachina.org website.",2016,Knowledge Discovery and Data Mining,Fields of study: answer set programminglatent variableselection biasdata sciencedata miningartificial intelligencemachine learningstatisticscomputer science
CompanyDepot: Employer Name Normalization in the Online Recruitment Industry,Qiaoling LiuFaizan JavedMatt Mcnair,"2486156090,2627345740,2124723780","Entity linking links entity mentions in text to the corresponding entities in a knowledge base (KB) and has many applications in both open domain and specific domains. For example, in the recruitment domain, linking employer names in job postings or resumes to entities in an employer KB is very important to many business applications. In this paper, we focus on this employer name normalization task, which has several unique challenges: handling employer names from both job postings and resumes, leveraging the corresponding location context, and handling name variations, irrelevant input data, and noises in the KB. We present a system called CompanyDepot which contains a machine learning based approach CompanyDepot-ML and a heuristic approach CompanyDepot-H to address these challenges in three steps: (1) searching for candidate entities based on a customized search engine for the KB; (2) ranking the candidate entities using learning-to-rank methods or heuristics; and (3) validating the top-ranked entity via binary classification or heuristics. While CompanyDepot-ML shows better extendability and flexibility, CompanyDepot-H serves as a strong baseline and useful way to collect training data for CompanyDepot-ML. The proposed system achieves 2.5%-21.4% higher coverage at the same precision level compared to an existing system used at CareerBuilder over multiple real-world datasets. Applying the system to a similar task of academic institution name normalization further shows the generalization ability of the method.",2016,Knowledge Discovery and Data Mining,Fields of study: entity linkingworld wide webinformation retrievaldata miningmachine learningcomputer science
Recruitment Market Trend Analysis with Sequential Latent Variable Models,Chen Zhu (Baidu)Hengshu Zhu (Baidu)Hui Xiong (Rutgers–Newark)Pengliang Ding (Baidu)Fang Xie (Baidu),"2529879468,2098414524,2153710278,2515451369,2507864332","Recruitment market analysis provides valuable understanding of industry-specific economic growth and plays an important role for both employers and job seekers. With the rapid development of online recruitment services, massive recruitment data have been accumulated and enable a new paradigm for recruitment market analysis. However, traditional methods for recruitment market analysis largely rely on the knowledge of domain experts and classic statistical models, which are usually too general to model large-scale dynamic recruitment data, and have difficulties to capture the fine-grained market trends. To this end, in this paper, we propose a new research paradigm for recruitment market analysis by leveraging unsupervised learning techniques for automatically discovering recruitment market trends based on large-scale recruitment data. Specifically, we develop a novel sequential latent variable model, named MTLVM, which is designed for capturing the sequential dependencies of corporate recruitment states and is able to automatically learn the latent recruitment topics within a Bayesian generative framework. In particular, to capture the variability of recruitment topics over time, we design hierarchical dirichlet processes for MTLVM. These processes allow to dynamically generate the evolving recruitment topics. Finally, we implement a prototype system to empirically evaluate our approach based on real-world recruitment data in China. Indeed, by visualizing the results from MTLVM, we can successfully reveal many interesting findings, such as the popularity of LBS related jobs reached the peak in the 2nd half of 2014, and decreased in 2015.",2016,Knowledge Discovery and Data Mining,Fields of study: latent variable modeltrend analysisdata miningsimulationstatisticscomputer science
Fast Memory-efficient Anomaly Detection in Streaming Heterogeneous Graphs,Emaad A. Manzoor (Stony Brook University)Sadegh M. Milajerdi (University of Illinois at Chicago)Leman Akoglu (Stony Brook University),"1532370067,2511109482,2288278917","Given a stream of heterogeneous graphs containing different types of nodes and edges, how can we spot anomalous ones in real-time while consuming bounded memory? This problem is motivated by and generalizes from its application in security to host-level advanced persistent threat (APT) detection. We propose StreamSpot, a clustering based anomaly detection approach that addresses challenges in two key fronts: (1) heterogeneity, and (2) streaming nature. We introduce a new similarity function for heterogeneous graphs that compares two graphs based on their relative frequency of local substructures, represented as short strings. This function lends itself to a vector representation of a graph, which is (a) fast to compute, and (b) amenable to a sketched version with bounded size that preserves similarity. StreamSpot exhibits desirable properties that a streaming application requires: it is (i) fully-streaming; processing the stream one edge at a time as it arrives, (ii) memory-efficient; requiring constant space for the sketches and the clustering, (iii) fast; taking constant time to update the graph sketches and the cluster summaries that can process over 100,000 edges per second, and (iv) online; scoring and flagging anomalies in real time. Experiments on datasets containing simulated system-call flow graphs from normal browser activity and various attack scenarios (ground truth) show that StreamSpot is high-performance; achieving above 95% detection accuracy with small delay, as well as competitive time and memory usage.",2016,Knowledge Discovery and Data Mining,Fields of study: indifference graphhopcroft karp algorithm1 planar graphgraph productmodular decompositionmaximal independent setpathwidthlongest path problemanomaly detectiontheoretical computer sciencecombinatoricsworld wide webdata miningmachine learningcomputer sciencemathematics
Robust and Effective Metric Learning Using Capped Trace Norm: Metric Learning via Capped Trace Norm,Zhouyuan Huo (University of Texas at Arlington)Feiping Nie (University of Texas at Arlington)Heng Huang (University of Texas at Arlington),"2668104805,2245267964,2137533801","Metric learning aims at automatically learning a metric from pair or triplet based constraints in data, and it can be potentially beneficial whenever the notion of metric between instances plays a nontrivial role. In Mahalanobis distance metric learning, distance matrix M is in symmetric positive semi-definite cone, and in order to avoid overfitting and to learn a better Mahalanobis distance from weakly supervised constraints, the low-rank regularization has been often imposed on matrix M to learn the correlations between features and samples. As the approximations of the rank minimization function, the trace norm and Fantope have been utilized to regularize the metric learning objectives and achieve good performance. However, these low-rank regularization models are either not tight enough to approximate rank minimization or time-consuming to tune an optimal rank. In this paper, we introduce a novel metric learning model using the capped trace norm based regularization, which uses a singular value threshold to constraint the metric matrix M as low-rank explicitly such that the rank of matrix M is stable when the large singular values vary. The capped trace norm regularization can also be viewed as the adaptive Fantope regularization. We minimize singular values which are less than threshold value and the rank of M is not necessary to be k , thus our method is more stable and applicable in practice when we do not know the optimal rank of matrix M . We derive an efficient optimization algorithm to solve the proposed new model and the algorithm convergence proof is also provided in this paper. We evaluate our method on a variety of challenging benchmarks, such as LFW and Pubfig datasets. Face verification experiments are performed and results show that our method consistently outperforms the state-of-the-art metric learning algorithms.",2016,Knowledge Discovery and Data Mining,Fields of study: convex metric spaceintrinsic metriclow rank approximationmetricdiscrete mathematicscombinatoricsmachine learningmathematical optimizationcomputer sciencemathematics
Structural Neighborhood Based Classification of Nodes in a Network,Sharad Nandanwar (Indian Institute of Science)Musti Narasimha Murty (Indian Institute of Science),"2514386424,2222915647","Classification of entities based on the underlying network structure is an important problem. Networks encountered in practice are sparse and have many missing and noisy links. Statistical learning techniques have been used in intra-network classification; however, they typically exploit only the local neighborhood, so may not perform well. In this paper, we propose a novel structural neighborhood-based classifier learning using a random walk. For classifying a node, we take a random walk from the node and make a decision based on how nodes in the respective k^th-level neighborhood are labeled. We observe that random walks of short length are helpful in classification. Emphasizing role of longer random walks may cause the underlying Markov chain to converge to a stationary distribution. Considering this, we take a lazy random walk based approach with variable termination probability for each node, based on the node's structural properties including its degree. Our experimental study on real world datasets demonstrates the superiority of the proposed approach over the existing state-of-the-art approaches.",2016,Knowledge Discovery and Data Mining,Fields of study: statistical relational learningdata miningpattern recognitionmachine learningcomputer sciencemathematics
Streaming-LDA: A Copula-based Approach to Modeling Topic Dependencies in Document Streams,Hesam Amoualian (University of Grenoble)Marianne Clausel (University of Grenoble)Eric Gaussier (University of Grenoble)Massih-Reza Amini (University of Grenoble),"2512811048,2031349992,2667237248,2687710440","We propose in this paper two new models for modeling topic and word-topic dependencies between consecutive documents in document streams. The first model is a direct extension of Latent Dirichlet Allocation model (LDA) and makes use of a Dirichlet distribution to balance the influence of the LDA prior parameters wrt to topic and word-topic distribution of the previous document. The second extension makes use of copulas, which constitute a generic tools to model dependencies between random variables. We rely here on Archimedean copulas, and more precisely on Franck copulas, as they are symmetric and associative and are thus appropriate for exchangeable random variables. Our experiments, conducted on three standard collections that have been used in several studies on topic modeling, show that our proposals outperform previous ones (as dynamic topic models and temporal \LDA), both in terms of perplexity and for tracking similar topics in a document stream.",2016,Knowledge Discovery and Data Mining,Fields of study: dynamic topic modelcopulalatent dirichlet allocationdata miningpattern recognitionmachine learningcomputer science
Compact and Scalable Graph Neighborhood Sketching,Takuya Akiba (National Institute of Informatics)Yosuke Yano (National Institute of Informatics),"2700554244,2507740877","The all-distances sketch (ADS) has recently emerged as a promising paradigm of graph neighborhood sketching. An ADS is a probabilistic data structure that is defined for each vertex of a graph. ADSs facilitate accurate estimation of many useful indicators for network analysis with the guarantee of accuracy, and the ADSs for all the vertices in a graph can be computed in near-linear time. Because of these useful properties, ADS has attracted considerable attention. However, a critical drawback of ADS is its space requirement, which tends to be much larger than that of the graph itself. In the present study, we address this issue by designing a new graph sketching scheme, namely, sketch retrieval shortcuts (SRS). Although SRSs are more space-efficient than ADSs by an order of magnitude, an ADS of any vertex can be quickly retrieved from the SRSs. The retrieved ADSs can be used to estimate the aforementioned indicators in exactly the same manner as with plain ADSs, inheriting the same accuracy guarantee. Our experiments on real-world networks demonstrate the usefulness of SRSs as a practical back-end of large-scale graph data mining.",2016,Knowledge Discovery and Data Mining,Fields of study: graphtheoretical computer sciencediscrete mathematicscombinatoricsmathematics
The Wisdom of Crowds: Best Practices for Data Prep & Machine Learning Derived from Millions of Data Science Workflows,Ingo Mierswa (Technical University of Dortmund),278133066,"With hundreds of thousands of users, RapidMiner is the most frequently used visual workflow platform for machine learning. It covers the full spectrum of analytics from data preparation to machine learning and model validation. In this presentation, I will take you on a tour of machine learning which spans the last 15 years of research and industry applications and share key insights with you about how data scientists perform their daily analysis tasks. These patterns are extracted from mining millions of analytical workflows that have been created with RapidMiner over the past years. This talk will address important questions around the data mining process such as: What are the most frequently used solutions for typical data quality problems? How often are analysts using decision trees or neural networks? And does this behavior change over time or depend on the users experience level?",2016,Knowledge Discovery and Data Mining,Fields of study: analyticsdata visualizationdata scienceworld wide webdata miningmachine learningcomputer science
An Empirical Study on Recommendation with Multiple Types of Feedback,Liang Tang (LinkedIn)Bo Long (Yahoo!)Bee-Chung Chen (LinkedIn)Deepak Agarwal (LinkedIn),"2520481619,2512786853,2152441490,2116605949","User feedback like clicks and ratings on recommended items provides important information for recommender systems to predict users' interests in unseen items. Most systems rely on models trained using a single type of feedback, e.g., ratings for movie recommendation and clicks for online news recommendation. However, in addition to the primary feedback, many systems also allow users to provide other types of feedback, e.g., liking or sharing an article, or hiding all articles from a source. These additional feedback potentially provides extra information for the recommendation models. To optimize user experience and business objectives, it is important for a recommender system to use both the primary feedback and additional feedback. This paper presents an empirical study on various training methods for incorporating multiple user feedback types based on LinkedIn recommendation products. We study three important problems that we face at LinkedIn: (1) Whether to send an email based on clicks and complaints, (2) how to rank updates in LinkedIn feeds based on clicks and hides and (3) how jointly optimize for viral actions and clicks in LinkedIn feeds. Extensive offline experiments on historical data show the effectiveness of these methods in different situations. Online A/B testing results further demonstrate the impact of these methods on LinkedIn production systems.",2016,Knowledge Discovery and Data Mining,Fields of study: multi objective optimizationrecommender systemmultimediaworld wide webdata miningmachine learningcomputer science
Talent Circle Detection in Job Transition Networks,Huang Xu (Northwestern Polytechnical University)Zhiwen Yu (Northwestern Polytechnical University)Jingyuan Yang (Rutgers–Newark)Hui Xiong (Rutgers–Newark)Hengshu Zhu (Baidu),"2132199107,2118377178,2521081551,2153710278,2098414524","With the high mobility of talent, it becomes critical for the recruitment team to find the right talent from the right source in an efficient manner. The prevalence of Online Professional Networks (OPNs), such as LinkedIn, enables the new paradigm for talent recruitment and job search. However, the dynamic and complex nature of such talent information imposes significant challenges to identify prospective talent sources from large-scale professional networks. Therefore, in this paper, we propose to create a job transition network where vertices stand for organizations and a directed edge represents the talent flow between two organizations for a time period. By analyzing this job transition network, it is able to extract talent circles in a way such that every circle includes the organizations with similar talent exchange patterns. Then, the characteristics of these talent circles can be used for talent recruitment and job search. To this end, we develop a talent circle detection model and design the corresponding learning method by maximizing the Normalized Discounted Cumulative Gain (NDCG) of inferred probability for the edge existence based on edge weights. Then, the identified circles will be labeled by the representative organizations as well as keywords in job descriptions. Moreover, based on these identified circles, we develop a talent exchange prediction method for talent recommendation. Finally, we have performed extensive experiments on real-world data. The results show that, our method can achieve much higher modularity when comparing to the benchmark approaches, as well as high precision and recall for talent exchange prediction.",2016,Knowledge Discovery and Data Mining,Fields of study: simulationcomputer science
Compute Job Memory Recommender System Using Machine Learning,Taraneh Taghavi (Qualcomm)Maria Lupetini (Qualcomm)Yaron Kretchmer (Qualcomm),"2635545676,2511213601,2650071953","This paper presents a machine learning approach to predict the amount of compute memory needed by jobs which are submitted to Load Sharing Facility (LSF® ) with a high level of accuracy. LSF® is the compute resource manager and job scheduler for Qualcomm chip design process. It schedules the jobs based on available resources: CPU, memory, storage, and software licenses. Memory is one of the key resources and its proper utilization leads to a substantial improvement in saving machine resources which in turn results in a significant reduction in overall job pending time. In addition, efficient memory utilization helps to reduce the operations cost by decreasing the number of servers needed for the end-to-end design process. In this paper, we explored a suite of statistical and machine learning techniques to develop a Compute Memory Recommender System for the Qualcomm chip design process with over 90% accuracy in predicting the amount of memory a job needs. Moreover, it demonstrates the potential to significantly reduce job pending time.",2016,Knowledge Discovery and Data Mining,Fields of study: computing with memoryflat memory modeluniform memory accessshared memorymemory managementgrid computingtheoretical computer sciencedata miningreal time computingmachine learningsimulationcomputer science
Burstiness Scale: A Parsimonious Model for Characterizing Random Series of Events,Rodrigo Augusto da Silva Alves (Centro Federal de Educação Tecnológica de Minas Gerais)Renato Martins Assuncao (Universidade Federal de Minas Gerais)Pedro Olmo Stancioli Vaz de Melo (Universidade Federal de Minas Gerais),"2683066055,2001700347,2162838672","The problem to accurately and parsimoniously characterize random series of events (RSEs) seen in the Web, such as Yelp reviews or Twitter hashtags, is not trivial. Reports found in the literature reveal two apparent conflicting visions of how RSEs should be modeled. From one side, the Poissonian processes, of which consecutive events follow each other at a relatively regular time and should not be correlated. On the other side, the self-exciting processes, which are able to generate bursts of correlated events. The existence of many and sometimes conflicting approaches to model RSEs is a consequence of the unpredictability of the aggregated dynamics of our individual and routine activities, which sometimes show simple patterns, but sometimes results in irregular rising and falling trends. In this paper we propose a parsimonious way to characterize general RSEs, namely the Burstiness Scale ( BuSca ) model. BuSca views each RSE as a mix of two independent process: a Poissonian and a self-exciting one. Here we describe a fast method to extract the two parameters of BuSca that, together, gives the burstiness scale ψ, which represents how much of the RSE is due to bursty and viral effects. We validated our method in eight diverse and large datasets containing real random series of events seen in Twitter, Yelp, e-mail conversations, Digg, and online forums. Results showed that, even using only two parameters, BuSca is able to accurately describe RSEs seen in these diverse systems, what can leverage many applications.",2016,Knowledge Discovery and Data Mining,Fields of study: social mediadata miningsimulationstatisticscomputer science
Temporal Order-based First-Take-All Hashing for Fast Attention-Deficit-Hyperactive-Disorder Detection,Hao Hu (University of Central Florida)Joey Velez-Ginorio (University of Central Florida)Guo-Jun Qi (University of Central Florida),"2431064401,2512677353,2237849324","Attention Deficit Hyperactive Disorder (ADHD) is one of the most common childhood disorders and can continue through adolescence and adulthood. Although the root cause of the problem still remains unknown, recent advancements in brain imaging technology reveal there exists differences between neural activities of Typically Developing Children (TDC) and ADHD subjects. Inspired by this, we propose a novel First-Take-All (FTA) hashing framework to investigate the problem of fast ADHD subjects detection through the fMRI time-series of neuron activities. By hashing time courses from regions of interests (ROIs) in the brain into fixed-size hash codes, FTA can compactly encode the temporal order differences between the neural activity patterns that are key to distinguish TDC and ADHD subjects. Such patterns can be directly learned via minimizing the training loss incurred by the generated FTA codes. By conducting similarity search on the resultant FTA codes, data-driven ADHD detection can be achieved in an efficient fashion. The experiments' results on real-world ADHD detection benchmarks demonstrate the FTA can outperform the state-of-the-art baselines using only neural activity time series without any phenotypic information.",2016,Knowledge Discovery and Data Mining,Fields of study: speech recognitionmachine learningsimulationcomputer science
Optimal Reserve Prices in Upstream Auctions: Empirical Application on Online Video Advertising,Miguel Angel Alcobendas Lisbona (Yahoo!)Sheide Chammas (Yahoo!)Kuang-chih Lee (Yahoo!),"2508079860,2513521846,2696605660","We consider optimal reserve prices in BrightRoll Video Exchange when the inventory opportunity comes from other exchanges (downstream marketplaces). We show that the existence of downstream auctions impacts the optimal floor. Moreover, it renders the classical derivation of the floor set by a monopolist inadequate and suboptimal. We derive the new downstream-corrected reserve price and compare its performance with respect to existing floors and the classical optimal monopoly price. In our application, the downstream-corrected reserve price proves superior to both. The proposed model also deals with data challenges commonly faced by exchanges: limited number of logged bids in an auction, and uncertainty regarding the bidding behavior in other exchanges. The relevance of this study transcends its particular context and is applicable to a wide range of scenarios where sequential auctions exist, and where marketplaces interact with each other.",2016,Knowledge Discovery and Data Mining,Fields of study: reservation pricecommon value auctiononline advertising
"The Limits of Popularity-Based Recommendations, and the Role of Social Ties",Marco Bressan (Sapienza University of Rome)Stefano Leucci (Sapienza University of Rome)Alessandro Panconesi (Sapienza University of Rome)Prabhakar Raghavan (Google)Erisa Terolli (Sapienza University of Rome),"2516098459,2290155465,2193688032,2326868227,2545383834","In this paper we introduce a mathematical model that captures some of the salient features of recommender systems that are based on popularity and that try to exploit social ties among the users. We show that, under very general conditions, the market always converges to a steady state, for which we are able to give an explicit form. Thanks to this we can tell rather precisely how much a market is altered by a recommendation system, and determine the power of users to influence others. Our theoretical results are complemented by experiments with real world social networks showing that social graphs prevent large market distortions in spite of the presence of highly influential users.",2016,Knowledge Discovery and Data Mining,Fields of study: shapley valuesocial influenceinterpersonal tiesrecommender systemsocial psychologyworld wide webcomputer science
Convex Optimization for Linear Query Processing under Approximate Differential Privacy,Ganzhao Yuan (South China University of Technology)Yin Yang (Khalifa University)Zhenjie Zhang (National University of Singapore)Zhifeng Hao (Foshan University),"2118188494,2280962535,2161417483,2684555521","Differential privacy enables organizations to collect accurate aggregates over sensitive data with strong, rigorous guarantees on individuals' privacy. Previous work has found that under differential privacy, computing multiple correlated aggregates as a batch, using an appropriate strategy , may yield higher accuracy than computing each of them independently. However, finding the best strategy that maximizes result accuracy is non-trivial, as it involves solving a complex constrained optimization program that appears to be non-convex. Hence, in the past much effort has been devoted in solving this non-convex optimization program. Existing approaches include various sophisticated heuristics and expensive numerical solutions. None of them, however, guarantees to find the optimal solution of this optimization problem. This paper points out that under (e, ཬ)-differential privacy, the optimal solution of the above constrained optimization problem in search of a suitable strategy can be found, rather surprisingly, by solving a simple and elegant convex optimization program. Then, we propose an efficient algorithm based on Newton's method, which we prove to always converge to the optimal solution with linear global convergence rate and quadratic local convergence rate. Empirical evaluations demonstrate the accuracy and efficiency of the proposed solution.",2016,Knowledge Discovery and Data Mining,Fields of study: meta optimizationdifferential privacyquadratically constrained quadratic programconvex optimizationtheoretical computer sciencemathematical optimizationcomputer science
Modeling Precursors for Event Forecasting via Nested Multi-Instance Learning,Yue Ning (Virginia Tech)Sathappan Muthiah (Virginia Tech)Huzefa Rangwala (George Mason University)Naren Ramakrishnan (Virginia Tech),"2493066754,1509108369,2096698710,2199255697","Forecasting large-scale societal events like civil unrest movements, disease outbreaks, and elections is an important and challenging problem. From the perspective of human analysts and policy makers, forecasting algorithms must not only make accurate predictions but must also provide supporting evidence, e.g., the causal factors related to the event of interest. We develop a novel multiple instance learning based approach that jointly tackles the problem of identifying evidence-based precursors and forecasts events into the future. Specifically, given a collection of streaming news articles from multiple sources we develop a nested multiple instance learning approach to forecast significant societal events such as protests. Using data from three countries in Latin America, we demonstrate how our approach is able to consistently identify news articles considered as precursors for protests. Our empirical evaluation demonstrates the strengths of our proposed approach in filtering candidate precursors, in forecasting the occurrence of events with a lead time advantage and in accurately predicting the characteristics of civil unrest events.",2016,Knowledge Discovery and Data Mining,Fields of study: text miningdata sciencedata miningmachine learningsimulationcomputer science
Online Dual Decomposition for Performance and Delivery-Based Distributed Ad Allocation,Jim C. Huang (Amazon.com)Rodolphe Jenatton (Amazon.com)Cedric Archambeau (Amazon.com),"2518843193,90224947,2006974593","Online optimization is central to display advertising, where we must sequentially allocate ad impressions to maximize the total welfare among advertisers, while respecting various advertiser-specified long-term constraints (e.g., total amount of the ad's budget that is consumed at the end of the campaign). In this paper, we present the online dual decomposition (ODD) framework for large-scale, online, distributed ad allocation, which combines dual decomposition and online convex optimization. ODD allows us to account for the distributed and the online nature of the ad allocation problem and is extensible to a variety of ad allocation problems arising in real-world display advertising systems. Moreover, ODD does not require assumptions about auction dynamics, stochastic or adversarial feedback, or any other characteristics of the ad marketplace. We further provide guarantees for the online solution as measured by bounds on cumulative regret. The regret analysis accounts for the impact of having to estimate constraints in an online setting before they are observed and for the dependence on the smoothness with which constraints and constraint violations are generated. We provide an extensive set of results from a large-scale production advertising system at Amazon to validate the framework and compare its behavior to various ad allocation algorithms.",2016,Knowledge Discovery and Data Mining,Fields of study: simulation
Can You Teach the Elephant to Dance? AKA: Culture Eats Data Science for Breakfast,Jonathan D. Becher,2508866078,"In the past 20 years, the practical examples of KDD/data mining have become so ubiquitous that it's almost impossible to imagine a new venture that isn't based on data science. Uber, Facebook, 23andMe, Tesla -- they aren't just technology companies; they are data companies. And yet the reality is that these companies are still anomalies. Large, successful companies usually still treat KDD as either an afterthought or as an experiment. It's not core to how they run the business. As practitioners we compound this problem by concentrating our efforts on valuable business problems; but ones which are usually on the periphery of the business. We do this because changing the heart of how a company operates requires more than just process or technology changes. It requires cultural changes. And these cultural changes usually trigger corporate antibodies adverse to anything new. This talk will review some practical realities of instituting data-driven decisions in a very large multi-national company.",2016,Knowledge Discovery and Data Mining,Fields of study: enterprise softwareanalyticsmobile technologydata miningsimulationcomputer science
CaSMoS: A Framework for Learning Candidate Selection Models over Structured Queries and Documents,Fedor Borisyuk (LinkedIn)Krishnaram Kenthapadi (LinkedIn)David Stein (LinkedIn)Bo Zhao (LinkedIn),"2531515047,2530367615,2507570935,2674375462","User experience at social media and web platforms such as LinkedIn is heavily dependent on the performance and scalability of its products. Applications such as personalized search and recommendations require real-time scoring of millions of structured candidate documents associated with each query, with strict latency constraints. In such applications, the query incorporates the context of the user (in addition to search keywords if present), and hence can become very large, comprising of thousands of Boolean clauses over hundreds of document attributes. Consequently, candidate selection techniques need to be applied since it is infeasible to retrieve and score all matching documents from the underlying inverted index. We propose CaSMoS, a machine learned candidate selection framework that makes use of Weighted AND (WAND) query. Our framework is designed to prune irrelevant documents and retrieve documents that are likely to be part of the top-k results for the query. We apply a constrained feature selection algorithm to learn positive weights for feature combinations that are used as part of the weighted candidate selection query. We have implemented and deployed this system to be executed in real time using LinkedIn's Galene search platform. We perform extensive evaluation with different training data approaches and parameter settings, and investigate the scalability of the proposed candidate selection model. Our deployment of this system as part of LinkedIn's job recommendation engine has resulted in significant reduction in latency (up to 25%) without sacrificing the quality of the retrieved results, thereby paving the way for more sophisticated scoring models.",2016,Knowledge Discovery and Data Mining,Fields of study: web search queryquery expansionworld wide webinformation retrievaldata miningmachine learningcomputer science
Large-Scale Machine Learning at Verizon: Theory and Applications,Ashok Srivastava (Verizon Communications),2529005834,"This talk will cover recent innovations in large-scale machine learning and their applications on massive, real-world data sets at Verizon. These applications power new revenue generating products and services for the company and are hosted on a massive computing and storage platform known as Orion. We will discuss the architecture of Orion and the underlying algorithmic framework. We will also cover some of the real-world aspects of building a new organization dedicated to creating new product lines based on data science.",2016,Knowledge Discovery and Data Mining,Fields of study: data sciencetheoretical computer sciencedata miningmachine learningsimulationcomputer science
How Machine Learning has Finally Solved Wanamaker's Dilemma,Oliver Downs,2674567224,"It has become a cliche to talk about Wanamaker's dilemma, his famous quote that ""Half the money I spend on advertising is wasted; the trouble is I don't know which half"" is well known. So why talk about it today? Well, I'll show you that some of the best targeted consumer marketing still suffers exactly from this problem, and tell you why -- it's not humanly possible to solve it! At Amplero we've finally solved it and made that solution accessible to marketers, using machine learning in combination with the revolution in online experimentation that the advent of the multi-armed bandit has brought about.",2016,Knowledge Discovery and Data Mining,Fields of study: operations researchartificial intelligence
A Survey on Social Media Anomaly Detection,Rose Yu (University of Southern California)Huida Qiu (University of Southern California)Zhen Wen (IBM)ChingYung Lin (IBM)Yan Liu (University of Southern California),"2107161032,2646186367,2129596114,2134425921,2240541904","Social media anomaly detection is of critical importance to prevent malicious activities such as bullying, terrorist attack planning, and fraud information dissemination. With the recent popularity of social media, new types of anomalous behaviors arise, causing concerns from various parties. While a large amount of work have been dedicated to traditional anomaly detection problems, we observe a surge of research interests in the new realm of social media anomaly detection. In this paper, we present a survey on existing approaches to address this problem. We focus on the new type of anomalous phenomena in the social media and review the recent developed techniques to detect those special types of anomalies. We provide a general overview of the problem domain, common formulations, existing methodologies and potential directions. With this work, we hope to call out the attention from the research community on this challenging problem and open up new directions that we can contribute in the future",2016,Knowledge Discovery and Data Mining,Fields of study: computer security
Current and Future Challenges in Mining Large Networks: Report on the Second SDM Workshop on Mining Networks and Graphs,Lawrence B. Holder (Washington State University)Rajmonda Caceres (Massachusetts Institute of Technology)David F. Gleich (Purdue University)E. Jason Riedy (Georgia Institute of Technology)Maleq Khan (Virginia Tech)Nitesh V. Chawla (University of Notre Dame)Ravi Kumar 0001 (Google)Yinghui Wu (Washington State University)Christine Klymko (Lawrence Livermore National Laboratory)Tina Eliassi-Rad (Rutgers University)B. Aditya Prakash (Virginia Tech),"639663078,2102944021,2148810670,2278635082,2257365121,1979796846,2232709231,2134127457,2528138532,218538652,2124002246","We report on the Second Workshop on Mining Networks and Graphs held at the 2015 SIAM International Conference on Data Mining. This half-day workshop consisted of a keynote talk, four technical paper presentations, one demonstration, and a panel on future challenges in mining large networks. We summarize the main highlights of the workshop, including expanded written summaries of the future challenges provided by the panelists. The current and future challenges discussed at the workshop and elaborated here provide valuable guidance for future research in the field",2016,Knowledge Discovery and Data Mining,Fields of study: big datadata sciencedata miningcomputer science
On the effect of endpoints on dynamic time warping,"Diego Furtado Silva (Spanish National Research Council)Gustavo Enrique de Almeida Prado Alves Batista (University of São Paulo)Eamonn Keogh (University of California, Riverside)","2138204294,2165222361,2170070822",-,2016,Knowledge Discovery and Data Mining,Fields of study: dynamic time warpingtime seriesstatisticscomputer science
The Internet of Things: Opportunities and Challenges for Distributed Data Analysis,Marco Stolpe (Technical University of Dortmund),2159003941,"Nowadays, data is created by humans as well as automatically collected by physical things, which embed electronics, software, sensors and network connectivity. Together, these entities constitute the Internet of Things (IoT). The automated analysis of its data can provide insights into previously unknown relationships between things, their environment and their users, facilitating an optimization of their behavior. Especially the real-time analysis of data, embedded into physical systems, can enable new forms of autonomous control. These in turn may lead to more sustainable applications, reducing waste and saving resources IoT's distributed and dynamic nature, resource constraints of sensors and embedded devices as well as the amounts of generated data are challenging even the most advanced automated data analysis methods known today. In particular, the IoT requires a new generation of distributed analysis methods. Many existing surveys have strongly focused on the centralization of data in the cloud and big data analysis, which follows the paradigm of parallel high-performance computing. However, bandwidth and energy can be too limited for the transmission of raw data, or it is prohibited due to privacy constraints. Such communication-constrained scenarios require decentralized analysis algorithms which at least partly work directly on the generating devices. After listing data-driven IoT applications, in contrast to existing surveys, we highlight the differences between cloudbased and decentralized analysis from an algorithmic perspective. We present the opportunities and challenges of research on communication-efficient decentralized analysis algorithms. Here, the focus is on the difficult scenario of vertically partitioned data, which covers common IoT use cases. The comprehensive bibliography aims at providing readers with a good starting point for their own work",2016,Knowledge Discovery and Data Mining,Fields of study: world wide webdata miningsimulation
Strategy discovery in professional soccer match data,Jan Van Haaren (Katholieke Universiteit Leuven)Siebe Hannosset (Katholieke Universiteit Leuven)Jesse Davis (Katholieke Universiteit Leuven),"2036917247,2241357928,2144802550",-,2016,Knowledge Discovery and Data Mining,-
"MultiClust 2013: Multiple Clusterings, Multiview Data, and Multisource Knowledgedriven Clustering: [Workshop Report]",Ira Assent (Aarhus University)Carlotta Domeniconi (George Mason University)Francesco Gullo (Yahoo!)Andrea Tagarelli (University of Calabria)Arthur Zimek (Ludwig Maximilian University of Munich),"145164693,45678088,1979201319,273425128,242745652","In this workshop report, we give a summary of the Multi-Clust workshop held in Chicago in conjunction with KDD 2013. We provide an overview on the history of this workshop series and the general topics covered. Furthermore, we provide summaries of the invited talks and of the contributed papers.",2016,Knowledge Discovery and Data Mining,Fields of study: data scienceinformation retrievaldata miningcomputer science
Catching Synchronized Behaviors in Large Networks: A Graph Mining Approach,Meng Jiang (Tsinghua University)Peng Cui (Tsinghua University)Alex Beutel (Carnegie Mellon University)Christos Faloutsos (Carnegie Mellon University)Shiqiang Yang (Tsinghua University),"2115305989,2113115369,2045447989,2198983026,2127183023","Given a directed graph of millions of nodes, how can we automatically spot anomalous, suspicious nodes judging only from their connectivity patterns? Suspicious graph patterns show up in many applications, from Twitter users who buy fake followers, manipulating the social network, to botnet members performing distributed denial of service attacks, disturbing the network traffic graph. We propose a fast and effective method, C atch S ync , which exploits two of the tell-tale signs left in graphs by fraudsters: (a) synchronized behavior: suspicious nodes have extremely similar behavior patterns because they are often required to perform some task together (such as follow the same user); and (b) rare behavior: their connectivity patterns are very different from the majority. We introduce novel measures to quantify both concepts (“synchronicity” and “normality”) and we propose a parameter-free algorithm that works on the resulting synchronicity-normality plots. Thanks to careful design, C atch S ync has the following desirable properties: (a) it is scalable to large datasets, being linear in the graph size; (b) it is parameter free ; and (c) it is side-information-oblivious : it can operate using only the topology, without needing labeled data, nor timing information, and the like., while still capable of using side information if available. We applied C atch S ync on three large, real datasets, 1-billion-edge Twitter social graph, 3-billion-edge, and 12-billion-edge Tencent Weibo social graphs, and several synthetic ones; C atch S ync consistently outperforms existing competitors, both in detection accuracy by 36p on Twitter and 20p on Tencent Weibo, as well as in speed.",2016,Knowledge Discovery and Data Mining,Fields of study: anomaly detectionworld wide webdata miningmachine learningcomputer science
Less is More: Building Selective Anomaly Ensembles,Shebuti Rayana (Stony Brook University)Leman Akoglu (Stony Brook University),"1722036351,2288278917","Ensemble learning for anomaly detection has been barely studied, due to difficulty in acquiring ground truth and the lack of inherent objective functions. In contrast, ensemble approaches for classification and clustering have been studied and effectively used for long. Our work taps into this gap and builds a new ensemble approach for anomaly detection, with application to event detection in temporal graphs as well as outlier detection in no-graph settings. It handles and combines multiple heterogeneous detectors to yield improved and robust performance. Importantly, trusting results from all the constituent detectors may deteriorate the overall performance of the ensemble, as some detectors could provide inaccurate results depending on the type of data in hand and the underlying assumptions of a detector. This suggests that combining the detectors selectively is key to building effective anomaly ensembles—hence “less is more”. In this paper we propose a novel ensemble approach called SELECT for anomaly detection, which automatically and systematically selects the results from constituent detectors to combine in a fully unsupervised fashion. We apply our method to event detection in temporal graphs and outlier detection in multi-dimensional point data (no-graph), where SELECT successfully utilizes five base detectors and seven consensus methods under a unified ensemble framework. We provide extensive quantitative evaluation of our approach for event detection on five real-world datasets (four with ground truth events), including Enron email communications, RealityMining SMS and phone call records, New York Times news corpus, and World Cup 2014 Twitter news feed. We also provide results for outlier detection on seven real-world multi-dimensional point datasets from UCI Machine Learning Repository. Thanks to its selection mechanism, SELECT yields superior performance compared to the individual detectors alone, the full ensemble (naively combining all results), an existing diversity-based ensemble, and an existing weighted ensemble approach.",2016,Knowledge Discovery and Data Mining,Fields of study: ensemble learningunsupervised learningdata miningpattern recognitionmachine learningcomputer science
Eigen-Optimization on Large Graphs by Edge Manipulation,Chen Chen (Arizona State University)Hanghang Tong (Arizona State University)B. Aditya Prakash (Virginia Tech)Tina Eliassi-Rad (Rutgers University)Michalis Faloutsos (University of New Mexico)Christos Faloutsos (Carnegie Mellon University),"2311808467,2667261544,2124002246,218538652,2703613186,2198983026","Large graphs are prevalent in many applications and enable a variety of information dissemination processes, e.g., meme, virus, and influence propagation. How can we optimize the underlying graph structure to affect the outcome of such dissemination processes in a desired way (e.g., stop a virus propagation, facilitate the propagation of a piece of good idea, etc)? Existing research suggests that the leading eigenvalue of the underlying graph is the key metric in determining the so-called epidemic threshold for a variety of dissemination models. In this paper, we study the problem of how to optimally place a set of edges (e.g., edge deletion and edge addition) to optimize the leading eigenvalue of the underlying graph, so that we can guide the dissemination process in a desired way. We propose effective, scalable algorithms for edge deletion and edge addition, respectively. In addition, we reveal the intrinsic relationship between edge deletion and node deletion problems. Experimental results validate the effectiveness and efficiency of the proposed algorithms.",2016,Knowledge Discovery and Data Mining,Fields of study: scalabilitytheoretical computer sciencecombinatoricsdistributed computingcomputer sciencemathematics
Spatial-Proximity Optimization for Rapid Task Group Deployment,Chih-Ya Shen (Academia Sinica)De-Nian Yang (Academia Sinica)Wang-Chien Lee (Pennsylvania State University)Ming-Syan Chen (National Taiwan University),"2122520210,2096343151,2143778659,2122365371","Spatial proximity is one of the most important factors for the quick deployment of the task groups in various time-sensitive missions. This article proposes a new spatial query, Spatio-Social Team Query (SSTQ) , that forms a strong task group by considering (1) the group’s spatial distance (i.e., transportation time), (2) skills of the candidate group members, and (3) social rapport among the candidates. Efficient processing of SSTQ is very challenging, because the aforementioned spatial, skill, and social factors need to be carefully examined. In this article, therefore, we first formulate two subproblems of SSTQ, namely Hop-Constrained Team Problem (HCTP) and Connection-Oriented Team Query (COTQ) . HCTP is a decision problem that considers only social and skill dimensions. We prove that HCTP is NP-Complete. Moreover, based on the hardness of HCTP, we prove that SSTQ is NP-Hard and inapproximable within any factor . On the other hand, COTQ is a special case of SSTQ that relaxes the social constraint. We prove that COTQ is NP-Hard and propose an approximation algorithm for COTQ, namely COTprox . Furthermore, based on the observations on COTprox, we devise an approximation algorithm, SSTprox , with a guaranteed error bound for SSTQ. Finally, to efficiently obtain the optimal solution to SSTQ for small instances, we design two efficient algorithms, SpatialFirst and SkillFirst , with different scenarios in mind. These two algorithms incorporate various effective ordering and pruning techniques to reduce the search space for answering SSTQ. Experimental results on real datasets indicate that the proposed algorithms can efficiently answer SSTQ under various parameter settings.",2016,Knowledge Discovery and Data Mining,Fields of study: spatial databasesocial networkdata miningmachine learningsimulationcomputer sciencemathematics
Heterogeneous Translated Hashing: A Scalable Solution Towards Multi-Modal Similarity Search,Ying Wei (Hong Kong University of Science and Technology)Yangqiu Song (University of Illinois at Urbana–Champaign)Yi Zhen (Georgia Institute of Technology)Bo Liu (Hong Kong University of Science and Technology)Qiang Yang (Hong Kong University of Science and Technology),"2235263654,2099747503,2706898368,2423441494,2109031554","Multi-modal similarity search has attracted considerable attention to meet the need of information retrieval across different types of media. To enable efficient multi-modal similarity search in large-scale databases recently, researchers start to study multi-modal hashing. Most of the existing methods are applied to search across multi-views among which explicit correspondence is provided. Given a multi-modal similarity search task, we observe that abundant multi-view data can be found on the Web which can serve as an auxiliary bridge. In this paper, we propose a Heterogeneous Translated Hashing (HTH) method with such auxiliary bridge incorporated not only to improve current multi-view search but also to enable similarity search across heterogeneous media which have no direct correspondence. HTH provides more flexible and discriminative ability by embedding heterogeneous media into different Hamming spaces, compared to almost all existing methods that map heterogeneous data in a common Hamming space. We formulate a joint optimization model to learn hash functions embedding heterogeneous media into different Hamming spaces, and a translator aligning different Hamming spaces. The extensive experiments on two real-world datasets, one publicly available dataset of Flickr, and the other MIRFLICKR-Yahoo Answers dataset, highlight the effectiveness and efficiency of our algorithm.",2016,Knowledge Discovery and Data Mining,Fields of study: locality sensitive hashingscalabilityworld wide webinformation retrievaldata miningmachine learningcomputer science
Introduction to the Special Issue of Best Papers in ACM SIGKDD 2014,"Wei Wang (University of California, Los Angeles)Jure Leskovec (Stanford University)","2315689540,1878631932",-,2016,Knowledge Discovery and Data Mining,-
Co-Clustering Structural Temporal Data with Applications to Semiconductor Manufacturing,Yada Zhu (IBM)Jingrui He (Arizona State University),"2712115745,2693123770","Recent years have witnessed data explosion in semiconductor manufacturing due to advances in instrumentation and storage techniques. The large amount of data associated with process variables monitored over time form a rich reservoir of information, which can be used for a variety of purposes, such as anomaly detection, quality control, and fault diagnostics. In particular, following the same recipe for a certain Integrated Circuit device, multiple tools and chambers can be deployed for the production of this device, during which multiple time series can be collected, such as temperature, impedance, gas flow, electric bias, etc. These time series naturally fit into a two-dimensional array (matrix), i.e., each element in this array corresponds to a time series for one process variable from one chamber. To leverage the rich structural information in such temporal data, in this article, we propose a novel framework named C-Struts to simultaneously cluster on the two dimensions of this array. In this framework, we interpret the structural information as a set of constraints on the cluster membership, introduce an auxiliary probability distribution accordingly, and design an iterative algorithm to assign each time series to a certain cluster on each dimension. Furthermore, we establish the equivalence between C-Struts and a generic optimization problem, which is able to accommodate various distance functions. Extensive experiments on synthetic, benchmark, as well as manufacturing datasets demonstrate the effectiveness of the proposed method.",2016,Knowledge Discovery and Data Mining,Fields of study: structuresemiconductorbiclusteringdata miningmachine learningsimulationstatisticscomputer science
CGC: A Flexible and Robust Approach to Integrating Co-Regularized Multi-Domain Graph for Clustering,"Wei Cheng (University of North Carolina at Chapel Hill)Zhishan Guo (University of North Carolina at Chapel Hill)Xiang Zhang (Case Western Reserve University)Wei Wang (University of California, Los Angeles)","2620045292,2165841276,2553248206,2315689540","Multi-view graph clustering aims to enhance clustering performance by integrating heterogeneous information collected in different domains. Each domain provides a different view of the data instances. Leveraging cross-domain information has been demonstrated an effective way to achieve better clustering results. Despite the previous success, existing multi-view graph clustering methods usually assume that different views are available for the same set of instances. Thus, instances in different domains can be treated as having strict one-to-one relationship. In many real-life applications, however, data instances in one domain may correspond to multiple instances in another domain. Moreover, relationships between instances in different domains may be associated with weights based on prior (partial) knowledge. In this article, we propose a flexible and robust framework, Co-regularized Graph Clustering (CGC), based on non-negative matrix factorization (NMF), to tackle these challenges. CGC has several advantages over the existing methods. First, it supports many-to-many cross-domain instance relationship. Second, it incorporates weight on cross-domain relationship. Third, it allows partial cross-domain mapping so that graphs in different domains may have different sizes. Finally, it provides users with the extent to which the cross-domain instance relationship violates the in-domain clustering structure, and thus enables users to re-evaluate the consistency of the relationship. We develop an efficient optimization method that guarantees to find the global optimal solution with a given confidence requirement. The proposed method can automatically identify noisy domains and assign smaller weights to them. This helps to obtain optimal graph partition for the focused domain. Extensive experimental results on UCI benchmark datasets, newsgroup datasets, and biological interaction networks demonstrate the effectiveness of our approach.",2016,Knowledge Discovery and Data Mining,Fields of study: correlation clusteringconstrained clusteringclustering coefficientfuzzy clusteringnon negative matrix factorizationcluster analysisdata miningmachine learningmathematical optimizationcomputer sciencemathematics
Jointly Modeling Label and Feature Heterogeneity in Medical Informatics,Pei Yang (Arizona State University)Hongxia Yang (IBM)Haoda Fu (Eli Lilly and Company)Dawei Zhou (Arizona State University)Jieping Ye (University of Michigan)Theodoros Lappas (Stevens Institute of Technology)Jingrui He (Arizona State University),"2309210939,2309208648,2683856079,2303436841,2305258894,2028397797,2693123770","Multiple types of heterogeneity including label heterogeneity and feature heterogeneity often co-exist in many real-world data mining applications, such as diabetes treatment classification, gene functionality prediction, and brain image analysis. To effectively leverage such heterogeneity, in this article, we propose a novel graph-based model for Learning with both Label and Feature heterogeneity, namely L 2 F . It models the label correlation by requiring that any two label-specific classifiers behave similarly on the same views if the associated labels are similar, and imposes the view consistency by requiring that view-based classifiers generate similar predictions on the same examples. The objective function for L 2 F is jointly convex. To solve the optimization problem, we propose an iterative algorithm, which is guaranteed to converge to the global optimum. One appealing feature of L 2 F is that it is capable of handling data with missing views and labels. Furthermore, we analyze its generalization performance based on Rademacher complexity, which sheds light on the benefits of jointly modeling the label and feature heterogeneity. Experimental results on various biomedical datasets show the effectiveness of the proposed approach.",2016,Knowledge Discovery and Data Mining,Fields of study: health informaticsdata sciencedata miningmachine learningstatisticscomputer science
"Mining Dual Networks: Models, Algorithms, and Applications",Yubao Wu (Case Western Reserve University)Xiaofeng Zhu (Case Western Reserve University)Li Li (Case Western Reserve University)Wei Fan (Baidu)Ruoming Jin (Kent State University)Xiang Zhang (Case Western Reserve University),"2481087172,2212746422,2694743028,2422054197,2119237514,2553248206","Finding the densest subgraph in a single graph is a fundamental problem that has been extensively studied. In many emerging applications, there exist dual networks. For example, in genetics, it is important to use protein interactions to interpret genetic interactions. In this application, one network represents physical interactions among nodes, for example, protein--protein interactions, and another network represents conceptual interactions, for example, genetic interactions. Edges in the conceptual network are usually derived based on certain correlation measure or statistical test measuring the strength of the interaction. Two nodes with strong conceptual interaction may not have direct physical interaction. In this article, we propose the novel dual-network model and investigate the problem of finding the densest connected subgraph (DCS), which has the largest density in the conceptual network and is also connected in the physical network. Density in the conceptual network represents the average strength of the measured interacting signals among the set of nodes. Connectivity in the physical network shows how they interact physically. Such pattern cannot be identified using the existing algorithms for a single network. We show that even though finding the densest subgraph in a single network is polynomial time solvable, the DCS problem is NP-hard. We develop a two-step approach to solve the DCS problem. In the first step, we effectively prune the dual networks, while guarantee that the optimal solution is contained in the remaining networks. For the second step, we develop two efficient greedy methods based on different search strategies to find the DCS. Different variations of the DCS problem are also studied. We perform extensive experiments on a variety of real and synthetic dual networks to evaluate the effectiveness and efficiency of the developed methods.",2016,Knowledge Discovery and Data Mining,Fields of study: interaction networkcombinatoricsmachine learningalgorithmcomputer sciencemathematics
Inferring Dynamic Diffusion Networks in Online Media,Maryam Tahani (Sharif University of Technology)Ali Mohammad Afshin Hemmatyar (Sharif University of Technology)Hamid R. Rabiee (Sharif University of Technology)Maryam Ramezani (Sharif University of Technology),"2499910170,1906074235,2032985511,2493360359","Online media play an important role in information societies by providing a convenient infrastructure for different processes. Information diffusion that is a fundamental process taking place on social and information networks has been investigated in many studies. Research on information diffusion in these networks faces two main challenges: (1) In most cases, diffusion takes place on an underlying network, which is latent and its structure is unknown. (2) This latent network is not fixed and changes over time. In this article, we investigate the diffusion network extraction (DNE) problem when the underlying network is dynamic and latent. We model the diffusion behavior (existence probability) of each edge as a stochastic process and utilize the Hidden Markov Model (HMM) to discover the most probable diffusion links according to the current observation of the diffusion process, which is the infection time of nodes and the past diffusion behavior of links. We evaluate the performance of our Dynamic Diffusion Network Extraction (DDNE) method, on both synthetic and real datasets. Experimental results show that the performance of the proposed method is independent of the cascade transmission model and outperforms the state of art method in terms of F-measure.",2016,Knowledge Discovery and Data Mining,Fields of study: dynamic network analysisdigital mediahidden markov modelartificial intelligencemachine learningsimulationcomputer science
Biomedical Ontology Quality Assurance Using a Big Data Approach,Licong Cui (University of Kentucky)Shiqiang Tao (University of Kentucky)Guo Qiang Zhang (University of Kentucky),"2147120483,2119318853,2588264867","This article presents recent progresses made in using scalable cloud computing environment, Hadoop and MapReduce, to perform ontology quality assurance (OQA), and points to areas of future opportunity. The standard sequential approach used for implementing OQA methods can take weeks if not months for exhaustive analyses for large biomedical ontological systems. With OQA methods newly implemented using massively parallel algorithms in the MapReduce framework, several orders of magnitude in speed-up can be achieved (e.g., from three months to three hours). Such dramatically reduced time makes it feasible not only to perform exhaustive structural analysis of large ontological hierarchies, but also to systematically track structural changes between versions for evolutional analysis. As an exemplar, progress is reported in using MapReduce to perform evolutional analysis and visualization on the Systemized Nomenclature of Medicine—Clinical Terms (SNOMED CT), a prominent clinical terminology system. Future opportunities in three areas are described: one is to extend the scope of MapReduce-based approach to existing OQA methods, especially for automated exhaustive structural analysis. The second is to apply our proposed MapReduce Pipeline for Lattice-based Evaluation (MaPLE) approach, demonstrated as an exemplar method for SNOMED CT, to other biomedical ontologies. The third area is to develop interfaces for reviewing results obtained by OQA methods and for visualizing ontological alignment and evolution, which can also take advantage of cloud computing technology to systematically pre-compute computationally intensive jobs in order to increase performance during user interactions with the visualization interface. Advances in these directions are expected to better support the ontological engineering lifecycle.",2016,Knowledge Discovery and Data Mining,Fields of study: snomed ctlatticedata sciencedata miningdatabasecomputer science
Guest Editorial: Special Issue on Connected Health at Big Data Era (BigChat): A TKDD Special Issue,Hanghang Tong (Arizona State University)Fei Wang (University of Connecticut)Munmun De Choudhury (Georgia Institute of Technology)Zoran Obradovic (Temple University),"2667261544,2465953593,2123618928,2029694244",-,2016,Knowledge Discovery and Data Mining,Fields of study: universal hashingscalabilitydata sciencetheoretical computer scienceinformation retrievalcomputer science
Unsupervised Rare Pattern Mining: A Survey,Yun Sing Koh (University of Auckland)Sri Devi Ravana (University of Malaya),"1987110578,2557241765","Association rule mining was first introduced to examine patterns among frequent items. The original motivation for seeking these rules arose from need to examine customer purchasing behaviour in supermarket transaction data. It seeks to identify combinations of items or itemsets, whose presence in a transaction affects the likelihood of the presence of another specific item or itemsets. In recent years, there has been an increasing demand for rare association rule mining. Detecting rare patterns in data is a vital task, with numerous high-impact applications including medical, finance, and security. This survey aims to provide a general, comprehensive, and structured overview of the state-of-the-art methods for rare pattern mining. We investigate the problems in finding rare rules using traditional association rule mining. As rare association rule mining has not been well explored, there is still specific groundwork that needs to be established. We will discuss some of the major issues in rare association rule mining and also look at current algorithms. As a contribution, we give a general framework for categorizing algorithms: Apriori and Tree based. We highlight the differences between these methods. Finally, we present several real-world application using rare pattern mining in diverse domains. We conclude our survey with a discussion on open and practical challenges in the field.",2016,Knowledge Discovery and Data Mining,Fields of study: k optimal pattern discoveryassociation rule learningdata scienceoperations researchdata miningmachine learningcomputer science
Kernelized Information-Theoretic Metric Learning for Cancer Diagnosis Using High-Dimensional Molecular Profiling Data,Feiyu Xiong (Drexel University)Moshe Kam (New Jersey Institute of Technology)Leonid Hrebien (Drexel University)Beilun Wang (University of Virginia)Yanjun Qi (University of Virginia),"2131398225,2678317542,221359174,2357637447,2255911127","With the advancement of genome-wide monitoring technologies, molecular expression data have become widely used for diagnosing cancer through tumor or blood samples. When mining molecular signature data, the process of comparing samples through an adaptive distance function is fundamental but difficult, as such datasets are normally heterogeneous and high dimensional. In this article, we present kernelized information-theoretic metric learning (KITML) algorithms that optimize a distance function to tackle the cancer diagnosis problem and scale to high dimensionality. By learning a nonlinear transformation in the input space implicitly through kernelization, KITML permits efficient optimization, low storage, and improved learning of distance metric. We propose two novel applications of KITML for diagnosing cancer using high-dimensional molecular profiling data: (1) for sample-level cancer diagnosis, the learned metric is used to improve the performance of k -nearest neighbor classification; and (2) for estimating the severity level or stage of a group of samples, we propose a novel set-based ranking approach to extend KITML. For the sample-level cancer classification task, we have evaluated on 14 cancer gene microarray datasets and compared with eight other state-of-the-art approaches. The results show that our approach achieves the best overall performance for the task of molecular-expression-driven cancer sample diagnosis. For the group-level cancer stage estimation, we test the proposed set-KITML approach using three multi-stage cancer microarray datasets, and correctly estimated the stages of sample groups for all three studies.",2016,Knowledge Discovery and Data Mining,Fields of study: clustering high dimensional databioinformaticsdata miningmachine learningcomputer sciencemathematics
"Featuring, Detecting, and Visualizing Human Sentiment in Chinese Micro-Blog",Zhiwen Yu (Northwestern Polytechnical University)Zhitao Wang (Northwestern Polytechnical University)Liming Chen (De Montfort University)Bin Guo (Northwestern Polytechnical University)Wenjie Li (Hong Kong Polytechnic University),"2118377178,2226468868,2155463703,2105384808,2139868060","Micro-blog has been increasingly used for the public to express their opinions, and for organizations to detect public sentiment about social events or public policies. In this article, we examine and identify the key problems of this field, focusing particularly on the characteristics of innovative words, multi-media elements, and hierarchical structure of Chinese “Weibo.” Based on the analysis, we propose a novel approach and develop associated theoretical and technological methods to address these problems. These include a new sentiment word mining method based on three wording metrics and point-wise information, a rule set model for analyzing sentiment features of different linguistic components, and the corresponding methodology for calculating sentiment on multi-granularity considering emoticon elements as auxiliary affective factors. We evaluate our new word discovery and sentiment detection methods on a real-life Chinese micro-blog dataset. Initial results show that our new diction can improve sentiment detection, and they demonstrate that our multi-level rule set method is more effective, with the average accuracy being 10.2p and 1.5p higher than two existing methods for Chinese micro-blog sentiment analysis. In addition, we exploit visualization techniques to study the relationships between online sentiment and real life. The visualization of detected sentiment can help depict temporal patterns and spatial discrepancy.",2016,Knowledge Discovery and Data Mining,Fields of study: visualizationsentiment analysisdata scienceworld wide webdata miningcomputer science
"Matrices, Compression, Learning Curves: Formulation, and the GroupNteach Algorithms",Bryan Hooi (Carnegie Mellon University)Hyun Ah Song (Carnegie Mellon University)Evangelos E. Papalexakis (Carnegie Mellon University)Rakesh Agrawal (IBM)Christos Faloutsos (Carnegie Mellon University),"1755863881,2158947101,1418764031,2138427228,2198983026","Suppose you are a teacher, and have to convey a set of object-property pairs 'lions eat meat'. A good teacher will convey a lot of information, with little effort on the student side. What is the best and most intuitive way to convey this information to the student, without the student being overwhelmed? A related, harder problem is: how can we assign a numerical score to each lesson plan i.e., way of conveying information? Here, we give a formal definition of this problem of forming learning units and we provide a metric for comparing different approaches based on information theory. We also design an algorithm, groupNteach, for this problem. Our proposed groupNteach is scalable near-linear in the dataset size; it is effective, achieving excellent results on real data, both with respect to our proposed metric, but also with respect to encoding length; and it is intuitive, conforming to well-known educational principles. Experiments on real and synthetic datasets demonstrate the effectiveness of groupNteach.",2016,Knowledge Discovery and Data Mining,Fields of study: theoretical computer sciencedata miningartificial intelligencemachine learningsimulationstatisticscomputer sciencemathematics
Hashing-Based Distributed Multi-party Blocking for Privacy-Preserving Record Linkage,Thilina Ranbaduge (Australian National University)Dinusha Vatsalan (Australian National University)Peter Christen (Australian National University)Vassilios S. Verykios (Hellenic Open University),"993935970,145048928,2023765750,77221159","In many application domains organizations require information from multiple sources to be integrated. Due to privacy and confidentiality concerns often these organizations are not willing or allowed to reveal their sensitive and personal data to other database owners, and to any external party. This has led to the emerging research discipline of privacy-preserving record linkage PPRL. We propose a novel blocking approach for multi-party PPRL to efficiently and effectively prune the record sets that are unlikely to match. Our approach allows each database owner to perform blocking independently except for the initial agreement of parameter settings and a final central hashing-based clustering. We provide an analysis of our technique in terms of complexity, quality, and privacy, and conduct an empirical study with large datasets. The results show that our approach is scalable with the size of the datasets and the number of parties, while providing better quality and privacy than previous multi-party private blocking approaches.",2016,Knowledge Discovery and Data Mining,Fields of study: locality sensitive hashingbloom filtercluster analysisworld wide webdata miningdatabasemachine learningcomputer science
Dual Similarity Regularization for Recommendation,Jing Zheng (Beijing University of Posts and Telecommunications)Jian Liu (Beijing University of Posts and Telecommunications)Chuan Shi (Beijing University of Posts and Telecommunications)Fuzhen Zhuang (Chinese Academy of Sciences)Jingzhi Li (Southern University and A&M College)Bin Wu (Beijing University of Posts and Telecommunications),"2624298923,2501726924,2252461150,2050314250,2656297484,2464938123","Recently, social recommendation becomes a hot research direction, which leverages social relations among users to alleviate data sparsity and cold-start problems in recommender systems. The social recommendation methods usually employ simple similarity information of users as social regularization on users. Unfortunately, the widely used social regularization may suffer from several aspects: 1 the similarity information of users only stems from users' social relations; 2 it only has constraint on users; 3 it may not work well for users with low similarity. In order to overcome the shortcomings of social regularization, we propose a new dual similarity regularization to impose the constraint on users and items with high and low similarities simultaneously. With the dual similarity regularization, we design an optimization function to integrate the similarity information of users and items, and a gradient descend solution is derived to optimize the objective function. Experiments on two real datasets validate the effectiveness of the proposed solution.",2016,Knowledge Discovery and Data Mining,Fields of study: regularization perspectives on support vector machinesregularizationinformation retrievaldata miningmachine learningcomputer sciencemathematics
Indoor Positioning System for Smart Homes Based on Decision Trees and Passive RFID,Frédéric Bergeron (Université de Sherbrooke)Kevin Bouchard (Université de Sherbrooke)Sébastien Gaboury (Université du Québec)Sylvain Giroux (Université de Sherbrooke)Bruno Bouchard (Université du Québec),"2224459328,2132358045,2011809654,2146759314,2147421945","This paper presents a novel Indoor Positioning System IPS for objects of daily life equipped with passive RFID tags. The goal is to provide a simple to use, yet accurate, qualitative IPS for housing enhanced with technology sensors, effectors, etc.. With such a service, the housing, namely called smart home, could enable a wide range of services by being able to better understand the context and the current progression of activities of daily living. The paper shows that classical data mining techniques can be applied to raw data from RFID readers and passive tags. In particular, it explains how we built several datasets using a tagged object in a real smart home infrastructure. Our method was proven very effective as most algorithms result in high accuracy for the majority of the smart home.",2016,Knowledge Discovery and Data Mining,Fields of study: random forestradio frequency identificationdecision treeembedded systemtelecommunicationscomputer securitymachine learningcomputer science
Incremental Hierarchical Clustering ofźStochastic Pattern-Based Symbolic Data,Xin XuJiaheng Lu (University of Helsinki)Wei Wang (Nanjing University),"2717337900,2695000131,2658432775","Classic data analysis techniques generally assume that variables have single values only. However, the data complexity during the age of big data has gone beyond the classic framework such that variable values probably take the form of a set of stochastic measurements instead. We refer to the above case as the stochastic pattern-based symbolic data where each measurement set is an instance of an underlying stochastic pattern. In such a case, non existing classic data analysis approaches, such as the crystal item or fuzzy region ones, could apply yet. For this reason, we put forward a novel Incremental Hierarchical Clustering algorithm for stochastic Pattern-based Symbolic Data IHCPSD. IHCPSD is robust to overlapping and missing measurements and well adapted for incremental learning. Experiments on synthetic and application on real-life emitter parameter data have validated its effectiveness.",2016,Knowledge Discovery and Data Mining,Fields of study: symbolic data analysishierarchical clusteringdata miningpattern recognitionmachine learningstatisticscomputer science
Social Group Based Video Recommendation Addressing the Cold-Start Problem,Chunfeng Yang (The Chinese University of Hong Kong)Yipeng Zhou (Shenzhen University)Liang Chen (Shenzhen University)Xiaopeng Zhang (Tencent)Dah Ming Chiu (The Chinese University of Hong Kong),"2237381698,2096849978,2714252513,2520293188,2144653979","Video recommendation has become an essential part of online video services. Cold start, a problem relatively common in the practical online video recommendation service, occurs when the user who needs video recommendation has no viewing history Cold start consists of the new-user problem and the new-item problem. In this paper, we discuss the new-user one. A promising approach to resolve this problem is to capitalize on information in online social networks OSNs: Videos viewed by a user's friends may be good candidates for recommendation. However, in practice, this information is also quite limited, either because of insufficient friends or lack of abundant viewing history of friends. In this work, we utilize social groups with richer information to recommend videos. It is common that users may be affiliated with multiple groups in OSNs. Through members within the same group, we can reach a considerably larger set of users, hence more candidate videos for recommendation. In this paper, by collaborating with Tencent Video, we propose a social-group-based algorithm to produce personalized video recommendations by ranking candidate videos from the groups a user is affiliated with. This algorithm was implemented and tested in the Tencent Video service system. Compared with two state-of-the-art methods, the proposed algorithm not only improves the click-through rate, but also recommends more diverse videos.",2016,Knowledge Discovery and Data Mining,Fields of study: internet privacymultimediaworld wide webdata miningcomputer science
DeepCare: A Deep Dynamic Memory Model forźPredictive Medicine,Trang Pham (Deakin University)Truyen Tran 0001 (Deakin University)Dinh Q. Phung (Deakin University)Svetha Venkatesh (Deakin University),"2503142456,2157392948,2314522249,2146461601","Personalized predictive medicine necessitates modeling of patient illness and care processes, which inherently have long-term temporal dependencies. Healthcare observations, recorded in electronic medical records, are episodic and irregular in time. We introduce DeepCare, a deep dynamic neural network that reads medical records and predicts future medical outcomes. At the data level, DeepCare models patient health state trajectories with explicit memory of illness. Built on Long Short-Term Memory LSTM, DeepCare introduces time parameterizations to handle irregular timing by moderating the forgetting and consolidation of illness memory. DeepCare also incorporates medical interventions that change the course of illness and shape future medical risk. Moving upi¾źto the health state level, historical and present health states are then aggregated through multiscale temporal pooling, before passing through a neural network that estimates future outcomes. We demonstrate the efficacy of DeepCare for disease progression modeling and readmission prediction in diabetes, a chronic disease with large economic burden. The results show improved modeling and risk prediction accuracy.",2016,Knowledge Discovery and Data Mining,Fields of study: data sciencedata miningmachine learningsimulationcomputer sciencemathematics
Joint Recognition and Segmentation of Actions via Probabilistic Integration of Spatio-Temporal Fisher Vectors,Johanna Carvajal (University of Queensland)Chris McCool (Queensland University of Technology)Brian C. Lovell (University of Queensland)Conrad Sanderson (University of Queensland),"2284811930,2307648760,2689104063,2090845954","We propose a hierarchical approach to multi-action recognition that performs joint classification and segmentation. Ai¾?given video containing several consecutive actions is processed via a sequence of overlapping temporal windows. Each frame in a temporal window is represented through selective low-level spatio-temporal features which efficiently capture relevant local dynamics. Features from each window are represented as a Fisher vector, which captures first and second order statistics. Instead of directly classifying each Fisher vector, it is converted into a vector of class probabilities. The final classification decision for each frame is then obtained by integrating the class probabilities at the frame level, which exploits the overlapping of the temporal windows. Experiments were performed on two datasets: s-KTH ai¾?stitched version of the KTH dataset to simulate multi-actions, and the challenging CMU-MMAC dataset. On s-KTH, the proposed approach achieves an accuracy of 85.0i¾?%, significantly outperforming two recent approaches based on GMMs and HMMs which obtained 78.3i¾?% and 71.2i¾?%, respectively. On CMU-MMAC, the proposed approach achieves an accuracy of 40.9i¾?%, outperforming the GMM and HMM approaches which obtained 33.7i¾?% and 38.4i¾?%, respectively. Furthermore, the proposed system is on average 40 times faster than the GMM based approach.",2016,Knowledge Discovery and Data Mining,Fields of study: segmentationspeech recognitionpattern recognitionmachine learningstatisticscomputer science
Robust Multi-view Manifold Ranking for Image Retrieval,Jun Wu (Beijing Jiaotong University)Jianbo Yuan (University of Rochester)Jiebo Luo (University of Rochester),"2639188716,2128154613,2059910451","Graph-based similarity ranking plays a key role in improving image retrieval performance. Its current trend is to fuse the ranking results from multiple feature sets, including textual feature, visual feature and query log feature, to elevate the retrieval effectiveness. The primary challenge is how to effectively exploit the complementary properties of different features. Another tough issue is the highly noisy features contributed by users, such as textual tags and query logs, which makes the exploration of such complementary properties difficult. This paper proposes a Multi-view Manifold Ranking M2R framework, in which multiple graphs built on different features are integrated to simultaneously encode the similarity ranking. To deal with the high noise issue inherent in the user-contributed features, a data cleaning solution based on visual-neighbor voting is embedded into M2R, thus called Robust M2R RM2R. Experimental results show that the proposed method significantly outperforms the existing approaches, especially when the user-contributed features are highly noisy.",2016,Knowledge Discovery and Data Mining,Fields of study: rankingranking svmvisual worddata cleansingimage retrievalinformation retrievaldata miningpattern recognitionmachine learningcomputer science
Normalized Cross-Match: Pattern Discovery Algorithm from Biofeedback Signals,Xueyuan Gong (University of Macau)Simon Fong (University of Macau)Yain-Whar Si (University of Macau)Robert P. Biuk-Agha (University of Macau)Raymond K. Wong (University of New South Wales)Athanasios V. Vasilakos (Luleå University of Technology),"2109812359,2132140839,2137049483,2530744559,2290077475,2567820109","Biofeedback signals are important elements in critical care applications, such as monitoring ECG data of a patient, discovering patterns from large amount of ECG data sets, detecting outliers from ECG data, etc. Because the signal data update continuously and the sampling rates may be different, time-series data stream is harder to be dealt with compared to traditional historical time-series data. For the pattern discovery problem on time-series streams, Toyoda proposed the CrossMatch CM approach to discover the patterns between two time-series data streams sequences, which requires only On time per data update, where n is the length of one sequence. CM, however, does not support normalization, which is required for some kinds of sequences e.g. EEG data, ECG data. Therefore, we propose a normalized-CrossMatch approach NCM that extends CM to enforce normalization while maintaining the same performance capabilities.",2016,Knowledge Discovery and Data Mining,Fields of study: data sciencedata miningdatabasecomputer science
Enhanced SVD for Collaborative Filtering,Xin Guan (University of Warwick)Chang-Tsun Li (University of Warwick)Yu Guan (Newcastle University),"2551562102,2130874684,2099546537","Matrix factorization is one of the most popular techniques for prediction problems in the fields of intelligent systems and data mining. It has shown its effectiveness in many real-world applications such as recommender systems. As a collaborative filtering method, it gives users recommendations based on their previous preferences or ratings. Due to the extreme sparseness of the ratings matrix, active learning is used for eliciting ratings for a user to get better recommendations. In this paper, we propose a new matrix factorization model called Enhanced SVD ESVD which combines the classic matrix factorization method with a specific rating elicitation strategy. We evaluate the proposed ESVD method on the Movielens data set, and the experimental results suggest its effectiveness in terms of both accuracy and efficiency, when compared with traditional matrix factorization methods and active learning methods.",2016,Knowledge Discovery and Data Mining,Fields of study: non negative matrix factorizationmatrix decompositionrecommender systeminformation retrievaldata miningmachine learningcomputer science
Unsupervised Parameter Estimation for One-Class Support Vector Machines,Zahra Ghafoori (University of Melbourne)Sutharshan Rajasegarar (Deakin University)Sarah M. Erfani (University of Melbourne)Shanika Karunasekera (University of Melbourne)Christopher A. Leckie (University of Melbourne),"2537358011,249221098,1965944821,1271724398,2111831791","Although the hyper-plane based One-Class Support Vector Machine OCSVM and the hyper-spherical based Support Vector Data Description SVDD algorithms have been shown to be very effective in detecting outliers, their performance on noisy and unlabeled training data has not been widely studied. Moreover, only a few heuristic approaches have been proposed to set the different parameters of these methods in an unsupervised manner. In this paper, we propose two unsupervised methods for estimating the optimal parameter settings to train OCSVM and SVDD models, based on analysing the structure of the data. We show that our heuristic is substantially faster than existing parameter estimation approaches while its accuracy is comparable with supervised parameter learning methods, such as grid-search with cross-validation on labeled data. In addition, our proposed approaches can be used to prepare a labeled data set for a OCSVM or a SVDD from unlabeled data.",2016,Knowledge Discovery and Data Mining,Fields of study: estimation theoryanomaly detectiondata miningpattern recognitionmachine learningstatisticscomputer science
Matching Product Offers of E-Shops,Andrea Horch (Fraunhofer Society)Holger Kett (Fraunhofer Society)Anette Weisbecker (Fraunhofer Society),"2229710468,2014678682,1976770254","E-commerce is a continuously growing and competitive market. There are several motivations for e-shoppers, sellers and manufacturers to require an automated approach for matching product offers from various online sources referring to the same or a similar real-world product. Currently, there are several approaches for the assignment of identical and similar product offers. These existing approaches are not sufficient for performing a precise comparison as they only return a similarity value for two compared products but do not give any information for further calculations and analyses. The contribution of this paper is a novel approach and an algorithm for matching identical and very similar product offers based on the pairwise comparison of the product names. For this purpose the approach uses different similarity values which are based on an existing string similarity measure. The approach is independent from a specific product domain or data source.",2016,Knowledge Discovery and Data Mining,Fields of study: web miningdata miningcomputer science
Collaborative Deep Ranking: A Hybrid Pair-Wise Recommendation Algorithm with Implicit Feedback,Haochao Ying (Zhejiang University)Liang Chen (RMIT University)Yuwen Xiong (Zhejiang University)Jian Wu (Zhejiang University),"2146051823,2685275606,2436419321,2525822525","Collaborative Filtering with Implicit Feedbacks e.g., browsing or clicking records, named as CF-IF, is demonstrated to be an effective way in recommender systems. Existing works of CF-IF can be mainly classified into two categories, i.e., point-wise regression based and pair-wise ranking based, where the latter one relaxes assumption and usually obtains better performance in empirical studies. In real applications, implicit feedback is often very sparse, causing CF-IF based methods to degrade significantly in recommendation performance. In this case, side information e.g., item content is usually introduced and utilized to address the data sparsity problem. Nevertheless, the latent feature representation learned from side information by topic model may not be very effective when the data is too sparse. To address this problem, we propose collaborative deep ranking CDR, a hybrid pair-wise approach with implicit feedback, which leverages deep feature representation of item content into Bayesian framework of pair-wise ranking model in this paper. The experimental analysis on a real-world dataset shows CDR outperforms three state-of-art methods in terms of recall metric under different sparsity level.",2016,Knowledge Discovery and Data Mining,Fields of study: information retrievaldata miningmachine learningcomputer science
Multiple Seeds Based Evolutionary Algorithm for Mining Boolean Association Rules,Mir Md. Jahangir Kabir (University of Tasmania)Shuxiang Xu (University of Tasmania)Byeong Ho Kang (University of Tasmania)Zongyuan Zhao (University of Tasmania),"2139298541,2101723723,2162029091,2095713079","Most of the association rule mining algorithms use a single seed for initializing a population without paying attention to the effectiveness of an initial population in an evolutionary learning. Recently, researchers show that an initial population has significant effects on producing good solutions over several generations of a genetic algorithm. There are two significant challenges raised by single seed based genetic algorithms for real world applications: 1 solutions of a genetic algorithm are varied, since different seeds generate different initial populations, 2 it is a hard process to define an effective seed for a specific application. To avoid these problems, in this paper we propose a new multiple seeds based genetic algorithm MSGA which generates multiple seeds from different domains of a solution space to discover high quality rules from a large data set. This approach introduces m-domain model and m-seeds selection process through which the whole solution space is subdivided into m-number of same size domains and from each domain it selects a seed. By using these seeds, this method generates an effective initial population to perform an evolutionary learning of the fitness value of each rule. As a result, this method obtains strong searching efficiency at the beginning of the evolution and achieves fast convergence along with the evolution. MSGA is tested with different mutation and crossover operators for mining interesting Boolean association rules from different real world data sets and compared the results with different single seeds based genetic algorithms.",2016,Knowledge Discovery and Data Mining,Fields of study: population based incremental learningcultural algorithmconditional probabilitygenetic algorithmbioinformaticsdata miningmachine learningstatisticscomputer sciencemathematics
Mirror on the Wall: Finding Similar Questions with Deep Structured Topic Modeling,"Arpita Das (International Institute of Information Technology, Hyderabad)Manish Shrivastava (International Institute of Information Technology, Hyderabad)Manoj Kumar Chinnakotla (Microsoft)","2489785726,2251125838,2250862700","Internet users today prefer getting precise answers to their questions rather than sifting through a bunch of relevant documents provided by search engines. This has led to the huge popularity of Community Question Answering cQA services like Yahoo! Answers, Baidu Zhidao, Quora, StackOverflowetc., where forum users respond to questions with precise answers. Over time, such cQA archives become rich repositories of knowledge encoded in the form of questions and user generated answers. In cQA archives, retrieval of similar questions, which have already been answered in some form, is important for improving the effectiveness of such forums. The main challenge while retrieving similar questions is the ""lexico-syntactic"" gap between the user query and the questions already present in the forum. In this paper, we propose a novel approach called ""Deep Structured Topic Model DSTM"" to bridge the lexico-syntactic gap between the question posed by the user and forum questions. DSTM employs a two-step process consisting of initially retrieving similar questions that lie in the vicinity of the query and latent topic vector space and then re-ranking them using a deep layered semantic model. Experiments on large scale real-life cQA dataset show that our approach outperforms the state-of-the-art translation and topic based baseline approaches.",2016,Knowledge Discovery and Data Mining,Fields of study: world wide webinformation retrievaldata miningmachine learningcomputer science
A Precise and Robust Clustering Approach Using Homophilic Degrees of Graph Kernel,Haolin Yang (Tsinghua University)Deli Zhao (Microsoft)Lele Cao (Tsinghua University)Fuchun Sun (Tsinghua University),"2154347452,2139968061,2113194918,2118495735","To address the difficulties of ""data noise sensitivity"" and ""cluster center variance"" in mainstream clustering algorithms, we propose a novel robust approach for identifying cluster centers unambiguously from data contaminated with noise; it incorporates the strength of homophilic degrees and graph kernel. Exploiting that in-degrees can breed the homophilic distribution if ordered by their associated sorted out-degrees, it is easy to separate clusters from noise. Then we apply the diffusion kernel to the graph formed by clusters so as to obtain graph kernel matrix, which is treated as the measurement of global similarities. Based on local data densities and global similarities, the proposed approach manages to identify cluster centers precisely. Experiments on various synthetic and real-world databases verify the superiority of our algorithm in comparison with state-of-the-art algorithms.",2016,Knowledge Discovery and Data Mining,Fields of study: theoretical computer sciencedata miningmachine learningstatisticscomputer science
Image Segmentation with Superpixel Based Covariance Descriptor,Xianbin Gu (University of Otago)Martin K. Purvis (University of Otago),"2224368314,2010549645","This paper investigates the problem of image segmentation using superpixels. We propose two approaches to enhance the discriminative ability of the superpixel's covariance descriptors. In the first one, we employ the Log-Euclidean distance as the metric on the covariance manifolds, and then use the RBF kernel to measure the similarities between covariance descriptors. The second method is focused on extracting the subspace structure of the set of covariance descriptors by extending a low rank representation algorithm on to the covariance manifolds. Experiments are carried out with the Berkly Segmentation Dataset, and compared with the state-of-the-art segmentation algorithms, both methods are competitive.",2016,Knowledge Discovery and Data Mining,Fields of study: scale space segmentationcovariance intersectionmanifoldimage segmentationcovariance matrixcomputer visionpattern recognitionmachine learningstatisticscomputer sciencemathematics
A Greedy Algorithm to Construct L1 Graph with Ranked Dictionary,Shuchu Han (Stony Brook University)Hong Qin (Stony Brook University),"2276591736,2157485457","$$\mathcal {L}_1$$ graph is an effective way to represent data samples in many graph-oriented machine learning applications. Its original construction algorithm is nonparametric, and the graphs it generates may have high sparsity. Meanwhile, the construction algorithm also requires many iterative convex optimization calculations and is very time-consuming. Such characteristics would severely limit the application scope of $$\mathcal {L}_1$$ graph in many real-world tasks. In this paper, we design a greedy algorithm to speed up the construction of $$\mathcal {L}_1$$ graph. Moreover, we introduce the concept of ""Ranked Dictionary"" for $$\mathcal {L}_1$$ minimization. This ranked dictionary not only preserves the locality but also removes the randomness of neighborhood selection during the process of graph construction. To demonstrate the effectiveness of our proposed algorithm, we present our experimental results on several commonly-used datasets using two different ranking strategies: one is based on Euclidean metric, and another is based on diffusion metric.",2016,Knowledge Discovery and Data Mining,Fields of study: factor critical graphdistance hereditary graphsimplex graphstrength of a graphvoltage graphcomplement graphgraph bandwidthgraph powerbutterfly graphstring graphlattice graphnull graphclique widthdense graphgraph propertycubic graphdistance regular graphline graphgraphcluster analysiscombinatoricspattern recognitionmachine learningcomputer sciencemathematics
A Music Recommendation System Based on Acoustic Features and User Personalities,Rui Cheng (University of Arizona)Boyang Tang (Delft University of Technology),"2544609326,2545145396","Music recommendation attracts great attention for music providers to improve their services as the volume of new music increases quickly. It is a great challenge for users to find their interested songs from such a large size of collections. In the previous studies, common strategies can be categorized into content-based music recommendation and collaborative music filtering. Content-based recommendation systems predict users' preferences in terms of the music content. Collaborative filtering systems predict users' ratings based on the preferences of the friends of the targeting user. In this study, we proposed a hybrid approach to provide personalized music recommendations. This is achieved by extracting audio features of songs and integrating these features and user personalities for context-aware recommendation using the state-of-the-art support vector machines SVM. Our experiments show the effectiveness of this proposed approach for personalized music recommendation.",2016,Knowledge Discovery and Data Mining,Fields of study: support vector machinemultimediaworld wide webmachine learningcomputer science
Ensembles of Interesting Subgroups for Discovering High Potential Employees,Girish Keshav Palshikar (Tata Consultancy Services)Kuleshwar Sahu (Tata Consultancy Services)Rajiv Srivastava (Tata Consultancy Services),"2602291868,2406347466,2709677202","We propose a new method for building a classifier ensemble, based on subgroup discovery techniques in data mining. We apply subgroup discovery techniques to a labeled training dataset to discover interesting subsets, characterized by a conjuctive logical expression rule, where such subset has an unusually high dominance of one class. Treating these rules as base classifiers, we propose several simple ensemble methods to construct a single classifier. Another novel aspect of the paper is that it applies these ensemble methods, along with standard anomaly detection and classification, to automatically identify high potential HIPO employees - an important problem in management. HIPO employees are critical for future-proofing the organization in the face of attrition, economic uncertainties and business challenges. Current HR processes for HIPO identification are manual and suffer from subjectivity, bias and disagreements. Proposed data-driven analytics algorithms address some of these issues. We show that the new ensemble methods perform better than other methods, including other ensemble methods on a real-life case-study dataset of a large multinational IT services company.",2016,Knowledge Discovery and Data Mining,Fields of study: data sciencedata miningmachine learningstatisticscomputer science
Reusing Extracted Knowledge in Genetic Programming to Solve Complex Texture Image Classification Problems,Muhammad Iqbal (Victoria University of Wellington)Bing Xue (Victoria University of Wellington)Mengjie Zhang (Victoria University of Wellington),"2106323235,2167531144,2097529606","Transfer learning is a process to transfer knowledge learned in one or more source tasks to a related but more complex, unseen target task, in an effort to facilitate learning in the target task. Genetic programming GP is an evolutionary approach to generating computer programs for solving a given problem automatically. Transfer learning in GP has been investigated in complex Boolean and symbolic regression problems, but not much in image classification. In this paper, we propose a novel approach to use transfer learning in GP for image classification problems. Specifically, the proposed novel approach extends an existing state-of-the-art GP method by incorporating the ability to extract useful knowledge from simpler problems of a domain and reuse the extracted knowledge to solve complex problems of the domain. The proposed system has been compared with the baseline system i.e., GP without using transfer learning on multi-class texture classification problems from three widely-used texture datasets with different rotations and different levels of noise. The experimental results showed that the ability to reuse the extracted knowledge in the proposed GP method helps achieve better classification accuracy than the baseline GP method.",2016,Knowledge Discovery and Data Mining,Fields of study: transfer of learninggenetic programmingdata miningartificial intelligencemachine learningcomputer science
Deep Feature Extraction from Trajectories forźTransportation Mode Estimation,Yuki Endo (University of Tsukuba)Hiroyuki Toda (Nippon Telegraph and Telephone)Kyosuke Nishida (Nippon Telegraph and Telephone)Akihisa Kawanobe,"2126580388,2028551331,2308334310,2560733909","This paper addresses the problem of feature extraction for estimating users' transportation modes from their movement trajectories. Previous studies have adopted supervised learning approaches and used engineers' skills to find effective features for accurate estimation. However, such hand-crafted features cannot always work well because human behaviors are diverse and trajectories include noise due to measurement error. To compensate for the shortcomings of hand-crafted features, we propose a method that automatically extracts additional features using a deep neural network DNN. In order that a DNN can easily handle input trajectories, our method converts a raw trajectory data structure into an image data structure while maintaining effective spatio-temporal information. A classification model is constructed in a supervised manner using both of the deep features and hand-crafted features. We demonstrate the effectiveness of the proposed method through several experiments using two real datasets, such as accuracy comparisons with previous methods and feature visualization.",2016,Knowledge Discovery and Data Mining,Fields of study: deep learningcomputer visiondata miningpattern recognitionmachine learningcomputer science
Learning Multi-faceted Activities from Heterogeneous Data with the Product Space Hierarchical Dirichlet Processes,Thanh-Binh Nguyen (Deakin University)Vu Nguyen (Deakin University)Svetha Venkatesh (Deakin University)Dinh Q. Phung (Deakin University),"2568150942,2097300135,2146461601,2314522249","Hierarchical Dirichlet processes HDP was originally designed and experimented for a single data channel. In this paper we enhanced its ability to model heterogeneous data using a richer structure for the base measure being a product-space. The enhanced model, called Product Space HDP PS-HDP, can 1 simultaneously model heterogeneous data from multiple sources in a Bayesian nonparametric framework and 2 discover multilevel latent structures from data to result in different types of topics/latent structures that can be explained jointly. We experimented with the MDC dataset, a large and real-world data collected from mobile phones. Our goal was to discover identity---location---time a.k.a who-where-when patterns at different levels globally for all groups and locally for each group. We provided analysis on the activities and patterns learned from our model, visualized, compared and contrasted with the ground-truth to demonstrate the merit of the proposed framework. We further quantitatively evaluated and reported its performance using standard metrics including F1-score, NMI, RI, and purity. We also compared the performance of the PS-HDP model with those of popular existing clustering methods including K-Means, NNMF, GMM, DP-Means, and AP. Lastly, we demonstrate the ability of the model in learning activities with missing data, a common problem encountered in pervasive and ubiquitous computing applications.",2016,Knowledge Discovery and Data Mining,Fields of study: health informaticsdata sciencedata miningmachine learningstatisticscomputer science
Modeling Adversarial Learning as Nested Stackelberg Games,Yan Zhou (University of Texas at Dallas)Murat Kantarcioglu (University of Texas at Dallas),"2620242970,332400322","Many data mining applications potentially operate in an adversarial environment where adversaries adapt their behavior to evade detection. Typically adversaries alter data under their control to cause a large divergence of distribution between training and test data. Existing state-of-the-art adversarial learning techniques try to address this problem in which there is only a single type of adversary. In practice, a learner often has to face multiple types of adversaries that may employ different attack tactics. In this paper, we tackle the challenges of multiple types of adversaries with a nested Stackelberg game framework. We demonstrate the effectiveness of our framework with extensive empirical results on both synthetic and real data sets. Our results demonstrate that the nested game framework offers more reliable defense against multiple types of attackers.",2016,Knowledge Discovery and Data Mining,Fields of study: distributed computingcomputer securitysimulationcomputer science
Secure k-NN Query on Encrypted Cloud Data with Limited Key-Disclosure and Offline Data Owner,Youwen Zhu (Nanjing University of Aeronautics and Astronautics)Zhikuan Wang (Nanjing University of Aeronautics and Astronautics)Yue Zhang (Nanjing University of Aeronautics and Astronautics),"2645164142,2506532745,2423907867","Recently, many schemes have been proposed to support k-nearest neighbors k-NN query on encrypted cloud data. However, existing approaches either assume query users are fully-trusted, or require data owner to be online all the time. Query users in fully-trusted assumption can access the key to encrypt/decrypt outsourced data, thus, untrusted cloud server can completely break the data upon obtaining the key from any untrustworthy query user. The online requirement introduces much cost to data owner. This paper presents a new scheme to support k-NN query on encrypted cloud database while preserving the privacy of database and query points. Our proposed approach only discloses limited information about the key to query users, and does not require an online data owner. Theoretical analysis and extensive experiments confirm the security and efficiency of our scheme.",2016,Knowledge Discovery and Data Mining,Fields of study: sargableonline aggregationweb search queryweb query classificationviewquery by examplequery expansionquery optimizationcloud computingprivacyk nearest neighbors algorithminternet privacyworld wide webdata miningdatabasemachine learningcomputer science
Efficient Page-Level Data Extraction via Schema Induction and Verification,Chia-Hui Chang (National Central University)Tian-Sheng Chen (National Central University)Ming-Chuan Chen (National Central University)Jhung-Li Ding (National Central University),"2144182650,2528017490,2528246021,2529607081","Page-level data extraction provides a complete solution for all kinds of information requirement, however very few researches focus on this task because of the difficulties and complexities in the problem. On the other hands, previous page-level systems focus on how to achieve unsupervised data extraction and pay less attention on schema/wrapper generation and verification. In this paper, we emphasize the importance of schema verification for large-scale extraction tasks. Given a large amount of web pages for data extraction, the system uses part of the input pages for training the schema without supervision, and then extracts data from the rest of the input pages through schema verification. To speed up the processing, we utilize leaf nodes of the DOM trees as the processing units and dynamically adjust the encoding for better alignment. The proposed system works better than other page-level extraction systems in terms of schema correctness and extraction efficiency. Overall, the extraction efficiency is 2.7 times faster than state-of-the-art unsupervised approaches that extract data page by page without schema verification.",2016,Knowledge Discovery and Data Mining,Fields of study: theoretical computer sciencedata miningdatabasemachine learningcomputer science
Neural Choice by Elimination via Highway Networks,Truyen Tran 0001 (Deakin University)Dinh Q. Phung (Deakin University)Svetha Venkatesh (Deakin University),"2157392948,2314522249,2146461601","We introduce Neural Choice by Elimination, a new framework that integrates deep neural networks into probabilistic sequential choice models for learning to rank. Given a set of items to chose from, the elimination strategy starts with the whole item set and iteratively eliminates the least worthy item in the remaining subset. We prove that the choice by elimination is equivalent to marginalizing out the random Gompertz latent utilities. Coupled with the choice model is the recently introduced Neural Highway Networks for approximating arbitrarily complex rank functions. We evaluate the proposed framework on a large-scale public dataset with over 425K items, drawn from the Yahoo! learning to rank challenge. It is demonstrated that the proposed method is competitive against state-of-the-art learning to rank methods.",2016,Knowledge Discovery and Data Mining,Fields of study: data miningartificial intelligencemachine learningstatisticscomputer science
Frequent Pattern Outlier Detection Without Exhaustive Mining,Arnaud Giacometti (François Rabelais University)Arnaud Soulet (François Rabelais University),"2129444726,2470748226","Outlier detection consists in detecting anomalous observations from data. During the past decade, pattern-based outlier detection methods have proposed to mine all frequent patterns in order to compute the outlier factor of each transaction. This approach remains too expensive despite recent progress in pattern mining field. In this paper, we provide exact and approximate methods for calculating the frequent pattern outlier factor FPOF without extracting any pattern or by extracting a small sample. We propose an algorithm that returns the exact FPOF without mining any pattern. Surprisingly, it works in polynomial time on the size of the dataset. We also present an approximate method where the end-user controls the maximum error on the estimated FPOF. Experiments show the interest of both methods for very large datasets where exhaustive mining fails to provide the exact solution. The accuracy of our approximate method outperforms the baseline approach for a same budget in time or number of patterns.",2016,Knowledge Discovery and Data Mining,Fields of study: data sciencedata miningstatisticsmathematics
"Comparative Evaluation of Action Recognition Methods via Riemannian Manifolds, Fisher Vectors and GMMs: Ideal and Challenging Conditions",Johanna Carvajal (University of Queensland)Arnold Wiliem (University of Queensland)Chris McCool (Queensland University of Technology)Brian C. Lovell (University of Queensland)Conrad Sanderson (University of Queensland),"2284811930,2230955268,2307648760,2689104063,2090845954","We present a comparative evaluation of various techniques for action recognition while keeping as many variables as possible controlled. We employ two categories of Riemannian manifolds: symmetric positive definite matrices and linear subspaces. For both categories we use their corresponding nearest neighbour classifiers, kernels, and recent kernelised sparse representations. We compare against traditional action recognition techniques based on Gaussian mixture models and Fisher vectors FVs. We evaluate these action recognition techniques under ideal conditions, as well as their sensitivity in more challenging conditions variations in scale and translation. Despite recent advancements for handling manifolds, manifold based techniques obtain the lowest performance and their kernel representations are more unstable in the presence of challenging conditions. The FV approach obtains the highest accuracy under ideal conditions. Moreover, FV best deals with moderate scale and translation changes.",2016,Knowledge Discovery and Data Mining,Fields of study: combinatoricsgeometrypattern recognitionmachine learningmathematics
Dynamic Grouped Mixture Models for Intermittent Multivariate Sensor Data,Naoya Takeishi (University of Tokyo)Takehisa Yairi (University of Tokyo)Naoki NishimuraYuta NakajimaNoboru Takata,"2032596288,226036427,2571734071,2649398116,2650874421","For secure and efficient operation of engineering systems, it is of great importance to watch daily logs generated by them, which mainly consist of multivariate time-series obtained with many sensors. This work focuses on challenges in practical analyses of those sensor data: temporal unevenness and sparseness. To handle the unevenly and sparsely spaced multivariate time-series, this work presents a novel method, which roughly models temporal information that still remains in the data. The proposed model is a mixture model with dynamic hierarchical structure that considers dependency between temporally close batches of observations, instead of every single observation. We conducted experiments with synthetic and real dataset, and confirmed validity of the proposed model quantitatively and qualitatively.",2016,Knowledge Discovery and Data Mining,Fields of study: mixture modeleconometricsdata miningmachine learningstatisticscomputer science
Parallel Discord Discovery,"Tian Huang (Shanghai Jiao Tong University)Yongxin Zhu (Shanghai Jiao Tong University)Yishu Mao (Shanghai Jiao Tong University)Xinyang Li (Shanghai Jiao Tong University)Mengyun Liu (Shanghai Jiao Tong University)Yafei Wu (Shanghai Jiao Tong University)Yajun Ha (Agency for Science, Technology and Research)Gillian Dobbie (University of Auckland)","2127525933,2147820062,2246571419,2405658016,2475405144,2637191053,2690893769,2002900660","Discords are the most unusual subsequences of a time series. Sequential discovery of discords is time consuming. As the scale of datasets increases unceasingly, datasets have to be kept on hard disk, which degrades the utilization of computing resources. Furthermore, the results discovered from segmentations of a time series are non-combinable, which makes discord discovery hard to parallelize. In this paper, we propose Parallel Discord Discovery PDD, which divides the discord discovery problem in a combinable manner and solves its sub-problems in parallel. PDD accelerates discord discovery with multiple computing nodes and guarantees the correctness of the results. PDD stores large time series in distributed memory and takes advantage of in-memory computing to improve the utilization of computing resources. Experiments show that given 10 computing nodes, PDD is seven times faster than the sequential method HOTSAX. PDD is able to handle larger datasets than HOTSAX does. PDD achieves over 90i¾ź% utilization of computing resources, nearly twice as much as the disk-aware method does.",2016,Knowledge Discovery and Data Mining,Fields of study: in memory processingparalleltheoretical computer scienceparallel computingdistributed computingcomputer science
Image Representation Optimization Based on Locally Aggregated Descriptors,Shijiang Chen (Tsinghua University)Guiguang Ding (Tsinghua University)Chenxiao Li (Tsinghua University)Yuchen Guo (Tsinghua University),"2582827174,2697274257,2265967718,2664008308","Aggregating local descriptors into super vectors achives excellent performance in image classification and retrieval tasks. Vector of locally aggregated descriptorsVLAD, which indexes images to compact representations by aggregating the residuals of descriptors and visual words, is a popular super vector encoding method among this kind. This paper will focus on the biggest difficulty of VLAD, the ""visual burstiness"", reviste the basic assumptions and solutions along this line, then make modifications to two key steps of the initial VLAD process. The main contributions are twofold. Firstly, we start from local coordinate systemLCS and propose the aggregated versionaggrLCS, which changes the objective and timing of coordinate rotation, for better captures of bursts. Secondly, an adaptive power-law normalization method is adopted to magnify the positive effect of power-law normalization by weighting each dimension respectively. Experiments on image retrieval tasks demonstrate that the proposed modifications show superior performance over the original and several variants of VLAD.",2016,Knowledge Discovery and Data Mining,Fields of study: visual wordpower lawimage retrievalcomputer visiondata miningpattern recognitionmachine learningcomputer sciencemathematics
A Clustering-Based Framework for Incrementally Repairing Entity Resolution,Qing Wang (Australian National University)Jingyi Gao (Australian National University)Peter Christen (Australian National University),"2287519903,2556752487,2023765750","Although entity resolution ER is known to be an important problem that has wide-spread applications in many areas, including e-commerce, health-care, social science, and crime and fraud detection, one aspect that has largely been neglected is to monitor the quality of entity resolution and repair erroneous matching decisions over time. In this paper we develop an efficient method for incrementally repairing ER, i.e., fix detected erroneous matches and non-matches. Our method is based on an efficient clustering algorithm that eliminates inconsistencies among matching decisions, and an efficient provenance indexing data structure that allows us to trace the evidence of clustering for supporting ER repairing. We have evaluated our method over real-world databases, and our experimental results show that the quality of entity resolution can be significantly improved through repairing over time.",2016,Knowledge Discovery and Data Mining,Fields of study: data deduplicationrecord linkageworld wide webdata miningdatabasecomputer science
Constraint Based Subspace Clustering for High Dimensional Uncertain Data,Xianchao Zhang (Dalian University of Technology)Lu Gao (Dalian University of Technology)Hong Yu (Dalian University of Technology),"2132650836,2565296157,2250407094","Both uncertain data and high-dimensional data pose huge challenges to traditional clustering algorithms. It is even more challenging for clustering high dimensional uncertain data and there are few such algorithms. In this paper, based on the classical FINDIT subspace clustering algorithm for high dimensional data, we propose a constraint based semi-supervised subspace clustering algorithm for high dimensional uncertain data, UFINDIT. We extend both the distance functions and dimension voting rules of FINDIT to deal with high dimensional uncertain data. Since the soundness criteria of FINDIT fails for uncertain data, we introduce constraints to solve the problem. We also use the constraints to improve FINDIT in eliminating parameters' effect on the process of merging medoids. Furthermore, we propose some methods such as sampling to get an more efficient algorithm. Experimental results on synthetic and real data sets show that our proposed UFINDIT algorithm outperforms the existing subspace clustering algorithm for uncertain data.",2016,Knowledge Discovery and Data Mining,Fields of study: canopy clustering algorithmcorrelation clusteringconstrained clusteringdata stream clusteringcure data clustering algorithmfuzzy clusteringcluster analysisdata miningmachine learningmathematical optimizationmathematics
Active Learning Based Entity Resolution Using Markov Logic,Jeffrey Fisher (Australian National University)Peter Christen (Australian National University)Qing Wang (Australian National University),"2148966539,2023765750,2287519903","Entity resolution is a common data cleaning and data integration problem that involves determining which records in one or more data sets refer to the same real-world entities. It has numerous applications for commercial, academic and government organisations. For most practical entity resolution applications, training data does not exist which limits the type of classification models that can be applied. This also prevents complex techniques such as Markov logic networks from being used on real-world problems. In this paper we apply an active learning based technique to generate training data for a Markov logic network based entity resolution model and learn the weights for the formulae in a Markov logic network. We evaluate our technique on real-world data sets and show that we can generate balanced training data and learn and also learn approximate weights for the formulae in the Markov logic network.",2016,Knowledge Discovery and Data Mining,Fields of study: markov modeldata miningmachine learningstatisticsalgorithmcomputer science
Dboost: A Fast Algorithm for DBSCAN-based Clustering on High Dimensional Data,Yuxiao Zhang (Peking University)Xiaorong Wang (Strategy&)Bingyang Li (University of International Relations)Wei Chen (Peking University)Tengjiao Wang (Peking University)Kai Lei (Peking University),"2251366251,2561341371,2566690164,2527738285,2144373700,2262537035","DBSCAN is a classic density-based clustering technique, which is well known in discovering clusters of arbitrary shapes and handling noise. However, it is very time-consuming in density calculation when facing high dimensional data, which makes it inefficient in many areas, such as multi-document summarization, product recommendation, etc. Therefore, how to efficiently calculate the density on high dimensional data becomes one key issue for DBSCAN-based clustering technique. In this paper, we propose a fast algorithm for DBSCAN-based clustering on high dimensional data, named Dboost. In our algorithm, a ranked retrieval technique adaption named $$WAND^\#$$ is novelly applied to improving the density calculations without accuracy loss, and we further improve this acceleration by reducing the invoking times of $$WAND^\#$$. Experiments were conducted on wire voltage data, Netflix dataset and microblog corpora. The results showed that an acceleration of over 50 times were achieved on wire voltage data and Netflix dataset, and 100 more times can be expected on microblog data.",2016,Knowledge Discovery and Data Mining,Fields of study: subcluoptics algorithmdbscanclustering high dimensional datacluster analysisworld wide webdata miningdatabasemachine learningcomputer science
Personal Credit Profiling via Latent User Behavior Dimensions on Social Media,Guangming Guo (University of Science and Technology of China)Feida Zhu (Singapore Management University)Enhong Chen (University of Science and Technology of China)Le Wu (University of Science and Technology of China)Qi Liu (University of Science and Technology of China)Yingling Liu (University of Science and Technology of China)Minghui Qiu (Singapore Management University),"2165049579,2160602068,2136372366,2181678313,2420624292,2439722024,2158082241","Consumer credit scoring and credit risk management have been the core research problem in financial industry for decades. In this paper, we target at inferring this particular user attribute called credit, i.e., whether a user is of the good credit class or not, from online social data. However, existing credit scoring methods, mainly relying on financial data, face severe challenges when tackling the heterogeneous social data. Moreover, social data only contains extremely weak signals about users' credit label. To that end, we put forward a Latent User Behavior Dimension based Credit Modeli¾źLUBD-CM to capture these small signals for personal credit profiling. LUBD-CM learns users' hidden behavior habits and topic distributions simultaneously, and represents each user at a much finer granularity. Specifically, we take a real-world Sina Weibo dataset as the testbed for personal credit profiling evaluation. Experiments conducted on the dataset demonstrate the effectiveness of our approach: 1 User credit label can be predicted using LUBD-CM with a considerable performance improvement over state-of-the-art baselines; 2 The latent behavior dimensions have very good interpretability in personal credit profiling.",2016,Knowledge Discovery and Data Mining,Fields of study: social networkcomputer securitydata miningcomputer science
Attribute Selection and Classification of Prostate Cancer Gene Expression Data Using Artificial Neural Networks,Sreenivas Sremath Tirumala (Auckland University of Technology)A. Narayanan (Auckland University of Technology),"2304086066,2179871709","Artificial Intelligence AI approaches for medical diagnosis and prediction of cancer are important and ever growing areas of research. Artificial Neural Networks ANN is one such approach that have been successfully applied in these areas. Various types of clinical datasets have been used in intelligent decision making systems for medical diagnosis, especially cancer for over three decades. However, gene expression datasets are complex with large numbers of attributes which make it more difficult for AI approaches to classification and prediction. Prostate Cancer dataset is one such dataset with 12600 attributes and only 102 samples. In this paper, we propose an extended ANN based approach for classification and prediction of prostate cancer using gene expression data. Firstly, we use four attribute selection approaches, namely Sequential Floating Forward Selection SFFS, RELIEFF, Sequential Backward Feature Section SFBS and Significant Attribute Evaluation SAE to identify the most influential attributes among 12600. We use ANNs and Naive Bayes for classification with complete sets of attributes as well as various sets obtained from attribute selection methods. Experimental results show that ANN outperformed Naive Bayes by achieving a classification accuracy of 98.2i¾?% compared to 62.74i¾?% with the full set of attributes. Further, with 21 selected attributes obtained with SFFS, ANNs achieved better accuracy 100i¾?% for classification compared to Naive Bayes. For prediction using ANNs, SFFS was able achieve best results with 92.31i¾?% of accuracy by correctly predicting 24 out of 26 samples provided for independent sample testing. Moreover, some of the gene selected by SFFS are identified to have a direct reference to cancer and tumour. Our results indicate that a combination of standard feature selection methods in conjunction with ANNs provide the most impressive results.",2016,Knowledge Discovery and Data Mining,Fields of study: artificial neural networkdata miningpattern recognitionmachine learningcomputer science
Adaptive Seeding for Gaussian Mixture Models,Johannes Blömer (University of Paderborn)Kathrin Bujna (University of Paderborn),"138736257,101389126","We present new initialization methods for the expectation-maximization algorithm for multivariate Gaussian mixture models. Our methods are adaptions of the well-known K-means++ initialization and the Gonzalez algorithm. Thereby we aim to close the gap between simple random, e.g. uniform, and complex methods, that crucially depend on the right choice of hyperparameters. Our extensive experiments indicate the usefulness of our methods compared to common techniques and methods, which e.g. apply the original K-means++ and Gonzalez directly, with respect to artificial as well as real-world data sets.",2016,Knowledge Discovery and Data Mining,Fields of study: econometricsmachine learningstatisticscomputer science
Denoising Time Series by Way of a Flexible Model for Phase Space Reconstruction,Minhazul Islam Sk (University of Florida)Arunava Banerjee (University of Florida),"2501234868,2119802459","We present a denoising technique in the domain of time series data that presumes a model for the uncorrupted underlying signal rather than a model for noise. Specifically, we show how the non-linear reconstruction of the underlying dynamical system by way of time delay embedding yields a new solution for denoising where the underlying dynamics is assumed to be highly non-linear yet low-dimensional. The model for the underlying data is recovered using a non-parametric Bayesian approach and is therefore very flexible. The proposed technique first clusters the reconstructed phase space through a Dirichlet Process Mixture of Exponential density, an infinite mixture model. Phase Space Reconstruction is accomplished by time delay embedding in the framework of Taken's Embedding Theorem with the underlying dimension being determined by the False Neighborhood method. Next, an Infinite Mixtures of Linear Regression via Dirichlet Process is used to non-linearly map the phase space data points to their respective temporally subsequent points in the phase space. Finally, a convex optimization based approach is used to restructure the dynamics by perturbing the phase space points to create the new denoised time series. We find that this method yields significantly better performance in noise reduction, power spectrum analysis and prediction accuracy of the phase space.",2016,Knowledge Discovery and Data Mining,Fields of study: machine learningmathematical optimizationstatisticsmathematics
Predicting Phone Usage Behaviors with Sensory Data Using a Hierarchical Generative Model,Chuankai An (Dartmouth College)Dan Rockmore (Dartmouth College),"2693990751,53499937","Using a sizable set of sensory data and related usage records on Android devices, we are able to give a reasonable prediction of three imporant aspects of phone usage: messages, phone calls and cellular data. We solve the problem via an estimation of a user's daily routine, on which we can train a hierarchical generative model on phone usages in all time slots of a day. The model generates phone usage behaviors in terms of three kinds of data: the state of user-phone interaction, occurrence times of an activity and the duration of the activity in each occurrence. We apply the model on a dataset with 107 frequent users, and find the prediction error of generative model is the smallest when compare with several other baseline methods. In addition, CDF curves illustrate the availability of generative model for most users with the distribution of prediction error for all test cases. We also explore the effects of time slots in a day, as well as size of training and test sets. The results suggest several interesting directions for further research.",2016,Knowledge Discovery and Data Mining,Fields of study: generative modelspeech recognitiondata miningmachine learningcomputer science
Linear Upper Confidence Bound Algorithm forźContextual Bandit Problem with Piled Rewards,Kuan-Hao Huang (National Taiwan University)Hsuan-Tien Lin (National Taiwan University),"2113276713,2127632057","We study the contextual bandit problem with linear payoff function. In the traditional contextual bandit problem, the algorithm iteratively chooses an action based on the observed context, and immediately receives a reward for the chosen action. Motivated by a practical need in many applications, we study the design of algorithms under the piled-reward setting, where the rewards are received as a pile instead of immediately. We present how the Linear Upper Confidence Bound LinUCB algorithm for the traditional problem can be naively applied under the piled-reward setting, and prove its regret bound. Then, we extend LinUCB to a novel algorithm, called Linear Upper Confidence Bound with Pseudo Reward LinUCBPR, which digests the observed contexts to choose actions more strategically before the piled rewards are received. We prove that LinUCBPR can match LinUCB in the regret bound under the piled-reward setting. Experiments on the artificial and real-world datasets demonstrate the strong performance of LinUCBPR in practice.",2016,Knowledge Discovery and Data Mining,Fields of study: artificial intelligencemachine learning
Distributed Sequential Pattern Mining in Large Scale Uncertain Databases,Jiaqi Ge (Indiana University – Purdue University Indianapolis)Yuni Xia (Indiana University – Purdue University Indianapolis),"2107884066,2334154954","While sequential pattern mining SPM is an import application in uncertain databases, it is challenging in efficiency and scalability. In this paper, we develop a dynamic programming DP approach to mine probabilistic frequent sequential patterns in distributed computing platform Spark. Directly applying the DP method to Spark is impractical because its memory-consuming characteristic may cause heavy JVM garbage collection overhead in Spark. Therefore, we design a memory-efficient distributed DP approach and use an extended prefix-tree to save intermediate results efficiently. The extensive experimental results in various scales prove that our method is orders of magnitude faster than straight-forward approaches.",2016,Knowledge Discovery and Data Mining,Fields of study: sequential pattern miningdata miningdatabasereal time computingcomputer science
Privacy Aware K-Means Clustering with High Utility,Thanh Dai Nguyen (Deakin University)Sunil Kumar Gupta 0001 (Deakin University)Santu Rana (Deakin University)Svetha Venkatesh (Deakin University),"2549737843,2119406083,2142238370,2146461601","Privacy-preserving data mining aims to keep data safe, yet useful. But algorithms providing strong guarantees often end up with low utility. We propose a novel privacy preserving framework that thwarts an adversary from inferring an unknown data point by ensuring that the estimation error is almost invariant to the inclusion/exclusion of the data point. By focusing directly on the estimation error of the data point, our framework is able to significantly lower the perturbation required. We use this framework to propose a new privacy aware K-means clustering algorithm. Using both synthetic and real datasets, we demonstrate that the utility of this algorithm is almost equal to that of the unperturbed K-means, and at strict privacy levels, almost twice as good as compared to the differential privacy counterpart.",2016,Knowledge Discovery and Data Mining,Fields of study: internet privacycomputer securitydata miningcomputer science
Towards a New Evolutionary Subsampling Technique for Heuristic Optimisation of Load Disaggregators,Michael Mayo (University of Waikato)Sara Omranian (University of Waikato),"2166211886,2530592766","In this paper we present some preliminary work towards the development of a new evolutionary subsampling technique for solving the non-intrusive load monitoring NILM problem. The NILM problem concerns using predictive algorithms to analyse whole-house energy usage measurements, so that individual appliance energy usages can be disaggregated. The motivation is to educate home owners about their energy usage. However, by their very nature, the datasets used in this research are massively imbalanced in their target value distributions. Consequently standard machine learning techniques, which often rely on optimising for root mean squared error RMSE, typically fail. We therefore propose the target-weighted RMSE TW-RMSE metric as an alternative fitness function for optimising load disaggregators, and show in a simple initial study in which random search is utilised that TW-RMSE is a metric that can be optimised, and therefore has the potential to be included in a larger evolutionary subsampling-based solution to this problem.",2016,Knowledge Discovery and Data Mining,Fields of study: fitness functiondata miningartificial intelligencemachine learningstatisticscomputer science
Online Learning for Accurate Real-Time Map Matching,Biwei Liang (Peking University)Tengjiao Wang (Peking University)Shun Li (University of International Relations)Wei Chen (Peking University)Hongyan Li (Peking University)Kai Lei (Peking University),"2538735692,2144373700,2397239839,2527738285,2430329963,2262537035","For the reason that deviation exists between GPS traces obtained by real-time positioning system and actual paths, real-time map matching which identifies the correct traveling road segment, becomes increasingly important. In order to effectively improve map matching accuracy, most state-of-art real-time map matching algorithms use machine learning which calls for time-consuming human labeling in advance. We propose an accurate real-time map matching method using online learning called OLMM. It takes into account a small piece of trajectory data and their matching result to support the subsequent matching process. We evaluate the effectiveness of the proposed approach using ground truth data. The results demonstrate that our approach can obtain more accurate matching results than existing methods without any human labeling beforehand.",2016,Knowledge Discovery and Data Mining,Fields of study: optimal matchingcomputer visiondata miningmachine learningcomputer science
Transfer-Learning Based Model for Reciprocal Recommendation,Chia-Hsin Ting (National Taiwan University)Hung-Yi Lo (National Taiwan University)Shou-De Lin (National Taiwan University),"2559197646,2156980939,2114357324","This paper tackles the reciprocal recommendation task which has various applications such as online dating, employee recruitment and mentor-mentee matching. The major difference between traditional recommender systems and reciprocal recommender systems is that a reciprocal recommender has to satisfy the preference on both directions. This paper proposes a simple yet novel regularization term, the Mutual-Attraction Indicator, to model the mutual preferences of both parties. Given such indicator, we design a transfer-learning based CF model for reciprocal recommender. The experiments are based on two real world tasks, online dating and human resource matching, showing significantly improved performance over the original factorization model and state-of-the-art reciprocal recommenders.",2016,Knowledge Discovery and Data Mining,Fields of study: transfer of learningmultimediaartificial intelligencemachine learningcomputer science
Fast and Semantic Measurements on Collaborative Tagging Quality,Yuqing Sun (Shandong University)Haiqi Sun (Shandong University)Reynold Cheng (University of Hong Kong),"2706175349,2639631723,2138267588","This paper focuses on the problem of tagging quality evaluation in collaborative tagging systems. By investigating the dynamics of tagging process, we find that high frequency tags almost cover the main aspects of a resource content and can be determined stable much earlier than a whole tag set. Motivated by this finding, we design the swapping index and smart moving index on tagging quality. We also study the correlations in tag usage and propose the semantic measurement on tagging quality. The proposed methods are evaluated against real datasets and the results show that they are more efficient than previous methods, which are appropriate for a large number of web resources. The effectiveness is justified by the results in tag based applications. The light weight metrics bring a little loss on the performance, while the semantic metric is better than current methods.",2016,Knowledge Discovery and Data Mining,Fields of study: world wide webinformation retrievaldata miningcomputer science
An Improved Self-Structuring Neural Network,Rami M. Mohammad (University of Huddersfield)Fadi A. Thabtah (Nelson Marlborough Institute of Technology)Lee McCluskey (University of Huddersfield),"2153773975,2511251656,2094120944","Creating a neural network based classification model is traditionally accomplished using the trial and error technique. However, the trial and error structuring method nornally suffers from several difficulties including overtraining. In this article, a new algorithm that simplifies structuring neural network classification models has been proposed. It aims at creating a large structure to derive classifiers from the training dataset that have generally good predictive accuracy performance on domain applications. The proposed algorithm tunes crucial NN model thresholds during the training phase in order to cope with dynamic behavior of the learning process. This indeed may reduce the chance of overfitting the training dataset or early convergence of the model. Several experiments using our algorithm as well as other classification algorithms, have been conducted against a number of datasets from University of California Irvine UCI repository. The experiments' are performed to assess the pros and cons of our proposed NN method. The derived results show that our algorithm outperformed the compared classification algorithms with respect to several performance measures.",2016,Knowledge Discovery and Data Mining,Fields of study: phishingpruningstructurebiological classificationartificial neural networkdata miningartificial intelligencemachine learningstatisticscomputer science
Multi-hypergraph Incidence Consistent Sparse Coding for Image Data Clustering,Xiaodong Feng (University of Electronic Science and Technology of China)Sen Wu (University of Science and Technology Beijing)Wenjun Zhou (University of Tennessee)Zhiwei Tang (University of Electronic Science and Technology of China),"2437673952,2706952931,2097769770,2492638385","Sparse representation has been a powerful technique for modeling image data and thus enhance the performance of image clustering. Sparse coding, as an unsupervised way to extract sparse representation, learns a dictionary that represents high-level semantics and the new representations on the dictionary. Though existing sparse coding schemes are considering local manifold structure of the data with graph/hypergraph regularization, more from the manifold should be exploited to utilize intrinsic manifold characteristics in the data. In this paper, we firstly propose a Hypergraph Incidence Consistency regularization term by minimizing the reconstruction error of the hypergraph incidence matrix with sparse codes to further regulate the learned sparse codes with hypergraph-based manifold. Moreover, a multi-hypergraph learning framework to automatically select the optimal manifold structure is integrated into the objective of sparse coding learning, resulting in multi-hypergraph incidence Consistent Sparse Coding MultiCSC. We show that the MultiCSC objective function can be optimized efficiently, and that several existing sparse coding methods are special cases of MultiCSC. Extensive experimental results on image clustering demonstrate the effectiveness of our proposed method.",2016,Knowledge Discovery and Data Mining,Fields of study: k svdneural codingsparse approximationtheoretical computer sciencepattern recognitionmachine learningmathematics
Phishing Detection on Twitter Streams,Se Yeong Jeong (University of Auckland)Yun Sing Koh (University of Auckland)Gillian Dobbie (University of Auckland),"2531645972,1987110578,2002900660","With the prevalence of cutting-edge technology, the social media network is gaining popularity and is becoming a worldwide phenomenon. Twitter is one of the most widely used social media sites, with over 500 million users all around the world. Along with its rapidly growing number of users, it has also attracted unwanted users such as scammers, spammers and phishers. Research has already been conducted to prevent such issues using network or contextual features with supervised learning. However, these methods are not robust to changes, such as temporal changes or changes in phishing trends. Current techniques also use additional network information. However, these techniques cannot be used before spammers form a particular number of user relationships. We propose an unsupervised technique that detects phishing in Twitter using a 2-phase unsupervised learning algorithm called PDT Phishing Detector for Twitter. From the experiments we show that our technique has high accuracy ranging between 0.88 and 0.99.",2016,Knowledge Discovery and Data Mining,Fields of study: dbscaninternet privacyworld wide webcomputer securitymachine learningcomputer science
A Hierarchical Beta Process Approach for Financial Time Series Trend Prediction,Mojgan Ghanavati (University of New South Wales)Raymond K. Wong (University of New South Wales)Fang Chen (University of New South Wales)Yang Wang (NICTA)Joe Lee,"2153149482,2290077475,2656583616,2675046882,2677831969","An automatic stock market categorization system would be invaluable to investors and financial experts, providing them with the opportunity to predict a stock price changes with respect to the other stocks. In recent years, clustering all companies in the stock markets based on their similarities in shape of the stock market has increasingly become popular. However, existing approaches may not be practical because the stock price data are high-dimensional data and the changes in the stock price usually occur with shift, which makes the categorization more complex. In this paper, a hierarchical beta process HBP based approach is proposed for stock market trend prediction. Preliminary results show that the approach is promising and outperforms other popular approaches.",2016,Knowledge Discovery and Data Mining,Fields of study: actuarial science
A Rule Based Open Information Extraction Method Using Cascaded Finite-State Transducer,Hailun Lin (Chinese Academy of Sciences)Yuanzhuo Wang (Chinese Academy of Sciences)Peng Zhang (Chinese Academy of Sciences)Weiping Wang (Chinese Academy of Sciences)Yinliang Yue (Chinese Academy of Sciences)Zheng Lin (Chinese Academy of Sciences),"2103889526,2120380447,2120503182,2597937528,2717669663,2085002608","In this paper, we present R-OpenIE, a rule based open information extraction method using cascaded finite-state transducer. R-OpenIE defines contextual constraint declarative rules to generate relation extraction templates, which frees from the influence of syntactic parser errors, and it uses cascaded finite-state transducer model to match the satisfied relational tuples. It is noted that R-OpenIE creates inverted index for each matched state during the matching process of cascaded finite-state transducer, which improves the efficiency of pattern matching. The experimental results have shown that our R-OpenIE can achieve good adaptability and efficiency for open information extraction.",2016,Knowledge Discovery and Data Mining,Fields of study: relationship extractiondata miningpattern recognitionmachine learningcomputer science
Computing Hierarchical Summary of the Data Streams,Zubair Shah (University of New South Wales)Abdun Naser Mahmood (University of New South Wales)Michael Barlow (University of New South Wales),"2609482565,2095913062,2134092914","Data stream processing is an important function in many online applications such as network traffic analysis, web applications, and financial data analysis. Computing summaries of data stream is challenging since streaming data is generally unbounded, and cannot be permanently stored or accessed more than once. In this paper, we have proposed two counter based hierarchical CHS $$\epsilon $$---approximation algorithms to create hierarchical summaries of one dimensional data. CHS maintains a data structure, where each entry contains the incoming data item and an associated counter to store its frequency. Since every item in streaming data cannot be stored, CHS only maintains frequent items known as hierarchical heavy hitters at various levels of generalization hierarchy by exploiting the natural hierarchy of the data. The algorithm guarantees accuracy of count within an $$\epsilon $$ bound. Furthermore, using aperiodic CHS-A and periodic CHS-P compression strategy the proposed technique offers improved space complexities of $$O\frac{\eta }{\epsilon }$$ and $$O\frac{\eta }{\epsilon }\log \epsilon N$$, respectively. We provide theoretical proofs for both space and time requirements of CHS algorithm. We have also experimentally compared the proposed algorithm with the existing benchmark techniques. Experimental results show that the proposed algorithm requires fewer updates per element of data, and uses a moderate amount of bounded memory. Moreover, precision-recall analysis demonstrates that CHS algorithm provides a high quality output compared to existing benchmark techniques. For the experimental validation, we have used both synthetic data derived from an open source generator, and real benchmark data sets from an international Internet Service Provider.",2016,Knowledge Discovery and Data Mining,Fields of study: data stream miningworld wide webdata miningdatabasemachine learningstatisticscomputer science
Enabling Hierarchical Dirichlet Processes to Work Better for Short Texts at Large Scale,Khai Mai (Hanoi University of Science and Technology)Sang Mai (Hanoi University of Science and Technology)Anh Nguyen (Hanoi University of Science and Technology)Ngo Van Linh (Hanoi University of Science and Technology)Khoat Than (Hanoi University of Science and Technology),"2510839171,2512314896,2103572204,2102101224,2116469936","Analyzing texts from social media often encounters many challenges, including shortness, dynamic, and huge size. Short texts do not provide enough information so that statistical models often fail to work. In this paper, we present a very simple approach namely, bag-of-biterms that helps statistical models such as Hierarchical Dirichlet Processes HDP to work well with short texts. By using both terms words and biterms to represent documents, bag-of-biterms BoB provides significant benefits: 1 it naturally lengthens representation and thus helps us reduce bad effects of shortness; 2 it enables the posterior inference in a large class of probabilistic models including HDP to be less intractable; 3 no modification of existing models/methods is necessary, and thus BoB can be easily employed in a wide class of statistical models. To evaluate those benefits of BoB, we take Online HDP into account in that it can deal with dynamic and massive text collections, and we do experiments on three large corpora of short texts which are crawled from Twitter, Yahoo Q&A, and New York Times. Extensive experiments show that BoB can help HDP work significantly better in both predictiveness and quality.",2016,Knowledge Discovery and Data Mining,Fields of study: data sciencedata miningartificial intelligencemachine learningstatisticscomputer science
Clinical Decision Support for Stroke Using Multi---view Learning Based Models for NIHSS Scores,"Vaibhav Rajan (Xerox)Sakyajit Bhattacharya (Xerox)Ranjan Shetty (Kasturba Medical College, Manipal)Amith SitaramG. Vivek (Kasturba Medical College, Manipal)","1988331176,2112986011,2107658649,2521828683,2063671572","Cerebral stroke is a leading cause of physical disability and death in the world. The severity of a stroke is assessed by a neurological examination using a scale known as the NIH stroke scale NIHSS. As a measure of stroke severity, the NIHSS score is widely adopted and has been found to also be useful in outcome prediction, rehabilitation planning and treatment planning. In many applications, such as in patient triage in under---resourced primary health care centres and in automated clinical decision support tools, it would be valuable to obtain the severity of stroke with minimal human intervention using simple parameters like age, past conditions and blood investigations. In this paper we propose a new model for predicting NIHSS scores which, to our knowledge, is the first statistical model for stroke severity. Our multi---view learning approach can handle data from heterogeneous sources with mixed data distributions binary, categorical and numerical and is robust against missing values --- strengths that many other modeling techniques lack. In our experiments we achieve better predictive accuracy than other commonly used methods.",2016,Knowledge Discovery and Data Mining,Fields of study: data mining
Imbalanced ELM Based on Normal Density Estimation for Binary-Class Classification,Yulin He (Shenzhen University)Rana Aamir Raza Ashfaq (Shenzhen University)Joshua Zhexue Huang (Shenzhen University)Xizhao Wang (Shenzhen University),"2559741520,2297205250,2699884321,2617279703","The imbalanced Extreme Learning Machine based on kernel density estimation imELM-kde is a latest classification algorithm for handling the imbalanced binary-class classification. By adjusting the real outputs of training data with intersection point of two probability density functions p.d.f.s corresponding to the predictive outputs of majority and minority classes, imELM-kde updates ELM which is trained based on the original training data and thus improves the performance of ELM-based imbalanced classifier. In this paper, we analyze the shortcomings of imELM-kde and then propose an improved version of imELM-kde. The Parzen window method used in imELM-kde leads to multiple intersection points between p.d.f.s of majority and minority classes. In addition, it is unreasonable to update the real outputs with intersection point, because the p.d.f.s are estimated based on the predictive outputs. Thus, in order to improve the shortcomings of imELM-kde, an imbalanced ELM based on normal density estimation imELM-nde is proposed in this paper. In imELM-nde, the p.d.f.s of predictive outputs corresponding to majority and minority classes are computed with normal density estimation and the intersection point is used to update the predictive outputs instead of real outputs. This makes the training of probability density estimation-based imbalanced ELM simpler and more feasible. The comparative results show that our proposed imELM-nde performs better than unweighted ELM and imELM-kde for imbalanced binary-class classification problem.",2016,Knowledge Discovery and Data Mining,Fields of study: kernel density estimationprobability density functiondata miningpattern recognitionmachine learningstatisticsmathematics
FeRoSA: A Faceted Recommendation System for Scientific Articles,Tanmoy Chakraborty (Indian Institute of Technology Kharagpur)Amrith Krishna (Indian Institute of Technology Kharagpur)Mayank Singh (Indian Institute of Technology Kharagpur)Niloy Ganguly (Indian Institute of Technology Kharagpur)Pawan Goyal (Indian Institute of Technology Kharagpur)Animesh Mukherjee (Indian Institute of Technology Kharagpur),"2471175502,2223599978,2322263662,2097625090,2556932677,2134540012","The overwhelming number of scientific articles over the years calls for smart automatic tools to facilitate the process of literature review. Here, we propose for the first time a framework of faceted recommendation for scientific articles abbreviated as FeRoSA which apart from ensuring quality retrieval of scientific articles for a query paper, also efficiently arranges the recommended papers into different facets categories. Providing users with an interface which enables the filtering of recommendations across multiple facets can increase users' control over how the recommendation system behaves. FeRoSA is precisely built on a random walk based framework on an induced subnetwork consisting of nodes related to the query paper in terms of either citations or content similarity. Rigorous analysis based an experts' judgment shows that FeRoSA outperforms two baseline systems in terms of faceted recommendations overall precision of 0.65. Further, we show that the faceted results of FeRoSA can be appropriately combined to design a better flat recommendation system as well. An experimental version of FeRoSA is publicly available at www.ferosa.org receiving as many as 170 hits within the first 15 days of launch.",2016,Knowledge Discovery and Data Mining,Fields of study: world wide webinformation retrievaldata miningcomputer science
Query-Focused Multi-document Summarization Based on Concept Importance,Hai-Tao Zheng (Tsinghua University)Ji-Min Guo (Tsinghua University)Yong Jiang (Tsinghua University)Shu-Tao Xia (Tsinghua University),"2128052405,2644077339,2584127598,2142362674","With the exponential growth of the web documents and the requirement of limited bandwidth for mobile devices, it becomes more and more difficult for users to get information they look forward to from the vast amount of information. Query-focused summarization gets more attention from both the research and engineering area in recent years. However, existing query-focused summarization methods don't consider the conceptual relation and the concept importance that make up the sentences, a concept is the title of a wikipedia article and can express an entity or action. In this article. We propose a novel method called Query-focused Multi-document Summarization based on Concept Importance QMSCI. We first map sentence to concepts and get ranked weighted concepts by reinforcement between the concepts of sentences and concepts of the query in a bipartite graph, then we use the ranked weighted concepts to help to rank the sentences in a hyper-graph model, sentences that contain important concepts, related with the query and also central among sentences are ranked higher and comprise the summary. We experiment on the DUC datasets, the experimental result demonstrates the effectiveness of our proposed method compared to the state-of-art methods.",2016,Knowledge Discovery and Data Mining,Fields of study: multi document summarizationautomatic summarizationnatural language processinginformation retrievaldata miningmachine learningcomputer science
Keystroke Biometric Recognition on Chinese Long Text Input,Xiaodong Li (Southwestern University of Finance and Economics)Jiafen Liu (Southwestern University of Finance and Economics),"2545172367,2627148494","Keystroke Biometric is useful in distinguishing legal users from perpetrators in online activities. Most previous keystroke studies focus on short text, however short text keystroke can only be used in limited scenarios such as user name and password input and provide one-time authentication. In this paper, we concentrate on how to detect whether current user is the legal one during the whole activity, such as writing an E-mail and chat online. We developed a JAVA applet to collect raw data, and then extracted features and constructed 4 classifiers. In the experiment, we required 30 users to choose a topic randomly and then type in a text about 400 Chinese characters on it. This experiment repeated 9 times in different days under the same typing environment. The accuracy of different methods shifts from 94.07i¾?% to 98.15i¾?%, the FAR reaches to 0.74i¾?% and FRR to 1.15i¾?%. In summary, Chinese free long text keystroke biometric recognition can be used to authenticate users during the whole online activity with satisfactory precision.",2016,Knowledge Discovery and Data Mining,Fields of study: keystroke dynamicsauthenticationinternet privacyworld wide webspeech recognitioncomputer science
Event Prediction in Healthcare Analytics: Beyond Prediction Accuracy,Lina Fu (PARC)Faming Li (PARC)Jing Zhou (PARC)Xuejin Wen (PARC)Jinhui Yao (PARC)Michael Shepherd (PARC),"2252213683,2521841085,2432412799,2535590692,2628471814,2599164936","During the recent few years, the United States healthcare industry is under unprecedented pressure to improve outcome and reduce cost. Many healthcare organizations are leveraging healthcare analytics, especially predictive analytics in moving towards these goals and bringing better value to the patients. While many existing event prediction models provide helpful predictions in terms of accuracy, their use are typically limited to prioritizing individual patients for care management at fixed time points. In this paper we explore Enhanced Modeling approaches around two important aspects: 1 model interpretability; 2 flexible prediction window. Better interpretability of the model will guide us towards more effective intervention design. Flexible prediction window can provide a higher resolution picture of patients' risks of adverse events over time, and thereby enable timely interventions. We illustrate interpretation and insights from our Bayesian Hierarchical Model for readmission prediction, and demonstrate flexible prediction window with Random Survival Forests model for prediction of future emergency department visits.",2016,Knowledge Discovery and Data Mining,Fields of study: analyticspredictive analyticsbayesian hierarchical modelingdata sciencedata miningmachine learningsimulationcomputer science
Efficient Iris Image Segmentation for ATM Based Approach Through Fuzzy Entropy and Graph Cut,Shibai Yin (Southwestern University of Finance and Economics)Yibin Wang (Northwestern Polytechnical University)Tao Wang (Southwestern University of Finance and Economics),"2643875572,2714109551,2695630697","In order to realize accurate personal identification in the ATMS, an efficient iris image segmentation approach based on the fuzzy 4-partition entropy and graph cut is presented which can not only yield noisy segmentation results but short the running time. In this paper, an iterative calculation scheme is presented for reducing redundant computations in fuzzy 4-entropy evaluation. Then the presented algorithm uses the probabilities of 4 fuzzy events to define the costs of 4 label assignments iris, pupil, background and eyelash for each region in the graph cut. The final segmentation result is computed using graph cut, which produces smooth segmentation result and yields noise. The experimental results demonstrate the presented iterative calculation scheme can greatly reduce the running time. Quantitative evaluations over 20 classic iris images also show that our algorithm outperforms existing iris image segmentation approaches.",2016,Knowledge Discovery and Data Mining,Fields of study: segmentation based object categorizationscale space segmentationcutimage segmentationcomputer visionmachine learningmathematical optimizationmathematics
A Social Spam Detection Framework via Semi-supervised Learning,Xianchao Zhang (Dalian University of Technology)Haijun Bai (Dalian University of Technology)Wenxin Liang (Dalian University of Technology),"2132650836,2531861419,2103505816","With the increasing popularity of social networking websites such as Twitter, Facebook, Sina Weibo and MySpace, spammers on them are getting more and more rampant. Social spammers always create a mass of compromised or fake accounts to deceive users and lead them to access malicious websites which contain illegal, pornography or dangerous information. As we all know, most of the studies on social spam detection are based on supervised machine learning which requires plenty of annotated datasets. Unfortunately, labeling a large number of datasets manually is a complex, error-prone and tedious task which may costs a lot of human efforts and time. In this paper, we propose a novel semi-supervised classification framework for social spam detection, which combines co-training with k-medoids. First we utilize k-medoids clustering algorithm to acquire some informative and presentative samples for labelling as our initial seeds set. Then we take advantage of the content features and behavior features of users for our co-training classification framework. In order to illustrate the effectiveness of k-medoids, we compare the performance with random selecting strategy. Finally, we evaluate the effectiveness of our proposed detection framework compared with several classical supervised algorithms.",2016,Knowledge Discovery and Data Mining,Fields of study: spambotsocial spamk medoidssemi supervised learninginternet privacyworld wide webdata miningmachine learningcomputer science
Rigidly Self-Expressive Sparse Subspace Clustering,Linbo Qiao (National University of Defense Technology)Bofeng Zhang (National University of Defense Technology)Yipin Sun (National University of Defense Technology)Jinshu Su (National University of Defense Technology),"2095838241,2136605010,2116260882,2109843016","Sparse subspace clustering is a well-known algorithm, and it is widely used in many research field nowadays, and a lot effort has been contributed to improve it. In this paper, we propose a novel approach to obtain the coefficient matrix. Compared with traditional sparse subspace clustering SSC approaches, the key advantage of our approach is that it provides a new perspective of the self-expressive property. We call it rigidly self-expressive RSE property. This new formulation captures the rigidly self-expressive property of the data points in the same subspace, and provides a new formulation for sparse subspace clustering. Extensions to traditional SSC could also be cooperating with this new formulation. We present a first-order algorithm to solve the nonconvex optimization, and further prove that it converges to a KKT point of the nonconvex problem under certain standard assumptions. Extensive experiments on the Extended Yale B dataset, the USPS digital images dataset, and the Columbia Object Image Library shows that for images with upi¾?to 30i¾?% missing pixels the clustering quality achieved by our approach outperforms the original SSC.",2016,Knowledge Discovery and Data Mining,Fields of study: cluster analysisdata miningmachine learningmathematical optimizationstatisticsmathematics
Recommendation Algorithm Design in a Land Exchange Platform,Xubin LuoJiang Duan,"2655618462,2648965982","In China the majority of the farmlands are small pieces, which should be circulated and aggregated to a larger scale and pave the way for modern farms. A Platform needs to be built to connect the small landowners and the potential new farmers or investors. This paper proposes an efficient recommendation algorithm that takes both the space attributes and other properties of farmland pieces into consideration and produce best selection for the intended potential new farmers or investors with customized object functions.",2016,Knowledge Discovery and Data Mining,Fields of study: data miningsimulation
An Efficient Dynamic Programming Algorithm for STR-IC-STR-IC-LCS Problem,Daxin Zhu (Quanzhou Normal University)Yingjie Wu (Fuzhou University)Xiaodong Wang (Fujian University of Technology),"2100681708,2291935952,2677196269","In this paper, we consider a generalized longest common subsequence problem, in which a constraining sequence of length s must be included as a substring and the other constraining sequence of length t must be included as a subsequence of two main sequences and the length of the result must be maximal. For the two input sequences X and Y of lengths n and m, and the given two constraining sequences of length s and t, we present an Onmst time dynamic programming algorithm for solving the new generalized longest common subsequence problem. The time complexity can be reduced further to cubic time in a more detailed analysis. The correctness of the new algorithm is proved.",2016,Knowledge Discovery and Data Mining,Fields of study: hunt mcilroy algorithmlongest alternating subsequencelongest repeated substring problemlongest common substring problemlongest increasing subsequencesubstringlongest common subsequence problemdynamic programmingtime complexitydiscrete mathematicscombinatoricsmathematical optimizationalgorithmcomputer sciencemathematics
A Survey on Truth Discovery,Yaliang Li (University at Buffalo)Jing Gao (University at Buffalo)Chuishi Meng (University at Buffalo)Qi Li (University at Buffalo)Lu Su (University at Buffalo)Bo Zhao (LinkedIn)Wei Fan (Baidu)Jiawei Han (University of Illinois at Urbana–Champaign),"2116094297,2096731881,2116340933,2261907930,2148733542,2674375462,2422054197,2121939561","Thanks to information explosion, data for the objects of interest can be collected from increasingly more sources. However, for the same object, there usually exist conflicts among the collected multi-source information. To tackle this challenge, truth discovery, which integrates multi-source noisy information by estimating the reliability of each source, has emerged as a hot topic. Several truth discovery methods have been proposed for various scenarios, and they have been successfully applied in diverse application domains. In this survey, we focus on providing a comprehensive overview of truth discovery methods, and summarizing them from different aspects. We also discuss some future directions of truth discovery research. We hope that this survey will promote a better understanding of the current progress on truth discovery, and offer some guidelines on how to apply these approaches in application domains.",2016,Knowledge Discovery and Data Mining,Fields of study: data sciencedata miningartificial intelligence
An Interactive Data Repository with Visual Analytics,Ryan A. Rossi (PARC)Nesreen K. Ahmed (Intel),"2060818872,2119838086","Scientific data repositories have historically made data widely accessible to the scientific community, and have led to better research through comparisons, reproducibility, as well as further discoveries and insights. Despite the growing importance and utilization of data repositories in many scientific disciplines, the design of existing data repositories has not changed for decades. In this paper, we revisit the current design and envision interactive data repositories, which not only make data accessible, but also provide techniques for interactive data exploration, mining, and visualization in an easy, intuitive, and free-flowing manner.",2016,Knowledge Discovery and Data Mining,Fields of study: information repositorynetwork sciencegraph drawinginformation visualizationvisual analyticsinteractive visualizationdigital librarydata scienceworld wide webdata miningcomputer science
Shedding Light on the Performance of Solar Panels: A Data-Driven View,Sue A. Chen (IBM)Arun Vishwanath (IBM)Saket Sathe (IBM)Shivkumar Kalyanaraman (IBM),"2111710552,2085402668,2478550524,1969120992","The significant adoption of solar photovoltaic (PV) systems in both commercial and residential sectors has spurred an interest in monitoring the performance of these systems. This is facilitated by the increasing availability of regularly logged PV performance data in recent years. In this paper, we present a data-driven framework to systematically characterise the relationship between performance of an existing photovoltaic (PV) system and various environmental factors. We demonstrate the efficacy of our proposed framework by applying it to a PV generation dataset from a building located in northern Australia. We show how, in light of limited site-specific weather information, this data set may be coupled with publicly available data to yield rich insights on the performance of the building's PV system.",2016,Knowledge Discovery and Data Mining,Fields of study: simulationcomputer science
Web Content Extraction: a MetaAnalysis of its Past and Thoughts on its Future,"Tim Weninger (University of Notre Dame)Rodrigo Palacios (California State University, Fresno)Valter Crescenzi (Roma Tre University)Thomas GottronPaolo Merialdo (Worcester Polytechnic Institute)","2037649753,2595255449,1986707317,2610112695,262536330","In this paper, we present a meta-analysis of several Web content extraction algorithms, and make recommendations for the future of content extraction on the Web. First, we find that nearly all Web content extractors do not consider a very large, and growing, portion of modernWeb pages. Second, it is well understood that wrapper induction extractors tend to break as theWeb changes; ; heuristic/ feature engineering extractors were thought to be immune to a Web site's evolution, but we find that this is not the case: heuristic content extractor performance also tends to degrade over time due to the evolution of Web site forms and practices. We conclude with recommendations for future work that address these and other findings.",2016,Knowledge Discovery and Data Mining,Fields of study: multimediaworld wide webdata mining
Automated analytics: the organizational impact of analytics-as-a-service,Tine Van Calster (Katholieke Universiteit Leuven)Jasmien Lismont (Katholieke Universiteit Leuven)María Oskarsdottir (Katholieke Universiteit Leuven)Seppe vanden Broucke (Katholieke Universiteit Leuven)Jan Vanthienen (Katholieke Universiteit Leuven)Wilfried Lemahieu (Katholieke Universiteit Leuven)Bart Baesens (Katholieke Universiteit Leuven),"2519675195,2512307785,2522590243,783869731,153925120,1218880500,2061851337",-,2016,Knowledge Discovery and Data Mining,Fields of study: software analyticsweb analyticssemantic analyticsanalyticscloud computingdata analysisbusiness analyticsbusiness intelligencedata scienceworld wide webdatabasecomputer science
Interactive Constrained Boolean Matrix Factorization,Nelson Mukuze (Max Planck Society)Pauli Miettinen (Max Planck Society),"2586077000,2015634213",-,2016,Knowledge Discovery and Data Mining,Fields of study: standard boolean modelincomplete lu factorizationlogical matrixmatrix decompositiondiscrete mathematicscombinatoricsalgebra
SIDE : a web app for interactive visual data exploration with subjective feedback,Jefrey Lijffijt (University of Bristol)Bo Kang (Ghent University)Kai Puolamäki (Aalto University)Tijl De Bie (University of Bristol),"115479936,2497045425,96415260,2080198120",-,2016,Knowledge Discovery and Data Mining,Fields of study: multimediaworld wide websimulation
"Advances in knowledge discovery and data mining: 20th pacific-asia conference, PAKDD 2016 Auckland, New Zealand, April 19-22, 2016 proceedings, part I",James Bailey (University of Melbourne)Latifur Khan (University of Texas at Dallas)Takashi Washio (Osaka University)Gillian Dobbie (University of Auckland)Joshua Zhexue Huang (Shenzhen University)Ruili Wang (Massey University),"2131557737,2155983610,1794038515,2002900660,2699884321,2162153024",Classification.- Machine learning.- Applications.- Novel methods and algorithms.- Opinion mining and sentiment analysis.- Clustering.- Feature extraction and pattern mining.- Graph and network data.- Spatiotemporal and image data.- Anomaly detection and clustering.- Novel models and algorithms.- Text mining and recommender systems.,2016,Knowledge Discovery and Data Mining,Fields of study: text miningdata scienceinformation retrievaldata mining
Theoretical Foundations and Algorithms for Outlier Ensembles,Charu C. Aggarwal (IBM)Saket Sathe (IBM),"2146335907,2478550524","Ensemble analysis has recently been studied in the context of the outlier detection problem. In this paper, we investigate the theoretical underpinnings of outlier ensemble analysis. In spite of the significant differences between the classification and the outlier analysis problems, we show that the theoretical underpinnings between the two problems are actually quite similar in terms of the bias-variance trade-off. We explain the existing algorithms within this traditional framework, and clarify misconceptions about the reasoning underpinning these methods. We propose more effective variants of subsampling and feature bagging. We also discuss the impact of the combination function and discuss the specific trade-offs of the average and maximization functions. We use these insights to propose new combination functions that are robust in many settings.",2015,Knowledge Discovery and Data Mining,Fields of study: data sciencedata miningmachine learningstatisticscomputer science
Question Quality in Community Question Answering Forums: a survey,Antoaneta Baltadzhieva (Tilburg University)Grzegorz Chrupała (Tilburg University),"2229077795,2270694663","Community Question Answering websites (CQA) offer a new opportunity for users to provide, search and share knowledge. Although the idea of receiving a direct, targeted response to a question sounds very attractive, the quality of the question itself can have an important effect on the likelihood of getting useful answers. High quality questions improve the CQA experience and therefore it is essential for CQA forums to better understand what characterizes questions that are more appealing for the forum community. In this survey, we review existing research on question quality in CQA websites. We discuss the possible measures of question quality and the question features that have been shown to influence question quality.",2015,Knowledge Discovery and Data Mining,Fields of study: divergent questionworld wide webinformation retrievaldata mining
Report of the First International Workshop on Learning over Multiple Contexts (LMCE 2014),Cèsar Ferri (Polytechnic University of Valencia)Peter Flach (University of Bristol)Nicolas Lachiche (University of Strasbourg),"2170579254,1814273096,254277980","The first international workshop on Learning over Multiple Contexts, devoted to generalization and reuse of machine learning models over multiple contexts, was held on September 19th, 2014, as part of the 7th European machine learning and data mining conference (ECML-PKDD 2014) in Nancy, France. This short report summarizes the presentations and discussions held during the LMCE 2014 workshop, as well as the workshop conclusions and nthe future agenda.",2015,Knowledge Discovery and Data Mining,Fields of study: data sciencemultimediadata miningmachine learningcomputer science
Load-Balancing the Distance Computations in Record Linkage,Dimitrios Karapiperis (Hellenic Open University)Vassilios S. Verykios (Hellenic Open University),"1879788522,77221159","In this paper, we propose a novel method for distributing the distance computations of record pairs generated by a blocking mechanism to the reduce tasks of a Map/Reduce system. The proposed solutions in the literature analyze the blocks and then construct a profile, which contains the number of record pairs in each block. However, this deterministic process, including all its variants, might incur considerable overhead given massive data sets. In contrast, our method utilizes two Map/Reduce jobs where the first job formulates the record pairs while the second job distributes these pairs to the reduce tasks, which perform the distance computations, using repetitive allocation rounds. In each such round, we utilize all the available reduce tasks on a random basis by generating permutations of their indexes. A series of experiments demonstrate an almost-equal distribution of the record pairs, or equivalently of the distance computations, to the reduce tasks, which makes our method a simple, yet efficient, solution for applying a blocking mechanism given massive data sets.",2015,Knowledge Discovery and Data Mining,Fields of study: theoretical computer sciencedistributed computingalgorithmcomputer science
A Framework for Collocation Error Correction in Web Pages and Text Documents,Alan Varghese (Montclair State University)Aparna S. Varde (Montclair State University)Jing Peng (Montclair State University)Eileen Fitzpatrick (Montclair State University),"2541086108,74820909,2140174629,2166619265","Much of the English in text documents today comes from nonnative speakers. Web searches are also conducted very often by non-native speakers. Though highly qualified in their respective fields, these speakers could potentially make errors in collocation, e.g., ""dark money"" and ""stock agora"" (instead of the more appropriate English expressions ""black money"" and ""stock market"" respectively). These may arise due to literal translation from the respective speaker's native language or other factors. Such errors could cause problems in contexts such as querying over Web pages, correct understanding of text documents and more. This paper proposes a framework called CollOrder to detect such collocation errors and suggest correctly ordered collocated responses for improving the semantics. This framework integrates machine learning approaches with natural language processing techniques, proposing suitable heuristics to provide responses to collocation errors, ranked in the order of correctness. We discuss the proposed framework with algorithms and experimental evaluation in this paper. We claim that it would be useful in semantically enhancing Web querying e.g., financial news, online shopping etc. It would also help in providing automated error correction in machine translated documents and offering assistance to people using ESL tools.",2015,Knowledge Discovery and Data Mining,Fields of study: natural language processingworld wide webdata miningmachine learningcomputer science
Collaborative Deep Learning for Recommender Systems,Hao Wang (Hong Kong University of Science and Technology)Naiyan Wang (Hong Kong University of Science and Technology)Dit Yan Yeung (Hong Kong University of Science and Technology),"2625698375,2626808564,2109477161","Collaborative filtering (CF) is a successful approach commonly used by many recommender systems. Conventional CF-based methods use the ratings given to items by users as the sole source of information for learning to make recommendation. However, the ratings are often very sparse in many applications, causing CF-based methods to degrade significantly in their recommendation performance. To address this sparsity problem, auxiliary information such as item content information may be utilized. Collaborative topic regression (CTR) is an appealing recent method taking this approach which tightly couples the two components that learn from two different sources of information. Nevertheless, the latent representation learned by CTR may not be very effective when the auxiliary information is very sparse. To address this problem, we generalize recently advances in deep learning from i.i.d. input to non-i.i.d. (CF-based) input and propose in this paper a hierarchical Bayesian model called collaborative deep learning (CDL), which jointly performs deep representation learning for the content information and collaborative filtering for the ratings (feedback) matrix. Extensive experiments on three real-world datasets from different domains show that CDL can significantly advance the state of the art.",2015,Knowledge Discovery and Data Mining,Fields of study: topic modelcollaborative filteringdeep learningrecommender systemtext miningworld wide webdata miningmachine learningcomputer science
Inferring Networks of Substitutable and Complementary Products,"Julian J. McAuley (University of California, San Diego)Rahul PandeyJure Leskovec (Stanford University)","2041520510,2506670837,1878631932","To design a useful recommender system, it is important to understand how products relate to each other. For example, while a user is browsing mobile phones, it might make sense to recommend other phones, but once they buy a phone, we might instead want to recommend batteries, cases, or chargers. In economics, these two types of recommendations are referred to as substitutes and complements: substitutes are products that can be purchased instead of each other, while complements are products that can be purchased in addition to each other. Such relationships are essential as they help us to identify items that are relevant to a user's search. Our goal in this paper is to learn the semantics of substitutes and complements from the text of online reviews. We treat this as a supervised learning problem, trained using networks of products derived from browsing and co-purchasing logs. Methodologically, we build topic models that are trained to automatically discover topics from product reviews that are successful at predicting and explaining such relationships. Experimentally, we evaluate our system on the Amazon product catalog, a large dataset consisting of 9 million products, 237 million links, and 144 million reviews.",2015,Knowledge Discovery and Data Mining,Fields of study: topic modelrecommender systemworld wide webdata miningmachine learning +1 other
Certifying and Removing Disparate Impact,Michael Feldman (Haverford College)Sorelle A. Friedler (Haverford College)John Moeller (University of Utah)Carlos Scheidegger (University of Arizona)Suresh Venkatasubramanian (University of Utah),"2575945510,2622481509,2127350133,2119064567,2090109395","What does it mean for an algorithm to be biased? In U.S. law, unintentional bias is encoded via disparate impact, which occurs when a selection process has widely different outcomes for different groups, even as it appears to be neutral. This legal determination hinges on a definition of a protected class (ethnicity, gender) and an explicit description of the process. When computers are involved, determining disparate impact (and hence bias) is harder. It might not be possible to disclose the process. In addition, even if the process is open, it might be hard to elucidate in a legal setting how the algorithm makes its decisions. Instead of requiring access to the process, we propose making inferences based on the data it uses. We present four contributions. First, we link disparate impact to a measure of classification accuracy that while known, has received relatively little attention. Second, we propose a test for disparate impact based on how well the protected class can be predicted from the other attributes. Third, we describe methods by which data might be made unbiased. Finally, we present empirical evidence supporting the effectiveness of our test for disparate impact and our approach for both masking bias and preserving relevant information in the data. Interestingly, our approach resembles some actual selection practices that have recently received legal scrutiny.",2015,Knowledge Discovery and Data Mining,Fields of study: management sciencedata miningmachine learningcomputer science
SEISMIC: A Self-Exciting Point Process Model for Predicting Tweet Popularity,Qingyuan Zhao (Stanford University)Murat A. Erdogdu (Stanford University)Hera Y. He (Stanford University)Anand Rajaraman (Stanford University)Jure Leskovec (Stanford University),"2222925634,2222747345,2118127398,2001295541,1878631932","Social networking websites allow users to create and share content. Big information cascades of post resharing can form as users of these sites reshare others' posts with their friends and followers. One of the central challenges in understanding such cascading behaviors is in forecasting information outbreaks, where a single post becomes widely popular by being reshared by many users. In this paper, we focus on predicting the final number of reshares of a given post. We build on the theory of self-exciting point processes to develop a statistical model that allows us to make accurate predictions. Our model requires no training or expensive feature engineering. It results in a simple and efficiently computable formula that allows us to answer questions, in real-time, such as: Given a post's resharing history so far, what is our current estimate of its final number of reshares? Is the post resharing cascade past the initial stage of explosive growth? And, which posts will be the most reshared in the future? We validate our model using one month of complete Twitter data and demonstrate a strong improvement in predictive accuracy over existing approaches. Our model gives only 15% relative error in predicting final size of an average information cascade after observing it for just one hour.",2015,Knowledge Discovery and Data Mining,Fields of study: social mediaworld wide webdata miningartificial intelligencemachine learning +3 others
Forecasting Fine-Grained Air Quality Based on Big Data,Yu Zheng (Microsoft)Xiuwen Yi (Southwest Jiaotong University)Ming Li (Microsoft)Ruiyuan Li (Wuhan University)Zhangqing Shan (Fudan University) +2 others,"2145115012,2226911177,2517922615,2678369202,2635678311,2110283602,2717994837","In this paper, we forecast the reading of an air quality monitoring station over the next 48 hours, using a data-driven method that considers current meteorological data, weather forecasts, and air quality data of the station and that of other stations within a few hundred kilometers. Our predictive model is comprised of four major components: 1) a linear regression-based temporal predictor to model the local factors of air quality, 2) a neural network-based spatial predictor to model global factors, 3) a dynamic aggregator combining the predictions of the spatial and temporal predictors according to meteorological data, and 4) an inflection predictor to capture sudden changes in air quality. We evaluate our model with data from 43 cities in China, surpassing the results of multiple baseline methods. We have deployed a system with the Chinese Ministry of Environmental Protection, providing 48-hour fine-grained air quality forecasts for four major Chinese cities every hour. The forecast function is also enabled on Microsoft Bing Map and MS cloud platform Azure. Our technology is general and can be applied globally for other cities.",2015,Knowledge Discovery and Data Mining,Fields of study: big dataoperations researchdata miningsimulationcomputer science
Petuum: A New Platform for Distributed Machine Learning on Big Data,"Eric P. Xing (Carnegie Mellon University)Qirong Ho (Agency for Science, Technology and Research)Wei Dai (Carnegie Mellon University)Jin Kyu Kim (Carnegie Mellon University)Jinliang Wei (Carnegie Mellon University) +5 others","351197510,2171689288,2194200048,2308163177,2113580177,2115333817,2132416899,2157788543,2132492541,2119954973","How can one build a distributed framework that allows efficient deployment of a wide spectrum of modern advanced machine learning (ML) programs for industrial-scale problems using Big Models (100s of billions of parameters) on Big Data (terabytes or petabytes)- Contemporary parallelization strategies employ fine-grained operations and scheduling beyond the classic bulk-synchronous processing paradigm popularized by MapReduce, or even specialized operators relying on graphical representations of ML programs. The variety of approaches tends to pull systems and algorithms design in different directions, and it remains difficult to find a universal platform applicable to a wide range of different ML programs at scale. We propose a general-purpose framework that systematically addresses data- and model-parallel challenges in large-scale ML, by leveraging several fundamental properties underlying ML programs that make them different from conventional operation-centric programs: error tolerance, dynamic structure, and nonuniform convergence; all stem from the optimization-centric nature shared in ML programs' mathematical definitions, and the iterative-convergent behavior of their algorithmic solutions. These properties present unique opportunities for an integrative system design, built on bounded-latency network synchronization and dynamic load-balancing scheduling, which is efficient, programmable, and enjoys provable correctness guarantees. We demonstrate how such a design in light of ML-first principles leads to significant performance improvements versus well-known implementations of several ML programs, allowing them to run in much less time and at considerably larger model sizes, on modestly-sized computer clusters.",2015,Knowledge Discovery and Data Mining,Fields of study: data parallelismtheorybig datatheoretical computer sciencedistributed computing +4 others
Heterogeneous Network Embedding via Deep Architectures,Shiyu Chang (University of Illinois at Urbana–Champaign)Wei Han (University of Illinois at Urbana–Champaign)Jiliang Tang (Arizona State University)Guo-Jun Qi (University of Central Florida)Charu C. Aggarwal (IBM) +1 other,"2098291119,2692381660,2147392410,2237849324,2146335907,2149631809","Data embedding is used in many machine learning applications to create low-dimensional feature representations, which preserves the structure of data points in their original space. In this paper, we examine the scenario of a heterogeneous network with nodes and content of various types. Such networks are notoriously difficult to mine because of the bewildering combination of heterogeneous contents and structures. The creation of a multidimensional embedding of such data opens the door to the use of a wide variety of off-the-shelf mining techniques for multidimensional data. Despite the importance of this problem, limited efforts have been made on embedding a network of scalable, dynamic and heterogeneous data. In such cases, both the content and linkage structure provide important cues for creating a unified feature representation of the underlying network. In this paper, we design a deep embedding algorithm for networked data. A highly nonlinear multi-layered embedding function is used to capture the complex interactions between the heterogeneous data in a network. Our goal is to create a multi-resolution deep embedding function, that reflects both the local and global network structures, and makes the resulting embedding useful for a variety of data mining tasks. In particular, we demonstrate that the rich content and linkage information in a heterogeneous network can be captured by such an approach, so that similarities among cross-modal data can be measured directly in a common embedding space. Once this goal has been achieved, a wide variety of data mining problems can be solved by applying off-the-shelf algorithms designed for handling vector representations. Our experiments on real-world network datasets show the effectiveness and scalability of the proposed algorithm as compared to the state-of-the-art embedding methods.",2015,Knowledge Discovery and Data Mining,Fields of study: dimensionality reductionfeature learningdeep learningtheoretical computer sciencedata mining +2 others
FaitCrowd: Fine Grained Truth Discovery for Crowdsourced Data Aggregation,Fenglong Ma (University at Buffalo)Yaliang Li (University at Buffalo)Qi Li (University at Buffalo)Minghui Qiu (Singapore Management University)Jing Gao (University at Buffalo) +5 others,"2227076362,2116094297,2261907930,2158082241,2096731881,2150664895,2148733542,2674375462,2282310307,2121939561","In crowdsourced data aggregation task, there exist conflicts in the answers provided by large numbers of sources on the same set of questions. The most important challenge for this task is to estimate source reliability and select answers that are provided by high-quality sources. Existing work solves this problem by simultaneously estimating sources' reliability and inferring questions' true answers (i.e., the truths). However, these methods assume that a source has the same reliability degree on all the questions, but ignore the fact that sources' reliability may vary significantly among different topics. To capture various expertise levels on different topics, we propose FaitCrowd, a fine grained truth discovery model for the task of aggregating conflicting data collected from multiple users/sources. FaitCrowd jointly models the process of generating question content and sources' provided answers in a probabilistic model to estimate both topical expertise and true answers simultaneously. This leads to a more precise estimation of source reliability. Therefore, FaitCrowd demonstrates better ability to obtain true answers for the questions compared with existing approaches. Experimental results on two real-world datasets show that FaitCrowd can significantly reduce the error rate of aggregation compared with the state-of-the-art multi-source aggregation approaches due to its ability of learning topical expertise from question content and collected answers.",2015,Knowledge Discovery and Data Mining,Fields of study: crowdsourcingdata scienceinformation retrievaldata miningcomputer science
Intelligible Models for HealthCare: Predicting Pneumonia Risk and Hospital 30-day Readmission,Rich Caruana (Microsoft)Yin Lou (LinkedIn)Johannes Gehrke (Microsoft)Paul Koch (Microsoft)Marc Sturm (NewYork–Presbyterian Hospital) +1 other,"2680453709,2308121590,2083845045,2141080278,2120335324,1964966895","In machine learning often a tradeoff must be made between accuracy and intelligibility. More accurate models such as boosted trees, random forests, and neural nets usually are not intelligible, but more intelligible models such as logistic regression, naive-Bayes, and single decision trees often have significantly worse accuracy. This tradeoff sometimes limits the accuracy of models that can be applied in mission-critical applications such as healthcare where being able to understand, validate, edit, and trust a learned model is important. We present two case studies where high-performance generalized additive models with pairwise interactions (GA2Ms) are applied to real healthcare problems yielding intelligible models with state-of-the-art accuracy. In the pneumonia risk prediction case study, the intelligible model uncovers surprising patterns in the data that previously had prevented complex learned models from being fielded in this domain, but because it is intelligible and modular allows these patterns to be recognized and removed. In the 30-day hospital readmission case study, we show that the same methods scale to large datasets containing hundreds of thousands of patients and thousands of attributes while remaining intelligible and providing accuracy comparable to the best (unintelligible) machine learning methods.",2015,Knowledge Discovery and Data Mining,Fields of study: intelligibilitylogistic regressionbiological classificationdata miningmachine learning +2 others
Network Lasso: Clustering and Optimization in Large Graphs,David Hallac (Stanford University)Jure Leskovec (Stanford University)Stephen P. Boyd (Stanford University),"2226262640,1878631932,2107142044","Convex optimization is an essential tool for modern data analysis, as it provides a framework to formulate and solve many problems in machine learning and data mining. However, general convex optimization solvers do not scale well, and scalable solvers are often specialized to only work on a narrow class of problems. Therefore, there is a need for simple, scalable algorithms that can solve many common optimization problems. In this paper, we introduce the network lasso , a generalization of the group lasso to a network setting that allows for simultaneous clustering and optimization on graphs. We develop an algorithm based on the Alternating Direction Method of Multipliers (ADMM) to solve this problem in a distributed and scalable manner, which allows for guaranteed global convergence even on large graphs. We also examine a non-convex extension of this approach. We then demonstrate that many types of problems can be expressed in our framework. We focus on three in particular --- binary classification, predicting housing prices, and event detection in time series data --- comparing the network lasso to baseline approaches and showing that it is both a fast and accurate method of solving large optimization problems.",2015,Knowledge Discovery and Data Mining,Fields of study: proximal gradient methods for learningconvex optimizationcombinatoricsmachine learningmathematical optimizationmathematics
COSNET: Connecting Heterogeneous Social Networks with Local and Global Consistency,Yutao Zhang (Tsinghua University)Jie Tang (Tsinghua University)Zhilin Yang (Tsinghua University)Jian Pei (Simon Fraser University)Philip S. Yu (University of Illinois at Chicago),"2223115198,2158012360,2489788272,2126330539,2125104194","More often than not, people are active in more than one social network. Identifying users from multiple heterogeneous social networks and integrating the different networks is a fundamental issue in many applications. The existing methods tackle this problem by estimating pairwise similarity between users in two networks. However, those methods suffer from potential inconsistency of matchings between multiple networks. In this paper, we propose COSNET (COnnecting heterogeneous Social NETworks with local and global consistency), a novel energy-based model, to address this problem by considering both local and global consistency among multiple networks. An efficient subgradient algorithm is developed to train the model by converting the original energy-based objective function into its dual form. We evaluate the proposed model on two different genres of data collections: SNS and Academia, each consisting of multiple heterogeneous social networks. Our experimental results validate the effectiveness and efficiency of the proposed model. On both data collections, the proposed COSNET method significantly outperforms several alternative methods by up to 10-30% (p t -test) in terms of F1-score. We also demonstrate that applying the integration results produced by our method can improve the accuracy of expert finding, an important task in social networks.",2015,Knowledge Discovery and Data Mining,Fields of study: evolving networksdynamic network analysisnetwork sciencesocial networkdata miningartificial intelligencemachine learningcomputer science
PTE: Predictive Text Embedding through Large-scale Heterogeneous Text Networks,Jian Tang (Microsoft)Meng Qu (Peking University)Qiaozhu Mei (University of Michigan),"2625476853,2224800206,2166036605","Unsupervised text embedding methods, such as Skip-gram and Paragraph Vector, have been attracting increasing attention due to their simplicity, scalability, and effectiveness. However, comparing to sophisticated deep learning architectures such as convolutional neural networks, these methods usually yield inferior results when applied to particular machine learning tasks. One possible reason is that these text embedding methods learn the representation of text in a fully unsupervised way, without leveraging the labeled information available for the task. Although the low dimensional representations learned are applicable to many different tasks, they are not particularly tuned for any task. In this paper, we fill this gap by proposing a semi-supervised representation learning method for text data, which we call the predictive text embedding (PTE). Predictive text embedding utilizes both labeled and unlabeled data to learn the embedding of text. The labeled information and different levels of word co-occurrence information are first represented as a large-scale heterogeneous text network, which is then embedded into a low dimensional space through a principled and efficient algorithm. This low dimensional embedding not only preserves the semantic closeness of words and documents, but also has a strong predictive power for the particular task. Compared to recent supervised approaches based on convolutional neural networks, predictive text embedding is comparable or more effective, much more efficient, and has fewer parameters to tune.",2015,Knowledge Discovery and Data Mining,Fields of study: theoretical computer sciencenatural language processingpattern recognitionartificial intelligencemachine learningcomputer science
Modeling Truth Existence in Truth Discovery,Shi Zhi (University of Illinois at Urbana–Champaign)Bo Zhao (LinkedIn)Wenzhu Tong (University of Illinois at Urbana–Champaign)Jing Gao (University at Buffalo)Dian Yu (Rensselaer Polytechnic Institute)Heng Ji (Rensselaer Polytechnic Institute)Jiawei Han (University of Illinois at Urbana–Champaign),"2150664895,2674375462,2232598181,2096731881,2625375043,2282310307,2121939561","When integrating information from multiple sources, it is common to encounter conflicting answers to the same question. Truth discovery is to infer the most accurate and complete integrated answers from conflicting sources. In some cases, there exist questions for which the true answers are excluded from the candidate answers provided by all sources. Without any prior knowledge, these questions, named no-truth questions , are difficult to be distinguished from the questions that have true answers, named has-truth questions . In particular, these no-truth questions degrade the precision of the answer integration system. We address such a challenge by introducing source quality , which is made up of three fine-grained measures: silent rate, false spoken rate and true spoken rate. By incorporating these three measures, we propose a probabilistic graphical model, which simultaneously infers truth as well as source quality without any a priori training involving ground truth answers. Moreover, since inferring this graphical model requires parameter tuning of the prior of truth, we propose an initialization scheme based upon a quantity named truth existence score , which synthesizes two indicators, namely, participation rate and consistency rate . Compared with existing methods, our method can effectively filter out no-truth questions, which results in more accurate source quality estimation. Consequently, our method provides more accurate and complete answers to both has-truth and no-truth questions. Experiments on three real-world datasets illustrate the notable advantage of our method over existing state-of-the-art truth discovery methods.",2015,Knowledge Discovery and Data Mining,Fields of study: single version of the truthmedical researchinformation extractionknowledge basetext miningdata scienceinformation retrievaldata miningmachine learningcomputer science
On the Discovery of Evolving Truth,Yaliang Li (University at Buffalo)Qi Li (University at Buffalo)Jing Gao (University at Buffalo)Lu Su (University at Buffalo)Bo Zhao (LinkedIn)Wei Fan (Baidu)Jiawei Han (University of Illinois at Urbana–Champaign),"2116094297,2261907930,2096731881,2148733542,2674375462,2422054197,2121939561","In the era of big data, information regarding the same objects can be collected from increasingly more sources. Unfortunately, there usually exist conflicts among the information coming from different sources. To tackle this challenge, truth discovery, i.e., to integrate multi-source noisy information by estimating the reliability of each source, has emerged as a hot topic. In many real world applications, however, the information may come sequentially, and as a consequence, the truth of objects as well as the reliability of sources may be dynamically evolving. Existing truth discovery methods, unfortunately, cannot handle such scenarios. To address this problem, we investigate the temporal relations among both object truths and source reliability, and propose an incremental truth discovery framework that can dynamically update object truths and source weights upon the arrival of new data. Theoretical analysis is provided to show that the proposed method is guaranteed to converge at a fast rate. The experiments on three real world applications and a set of synthetic data demonstrate the advantages of the proposed method over state-of-the-art truth discovery methods.",2015,Knowledge Discovery and Data Mining,Fields of study: single version of the truthdynamic datadata sciencedata miningartificial intelligencecomputer science
Whither Social Networks for Web Search,Rakesh Agrawal (IBM)Behzad Golshan (Boston University)Evangelos E. Papalexakis (Carnegie Mellon University),"2138427228,2056540169,1418764031","Access to diverse perspectives nurtures an informed citizenry. Google and Bing have emerged as the duopoly that largely arbitrates which English language documents are seen by web searchers. A recent study shows that there is now a large overlap in the top organic search results produced by them. Thus, citizens may no longer be able to gain different perspectives by using different search engines. We present the results of our empirical study that indicates that by mining Twitter data one can obtain search results that are quite distinct from those produced by Google and Bing. Additionally, our user study found that these results were quite informative. The gauntlet is now on search engines to test whether our findings hold in their infrastructure for different social networks and whether enabling diversity has sufficient business imperative for them.",2015,Knowledge Discovery and Data Mining,Fields of study: search analyticsorganic searchsearch engine optimizationspamdexingsearch enginesemantic searchweb search engineinternet privacyworld wide webdata miningcomputer science
VEWS: A Wikipedia Vandal Early Warning System,"Srijan Kumar (University of Maryland, College Park)Francesca Spezzano (University of Maryland, College Park)V.S. Subrahmanian (University of Maryland, College Park)","2191206652,2203751546,2261167843","We study the problem of detecting vandals on Wikipedia before any human or known vandalism detection system reports flagging potential vandals so that such users can be presented early to Wikipedia administrators. We leverage multiple classical ML approaches, but develop 3 novel sets of features. Our Wikipedia Vandal Behavior (WVB) approach uses a novel set of user editing patterns as features to classify some users as vandals. Our Wikipedia Transition Probability Matrix (WTPM) approach uses a set of features derived from a transition probability matrix and then reduces it via a neural net auto-encoder to classify some users as vandals. The VEWS approach merges the previous two approaches. Without using any information (e.g. reverts) provided by other users, these algorithms each have over 85% classification accuracy. Moreover, when temporal recency is considered, accuracy goes to almost 90%. We carry out detailed experiments on a new data set we have created consisting of about 33K Wikipedia users (including both a black list and a white list of editors) and containing 770K edits. We describe specific behaviors that distinguish between vandals and non-vandals. We show that VEWS beats ClueBot NG and STiki, the best known algorithms today for vandalism detection. Moreover, VEWS detects far more vandals than ClueBot NG and on average, detects them 2.39 edits before ClueBot NG when both detect the vandal. However, we show that the combination of VEWS and ClueBot NG can give a fully automated vandal early warning system with even higher accuracy.",2015,Knowledge Discovery and Data Mining,Fields of study: brandbehavioral modelingdata scienceworld wide webdata miningmachine learningcomputer science
Dirichlet-Hawkes Processes with Applications to Clustering Continuous-Time Document Streams,Nan Du (Georgia Institute of Technology)Mehrdad Farajtabar (Georgia Institute of Technology)Amr Ahmed (Google)Alexander J. Smola (Carnegie Mellon University)Le Song (Georgia Institute of Technology),"2681008474,1198110563,2259645355,1972291593,2113868374","Clusters in document streams, such as online news articles, can be induced by their textual contents, as well as by the temporal dynamics of their arriving patterns. Can we leverage both sources of information to obtain a better clustering of the documents, and distill information that is not possible to extract using contents only? In this paper, we propose a novel random process, referred to as the Dirichlet-Hawkes process, to take into account both information in a unified framework. A distinctive feature of the proposed model is that the preferential attachment of items to clusters according to cluster sizes, present in Dirichlet processes, is now driven according to the intensities of cluster-wise self-exciting temporal point processes, the Hawkes processes. This new model establishes a previously unexplored connection between Bayesian Nonparametrics and temporal Point Processes, which makes the number of clusters grow to accommodate the increasing complexity of online streaming contents, while at the same time adapts to the ever changing dynamics of the respective continuous arrival time. We conducted large-scale experiments on both synthetic and real world news articles, and show that Dirichlet-Hawkes processes can recover both meaningful topics and temporal dynamics, which leads to better predictive performance in terms of content perplexity and arrival time of future documents.",2015,Knowledge Discovery and Data Mining,Fields of study: data sciencespeech recognitiondata miningmachine learningstatisticscomputer science
Geo-SAGE: A Geographical Sparse Additive Generative Model for Spatial Item Recommendation,"Weiqing Wang (University of Queensland)Hongzhi Yin (University of Queensland)Ling Chen (University of Technology, Sydney)Yizhou Sun (Northeastern University)Shazia Wasim Sadiq (University of Queensland)Xiaofang Zhou (University of Queensland)","2610700303,2145818752,2633547329,2131539564,2122552307,2128990482","With the rapid development of location-based social networks (LBSNs), spatial item recommendation has become an important means to help people discover attractive and interesting venues and events, especially when users travel out of town. However, this recommendation is very challenging compared to the traditional recommender systems. A user can visit only a limited number of spatial items, leading to a very sparse user-item matrix. Most of the items visited by a user are located within a short distance from where he/she lives, which makes it hard to recommend items when the user travels to a far away place. Moreover, user interests and behavior patterns may vary dramatically across different geographical regions. In light of this, we propose Geo-SAGE, a geographical sparse additive generative model for spatial item recommendation in this paper. Geo-SAGE considers both user personal interests and the preference of the crowd in the target region, by exploiting both the co-occurrence pattern of spatial items and the content of spatial items. To further alleviate the data sparsity issue, Geo-SAGE exploits the geographical correlation by smoothing the crowd's preferences over a well-designed spatial index structure called spatial pyramid. We conduct extensive experiments and the experimental results clearly demonstrate our Geo-SAGE model outperforms the state-of-the-art.",2015,Knowledge Discovery and Data Mining,Fields of study: cold startlocation based servicerecommender systemworld wide webinformation retrievaldata miningmachine learningcomputer science
Large-Scale Distributed Bayesian Matrix Factorization using Stochastic Gradient MCMC,"Sungjin Ahn (University of California, Irvine)Anoop Korattikara (Google)Nathan Liu (Yahoo!)Suju Rajan (Yahoo!)Max Welling (University of Amsterdam)","2525862771,40178510,2230629529,2229133469,2014508897","Despite having various attractive qualities such as high prediction accuracy and the ability to quantify uncertainty and avoid over-fitting, Bayesian Matrix Factorization has not been widely adopted because of the prohibitive cost of inference. In this paper, we propose a scalable distributed Bayesian matrix factorization algorithm using stochastic gradient MCMC. Our algorithm, based on Distributed Stochastic Gradient Langevin Dynamics, can not only match the prediction accuracy of standard MCMC methods like Gibbs sampling, but at the same time is as fast and simple as stochastic gradient descent. In our experiments, we show that our algorithm can achieve the same level of prediction accuracy as Gibbs sampling an order of magnitude faster. We also show that our method reduces the prediction error as fast as distributed stochastic gradient descent, achieving a 4.1% improvement in RMSE for the Netflix dataset and an 1.8% for the Yahoo music dataset.",2015,Knowledge Discovery and Data Mining,Fields of study: markov chain monte carlobayesian inferencematrix decompositionmachine learningmathematical optimizationstatisticsmathematics
Inside Jokes: Identifying Humorous Cartoon Captions,Dafna Shahaf (Microsoft)Eric Horvitz (Microsoft)Robert Mankoff,"1275334909,1970391018,2671073649","Humor is an integral aspect of the human experience. Motivated by the prospect of creating computational models of humor, we study the influence of the language of cartoon captions on the perceived humorousness of the cartoons. Our studies are based on a large corpus of crowdsourced cartoon captions that were submitted to a contest hosted by the New Yorker. Having access to thousands of captions submitted for the same image allows us to analyze the breadth of responses of people to the same visual stimulus. We first describe how we acquire judgments about the humorousness of different captions. Then, we detail the construction of a corpus where captions deemed funnier are paired with less-funny captions for the same cartoon. We analyze the caption pairs and find significant differences between the funnier and less-funny captions. Next, we build a classifier to identify funnier captions automatically. Given two captions and a cartoon, our classifier picks the funnier one 69% of the time for captions hinging on the same joke, and 64% of the time for any pair of captions. Finally, we use the classifier to find the best captions and study how its predictions could be used to significantly reduce the load on the cartoon contest's judges.",2015,Knowledge Discovery and Data Mining,Fields of study: comedymultimedia
SAME but Different: Fast and High Quality Gibbs Parameter Estimation,"Huasha Zhao (University of California, Berkeley)Biye Jiang (University of California, Berkeley)John F. Canny (University of California, Berkeley)Bobby Jaros (Yahoo!)","2161004278,2225151974,2101610026,2226871070","Gibbs sampling is a workhorse for Bayesian inference but has several limitations when used for parameter estimation, and is often much slower than non-sampling inference methods. SAME (State Augmentation for Marginal Estimation) [15, 8] is an approach to MAP parameter estimation which gives improved parameter estimates over direct Gibbs sampling. SAME can be viewed as cooling the posterior parameter distribution and allows annealed search for the MAP parameters, often yielding very high quality estimates. But it does so at the expense of additional samples per iteration and generally slower performance. On the other hand, SAME dramatically increases the parallelism in the sampling schedule, and is an excellent match for modern (SIMD) hardware. In this paper we explore the application of SAME to graphical model inference on modern hardware. We show that combining SAME with factored sample representation (or approximation) gives throughput competitive with the fastest symbolic methods, but with potentially better quality. We describe experiments on Latent Dirichlet Allocation, achieving speeds similar to the fastest reported methods (online Variational Bayes) and lower cross-validated loss than other LDA implementations. The method is simple to implement and should be applicable to many other models.",2015,Knowledge Discovery and Data Mining,Fields of study: latent dirichlet allocationgibbs samplingsimulated annealingeconometricsmachine learningmathematical optimizationstatisticscomputer sciencemathematics
From Group to Individual Labels Using Deep Features,"Dimitrios Kotzias (University of California, Irvine)Misha Denil (University of Oxford)Nando de Freitas (University of Oxford)Padhraic Smyth (University of California, Irvine)","2129043304,1895801446,2112824674,2137074633","In many classification problems labels are relatively scarce. One context in which this occurs is where we have labels for groups of instances but not for the instances themselves, as in multi-instance learning. Past work on this problem has typically focused on learning classifiers to make predictions at the group level. In this paper we focus on the problem of learning classifiers to make predictions at the instance level. To achieve this we propose a new objective function that encourages smoothness of inferred instance-level labels based on instance-level similarity, while at the same time respecting group-level label constraints. We apply this approach to the problem of predicting labels for sentences given labels for reviews, using a convolutional neural network to infer sentence similarity. The approach is evaluated using three large review data sets from IMDB, Yelp, and Amazon, and we demonstrate the proposed approach is both accurate and scalable compared to various alternatives.",2015,Knowledge Discovery and Data Mining,Fields of study: stabilitydeep learningsentiment analysisinstance based learningunsupervised learningdata miningpattern recognitionmachine learningcomputer science
Deep Model Based Transfer and Multi-Task Learning for Biological Image Analysis,Wenlu Zhang (Old Dominion University)Rongjian Li (Old Dominion University)Tao Zeng (Old Dominion University)Qian Sun (Arizona State University)Sudhir Kumar (Temple University)Jieping Ye (University of Michigan)Shuiwang Ji (Old Dominion University),"2303814361,2130205980,2310544659,2620185474,2149898553,2305258894,2149659377","A central theme in learning from image data is to develop appropriate image representations for the specific task at hand. Traditional methods used handcrafted local features combined with high-level image representations to generate image-level representations. Thus, a practical challenge is to determine what features are appropriate for specific tasks. For example, in the study of gene expression patterns in Drosophila melanogaster , texture features based on wavelets were particularly effective for determining the developmental stages from in situ hybridization (ISH) images. Such image representation is however not suitable for controlled vocabulary (CV) term annotation because each CV term is often associated with only a part of an image. Here, we developed problem-independent feature extraction methods to generate hierarchical representations for ISH images. Our approach is based on the deep convolutional neural networks (CNNs) that can act on image pixels directly. To make the extracted features generic, the models were trained using a natural image set with millions of labeled examples. These models were transferred to the ISH image domain and used directly as feature extractors to compute image representations. Furthermore, we employed multi-task learning method to fine-tune the pre-trained models with labeled ISH images, and also extracted features from the fine-tuned models. Experimental results showed that feature representations computed by deep models based on transfer and multi-task learning significantly outperformed other methods for annotating gene expression patterns at different stage ranges. We also demonstrated that the intermediate layers of deep models produced the best gene expression pattern representations.",2015,Knowledge Discovery and Data Mining,Fields of study: feature detectionmulti task learningtransfer of learningdeep learningimage analysiscomputer visiondata miningpattern recognitionmachine learningcomputer science
Deep Computational Phenotyping,Zhengping Che (University of Southern California)David C. Kale (University of Southern California)Wenzhe Li (University of Southern California)Mohammad Taha Bahadori (University of Southern California)Yan Liu (University of Southern California),"2117368485,2054676105,2156832614,2032867848,2240541904","We apply deep learning to the problem of discovery and detection of characteristic patterns of physiology in clinical time series data. We propose two novel modifications to standard neural net training that address challenges and exploit properties that are peculiar, if not exclusive, to medical data. First, we examine a general framework for using prior knowledge to regularize parameters in the topmost layers. This framework can leverage priors of any form, ranging from formal ontologies (e.g., ICD9 codes) to data-derived similarity. Second, we describe a scalable procedure for training a collection of neural networks of different sizes but with partially shared architectures. Both of these innovations are well-suited to medical applications, where available data are not yet Internet scale and have many sparse outputs (e.g., rare diagnoses) but which have exploitable structure (e.g., temporal order and relationships between labels). However, both techniques are sufficiently general to be applied to other problems and domains. We demonstrate the empirical efficacy of both techniques on two real-world hospital data sets and show that the resulting neural nets learn interpretable and clinically relevant features.",2015,Knowledge Discovery and Data Mining,Fields of study: phenotypedeep learningdata sciencedata miningmachine learningstatisticscomputer science
Large Scale Distributed Data Science using Apache Spark,"James G. Shanahan (University of California, Berkeley)Laing Dai","2225998645,2225725436","Apache Spark is an open-source cluster computing framework for big data processing. It has emerged as the next generation big data processing engine, overtaking Hadoop MapReduce which helped ignite the big data revolution. Spark maintains MapReduce's linear scalability and fault tolerance, but extends it in a few important ways: it is much faster (100 times faster for certain applications), much easier to program in due to its rich APIs in Python, Java, Scala (and shortly R), and its core data abstraction, the distributed data frame, and it goes far beyond batch applications to support a variety of compute-intensive tasks, including interactive queries, streaming, machine learning, and graph processing. This tutorial will provide an accessible introduction to Spark and its potential to revolutionize academic and commercial data science practices.",2015,Knowledge Discovery and Data Mining,Fields of study: sparkworld wide webdata miningdatabasereal time computingcomputer science
Temporal Phenotyping from Longitudinal Electronic Health Records: A Graph Based Framework,Chuanren Liu (Drexel University)Fei Wang (University of Connecticut)Jianying Hu (IBM)Hui Xiong (Rutgers–Newark),"2169554947,2465953593,2098665416,2153710278","The rapid growth in the development of healthcare information systems has led to an increased interest in utilizing the patient Electronic Health Records (EHR) for assisting disease diagnosis and phenotyping. The patient EHRs are generally longitudinal and naturally represented as medical event sequences, where the events include clinical notes, problems, medications, vital signs, laboratory reports, etc. The longitudinal and heterogeneous properties make EHR analysis an inherently difficult challenge. To address this challenge, in this paper, we develop a novel representation, namely the temporal graph, for such event sequences. The temporal graph is informative for a variety of challenging analytic tasks, such as predictive modeling, since it can capture temporal relationships of the medical events in each event sequence. By summarizing the longitudinal data, the temporal graphs are also robust and resistant to noisy and irregular observations. Based on the temporal graph representation, we further develop an approach for temporal phenotyping to identify the most significant and interpretable graph basis as phenotypes. This helps us better understand the disease evolving patterns. Moreover, by expressing the temporal graphs with the phenotypes, the expressing coefficients can be used for applications such as personalized medicine, disease diagnosis, and patient segmentation. Our temporal phenotyping framework is also flexible to incorporate semi-supervised/supervised information. Finally, we validate our framework on two real-world tasks. One is predicting the onset risk of heart failure. Another is predicting the risk of heart failure related hospitalization for patients with COPD pre-condition. Our results show that the diagnosis performance in both tasks can be improved significantly by the proposed approaches. Also, we illustrate some interesting phenotypes derived from the data.",2015,Knowledge Discovery and Data Mining,Fields of study: regularizationdata sciencebioinformaticsdata miningmachine learningcomputer science
Dynamically Modeling Patient's Health State from Electronic Medical Records: A Time Series Approach,"Karla L. Caballero Barajas (University of California, Santa Cruz)Ram Akella (University of California, Berkeley)","2226443371,2493748921","In this paper, we present a method to dynamically estimate the probability of mortality inside the Intensive Care Unit (ICU) by combining heterogeneous data. We propose a method based on Generalized Linear Dynamic Models that models the probability of mortality as a latent state that evolves over time. This framework allows us to combine different types of features (lab results, vital signs readings, doctor and nurse notes, etc) into a single state, which is updated each time new patient data is observed. In addition, we include the use of text features, based on medical noun phrase extraction and Statistical Topic Models. These features provide context about the patient that cannot be captured when only numerical features are used. We fill out the missing values using a Regularized Expectation Maximization based method assuming temporal data. We test our proposed approach using 15,000 Electronic Medical Records (EMRs) obtained from the MIMIC II public dataset. Experimental results show that the proposed model allows us to detect an increase in the probability of mortality before it occurs. We report an AUC 0.8657. Our proposed model clearly outperforms other methods of the literature in terms of sensitivity with 0.7885 compared to 0.6559 of Naive Bayes and F-score with 0.5929 compared to 0.4662 of Apache III score after 24 hours.",2015,Knowledge Discovery and Data Mining,Fields of study: text miningdata sciencedata miningpattern recognitionmachine learningstatisticscomputer science
RSC: Mining and Modeling Temporal Activity in Social Media,Alceu Ferraz Costa (University of São Paulo)Yuto Yamaguchi (University of Tsukuba)Agma Juci Machado Traina (University of São Paulo)Caetano Traina (University of São Paulo)Christos Faloutsos (Carnegie Mellon University),"2105073060,2150454284,2017812127,2108706841,2198983026","Can we identify patterns of temporal activities caused by human communications in social media? Is it possible to model these patterns and tell if a user is a human or a bot based only on the timing of their postings? Social media services allow users to make postings, generating large datasets of human activity time-stamps. In this paper we analyze time-stamp data from social media services and find that the distribution of postings inter-arrival times (IAT) is characterized by four patterns: (i) positive correlation between consecutive IATs, (ii) heavy tails, (iii) periodic spikes and (iv) bimodal distribution. Based on our findings, we propose Rest-Sleep-and-Comment (RSC), a generative model that is able to match all four discovered patterns. We demonstrate the utility of RSC by showing that it can accurately fit real time-stamp data from Reddit and Twitter. We also show that RSC can be used to spot outliers and detect users with non-human behavior, such as bots. We validate RSC using real data consisting of over 35 million postings from Twitter and Reddit. RSC consistently provides a better fit to real data and clearly outperform existing models for human dynamics. RSC was also able to detect bots with a precision higher than 94%.",2015,Knowledge Discovery and Data Mining,Fields of study: generative modelsocial mediatime seriesinternet privacyworld wide webdata miningmachine learningsimulationstatisticscomputer science
Differentially Private High-Dimensional Data Publication via Sampling-Based Inference,Rui Chen (Samsung)Qian Xiao (National University of Singapore)Yu Zhang (Hong Kong Baptist University)Jianliang Xu (Hong Kong Baptist University),"2690718549,2697978760,2648094648,2119615219","Releasing high-dimensional data enables a wide spectrum of data mining tasks. Yet, individual privacy has been a major obstacle to data sharing. In this paper, we consider the problem of releasing high-dimensional data with differential privacy guarantees. We propose a novel solution to preserve the joint distribution of a high-dimensional dataset. We first develop a robust sampling-based framework to systematically explore the dependencies among all attributes and subsequently build a dependency graph. This framework is coupled with a generic threshold mechanism to significantly improve accuracy. We then identify a set of marginal tables from the dependency graph to approximate the joint distribution based on the solid inference foundation of the junction tree algorithm while minimizing the resultant error. We prove that selecting the optimal marginals with the goal of minimizing error is NP-hard and, thus, design an approximation algorithm using an integer programming relaxation and the constrained concave-convex procedure. Extensive experiments on real datasets demonstrate that our solution substantially outperforms the state-of-the-art competitors.",2015,Knowledge Discovery and Data Mining,Fields of study: dependency graphdifferential privacyjoint probability distributionclustering high dimensional datatheoretical computer sciencedata miningmachine learningstatisticscomputer science
Regularity and Conformity: Location Prediction Using Heterogeneous Mobility Data,Yingzi Wang (University of Science and Technology of China)Nicholas Jing Yuan (Microsoft)Defu Lian (University of Electronic Science and Technology of China)Linli Xu (University of Science and Technology of China)Xing Xie (Microsoft)Enhong Chen (University of Science and Technology of China)Yong Rui (Microsoft),"2274817761,2096490164,2110195189,2326517690,2125800575,2136372366,2704008839","Mobility prediction enables appealing proactive experiences for location-aware services and offers essential intelligence to business and governments. Recent studies suggest that human mobility is highly regular and predictable. Additionally, social conformity theory indicates that people's movements are influenced by others. However, existing approaches for location prediction fail to organically combine both the regularity and conformity of human mobility in a unified model, and lack the capacity to incorporate heterogeneous mobility datasets to boost prediction performance. To address these challenges, in this paper we propose a hybrid predictive model integrating both the regularity and conformity of human mobility as well as their mutual reinforcement. In addition, we further elevate the predictive power of our model by learning location profiles from heterogeneous mobility datasets based on a gravity model. We evaluate the proposed model using several city-scale mobility datasets including location check-ins, GPS trajectories of taxis, and public transit data. The experimental results validate that our model significantly outperforms state-of-the-art approaches for mobility prediction in terms of multiple metrics such as accuracy and percentile rank. The results also suggest that the predictability of human mobility is time-varying, e.g., the overall predictability is higher on workdays than holidays while predicting users' unvisited locations is more challenging for workdays than holidays.",2015,Knowledge Discovery and Data Mining,Fields of study: conformitygravity model of trademobility modelcollaborative filteringdata miningartificial intelligencemachine learningsimulationcomputer science
Bayesian Poisson Tensor Factorization for Inferring Multilateral Relations from Sparse Dyadic Event Counts,Aaron Schein (University of Massachusetts Amherst)John William Paisley (Columbia University)David M. Blei (Columbia University)Hanna M. Wallach (Microsoft),"2158014761,2020801783,2289542319,2148452840","We present a Bayesian tensor factorization model for inferring latent group structures from dynamic pairwise interaction patterns. For decades, political scientists have collected and analyzed records of the form ""country i took action a toward country j at time t "" - known as dyadic events - in order to form and test theories of international relations. We represent these event data as a tensor of counts and develop Bayesian Poisson tensor factorization to infer a low-dimensional, interpretable representation of their salient patterns. We demonstrate that our model's predictive performance is better than that of standard non-negative tensor factorization methods. We also provide a comparison of our variational updates to their maximum likelihood counterparts. In doing so, we identify a better way to form point estimates of the latent factors than that typically used in Bayesian Poisson matrix factorization. Finally, we showcase our model as an exploratory analysis tool for political scientists. We show that the inferred latent factor matrices capture interpretable multilateral relations that both conform to and inform our knowledge of international a airs.",2015,Knowledge Discovery and Data Mining,Fields of study: bayesian inferenceinternational relationseconometricsmachine learningstatisticsmathematics
Inferring Air Quality for Station Location Recommendation Based on Urban Big Data,Hsun-Ping Hsieh (National Taiwan University)Shou-De Lin (National Taiwan University)Yu Zheng (Microsoft),"2109618838,2114357324,2145115012","This paper tries to answer two questions. First, how to infer real-time air quality of any arbitrary location given environmental data and historical air quality data from very sparse monitoring locations. Second, if one needs to establish few new monitoring stations to improve the inference quality, how to determine the best locations for such purpose? The problems are challenging since for most of the locations (>99%) in a city we do not have any air quality data to train a model from. We design a semi-supervised inference model utilizing existing monitoring data together with heterogeneous city dynamics, including meteorology, human mobility, structure of road networks, and point of interests (POIs). We also propose an entropy-minimization model to suggest the best locations to establish new monitoring stations. We evaluate the proposed approach using Beijing air quality data, resulting in clear advantages over a series of state-of-the-art and commonly used methods.",2015,Knowledge Discovery and Data Mining,Fields of study: air quality indexoperations researchdata miningsimulation
ALOJA-ML: A Framework for Automating Characterization and Knowledge Discovery in Hadoop Deployments,Josep Lluís Berral (Birmingham–Southern College)Nicolas Poggi (Birmingham–Southern College)David Carrera (Birmingham–Southern College)Aaron Call (Birmingham–Southern College)Rob Reinauer (Microsoft)Daron Green (Microsoft),"2026048867,160053592,2167049718,2100972987,2035359515,2111202840","This article presents ALOJA-Machine Learning (ALOJA-ML) an extension to the ALOJA project that uses machine learning techniques to interpret Hadoop benchmark performance data and performance tuning; here we detail the approach, efficacy of the model and initial results. The ALOJA-ML project is the latest phase of a long-term collaboration between BSC and Microsoft, to automate the characterization of cost-effectiveness on Big Data deployments, focusing on Hadoop. Hadoop presents a complex execution environment, where costs and performance depends on a large number of software (SW) configurations and on multiple hardware (HW) deployment choices. Recently the ALOJA project presented an open, vendor-neutral repository, featuring over 16.000 Hadoop executions. These results are accompanied by a test bed and tools to deploy and evaluate the cost-effectiveness of the different hardware configurations, parameter tunings, and Cloud services. Despite early success within ALOJA from expert-guided benchmarking, it became clear that a genuinely comprehensive study requires automation of modeling procedures to allow a systematic analysis of large and resource-constrained search spaces. ALOJA-ML provides such an automated system allowing knowledge discovery by modeling Hadoop executions from observed benchmarks across a broad set of configuration parameters. The resulting empirically-derived performance models can be used to forecast execution behavior of various workloads; they allow a-priori prediction of the execution times for new configurations and HW choices and they offer a route to model-based anomaly detection. In addition, these models can guide the benchmarking exploration efficiently, by automatically prioritizing candidate future benchmark tests. Insights from ALOJA-ML's models can be used to reduce the operational time on clusters, speed-up the data acquisition and knowledge discovery process, and importantly, reduce running costs. In addition to learning from the methodology presented in this work, the community can benefit in general from ALOJA data-sets, framework, and derived insights to improve the design and deployment of Big Data applications.",2015,Knowledge Discovery and Data Mining,Fields of study: data miningdatabasereal time computingmachine learningcomputer science
Who Supported Obama in 2012?: Ecological Inference through Distribution Regression,Seth R. Flaxman (Carnegie Mellon University)Yu-Xiang Wang (Carnegie Mellon University)Alexander J. Smola (Carnegie Mellon University),"2050534514,2231647228,1972291593","We present a new solution to the ``ecological inference'' problem, of learning individual-level associations from aggregate data. This problem has a long history and has attracted much attention, debate, claims that it is unsolvable, and purported solutions. Unlike other ecological inference techniques, our method makes use of unlabeled individual-level data by embedding the distribution over these predictors into a vector in Hilbert space. Our approach relies on recent learning theory results for distribution regression, using kernel embeddings of distributions. Our novel approach to distribution regression exploits the connection between Gaussian process regression and kernel ridge regression, giving us a coherent, Bayesian approach to learning and inference and a convenient way to include prior information in the form of a spatial covariance function. Our approach is highly scalable as it relies on FastFood, a randomized explicit feature representation for kernel embeddings. We apply our approach to the challenging political science problem of modeling the voting behavior of demographic groups based on aggregate voting data. We consider the 2012 US Presidential election, and ask: what was the probability that members of various demographic groups supported Barack Obama, and how did this vary spatially across the country? Our results match standard survey-based exit polling data for the small number of states for which it is available, and serve to fill in the large gaps in this data, at a much higher degree of granularity.",2015,Knowledge Discovery and Data Mining,Fields of study: kernel embedding of distributionskernel methodgaussian processsupervised learningeconometricsdata miningmachine learningstatisticscomputer sciencemathematics
ClusType: Effective Entity Recognition and Typing by Relation Phrase-Based Clustering,Xiang Ren (University of Illinois at Urbana–Champaign)Ahmed El-Kishky (University of Illinois at Urbana–Champaign)Chi Wang (Microsoft)Fangbo Tao (University of Illinois at Urbana–Champaign)Clare R. Voss (United States Army Research Laboratory)Jiawei Han (University of Illinois at Urbana–Champaign),"2129405715,2231529517,2461963590,2130643217,2118725505,2121939561","Entity recognition is an important but challenging research problem. In reality, many text collections are from specific, dynamic, or emerging domains, which poses significant new challenges for entity recognition with increase in name ambiguity and context sparsity, requiring entity detection without domain restriction. In this paper, we investigate entity recognition (ER) with distant-supervision and propose a novel relation phrase-based ER framework, called ClusType, that runs data-driven phrase mining to generate entity mention candidates and relation phrases, and enforces the principle that relation phrases should be softly clustered when propagating type information between their argument entities. Then we predict the type of each entity mention based on the type signatures of its co-occurring relation phrases and the type indicators of its surface name, as computed over the corpus. Specifically, we formulate a joint optimization problem for two tasks, type propagation with relation phrases and multi-view relation phrase clustering. Our experiments on multiple genres---news, Yelp reviews and tweets---demonstrate the effectiveness and robustness of ClusType, with an average of 37% improvement in F1 score over the best compared method.",2015,Knowledge Discovery and Data Mining,Fields of study: weak entityentity linkingnatural language processingdata miningpattern recognitionmachine learningcomputer science
Predicting Winning Price in Real Time Bidding with Censored Data,Wush Chi-Hsuan Wu (National Taiwan University)Mi-Yen Yeh (Academia Sinica)Ming-Syan Chen (National Taiwan University),"2103160970,2120443347,2122365371","In the aspect of a Demand-Side Platform (DSP), which is the agent of advertisers, we study how to predict the winning price such that the DSP can win the bid by placing a proper bidding value in the real-time bidding (RTB) auction. We propose to leverage the machine learning and statistical methods to train the winning price model from the bidding history. A major challenge is that a DSP usually suffers from the censoring of the winning price, especially for those lost bids in the past. To solve it, we utilize the censored regression model, which is widely used in the survival analysis and econometrics, to fit the censored bidding data. Note, however, the assumption of censored regression does not hold on the real RTB data. As a result, we further propose a mixture model, which combines linear regression on bids with observable winning prices and censored regression on bids with the censored winning prices, weighted by the winning rate of the DSP. Experiment results show that the proposed mixture model in general prominently outperforms linear regression in terms of the prediction accuracy.",2015,Knowledge Discovery and Data Mining,Fields of study: real time biddingdata mining
Annotating Needles in the Haystack without Looking: Product Information Extraction from Emails,Weinan Zhang (University College London)Amr Ahmed (Google)Jie Yang (Google)Vanja Josifovski (Yahoo!)Alexander J. Smola (Carnegie Mellon University),"2527611484,2259645355,2304724589,344688379,1972291593","Business-to-consumer (B2C) emails are usually generated by filling structured user data (e.g.purchase, event) into templates. Extracting structured data from B2C emails allows users to track important information on various devices. However, it also poses several challenges, due to the requirement of short response time for massive data volume, the diversity and complexity of templates, and the privacy and legal constraints. Most notably, email data is legally protected content, which means no one except the receiver can review the messages or derived information. In this paper we first introduce a system which can extract structured information automatically without requiring human review of any personal content. Then we focus on how to annotate product names from the extracted texts, which is one of the most difficult problems in the system. Neither general learning methods, such as binary classifiers, nor more specific structure learning methods, suchas Conditional Random Field (CRF), can solve this problem well. To accomplish this task, we propose a hybrid approach, which basically trains a CRF model using the labels predicted by binary classifiers (weak learners). However, the performance of weak learners can be low, therefore we use Expectation Maximization (EM) algorithm on CRF to remove the noise and improve the accuracy, without the need to label and inspect specific emails. In our experiments, the EM-CRF model can significantly improve the product name annotations over the weak learners and plain CRFs.",2015,Knowledge Discovery and Data Mining,Fields of study: data scienceworld wide webdata miningmachine learningcomputer science
Generic and Scalable Framework for Automated Time-series Anomaly Detection,Nikolay Laptev (Yahoo!)Saeed Amizadeh (Yahoo!)Ian Flint (Yahoo!),"2094259916,953339772,2224210665","This paper introduces a generic and scalable framework for automated anomaly detection on large scale time-series data. Early detection of anomalies plays a key role in maintaining consistency of person's data and protects corporations against malicious attackers. Current state of the art anomaly detection approaches suffer from scalability, use-case restrictions, difficulty of use and a large number of false positives. Our system at Yahoo, EGADS, uses a collection of anomaly detection and forecasting models with an anomaly filtering layer for accurate and scalable anomaly detection on time-series. We compare our approach against other anomaly detection systems on real and synthetic data with varying time-series characteristics. We found that our framework allows for 50-60% improvement in precision and recall for a variety of use-cases. Both the data and the framework are being open-sourced. The open-sourcing of the data, in particular, represents the first of its kind effort to establish the standard benchmark for anomaly detection.",2015,Knowledge Discovery and Data Mining,Fields of study: time seriesanomaly detectioninternet privacycomputer securitydata miningmachine learningstatisticscomputer science
Organizational Chart Inference,Jiawei Zhang (University of Illinois at Chicago)Philip S. Yu (University of Illinois at Chicago)Yuanhua Lv (Microsoft),"2305185572,2125104194,2132538679","Nowadays, to facilitate the communication and cooperation among employees, a new family of online social networks has been adopted in many companies, which are called the ""enterprise social networks"" (ESNs). ESNs can provide employees with various professional services to help them deal with daily work issues. Meanwhile, employees in companies are usually organized into different hierarchies according to the relative ranks of their positions. The company internal management structure can be outlined with the organizational chart visually, which is normally confidential to the public out of the privacy and security concerns. In this paper, we want to study the IOC (Inference of Organizational Chart) problem to identify company internal organizational chart based on the heterogeneous online ESN launched in it. IOC is very challenging to address as, to guarantee smooth operations, the internal organizational charts of companies need to meet certain structural requirements (about its depth and width). To solve the IOC problem, a novel unsupervised method Create (ChArT REcovEr) is proposed in this paper, which consists of 3 steps: (1) social stratification of ESN users into different social classes, (2) supervision link inference from managers to subordinates, and (3) consecutive social classes matching to prune the redundant supervision links. Extensive experiments conducted on real-world online ESN dataset demonstrate that Create can perform very well in addressing the IOC problem.",2015,Knowledge Discovery and Data Mining,Fields of study: knowledge managementworld wide webdata miningmachine learningcomputer science
Optimal Action Extraction for Random Forests and Boosted Trees,Zhicheng Cui (Washington University in St. Louis)Wenlin Chen (Washington University in St. Louis)Yujie He (Washington University in St. Louis)Yixin Chen (Washington University in St. Louis),"2229038560,2231133452,2231071039,2295009377","Additive tree models (ATMs) are widely used for data mining and machine learning. Important examples of ATMs include random forest, adaboost (with decision trees as weak learners), and gradient boosted trees, and they are often referred to as the best off-the-shelf classifiers. Though capable of attaining high accuracy, ATMs are not well interpretable in the sense that they do not provide actionable knowledge for a given instance. This greatly limits the potential of ATMs on many applications such as medical prediction and business intelligence, where practitioners need suggestions on actions that can lead to desirable outcomes with minimum costs. To address this problem, we present a novel framework to post-process any ATM classifier to extract an optimal actionable plan that can change a given input to a desired class with a minimum cost. In particular, we prove the NP-hardness of the optimal action extraction problem for ATMs and formulate this problem in an integer linear programming formulation which can be efficiently solved by existing packages. We also empirically demonstrate the effectiveness of the proposed framework by conducting comprehensive experiments on challenging real-world datasets.",2015,Knowledge Discovery and Data Mining,Fields of study: random forestdecision treeinteger programmingdata miningpattern recognitionmachine learningcomputer sciencemathematics
Predicting Future Scientific Discoveries Based on a Networked Analysis of the Past Literature,Meenakshi Nagarajan (IBM)Angela D. Wilkins (Baylor College of Medicine)Benjamin J. Bachman (Baylor College of Medicine)Ilya B. Novikov (Baylor College of Medicine)Shenghua Bao (IBM)Peter J. Haas (IBM)María E. Terrón-Díaz (Baylor College of Medicine)Sumit Bhatia (IBM)Anbu K. Adikesavan (Baylor College of Medicine)Jacques J. Labrie (IBM)Sam Regenbogen (Baylor College of Medicine)Christie M. Buchovecky (Baylor College of Medicine)Curtis R. Pickering (University of Texas at Austin)Linda Kato (IBM)Andreas Martin Lisewski (Baylor College of Medicine)Ana Lelescu (IBM)Houyin Zhang (Baylor College of Medicine)Stephen Boyer (IBM)Griff Weber (IBM)Ying Chen (IBM)Lawrence A. Donehower (Baylor College of Medicine)W. Scott Spangler (IBM)Olivier Lichtarge (Baylor College of Medicine),"2122423346,1999041300,2164310169,2230780052,2135877978,2118460571,2224655266,2152564817,2090122539,2114099301,2041939495,1983915615,1967103034,2156620596,1968450065,70079055,2564164394,2121651308,2230435086,2527554124,2268387923,2106251501,256939357","We present KnIT, the Knowledge Integration Toolkit, a system for accelerating scientific discovery and predicting previously unknown protein-protein interactions. Such predictions enrich biological research and are pertinent to drug discovery and the understanding of disease. Unlike a prior study, KnIT is now fully automated and demonstrably scalable. It extracts information from the scientific literature, automatically identifying direct and indirect references to protein interactions, which is knowledge that can be represented in network form. It then reasons over this network with techniques such as matrix factorization and graph diffusion to predict new, previously unknown interactions. The accuracy and scope of KnIT's knowledge extractions are validated using comparisons to structured, manually curated data sources as well as by performing retrospective studies that predict subsequent literature discoveries using literature available prior to a given date. The KnIT methodology is a step towards automated hypothesis generation from text, with potential application to other scientific domains.",2015,Knowledge Discovery and Data Mining,Fields of study: text miningdata sciencebioinformaticsdata miningmachine learningcomputer science
Assembler: Efficient Discovery of Spatial Co-evolving Patterns in Massive Geo-sensory Data,Chao Zhang (University of Illinois at Urbana–Champaign)Yu Zheng (Microsoft)Xiuli Ma (Peking University)Jiawei Han (University of Illinois at Urbana–Champaign),"2618902948,2145115012,2105391371,2121939561","Recent years have witnessed the wide proliferation of geo-sensory applications wherein a bundle of sensors are deployed at different locations to cooperatively monitor the target condition. Given massive geo-sensory data, we study the problem of mining spatial co-evolving patterns (SCPs), i.e., groups of sensors that are spatially correlated and co-evolve frequently in their readings. SCP mining is of great importance to various real-world applications, yet it is challenging because (1) the truly interesting evolutions are often flooded by numerous trivial fluctuations in the geo-sensory time series; and (2) the pattern search space is extremely large due to the spatiotemporal combinatorial nature of SCP. In this paper, we propose a two-stage method called Assember. In the first stage, Assember filters trivial fluctuations using wavelet transform and detects frequent evolutions for individual sensors via a segment-and-group approach. In the second stage, Assember generates SCPs by assembling the frequent evolutions of individual sensors. Leveraging the spatial constraint, it conceptually organizes all the SCPs into a novel structure called the SCP search tree, which facilitates the effective pruning of the search space to generate SCPs efficiently. Our experiments on both real and synthetic data sets show that Assember is effective, efficient, and scalable.",2015,Knowledge Discovery and Data Mining,Fields of study: wireless sensor networktext miningdata sciencebioinformaticsdata miningmachine learningcomputer science
Rubik: Knowledge Guided Tensor Factorization and Completion for Health Data Analytics,Yichen Wang (Georgia Institute of Technology)Robert Chen (Georgia Institute of Technology)Joydeep Ghosh (University of Texas at Austin)Joshua C. Denny (Vanderbilt University)Abel N. Kho (Northwestern University)You Chen (Vanderbilt University)Bradley A. Malin (Vanderbilt University)Jimeng Sun (Georgia Institute of Technology),"2272469598,2113247120,2148168557,2143892069,2423943427,2142542144,2145238237,2110385854","Computational phenotyping is the process of converting heterogeneous electronic health records (EHRs) into meaningful clinical concepts. Unsupervised phenotyping methods have the potential to leverage a vast amount of labeled EHR data for phenotype discovery. However, existing unsupervised phenotyping methods do not incorporate current medical knowledge and cannot directly handle missing, or noisy data. We propose Rubik, a constrained non-negative tensor factorization and completion method for phenotyping. Rubik incorporates 1) guidance constraints to align with existing medical knowledge, and 2) pairwise constraints for obtaining distinct, non-overlapping phenotypes. Rubik also has built-in tensor completion that can significantly alleviate the impact of noisy and missing data. We utilize the Alternating Direction Method of Multipliers (ADMM) framework to tensor factorization and completion, which can be easily scaled through parallel computing. We evaluate Rubik on two EHR datasets, one of which contains 647,118 records for 7,744 patients from an outpatient clinic, the other of which is a public dataset containing 1,018,614 CMS claims records for 472,645 patients. Our results show that Rubik can discover more meaningful and distinct phenotypes than the baselines. In particular, by using knowledge guidance constraints, Rubik can also discover sub-phenotypes for several major diseases. Rubik also runs around seven times faster than current state-of-the-art tensor methods. Finally, Rubik is scalable to large datasets containing millions of EHR records.",2015,Knowledge Discovery and Data Mining,Fields of study: tensorconstrained optimizationdata sciencebioinformaticsdata miningmachine learningcomputer science
Incorporating World Knowledge to Document Clustering via Heterogeneous Information Networks,Chenguang Wang (Peking University)Yangqiu Song (University of Illinois at Urbana–Champaign)Ahmed El-Kishky (University of Illinois at Urbana–Champaign)Dan Roth (University of Illinois at Urbana–Champaign)Ming Zhang (Peking University)Jiawei Han (University of Illinois at Urbana–Champaign),"2293946001,2099747503,2231529517,2122007671,2714461950,2121939561","One of the key obstacles in making learning protocols realistic in applications is the need to supervise them, a costly process that often requires hiring domain experts. We consider the framework to use the world knowledge as indirect supervision. World knowledge is general-purpose knowledge, which is not designed for any specific domain. Then the key challenges are how to adapt the world knowledge to domains and how to represent it for learning. In this paper, we provide an example of using world knowledge for domain dependent document clustering. We provide three ways to specify the world knowledge to domains by resolving the ambiguity of the entities and their types, and represent the data with world knowledge as a heterogeneous information network. Then we propose a clustering algorithm that can cluster multiple types and incorporate the sub-type information as constraints. In the experiments, we use two existing knowledge bases as our sources of world knowledge. One is Freebase, which is collaboratively collected knowledge about entities and their organizations. The other is YAGO2, a knowledge base automatically extracted from Wikipedia and maps knowledge to the linguistic knowledge base, WordNet. Experimental results on two text benchmark datasets (20newsgroups and RCV1) show that incorporating world knowledge as indirect supervision can significantly outperform the state-of-the-art clustering algorithms as well as clustering algorithms enhanced with world knowledge features.",2015,Knowledge Discovery and Data Mining,Fields of study: commonsense knowledgeopen knowledge base connectivitybody of knowledgedomain knowledgemedical researchdocument clusteringknowledge extractionknowledge baseknowledge based systemstext miningdata scienceknowledge managementdata miningmachine learningcomputer science
Scalable Large Near-Clique Detection in Large-Scale Networks via Sampling,Michael Mitzenmacher (Harvard University)Jakub Pachocki (Carnegie Mellon University)Richard Peng (Massachusetts Institute of Technology)Charalampos E. Tsourakakis (Harvard University)Shen Chen Xu (Carnegie Mellon University),"1988080645,2047269919,2144828366,750472553,2112170937","Extracting dense subgraphs from large graphs is a key primitive in a variety of graph mining applications, ranging from mining social networks and the Web graph to bioinformatics [41]. In this paper we focus on a family of poly-time solvable formulations, known as the k-clique densest subgraph problem (k-Clique-DSP) [57]. When k =2, the problem becomes the well-known densest subgraph problem (DSP) [22, 31, 33, 39]. Our main contribution is a sampling scheme that gives densest subgraph sparsifier , yielding a randomized algorithm that produces high-quality approximations while providing significant speedups and improved space complexity. We also extend this family of formulations to bipartite graphs by introducing the ( p , q ) -biclique densest subgraph problem ((p,q)-Biclique-DSP), and devise an exact algorithm that can treat both clique and biclique densities in a unified way. As an example of performance, our sparsifying algorithm extracts the 5-clique densest subgraph --which is a large-near clique on 62 vertices-- from a large collaboration network. Our algorithm achieves 100% accuracy over five runs, while achieving an average speedup factor of over 10,000. Specifically, we reduce the running time from ∼2 107 seconds to an average running time of 0.15 seconds. We also use our methods to study how the k -clique densest subgraphs change as a function of time in time-evolving networks for various small values of k . We observe significant deviations between the experimental findings on real-world networks and stochastic Kronecker graphs, a random graph model that mimics real-world networks in certain aspects. We believe that our work is a significant advance in routines with rigorous theoretical guarantees for scalable extraction of large near-cliques from networks.",2015,Knowledge Discovery and Data Mining,Fields of study: induced subgraph isomorphism problemsplit graphsubgraph isomorphism problemcliquetheoretical computer sciencediscrete mathematicscombinatoricsmathematics
Community Detection based on Distance Dynamics,Junming Shao (University of Electronic Science and Technology of China)Zhichao Han (University of Electronic Science and Technology of China)Qinli Yang (University of Electronic Science and Technology of China)Tao Zhou (University of Electronic Science and Technology of China),"2109747452,2152072322,2152976369,2717739459","In this paper, we introduce a new community detection algorithm, called Attractor, which automatically spots communities in a network by examining the changes of ""distances"" among nodes (i.e. distance dynamics). The fundamental idea is to envision the target network as an adaptive dynamical system, where each node interacts with its neighbors. The interaction will change the distances among nodes, while the distances will affect the interactions. Such interplay eventually leads to a steady distribution of distances, where the nodes sharing the same community move together and the nodes in different communities keep far away from each other. Building upon the distance dynamics, Attractor has several remarkable advantages: (a) It provides an intuitive way to analyze the community structure of a network, and more importantly, faithfully captures the natural communities (with high quality). (b) Attractor allows detecting communities on large-scale networks due to its low time complexity ( O (| E |)). (c) Attractor is capable of discovering communities of arbitrary size, and thus small-size communities or anomalies, usually existing in real-world networks, can be well pinpointed. Extensive experiments show that our algorithm allows the effective and efficient community detection and has good performance compared to state-of-the-art algorithms.",2015,Knowledge Discovery and Data Mining,Fields of study: community structureartificial intelligencemachine learningsimulationcomputer sciencemathematics
0-Bit Consistent Weighted Sampling,Ping Li (Rutgers University),2721445647,"We develop 0-bit consistent weighted sampling (CWS) for efficiently estimating min-max kernel, which is a generalization of the resemblance kernel originally designed for binary data. Because the estimator of 0-bit CWS constitutes a positive definite kernel, this method can be naturally applied to large-scale data mining problems. Basically, if we feed the sampled data from 0-bit CWS to a highly efficient linear classifier (e.g., linear SVM), we effectively (and approximately) train a nonlinear classifier based on the min-max kernel. The accuracy improves as we increase the sample size. In this paper, we first demonstrate, through an extensive classification study using kernel machines, that the min-max kernel often provides an effective measure of similarity for nonnegative data. This helps justify the use of min-max kernel. However, as the min-max kernel is nonlinear and might be difficult to be used for industrial applications with massive data, we propose to linearize the min-max kernel via 0-bit CWS, a simplification of the original CWS method. The previous remarkable work on {\em consistent weighted sampling (CWS)} produces samples in the form of ( i * , t * ) where the i * records the location (and in fact also the weights) information analogous to the samples produced by classical minwise hashing on binary data. Because the t * is theoretically unbounded, it was not immediately clear how to effectively implement CWS for building large-scale linear classifiers. We provide a simple solution by discarding t * (which we refer to as the ""0-bit"" scheme). Via an extensive empirical study, we show that this 0-bit scheme does not lose essential information. We then apply 0-bit CWS for building linear classifiers to approximate min-max kernel classifiers, as extensively validated on a wide range of public datasets. We expect this work will generate interests among data mining practitioners who would like to efficiently utilize the nonlinear information of non-binary and nonnegative data.",2015,Knowledge Discovery and Data Mining,Fields of study: kernel embedding of distributionstree kernelvariable kernel density estimationstring kernelradial basis function kernelhash functionbig datadata miningpattern recognitionmachine learningstatisticscomputer sciencemathematics
CoupledLP: Link Prediction in Coupled Networks,Yuxiao Dong (University of Notre Dame)Jing Zhang (Tsinghua University)Jie Tang (Tsinghua University)Nitesh V. Chawla (University of Notre Dame)Bai Wang (Beijing University of Posts and Telecommunications),"2157080782,2635071740,2158012360,1979796846,2582794223","We study the problem of link prediction in coupled networks , where we have the structure information of one (source) network and the interactions between this network and another (target) network. The goal is to predict the missing links in the target network. The problem is extremely challenging as we do not have any information of the target network. Moreover, the source and target networks are usually heterogeneous and have different types of nodes and links. How to utilize the structure information in the source network for predicting links in the target network? How to leverage the heterogeneous interactions between the two networks for the prediction task? We propose a unified framework, CoupledLP, to solve the problem. Given two coupled networks, we first leverage atomic propagation rules to automatically construct implicit links in the target network for addressing the challenge of target network incompleteness, and then propose a coupled factor graph model to incorporate the meta-paths extracted from the coupled part of the two networks for transferring heterogeneous knowledge. We evaluate the proposed framework on two different genres of datasets: disease-gene (DG) and mobile social networks. In the DG networks, we aim to use the disease network to predict the associations between genes. In the mobile networks, we aim to use the mobile communication network of one mobile operator to infer the network structure of its competitors. On both datasets, the proposed CoupledLP framework outperforms several alternative methods. The proposed problem of coupled link prediction and the corresponding framework demonstrate both the scientific and business applications in biology and social networks.",2015,Knowledge Discovery and Data Mining,Fields of study: organizational network analysisinterdependent networksevolving networkshierarchical network modeldynamic network analysisnetwork sciencenetwork formationnetwork simulationsocial networkdata miningartificial intelligencemachine learningcomputer science
Personalizing LinkedIn Feed,Deepak Agarwal (LinkedIn)Bee-Chung Chen (LinkedIn)Qi He (LinkedIn)Zhenhao Hua (LinkedIn)Guy Lebanon (LinkedIn)Yiming Ma (LinkedIn)Pannagadatta Shivaswamy (LinkedIn)Hsiao-Ping Tseng (LinkedIn)Jaewon Yang (LinkedIn)Liang Zhang (LinkedIn),"2116605949,2152441490,2658691334,2250976429,2312000221,2237551533,2227780436,2224987450,2131921352,2306956169","LinkedIn dynamically delivers update activities from a user's interpersonal network to more than 300 million members in the personalized feed that ranks activities according their ""relevance"" to the user. This paper discloses the implementation details behind this personalized feed system at LinkedIn which can not be found from related work, and addresses the scalability and data sparsity challenges for deploying the system online. More specifically, we focus on the personalization models by generating three kinds of affinity scores: Viewer-ActivityType Affinity, Viewer-Actor Affinity, and Viewer-Actor-ActivityType Affinity. Extensive experiments based on online bucket tests (A/B experiments) and offline evaluation illustrate the effect of our personalization models in LinkedIn feed.",2015,Knowledge Discovery and Data Mining,Fields of study: personalizationmultimediaworld wide webdata miningmachine learning
Voltage Correlations in Smart Meter Data,Rajendu Mitra (IBM)Ramachandra Kota (IBM)Sambaran Bandyopadhyay (IBM)Vijay Arya (IBM)Brian SullivanRichard MuellerHeather StoreyGerard Labut,"2107091510,2118663564,2131923610,2030278002,2283141195,2566394886,2230416497,2226393475","The connectivity model of a power distribution network can easily become outdated due to system changes occurring in the field. Maintaining and sustaining an accurate connectivity model is a key challenge for distribution utilities worldwide. This work shows that voltage time series measurements collected from customer smart meters exhibit correlations that are consistent with the hierarchical structure of the distribution network. These correlations may be leveraged to cluster customers based on common ancestry and help verify and correct an existing connectivity model. Additionally, customers may be clustered in combination with voltage data from circuit metering points, spatial data from the geographical information system, and any existing but partially accurate connectivity model to infer customer to transformer and phase connectivity relationships with high accuracy. We report analysis and validation results based on data collected from multiple feeders of a large electric distribution network in North America. To the best of our knowledge, this is the first large scale measurement study of customer voltage data and its use in inferring network connectivity information.",2015,Knowledge Discovery and Data Mining,Fields of study: cluster analysisdata miningreal time computingmachine learningcomputer science
On the Reliability of Profile Matching Across Large Online Social Networks,Oana Goga (Max Planck Society)Patrick Loiseau (Institut Eurécom)Robin Sommer (Institute of Company Secretaries of India)Renata Teixeira (French Institute for Research in Computer Science and Automation)Krishna P. Gummadi (Max Planck Society),"2029936416,2469497386,2107180103,2122330853,1982116827","Matching the profiles of a user across multiple online social networks brings opportunities for new services and applications as well as new insights on user online behavior, yet it raises serious privacy concerns. Prior literature has showed that it is possible to accurately match profiles, but their evaluation focused only on sampled datasets. In this paper, we study the extent to which we can reliably match profiles in practice, across real-world social networks, by exploiting public attributes, i.e., information users publicly provide about themselves. Today's social networks have hundreds of millions of users, which brings completely new challenges as a reliable matching scheme must identify the correct matching profile out of the millions of possible profiles. We first define a set of properties for profile attributes--Availability, Consistency, non-Impersonability, and Discriminability (ACID)--that are both necessary and sufficient to determine the reliability of a matching scheme. Using these properties, we propose a method to evaluate the accuracy of matching schemes in real practical cases. Our results show that the accuracy in practice is significantly lower than the one reported in prior literature. When considering entire social networks, there is a non-negligible number of profiles that belong to different users but have similar attributes, which leads to many false matches. Our paper sheds light on the limits of matching profiles in the real world and illustrates the correct methodology to evaluate matching schemes in realistic scenarios.",2015,Knowledge Discovery and Data Mining,Fields of study: reliabilitydata scienceworld wide webdata miningmachine learningstatisticscomputer sciencemathematics
Visual Search at Pinterest,"Yushi JingDavid C. LiuDmitry KislyukAndrew ZhaiJiajing XuJeff Donahue (University of California, Berkeley)Sarah Tavel","2310031371,2432702379,2231274369,2698904025,2722283809,2133088636,2228931359","We demonstrate that, with the availability of distributed computation platforms such as Amazon Web Services and open-source tools, it is possible for a small engineering team to build, launch and maintain a cost-effective, large-scale visual search system. We also demonstrate, through a comprehensive set of live experiments at Pinterest, that content recommendation powered by visual search improves user engagement. By sharing our implementation details and learnings from launching a commercial visual search engine from scratch, we hope visual search becomes more widely incorporated into today's commercial applications.",2015,Knowledge Discovery and Data Mining,Fields of study: deep learningmultimediaworld wide webcomputer visionmachine learningsimulationcomputer science
Modeling User Mobility for Location Promotion in Location-based Social Networks,Wen Yuan Zhu (National Chiao Tung University)Wen Chih Peng (National Chiao Tung University)Ling Jyh Chen (Academia Sinica)Kai Zheng (University of Queensland)Xiaofang Zhou (University of Queensland),"2128979492,2138208885,2122836793,2150403146,2128990482","With the explosion of smartphones and social network services, location-based social networks (LBSNs) are increasingly seen as tools for businesses (e.g., restaurants, hotels) to promote their products and services. In this paper, we investigate the key techniques that can help businesses promote their locations by advertising wisely through the underlying LBSNs. In order to maximize the benefit of location promotion, we formalize it as an influence maximization problem in an LBSN, i.e., given a target location and an LBSN, which a set of k users (called seeds) should be advertised initially such that they can successfully propagate and attract most other users to visit the target location. Existing studies have proposed different ways to calculate the information propagation probability, that is how likely a user may influence another, in the settings of static social network. However, it is more challenging to derive the propagation probability in an LBSN since it is heavily affected by the target location and the user mobility, both of which are dynamic and query dependent. This paper proposes two user mobility models, namely Gaussian-based and distance-based mobility models, to capture the check-in behavior of individual LBSN user, based on which location-aware propagation probabilities can be derived respectively. Extensive experiments based on two real LBSN datasets have demonstrated the superior effectiveness of our proposals than existing static models of propagation probabilities to truly reflect the information propagation in LBSNs.",2015,Knowledge Discovery and Data Mining,Fields of study: data miningsimulation
A Collective Bayesian Poisson Factorization Model for Cold-start Local Event Recommendation,Wei Zhang (Tsinghua University)Jianyong Wang (Tsinghua University),"2723820743,2105625159","Event-based social networks (EBSNs), in which organizers publish events to attract other users in local city to attend offline, emerge in recent years and grow rapidly. Due to the large volume of events in EBSNs, event recommendation is essential. A few recent works focus on this task, while almost all the methods need that each event to be recommended should have been registered by some users to attend. Thus they ignore two essential characteristics of events in EBSNs: (1) a large number of new events will be published every day which means many events have few participants in the beginning, (2) events have life cycles which means outdated events should not be recommended. Overall, event recommendation in EBSNs inevitably faces the cold-start problem. In this work, we address the new problem of cold-start local event recommendation in EBSNs. We propose a collective Bayesian Poisson factorization (CBPF) model for handling this problem. CBPF takes recently proposed Bayesian Poisson factorization as its basic unit to model user response to events, social relation, and content text separately. Then it further jointly connects these units by the idea of standard collective matrix factorization model. Moreover, in our model event textual content, organizer, and location information are utilized to learn representation of cold-start events for predicting user response to them. Besides, an efficient coordinate ascent algorithm is adopted to learn the model. We conducted comprehensive experiments on real datasets crawled from EBSNs and the results demonstrate our proposed model is effective and outperforms several alternative methods.",2015,Knowledge Discovery and Data Mining,Fields of study: data sciencedata miningmachine learningsimulationstatisticscomputer science
Debiasing Crowdsourced Batches,Honglei Zhuang (University of Illinois at Urbana–Champaign)Aditya G. Parameswaran (University of Illinois at Urbana–Champaign)Dan Roth (University of Illinois at Urbana–Champaign)Jiawei Han (University of Illinois at Urbana–Champaign),"2128637305,2077695977,2122007671,2121939561","Crowdsourcing is the de-facto standard for gathering annotated data. While, in theory, data annotation tasks are assumed to be attempted by workers independently, in practice, data annotation tasks are often grouped into batches to be presented and annotated by workers together, in order to save on the time or cost overhead of providing instructions or necessary background. Thus, even though independence is usually assumed between annotations on data items within the same batch, in most cases, a worker's judgment on a data item can still be affected by other data items within the batch, leading to additional errors in collected labels. In this paper, we study the data annotation bias when data items are presented as batches to be judged by workers simultaneously. We propose a novel worker model to characterize the annotating behavior on data batches, and present how to train the worker model on annotation data sets. We also present a debiasing technique to remove the effect of such annotation bias from adversely affecting the accuracy of labels obtained. Our experimental results on synthetic and real-world data sets demonstrate that our proposed method can achieve up to +57% improvement in F 1-score compared to the standard majority voting baseline.",2015,Knowledge Discovery and Data Mining,Fields of study: crowdsourcingtext miningdata scienceinformation retrievaldata miningcomputer science
Client Clustering for Hiring Modeling in Work Marketplaces,Vasilis Verroios (Stanford University)Panagiotis Papadimitriou (Stanford University)Ramesh Johari (Stanford University)Hector Garcia-Molina (Stanford University),"53618934,2027183093,2098118527,237419955","An important problem that online work marketplaces face is grouping clients into clusters, so that in each cluster clients are similar with respect to their hiring criteria. Such a separation allows the marketplace to ""learn"" more accurately the hiring criteria in each cluster and recommend the right contractor to each client, for a successful collaboration. We propose a Maximum Likelihood definition of the ""optimal"" client clustering along with an efficient Expectation-Maximization clustering algorithm that can be applied in large marketplaces. Our results on the job hirings at oDesk over a seven-month period show that our client-clustering approach yields significant gains compared to ""learning"" the same hiring criteria for all clients. In addition, we analyze the clustering results to find interesting differences between the hiring criteria in the different groups of clients.",2015,Knowledge Discovery and Data Mining,Fields of study: crowdsourcingoutsourcingdata miningcomputer science
Deep Graph Kernels,"Pinar Yanardag (Purdue University)S.V.N. Vishwanathan (University of California, Santa Cruz)","209877522,2584127737","In this paper, we present Deep Graph Kernels, a unified framework to learn latent representations of sub-structures for graphs, inspired by latest advancements in language modeling and deep learning. Our framework leverages the dependency information between sub-structures by learning their latent representations. We demonstrate instances of our framework on three popular graph kernels, namely Graphlet kernels, Weisfeiler-Lehman subtree kernels, and Shortest-Path graph kernels. Our experiments on several benchmark datasets show that Deep Graph Kernels achieve significant improvements in classification accuracy over state-of-the-art graph kernels.",2015,Knowledge Discovery and Data Mining,Fields of study: graphdata modelsocial networkdeep learningtheoretical computer sciencepattern recognitionmachine learningcomputer science
Online Influence Maximization,Siyu Lei (University of Hong Kong)Silviu Maniu (Huawei)Luyi Mo (University of Hong Kong)Reynold Cheng (University of Hong Kong)Pierre Senellart (Télécom ParisTech),"2105451361,1545145631,2099125123,2138267588,2039937633","Social networks are commonly used for marketing purposes. For example, free samples of a product can be given to a few influential social network users (or seed nodes), with the hope that they will convince their friends to buy it. One way to formalize this objective is through the problem of influence maximization (or IM), whose goal is to find the best seed nodes to activate under a fixed budget, so that the number of people who get influenced in the end is maximized. Solutions to IM rely on the influence probability that a user influences another one. However, this probability information may be unavailable or incomplete. In this paper, we study IM in the absence of complete information on influence probability. We call this problem Online Influence Maximization (OIM), since we learn influence probabilities at the same time we run influence campaigns. To solve OIM, we propose a multiple-trial approach, where (1) some seed nodes are selected based on existing influence information; (2) an influence campaign is started with these seed nodes; and (3) user feedback is used to update influence information. We adopt Explore-Exploit strategies, which can select seed nodes using either the current influence probability estimation (exploit), or the confidence bound on the estimation (explore). Any existing IM algorithm can be used in this framework. We also develop an incremental algorithm that can significantly reduce the overhead of handling user feedback information. Our experiments show that our solution is more effective than traditional IM methods on the partial information.",2015,Knowledge Discovery and Data Mining,Fields of study: reinforcement learningdata miningartificial intelligencemachine learningsimulationstatisticscomputer science
Collective Opinion Spam Detection: Bridging Review Networks and Metadata,Shebuti Rayana (Stony Brook University)Leman Akoglu (Stony Brook University),"1722036351,2288278917","Online reviews capture the testimonials of ""real"" people and help shape the decisions of other consumers. Due to the financial gains associated with positive reviews, however, opinion spam has become a widespread problem, with often paid spam reviewers writing fake reviews to unjustly promote or demote certain products or businesses. Existing approaches to opinion spam have successfully but separately utilized linguistic clues of deception, behavioral footprints, or relational ties between agents in a review system. In this work, we propose a new holistic approach called SPEAGLE that utilizes clues from all metadata (text, timestamp, rating) as well as relational data (network), and harness them collectively under a unified framework to spot suspicious users and reviews, as well as products targeted by spam. Moreover, our method can efficiently and seamlessly integrate semi-supervision, i.e., a (small) set of labels if available, without requiring any training or changes in its underlying algorithm. We demonstrate the effectiveness and scalability of SPEAGLE on three real-world review datasets from Yelp.com with filtered (spam) and recommended (non-spam) reviews, where it significantly outperforms several baselines and state-of-the-art methods. To the best of our knowledge, this is the largest scale quantitative evaluation performed to date for the opinion spam problem.",2015,Knowledge Discovery and Data Mining,Fields of study: forum spamspambotmetadatasemi supervised learninginternet privacyworld wide webdata miningmachine learningcomputer science
TimeCrunch: Interpretable Dynamic Graph Summarization,Neil Shah (Carnegie Mellon University)Danai Koutra (Carnegie Mellon University)Tianmin Zou (Carnegie Mellon University)Brian Gallagher (Lawrence Livermore National Laboratory)Christos Faloutsos (Carnegie Mellon University),"2397954333,1524801041,2228015889,2188853992,2198983026","How can we describe a large, dynamic graph over time? Is it random? If not, what are the most apparent deviations from randomness -- a dense block of actors that persists over time, or perhaps a star with many satellite nodes that appears with some fixed periodicity? In practice, these deviations indicate patterns -- for example, botnet attackers forming a bipartite core with their victims over the duration of an attack, family members bonding in a clique-like fashion over a difficult period of time, or research collaborations forming and fading away over the years. Which patterns exist in real-world dynamic graphs, and how can we find and rank them in terms of importance? These are exactly the problems we focus on in this work. Our main contributions are (a) formulation : we show how to formalize this problem as minimizing the encoding cost in a data compression paradigm, (b) algorithm : we propose TIMECRUNCH, an effective, scalable and parameter-free method for finding coherent, temporal patterns in dynamic graphs and (c) practicality : we apply our method to several large, diverse real-world datasets with up to 36 million edges and 6.3 million nodes. We show that TIMECRUNCH is able to compress these graphs by summarizing important temporal structures and finds patterns that agree with intuition.",2015,Knowledge Discovery and Data Mining,Fields of study: compressioncluster analysisautomatic summarizationtheoretical computer sciencedata miningmachine learningstatisticscomputer sciencemathematics
Influence at Scale: Distributed Computation of Complex Contagion in Networks,Brendan Lucier (Microsoft)Joel Oren (University of Toronto)Yaron Singer (Harvard University),"1906554206,2144239422,2155009540",We consider the task of evaluating the spread of influence in large networks in the well-studied independent cascade model. We describe a novel sampling approach that can be used to design scalable algorithms with provable performance guarantees. These algorithms can be implemented in distributed computation frameworks such as MapReduce. We complement these results with a lower bound on the query complexity of influence estimation in this model. We validate the performance of these algorithms through experiments that demonstrate the efficacy of our methods and related heuristics.,2015,Knowledge Discovery and Data Mining,Fields of study: submodular set functionsocial networktheoretical computer sciencedistributed computingmachine learningcomputer science
Accelerating Dynamic Time Warping Clustering with a Novel Admissible Pruning Strategy,"Nurjahan Begum (University of California, Riverside)Liudmila Ulanova (University of California, Riverside)Jun Wang (University of Texas at Dallas)Eamonn J. Keogh (University of California, Riverside)","2157373250,2089139422,2608497112,2170070822","Clustering time series is a useful operation in its own right, and an important subroutine in many higher-level data mining analyses, including data editing for classifiers, summarization, and outlier detection. While it has been noted that the general superiority of Dynamic Time Warping (DTW) over Euclidean Distance for similarity search diminishes as we consider ever larger datasets, as we shall show, the same is not true for clustering . Thus, clustering time series under DTW remains a computationally challenging task. In this work, we address this lethargy in two ways. We propose a novel pruning strategy that exploits both upper and lower bounds to prune off a large fraction of the expensive distance calculations. This pruning strategy is admissible; giving us provably identical results to the brute force algorithm, but is at least an order of magnitude faster. For datasets where even this level of speedup is inadequate, we show that we can use a simple heuristic to order the unavoidable calculations in a most-useful-first ordering, thus casting the clustering as an anytime algorithm. We demonstrate the utility of our ideas with both single and multidimensional case studies in the domains of astronomy, speech physiology, medicine and entomology.",2015,Knowledge Discovery and Data Mining,Fields of study: canopy clustering algorithmcorrelation clusteringconstrained clusteringdata stream clusteringcure data clustering algorithmfuzzy clusteringclustering high dimensional datacluster analysistime seriesdata miningmachine learningstatisticsalgorithmcomputer science
TOPTRAC: Topical Trajectory Pattern Mining,Younghoon Kim (Hanyang University)Jiawei Han (University of Illinois at Urbana–Champaign)Cangzhou Yuan (Beihang University),"2609526431,2121939561,2682805244","With the increasing use of GPS-enabled mobile phones, geo-tagging, which refers to adding GPS information to media such as micro-blogging messages or photos, has seen a surge in popularity recently. This enables us to not only browse information based on locations, but also discover patterns in the location-based behaviors of users. Many techniques have been developed to find the patterns of people's movements using GPS data, but latent topics in text messages posted with local contexts have not been utilized effectively. In this paper, we present a latent topic-based clustering algorithm to discover patterns in the trajectories of geo-tagged text messages. We propose a novel probabilistic model to capture the semantic regions where people post messages with a coherent topic as well as the patterns of movement between the semantic regions. Based on the model, we develop an efficient inference algorithm to calculate model parameters. By exploiting the estimated model, we next devise a clustering algorithm to find the significant movement patterns that appear frequently in data. Our experiments on real-life data sets show that the proposed algorithm finds diverse and interesting trajectory patterns and identifies the semantic regions in a finer granularity than the traditional geographical clustering methods.",2015,Knowledge Discovery and Data Mining,Fields of study: medical researchtext miningdata scienceinformation retrievaldata miningmachine learningcomputer science
Towards Interactive Construction of Topical Hierarchy: A Recursive Tensor Decomposition Approach,Chi Wang (Microsoft)Xueqing Liu (University of Illinois at Urbana–Champaign)Yanglei Song (University of Illinois at Urbana–Champaign)Jiawei Han (University of Illinois at Urbana–Champaign),"2461963590,2657831526,2155982158,2121939561","Automatic construction of user-desired topical hierarchies over large volumes of text data is a highly desirable but challenging task. This study proposes to give users freedom to construct topical hierarchies via interactive operations such as expanding a branch and merging several branches. Existing hierarchical topic modeling techniques are inadequate for this purpose because (1) they cannot consistently preserve the topics when the hierarchy structure is modified; and (2) the slow inference prevents swift response to user requests. In this study, we propose a novel method, called STROD, that allows efficient and consistent modification of topic hierarchies, based on a recursive generative model and a scalable tensor decomposition inference algorithm with theoretical performance guarantee. Empirical evaluation shows that STROD reduces the runtime of construction by several orders of magnitude, while generating consistent and quality hierarchies.",2015,Knowledge Discovery and Data Mining,Fields of study: topic modeltheoretical computer sciencedata miningmachine learningcomputer sciencemathematics
Diversifying Restricted Boltzmann Machine for Document Modeling,Pengtao Xie (Carnegie Mellon University)Yuntian Deng (Carnegie Mellon University)Eric P. Xing (Carnegie Mellon University),"2157788543,2123267846,351197510","Restricted Boltzmann Machine (RBM) has shown great effectiveness in document modeling. It utilizes hidden units to discover the latent topics and can learn compact semantic representations for documents which greatly facilitate document retrieval, clustering and classification. The popularity (or frequency) of topics in text corpora usually follow a power-law distribution where a few dominant topics occur very frequently while most topics (in the long-tail region) have low probabilities. Due to this imbalance, RBM tends to learn multiple redundant hidden units to best represent dominant topics and ignore those in the long-tail region, which renders the learned representations to be redundant and non-informative. To solve this problem, we propose Diversified RBM (DRBM) which diversifies the hidden units, to make them cover not only the dominant topics, but also those in the long-tail region. We define a diversity metric and use it as a regularizer to encourage the hidden units to be diverse. Since the diversity metric is hard to optimize directly, we instead optimize its lower bound and prove that maximizing the lower bound with projected gradient ascent can increase this diversity metric. Experiments on document retrieval and clustering demonstrate that with diversification, the document modeling power of DRBM can be greatly improved.",2015,Knowledge Discovery and Data Mining,Fields of study: topic modelpareto distributiondata miningartificial intelligencemachine learningstatisticscomputer sciencemathematics
SCRAM: A Sharing Considered Route Assignment Mechanism for Fair Taxi Route Recommendations,Shiyou Qian (Shanghai Jiao Tong University)Jian Cao (Shanghai Jiao Tong University)Frédéric Le Mouël (University of Lyon)Issam Sahel (University of Lyon)Minglu Li (Shanghai Jiao Tong University),"2153059418,2662156794,61616480,2228436515,2146282702","Recommending routes for a group of competing taxi drivers is almost untouched in most route recommender systems. For this kind of problem, recommendation fairness and driving efficiency are two fundamental aspects. In the paper, we propose SCRAM, a sharing considered route assignment mechanism for fair taxi route recommendations. SCRAM aims to provide recommendation fairness for a group of competing taxi drivers, without sacrificing driving efficiency. By designing a concise route assignment mechanism, SCRAM achieves better recommendation fairness for competing taxis. By considering the sharing of road sections to avoid unnecessary competition, SCRAM is more efficient in terms of driving cost per customer (DCC). We test SCRAM based on a large number of historical taxi trajectories and validate the recommendation fairness and driving efficiency of SCRAM with extensive evaluations. Experimental results show that SCRAM achieves better recommendation fairness and higher driving efficiency than three compared approaches.",2015,Knowledge Discovery and Data Mining,Fields of study: taxisrecommender systemcomputer securitymachine learningcomputer science
Probabilistic Modeling of a Sales Funnel to Prioritize Leads,"Brendan Andrew Duncan (University of California, San Diego)Charles Peter Elkan (University of California, San Diego)","2226537134,705676171","This paper shows how to learn probabilistic classifiers that model how sales prospects proceed through stages from first awareness to final success or failure. Specifically,we present two models, called DQM for direct qualification model and FFM for full funnel model, that can be used to rank initial leads based on their probability of conversion to a sales opportunity, probability of successful sale, and/or expected revenue. Training uses the large amount of historical data collected by customer relationship management or marketing automation software. The trained models can replace traditional lead scoring systems, which are hand-tuned and therefore error-prone and not probabilistic. DQM and FFM are designed to overcome the selection bias caused by available data being based on a traditional lead scoring system. Experimental results are shown on real sales data from two companies. Features in the training data include demographic and behavioral information about each lead. For both companies, both methods achieve high AUC scores. For one company, they result in a a 307% increase in number of successful sales, as well as a dramatic increase in total revenue. In addition, we describe the results of the DQM method in actual use. These results show that the method has additional benefits that include decreased time needed to qualify leads, and decreased number of calls placed to schedule a product demo. The proposed methods find high-quality leads earlier in the sales process because they focus on features that measure the fit of potential customers with the product being sold, in addition to their behavior.",2015,Knowledge Discovery and Data Mining,Fields of study: decision treedata miningmachine learningstatisticscomputer science
Matrix Completion with Queries,Natali Ruchansky (Boston University)Mark Crovella (Boston University)Evimaria Terzi (Boston University),"86489919,91397146,2110675235","In many applications, e.g., recommender systems and traffic monitoring, the data comes in the form of a matrix that is only partially observed and low rank. A fundamental data-analysis task for these datasets is matrix completion , where the goal is to accurately infer the entries missing from the matrix. Even when the data satisfies the low-rank assumption, classical matrix-completion methods may output completions with significant error -- in that the reconstructed matrix differs significantly from the true underlying matrix. Often, this is due to the fact that the information contained in the observed entries is insufficient. In this work, we address this problem by proposing an active version of matrix completion, where queries can be made to the true underlying matrix. Subsequently, we design Order&Extend , which is the first algorithm to unify a matrix-completion approach and a querying strategy into a single algorithm. Order&Extend is able identify and alleviate insufficient information by judiciously querying a small number of additional entries. In an extensive experimental evaluation on real-world datasets, we demonstrate that our algorithm is efficient and is able to accurately reconstruct the true matrix while asking only a small number of queries.",2015,Knowledge Discovery and Data Mining,Fields of study: document term matrixrecommender systemtheoretical computer sciencedata miningdatabase +3 others
Automatic Entity Recognition and Typing from Massive Text Corpora: A Phrase and Network Mining Approach,Xiang Ren (University of Illinois at Urbana–Champaign)Ahmed El-Kishky (University of Illinois at Urbana–Champaign)Chi Wang (Microsoft)Jiawei Han (University of Illinois at Urbana–Champaign),"2129405715,2231529517,2461963590,2121939561","In today's computerized and information-based society, we are soaked with vast amounts of text data, ranging from news articles, scientific publications, product reviews, to a wide range of textual information from social media. To unlock the value of these unstructured text data from various domains, it is of great importance to gain an understanding of entities and their relationships. In this tutorial, we introduce data-driven methods to recognize typed entities of interest in massive, domain-specific text corpora. These methods can automatically identify token spans as entity mentions in documents and label their types (e.g., people, product, food) in a scalable way. We demonstrate on real datasets including news articles and tweets how these typed entities aid in knowledge discovery and management.",2015,Knowledge Discovery and Data Mining,Fields of study: cluster analysistext miningdata scienceinformation retrievaldata mining +2 others
From Infrastructure to Culture: A/B Testing Challenges in Large Scale Social Networks,Ya Xu (LinkedIn)Nanyu Chen (LinkedIn)Addrian Fernandez (LinkedIn)Omar Sinno (LinkedIn)Anmol Bhasin (LinkedIn),"2310280520,2170586657,2297662169,2224087825,2157708525","A/B testing, also known as bucket testing, split testing, or controlled experiment, is a standard way to evaluate user engagement or satisfaction from a new service, feature, or product. It is widely used among online websites, including social network sites such as Facebook, LinkedIn, and Twitter to make data-driven decisions. At LinkedIn, we have seen tremendous growth of controlled experiments over time, with now over 400 concurrent experiments running per day. General A/B testing frameworks and methodologies, including challenges and pitfalls, have been discussed extensively in several previous KDD work [7, 8, 9, 10]. In this paper, we describe in depth the experimentation platform we have built at LinkedIn and the challenges that arise particularly when running A/B tests at large scale in a social network setting. We start with an introduction of the experimentation platform and how it is built to handle each step of the A/B testing process at LinkedIn, from designing and deploying experiments to analyzing them. It is then followed by discussions on several more sophisticated A/B testing scenarios, such as running offline experiments and addressing the network effect, where one user's action can influence that of another. Lastly, we talk about features and processes that are crucial for building a strong experimentation culture.",2015,Knowledge Discovery and Data Mining,Fields of study: social networkmeasurementworld wide webdata miningsimulation
Early Prediction of Cardiac Arrest (Code Blue) using Electronic Medical Records,Sriram Somanchi (Carnegie Mellon University)Samrachana Adhikari (Carnegie Mellon University)Allen Lin (Harvard University)Elena Eneva (Accenture)Rayid Ghani (University of Chicago),"1572451201,2669172339,2226339777,2307980829,2655769349","Code Blue is an emergency code used in hospitals to indicate when a patient goes into cardiac arrest and needs resuscitation. When Code Blue is called, an on-call medical team staffed by physicians and nurses is paged and rushes in to try to save the patient's life. It is an intense, chaotic, and resource-intensive process, and despite the considerable effort, survival rates are still less than 20% [4]. Research indicates that patients actually start showing clinical signs of deterioration some time before going into cardiac arrest [1][2[][3], making early prediction, and possibly intervention, feasible. In this paper, we describe our work, in partnership with NorthShore University HealthSystem, that preemptively flags patients who are likely to go into cardiac arrest, using signals extracted from demographic information, hospitalization history, vitals and laboratory measurements in patient-level electronic medical records. We find that early prediction of Code Blue is possible and when compared with state of the art existing method used by hospitals (MEWS - Modified Early Warning Score)[4], our methods perform significantly better. Based on these results, this system is now being considered for deployment in hospital settings.",2015,Knowledge Discovery and Data Mining,Fields of study: support vector machinemachine learningcomputer science
Reciprocity in Social Networks with Capacity Constraints,Bo Jiang (University of Massachusetts Amherst)Zhi Li Zhang (University of Minnesota)Don Towsley (University of Massachusetts Amherst),"2307088945,2164420873,2177075905","Directed links -- representing asymmetric social ties or interactions (e.g., ""follower-followee"") -- arise naturally in many social networks and other complex networks, giving rise to directed graphs (or digraphs) as basic topological models for these networks. Reciprocity, defined for a digraph as the percentage of edges with a reciprocal edge, is a key metric that has been used in the literature to compare different directed networks and provide ""hints"" about their structural properties: for example, are reciprocal edges generated randomly by chance or are there other processes driving their generation? In this paper we study the problem of maximizing achievable reciprocity for an ensemble of digraphs with the same prescribed in- and out-degree sequences. We show that the maximum reciprocity hinges crucially on the in- and out-degree sequences, which may be intuitively interpreted as constraints on some ""social capacities"" of nodes and impose fundamental limits on achievable reciprocity. We show that it is NP-complete to decide the achievability of a simple upper bound on maximum reciprocity, and provide conditions for achieving it. We demonstrate that many real networks exhibit reciprocities surprisingly close to the upper bound, which implies that users in these social networks are in a sense more ""social"" than suggested by the empirical reciprocity alone in that they are more willing to reciprocate, subject to their ""social capacity"" constraints. We find some surprising linear relationships between empirical reciprocity and the bound. We also show that a particular type of small network motifs that we call 3-paths are the major source of loss in reciprocity for real networks.",2015,Knowledge Discovery and Data Mining,Fields of study: reciprocitydirected graphsocial networkcombinatoricssocial psychology +2 others
MASCOT: Memory-efficient and Accurate Sampling for Counting Local Triangles in Graph Streams,Yongsub Lim (KAIST)U Kang (KAIST),"2118432040,2426051778","How can we estimate local triangle counts accurately in a graph stream without storing the whole graph? The local triangle counting which counts triangles for each node in a graph is a very important problem with wide applications in social network analysis, anomaly detection, web mining, etc. In this paper, we propose MASCOT, a memory-efficient and accurate method for local triangle estimation in a graph stream based on edge sampling. To develop MASCOT, we first present two naive local triangle counting algorithms in a graph stream: MASCOT-C and MASCOT-A. MASCOT-C is based on constant edge sampling, and MASCOT-A improves its accuracy by utilizing more memory spaces. MASCOT achieves both accuracy and memory-efficiency of the two algorithms by an unconditional triangle counting for a new edge, regardless of whether it is sampled or not. In contrast to the existing algorithm which requires prior knowledge on the target graph and appropriately set parameters, MASCOT requires only one simple parameter, the edge sampling probability. Through extensive experiments, we show that for the same number of edges sampled, MASCOT provides the best accuracy compared to the existing algorithm as well as MASCOT-C and MASCOT-A. Thanks to MASCOT, we also discover interesting anomalous patterns in real graphs, like core-peripheries in the web and ambiguous author names in DBLP.",2015,Knowledge Discovery and Data Mining,Fields of study: anomaly detectioncombinatoricsdata miningmachine learningcomputer science +1 other
User Conditional Hashtag Prediction for Images,Emily Denton (New York University)Jason Weston (Facebook)Manohar Paluri (Facebook)Lubomir D. Bourdev (Facebook)Rob Fergus (Facebook),"2024567059,2494296606,975267881,2475820207,2667537643","Understanding the content of user's image posts is a particularly interesting problem in social networks and web settings. Current machine learning techniques focus mostly on curated training sets of image-label pairs, and perform image classification given the pixels within the image. In this work we instead leverage the wealth of information available from users: firstly, we employ user hashtags to capture the description of image content; and secondly, we make use of valuable contextual information about the user. We show how user metadata (age, gender, etc.) combined with image features derived from a convolutional neural network can be used to perform hashtag prediction. We explore two ways of combining these heterogeneous features into a learning framework: (i) simple concatenation; and (ii) a 3-way multiplicative gating, where the image model is conditioned on the user metadata. We apply these models to a large dataset of de-identified Facebook posts and demonstrate that modeling the user can significantly improve the tag prediction quality over current state-of-the-art methods.",2015,Knowledge Discovery and Data Mining,Fields of study: social mediauser modelingdeep learningautomatic image annotationworld wide webinformation retrievaldata miningmachine learningcomputer science
Real Estate Ranking via Mixed Land-use Latent Models,Yanjie Fu (Rutgers–Newark)Guannan Liu (Tsinghua University)Spiros Papadimitriou (Rutgers–Newark)Hui Xiong (Rutgers–Newark)Yong Ge (University of North Carolina at Charlotte)Hengshu Zhu (Baidu)Chen Zhu (University of Science and Technology of China),"2168873515,2273869953,2650391232,2153710278,2218492437,2098414524,2229244414","Mixed land use refers to the effort of putting residential, commercial and recreational uses in close proximity to one another. This can contribute economic benefits, support viable public transit, and enhance the perceived security of an area. It is naturally promising to investigate how to rank real estate from the viewpoint of diverse mixed land use, which can be reflected by the portfolio of community functions in the observed area. To that end, in this paper, we develop a geographical function ranking method, named FuncDivRank, by incorporating the functional diversity of communities into real estate appraisal. Specifically, we first design a geographic function learning model to jointly capture the correlations among estate neighborhoods, urban functions, temporal effects, and user mobility patterns. In this way we can learn latent community functions and the corresponding portfolios of estates from human mobility data and Point of Interest (POI) data. Then, we learn the estate ranking indicator by simultaneously maximizing ranking consistency and functional diversity, in a unified probabilistic optimization framework. Finally, we conduct a comprehensive evaluation with real-world data. The experimental results demonstrate the enhanced performance of the proposed method for real estate appraisal.",2015,Knowledge Discovery and Data Mining,Fields of study: cost approachurban geographydata miningsimulation
A Decision Tree Framework for Spatiotemporal Sequence Prediction,Taehwan Kim (Toyota Technological Institute at Chicago)Yisong Yue (California Institute of Technology)Sarah L. Taylor (Disney Research)Iain A. Matthews (Disney Research),"2682673352,2693634657,2589602401,2227920884","We study the problem of learning to predict a spatiotemporal output sequence given an input sequence. In contrast to conventional sequence prediction problems such as part-of-speech tagging (where output sequences are selected using a relatively small set of discrete labels), our goal is to predict sequences that lie within a high-dimensional continuous output space. We present a decision tree framework for learning an accurate non-parametric spatiotemporal sequence predictor. Our approach enjoys several attractive properties, including ease of training, fast performance at test time, and the ability to robustly tolerate corrupted training data using a novel latent variable approach. We evaluate on several datasets, and demonstrate substantial improvements over existing decision tree based sequence learning frameworks such as SEARN and DAgger.",2015,Knowledge Discovery and Data Mining,Fields of study: incremental decision treedecision treedata miningpattern recognitionmachine learningcomputer science
A Deep Hybrid Model for Weather Forecasting,Aditya Grover (Indian Institute of Technology Delhi)Ashish Kapoor (Microsoft)Eric Horvitz (Microsoft),"2008277149,2161328394,1970391018",Weather forecasting is a canonical predictive challenge that has depended primarily on model-based methods. We explore new directions with forecasting weather as a data-intensive challenge that involves inferences across space and time. We study specifically the power of making predictions via a hybrid approach that combines discriminatively trained predictive models with a deep neural network that models the joint statistics of a set of weather-related variables. We show how the base model can be enhanced with spatial interpolation that uses learned long-range spatial dependencies. We also derive an efficient learning and inference procedure that allows for large scale optimization of the model parameters. We evaluate the methods with experiments on real-world meteorological data that highlight the promise of the approach.,2015,Knowledge Discovery and Data Mining,Fields of study: probabilistic forecastingweather forecastinggraphical modelgaussian processdeep learningdata sciencedata miningmachine learningcomputer science
Life-stage Prediction for Product Recommendation in E-commerce,"Peng Jiang (Alibaba Group)Yadong Zhu (Alibaba Group)Yi Zhang (University of California, Santa Cruz)Quan Yuan (Alibaba Group)","2259224012,2492899049,2671671868,2640699477","Although marketing researchers and sociologists have recognized the large impact of life stage on consumer's purchasing behaviors, existing recommender systems have not taken this impact into consideration. In this paper, we found obvious correlation between life stage and purchasing behavior in many E-commerce categories. For example, a mum may look for different suitable products when her baby is at different ages. Motivated by this, we introduce the conception of life stage into recommender systems and propose to predict a user's current life-stage and recommend products correspondingly. We propose a new Maximum Entropy Semi Markov Model to segment and label consumer life stage based on the observed purchasing data over time. In the mom-baby product category where the life stage transition is deterministic, we develop an efficient approximate solution using large scale logistic regression and a Viterbi-like algorithm. We also propose a Gaussian mixture model to efficiently handle multi-kids life stage prediction problem. We integrate the life stage information predicted into the recommender system behind the largest online shopping website taobao.com. Both offline and online experiments demonstrate the effectiveness of the proposed life-stage based recommendation approach.",2015,Knowledge Discovery and Data Mining,Fields of study: recommender systemdata miningcomputer science
Leveraging Social Context for Modeling Topic Evolution,"Janani Kalyanam (University of California, San Diego)Amin Mantrach (Yahoo!)Diego Sáez-Trumper (Yahoo!)Hossein Vahabi (Yahoo!)Gert R. G. Lanckriet (University of California, San Diego)","2305354088,2027707398,2655440142,2190435107,60139091","Topic discovery and evolution (TDE) has been a problem which has gained long standing interest in the research community. The goal in topic discovery is to identify groups of keywords from large corpora so that the information in those corpora are summarized succinctly. The nature of text corpora has changed dramatically in the past few years with the advent of social media. Social media services allow users to constantly share, follow and comment on posts from other users. Hence, such services have given a new dimension to the traditional text corpus. The new dimension being that today's corpora have a social context embedded in them in terms of the community of users interested in a particular post, their profiles etc. We wish to harness this social context that comes along with the textual content for TDE. In particular, our goal is to both qualitatively and quantitatively analyze when social context actually helps with TDE. Methodologically, we approach the problem of TDE by a proposing non-negative matrix factorization (NMF) based model that incorporates both the textual information and social context information. We perform experiments on large scale real world dataset of news articles, and use Twitter as the platform providing information about the social context of these news articles. We compare with and outperform several state-of-the-art baselines. Our conclusion is that using the social context information is most useful when faced with topics that are particularly difficult to detect.",2015,Knowledge Discovery and Data Mining,Fields of study: social networkdata scienceworld wide webdata miningmachine learningcomputer science
Spectral Ensemble Clustering,"Hongfu Liu (Northeastern University)Tongliang Liu (University of Technology, Sydney)Junjie Wu (Beihang University)Dacheng Tao (University of Technology, Sydney)Yun Fu (Northeastern University)","2108071053,2137595165,2149366604,2104129307,2123131494","Ensemble clustering, also known as consensus clustering, is emerging as a promising solution for multi-source and/or heterogeneous data clustering. The co-association matrix based method, which redefines the ensemble clustering problem as a classical graph partition problem, is a landmark method in this area. Nevertheless, the relatively high time and space complexity preclude it from real-life large-scale data clustering. We therefore propose SEC, an efficient Spectral Ensemble Clustering method based on co-association matrix. We show that SEC has theoretical equivalence to weighted K-means clustering and results in vastly reduced algorithmic complexity. We then derive the latent consensus function of SEC, which to our best knowledge is among the first to bridge co-association matrix based method to the methods with explicit object functions. The robustness and generalizability of SEC are then investigated to prove the superiority of SEC in theory. We finally extend SEC to meet the challenge rising from incomplete basic partitions, based on which a scheme for big data clustering can be formed. Experimental results on various real-world data sets demonstrate that SEC is an effective and efficient competitor to some state-of-the-art ensemble clustering methods and is also suitable for big data clustering.",2015,Knowledge Discovery and Data Mining,Fields of study: k medians clusteringflame clusteringbrown clusteringcanopy clustering algorithmdbscancorrelation clusteringconstrained clusteringdata stream clusteringcure data clustering algorithmsingle linkage clusteringaffinity propagationspectral clusteringfuzzy clusteringk means clusteringclustering high dimensional datacluster analysisconsensus clusteringbiclusteringdata miningpattern recognitionmachine learningstatisticscomputer sciencemathematics
Efficient Long-Term Degradation Profiling in Time Series for Complex Physical Systems,"Liudmila Ulanova (University of California, Riverside)Tan Yan (NEC)Haifeng Chen (NEC)Guofei Jiang (NEC)Eamonn J. Keogh (University of California, Riverside)Kai Zhang (NEC)","2089139422,2194417700,2571838709,2168090285,2170070822,2600447970","The long term operation of physical systems inevitably leads to their wearing out, and may cause degradations in performance or the unexpected failure of the entire system. To reduce the possibility of such unanticipated failures, the system must be monitored for tell-tale symptoms of degradation that are suggestive of imminent failure. In this work, we introduce a novel time series analysis technique that allows the decomposition of the time series into trend and fluctuation components, providing the monitoring software with actionable information about the changes of the system's behavior over time. We analyze the underlying problem and formulate it to a Quadratic Programming (QP) problem that can be solved with existing QP-solvers. However, when the profiling resolution is high, as generally required by real-world applications, such a decomposition becomes intractable to general QP-solvers. To speed up the problem solving, we further transform the problem and present a novel QP formulation, Non-negative QP, for the problem and demonstrate a tractable solution that bypasses the use of slow general QP-solvers. We demonstrate our ideas on both synthetic and real datasets, showing that our method allows us to accurately extract the degradation phenomenon of time series. We further demonstrate the generality of our ideas by applying them beyond classic machine prognostics to problems in identifying the influence of news events on currency exchange rates and stock prices. We fully implement our profiling system and deploy it into several physical systems, such as chemical plants and nuclear power plants, and it greatly helps detect the degradation phenomenon, and diagnose the corresponding components.",2015,Knowledge Discovery and Data Mining,Fields of study: time seriesdata miningsimulationstatistics
A Machine Learning Framework to Identify Students at Risk of Adverse Academic Outcomes,Himabindu Lakkaraju (Stanford University)Everaldo Aguiar (University of Notre Dame)Carl Shan (University of Chicago)David Miller (Northwestern University)Nasir Bhanpuri (University of Chicago)Rayid Ghani (University of Chicago)Kecia L. Addison,"2237122046,2125151847,2226776816,2171884165,2228032741,2655769349,2223177216","Many school districts have developed successful intervention programs to help students graduate high school on time. However, identifying and prioritizing students who need those interventions the most remains challenging. This paper describes a machine learning framework to identify such students, discusses features that are useful for this task, applies several classification algorithms, and evaluates them using metrics important to school administrators. To help test this framework and make it practically useful, we partnered with two U.S. school districts with a combined enrollment of approximately 200,000 students. We together designed several evaluation metrics to assess the goodness of machine learning algorithms from an educator's perspective. This paper focuses on students at risk of not finishing high school on time, but our framework lays a strong foundation for future work on other adverse academic outcomes.",2015,Knowledge Discovery and Data Mining,Fields of study: information technologydata scienceknowledge managementsimulationcomputer science
Transitive Transfer Learning,Ben Tan (Hong Kong University of Science and Technology)Yangqiu Song (University of Illinois at Urbana–Champaign)Erheng Zhong (Yahoo!)Qiang Yang (Hong Kong University of Science and Technology),"2102551043,2099747503,2108094379,2109031554","Transfer learning, which leverages knowledge from source domains to enhance learning ability in a target domain, has been proven effective in various applications. One major limitation of transfer learning is that the source and target domains should be directly related. If there is little overlap between the two domains, performing knowledge transfer between these domains will not be effective. Inspired by human transitive inference and learning ability, whereby two seemingly unrelated concepts can be connected by a string of intermediate bridges using auxiliary concepts, in this paper we study a novel learning problem: Transitive Transfer Learning (abbreviated to TTL). TTL is aimed at breaking the large domain distances and transfer knowledge even when the source and target domains share few factors directly. For example, when the source and target domains are documents and images respectively, TTL could use some annotated images as the intermediate domain to bridge them. To solve the TTL problem, we propose a learning framework to mimic the human learning process. The framework is composed of an intermediate domain selection component and a knowledge transfer component. Extensive empirical evidence shows that the framework yields state-of-the-art classification accuracies on several classification data sets.",2015,Knowledge Discovery and Data Mining,Fields of study: inductive transfermulti task learningtransfer of learningsemi supervised learningdata miningartificial intelligencemachine learningcomputer sciencemathematics
Portraying Collective Spatial Attention in Twitter,Émilien Antoine (Kyoto Sangyo University)Adam Jatowt (Kyoto University)Shoko Wakamiya (Kyoto Sangyo University)Yukiko Kawai (Kyoto Sangyo University)Toyokazu Akiyama (Kyoto Sangyo University),"2306443022,13250842,2141474645,2116005948,2190548400","Microblogging platforms such as Twitter have been recently frequently used for detecting real-time events. The spatial component, as reflected by user location, usually plays a key role in such systems. However, an often neglected source of spatial information are location mentions expressed in tweet contents. In this paper we demonstrate a novel visualization system for analyzing how Twitter users collectively talk about space and for uncovering correlations between geographical locations of Twitter users and the locations they tweet about. Our exploratory analysis is based on the development of a model of spatial information extraction and representation that allows building effective visual analytics framework for large scale datasets. We show visualization results based on half a year long dataset of Japanese tweets and a four months long collection of tweets from USA. The proposed system allows observing many space related aspects of tweet messages including the average scope of spatial attention of social media users and variances in spatial interest over time. The analytical framework we provide and the findings we outline can be valuable for scientists from diverse research areas and for any users interested in geographical and social aspects of shared online data.",2015,Knowledge Discovery and Data Mining,Fields of study: spatial analysisvisualizationsocial networkinternet privacymultimediaworld wide webdata miningcomputer science
Using Local Spectral Methods to Robustify Graph-Based Learning Algorithms,"David F. Gleich (Purdue University)Michael W. Mahoney (University of California, Berkeley)","2148810670,2104888899","Graph-based learning methods have a variety of names including semi-supervised and transductive learning. They typically use a diffusion to propagate labels from a small set of nodes with known class labels to the remaining nodes of the graph. While popular, these algorithms, when implemented in a straightforward fashion, are extremely sensitive to the details of the graph construction. Here, we provide four procedures to help make them more robust: recognizing implicit regularization in the diffusion, using a scalable push method to evaluate the diffusion, using rank-based rounding, and densifying the graph through a matrix polynomial. We study robustness with respect to the details of graph constructions, errors in node labeling, degree variability, and a variety of other real-world heterogeneities, studying these methods through a precise relationship with mincut problems. For instance, the densification strategy explicitly adds new weighted edges to a sparse graph. We find that this simple densification creates a graph where multiple diffusion methods are robust to several types of errors. This is demonstrated by a study with predicting product categories from an Amazon co-purchasing network.",2015,Knowledge Discovery and Data Mining,Fields of study: random geometric graphcluster analysissemi supervised learningdiscrete mathematicscombinatoricsdata miningmachine learningstatisticscomputer sciencemathematics
Edge-Weighted Personalized PageRank: Breaking A Decade-Old Performance Barrier,Wenlei Xie (Cornell University)David Bindel (Cornell University)Alan J. Demers (Cornell University)Johannes Gehrke (Cornell University),"2137075844,1964853290,2372019969,2083845045","Personalized PageRank is a standard tool for finding vertices in a graph that are most relevant to a query or user. To personalize PageRank, one adjusts node weights or edge weights that determine teleport probabilities and transition probabilities in a random surfer model. There are many fast methods to approximate PageRank when the node weights are personalized; however, personalization based on edge weights has been an open problem since the dawn of personalized PageRank over a decade ago. In this paper, we describe the first fast algorithm for computing PageRank on general graphs when the edge weights are personalized. Our method, which is based on model reduction, outperforms existing methods by nearly five orders of magnitude . This huge performance gain over previous work allows us --- for the very first time --- to solve learning-to-rank problems for edge weight personalization at interactive speeds , a goal that had not previously been achievable for this class of problems.",2015,Knowledge Discovery and Data Mining,Fields of study: theoretical computer scienceworld wide webdata miningcomputer science
Discovery of Meaningful Rules in Time Series,"Mohammad Shokoohi-Yekta (University of California, Riverside)Yanping Chen 0005 (University of California, Riverside)Bilson J. L. Campana (University of California, Riverside)Bing Hu 0001 (University of California, Riverside)Jesin Zakaria (University of California, Riverside)Eamonn J. Keogh (University of California, Riverside)","2310806999,2472401419,2125777994,2105942667,2129110089,2170070822","The ability to make predictions about future events is at the heart of much of science; so, it is not surprising that prediction has been a topic of great interest in the data mining community for the last decade. Most of the previous work has attempted to predict the future based on the current value of a stream. However, for many problems the actual values are irrelevant, whereas the shape of the current time series pattern may foretell the future. The handful of research efforts that consider this variant of the problem have met with limited success. In particular, it is now understood that most of these efforts allow the discovery of spurious rules. We believe the reason why rule discovery in real-valued time series has failed thus far is because most efforts have more or less indiscriminately applied the ideas of symbolic stream rule discovery to real-valued rule discovery. In this work, we show why these ideas are not directly suitable for rule discovery in time series. Beyond our novel definitions/representations, which allow for meaningful and extendable specifications of rules, we further show novel algorithms that allow us to quickly discover high quality rules in very large datasets that accurately predict the occurrence of future events.",2015,Knowledge Discovery and Data Mining,Fields of study: predictiontime seriesdata sciencedata miningartificial intelligencestatisticscomputer science
Efficient Online Evaluation of Big Data Stream Classifiers,Albert Bifet (Huawei)Gianmarco de Francisci Morales (Aalto University)Jess Read (Aalto University)Geoffrey Holmes (University of Waikato)Bernhard Pfahringer (University of Waikato),"307521372,2153118160,2579780028,2189262995,1991131908","The evaluation of classifiers in data streams is fundamental so that poorly-performing models can be identified, and either improved or replaced by better-performing models. This is an increasingly relevant and important task as stream data is generated from more sources, in real-time, in large quantities, and is now considered the largest source of big data . Both researchers and practitioners need to be able to effectively evaluate the performance of the methods they employ. However, there are major challenges for evaluation in a stream. Instances arriving in a data stream are usually time-dependent, and the underlying concept that they represent may evolve over time. Furthermore, the massive quantity of data also tends to exacerbate issues such as class imbalance. Current frameworks for evaluating streaming and online algorithms are able to give predictions in real-time, but as they use a prequential setting, they build only one model, and are thus not able to compute the statistical significance of results in real-time. In this paper we propose a new evaluation methodology for big data streams. This methodology addresses unbalanced data streams, data where change occurs on different time scales, and the question of how to split the data between training and testing, over multiple models.",2015,Knowledge Discovery and Data Mining,Fields of study: evaluationbiological classificationdata stream miningdata sciencedata miningmachine learningstatisticscomputer science
E-commerce in Your Inbox: Product Recommendations at Scale,Mihajlo Grbovic (Yahoo!)Vladan Radosavljevic (Yahoo!)Nemanja Djuric (Yahoo!)Narayan Bhamidipati (Yahoo!)Jaikit Savla (Yahoo!)Varun Bhagwan (Yahoo!)Doug Sharp (Yahoo!),"2000240052,1981991050,2128657275,1964376778,2133578042,2303161709,2227693147","In recent years online advertising has become increasingly ubiquitous and effective. Advertisements shown to visitors fund sites and apps that publish digital content, manage social networks, and operate e-mail services. Given such large variety of internet resources, determining an appropriate type of advertising for a given platform has become critical to financial success. Native advertisements, namely ads that are similar in look and feel to content, have had great success in news and social feeds. However, to date there has not been a winning formula for ads in e-mail clients. In this paper we describe a system that leverages user purchase history determined from e-mail receipts to deliver highly personalized product ads to Yahoo Mail users. We propose to use a novel neural language-based algorithm specifically tailored for delivering effective product recommendations, which was evaluated against baselines that included showing popular products and products predicted based on co-occurrence. We conducted rigorous offline testing using a large-scale product purchase data set, covering purchases of more than 29 million users from 172 e-commerce websites. Ads in the form of product recommendations were successfully tested on online traffic, where we observed a steady 9% lift in click-through rates over other ad formats in mail, as well as comparable lift in conversion rates. Following successful tests, the system was launched into production during the holiday season of 2014.",2015,Knowledge Discovery and Data Mining,Fields of study: contextual advertisingworld wide webdata miningartificial intelligencecomputer science
Predicting Serves in Tennis using Style Priors,Xinyu Wei (Queensland University of Technology)Patrick Lucey (Disney Research)Stuart Morgan (Australian Institute of Sport)Peter Carr (Disney Research)Machar Reid (University of Western Australia)Sridha Sridharan (Queensland University of Technology),"2142198809,1986214492,2156595135,2049576458,2263218702,2136340264","In professional sport, an enormous amount of fine-grain performance data can be generated at near millisecond intervals in the form of vision-based tracking data. One of the first sports to embrace this technology has been tennis, where Hawk-Eye technology has been used to both aid umpiring decisions, and to visualize shot trajectories for broadcast purposes. These data have tremendous untapped applications in terms of ""opponent planning'', where a large amount of recent data is used to learn contextual behavior patterns of individual players, and ultimately predict the likelihood of a particular type of serve. Since the type of serve selected by a player may be contingent on the match context (i.e., is the player down break-point, or is serving for the match etc.), the characteristics of the player (i.e., the player may have a very fast serve, hit heavy with topspin or kick, or slice serves into the body) as well as the characteristics of the opponent (e.g., the opponent may prefer to play from the baseline or ""chip-and-charge'' into the net). In this paper we present a method which recommends the most likely serves of a player in a given context. We show by utilizing a ""style prior"", we can improve the prediction/recommendation. Such an approach also allows us to quantify the similarity between players, which is useful in enriching the dataset for future prediction. We conduct our analysis on Hawk-Eye data collected from three recent Australian Open Grand-Slam Tournaments and show how our approach can be used in practice.",2015,Knowledge Discovery and Data Mining,Fields of study: investor profilemultimediasimulation
Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,"Longbing Cao (University of Technology, Sydney)Chengqi Zhang (University of Technology, Sydney)Thorsten Joachims (Cornell University)Geoff Webb (Monash University)Dragos D. MargineantuGraham Williams (Australian Taxation Office)","2115085568,2166080598,245171893,2558621969,2631568539,2106893266","It is our great pleasure to welcome you to the 21st ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD). Theannual ACM SIGKDD conference is the premier international forum for data science, data mining, knowledge discovery and big data. It brings together researchers and practitioners from academia, industry, and government to share their ideas, research results and experiences. KDD-2015 features 4 plenary keynote presentations, 12 invited talks, 228 paper presentations, a discussion panel, a poster session, 14 workshops, 12 tutorials, 27 exhibition booths, the KDD Cup competition, and a banquet at the Dockside Pavilion at the Sydney Darling Harbour. As always, KDD-2015 attracted presenters and delegates from around the world. It is with great pleasure that we bring this international conference for the first time to the southern hemisphere. This year we again had a strong set of submissions. There were 819 submissions to the Research Track, of which 160 papers were accepted. There were 189 submissions to the Industry & Government Track, of which 68 papers were accepted. All papers submitted to the Research Track and to the Industry & Government Tracks were subjected to a rigorous review process. They were initially screened by the Chairs of the respective tracks, and a small number of papers that did not comply with the formatting requirements or which violated the dual submission policy were summarily rejected. At least three reviewers and a metareviewer were assigned to all remaining papers based on the results of a bidding process. The authors were able to read the reviews and provide a response. The meta-reviewers then had an opportunity to consider all reviews and author responses. This then initiated a discussion during which all reviewers of a paper had the opportunity to read each other's reviews and the author responses and to update their reviews as appropriate. In a few cases, the meta-reviewers added another reviewer at this stage to gain expert opinion on specific issues. The meta-reviewers then made recommendations on acceptance or rejection to the track chairs. The track chairs then assessed the meta-reviews, reviews, author responses and discussions to make a final decision. In a few cases, they also solicited further expert reviews and meta-reviews to resolve specific questions. Thus, all papers were assessed by at least four and up to seven discipline experts. All accepted papers were presented both as a 20-minute talk and as a poster. The Industry & Government Invited Talk Track features 12 talks from world renowned experts who have played a significant role in developing and deploying large-scale data mining applications and systems in their respective fields with clearly measurable and meaningful impact. We trust that this opportunity for the KDD community to hear directly from senior leaders in industry and government will inspire new advances and broader interdisciplinary collaboration between researchers, industry and government counterparts. Featured topics in this year's Industry & Government Invited Talk Track include applications in cloud intelligence, personal finance, insurance, fraud, marketing, advertising, and telecommunications. Additionally, we have focused on key enabling BigData infrastructure for KDD technologies with talks on highly-scalable and open source solutions that are quickly becoming mainstream. We continued the conference's tradition of strong workshop and tutorial programs. This year there are 9 full-day workshops, 5 half-day workshops, and 12 tutorials.",2015,Knowledge Discovery and Data Mining,Fields of study: operations researchdata miningcomputer science
Panther: Fast Top-k Similarity Search on Large Networks,Jing Zhang (Tsinghua University)Jie Tang (Tsinghua University)Cong Ma (Tsinghua University)Hanghang Tong (Arizona State University)Yu Jing (Tsinghua University)Juanzi Li (Tsinghua University),"2635071740,2158012360,2639301103,2667261544,2723938845,2129156004","Estimating similarity between vertices is a fundamental issue in network analysis across various domains, such as social networks and biological networks. Methods based on common neighbors and structural contexts have received much attention. However, both categories of methods are difficult to scale up to handle large networks (with billions of nodes). In this paper, we propose a sampling method that provably and accurately estimates the similarity between vertices. The algorithm is based on a novel idea of random path . Specifically, given a network, we perform R random walks, each starting from a randomly picked vertex and walking T steps. Theoretically, the algorithm guarantees that the sampling size R = O (2e -2 log 2 T ) depends on the error-bound e, the confidence level (1 -- δ), and the path length T of each random walk. We perform extensive empirical study on a Tencent microblogging network of 1,000,000,000 edges. We show that our algorithm can return top-k similar vertices for any vertex in a network 300× faster than the state-of-the-art methods. We also use two applications-identity resolution and structural hole spanner finding--to evaluate the accuracy of the estimated similarities. Our results demonstrate that the proposed algorithm achieves clearly better performance than several alternative methods.",2015,Knowledge Discovery and Data Mining,Fields of study: social networktheoretical computer sciencecombinatoricsworld wide websocial sciencemachine learningstatisticsmathematics
Discerning Tactical Patterns for Professional Soccer Teams: An Enhanced Topic Model with Applications,Qing Wang (Peking University)Hengshu Zhu (Baidu)Wei Hu (Baidu)Zhiyong Shen (Baidu)Yuan Yao (Peking University),"2637914907,2098414524,2282356173,2096063433,2653849141","Analyzing team tactics plays an important role in the professional soccer industry. Recently, the progressing ability to track the mobility of ball and players makes it possible to accumulate extensive match logs, which open a venue for better tactical analysis. However, traditional methods for tactical analysis largely rely on the knowledge and manual labor of domain experts. To this end, in this paper we propose an unsupervised approach to automatically discerning the typical tactics, i.e., tactical patterns, of soccer teams through mining the historical match logs. To be specific, we first develop a novel model named Team Tactic Topic Model (T3M) for learning the latent tactical patterns, which can model the locations and passing relations of players simultaneously. Furthermore, we demonstrate several potential applications enabled by the proposed T3M, such as automatic tactical pattern discovery, pass segment annotation, and spatial analysis of player roles. Finally, we implement an intelligent demo system to empirically evaluate our approach based on the data collected from La Liga 2013-2014. Indeed, by visualizing the results obtained from T3M, we can successfully observe many meaningful tactical patterns and interesting discoveries, such as using which tactics a team is more likely to score a goal and how a team's playing tactic changes in sequential matches across a season.",2015,Knowledge Discovery and Data Mining,Fields of study: topic modelmultimediadata miningmachine learningsimulationcomputer science
Mining Frequent Itemsets through Progressive Sampling with Rademacher Averages,Matteo Riondato (Brown University)Eli Upfal (Brown University),"1555209364,265232414","We present an algorithm to extract an high-quality approximation of the (top- k ) Frequent itemsets (FIs) from random samples of a transactional dataset. With high probability the approximation is a superset of the FIs, and no itemset with frequency much lower than the threshold is included in it. The algorithm employs progressive sampling, with a stopping condition based on bounds to the empirical Rademacher average, a key concept from statistical learning theory. The computation of the bounds uses characteristic quantities that can be obtained efficiently with a single scan of the sample. Therefore, evaluating the stopping condition is fast, and does not require an expensive mining of each sample. Our experimental evaluation confirms the practicality of our approach on real datasets, outperforming approaches based on one-shot static sampling.",2015,Knowledge Discovery and Data Mining,Fields of study: samplingdata miningpattern recognitionstatisticsmathematics
Focusing on the Long-term: It's Good for Users and Business,Henning Hohnhold (Google)Deirdre O'Brien (Google)Diane Tang (Google),"2228726280,2106587893,2163328135","Over the past 10+ years, online companies large and small have adopted widespread A/B testing as a robust data-based method for evaluating potential product improvements. In online experimentation, it is straightforward to measure the short-term effect, i.e., the impact observed during the experiment. However, the short-term effect is not always predictive of the long-term effect, i.e., the final impact once the product has fully launched and users have changed their behavior in response. Thus, the challenge is how to determine the long-term user impact while still being able to make decisions in a timely manner. We tackle that challenge in this paper by first developing experiment methodology for quantifying long-term user learning. We then apply this methodology to ads shown on Google search, more specifically, to determine and quantify the drivers of ads blindness and sightedness, the phenomenon of users changing their inherent propensity to click on or interact with ads. We use these results to create a model that uses metrics measurable in the short-term to predict the long-term. We learn that user satisfaction is paramount: ads blindness and sightedness are driven by the quality of previously viewed or clicked ads, as measured by both ad relevance and landing page quality. Focusing on user satisfaction both ensures happier users but also makes business sense, as our results illustrate. We describe two major applications of our findings: a conceptual change to our search ads auction that further increased the importance of ads quality, and a 50% reduction of the ad load on Google's mobile search interface. The results presented in this paper are generalizable in two major ways. First, the methodology may be used to quantify user learning effects and to evaluate online experiments in contexts other than ads. Second, the ads blindness/sighted-ness results indicate that a focus on user satisfaction could help to reduce the ad load on the internet at large with long-term neutral, or even positive, business impact.",2015,Knowledge Discovery and Data Mining,Fields of study: randomized experimentworld wide webdata miningsimulationstatisticscomputer science
Measuring Causal Impact of Online Actions via Natural Experiments: Application to Display Advertising,"Daniel N. Hill (Amazon.com)Robert Moakler (New York University Stern School of Business)Alan E. Hubbard (University of California, Berkeley)Vadim TsemekhmanFoster J. Provost (New York University Stern School of Business)Kiril Tsemekhman","2484979804,2229869706,2096146599,2428456263,2158932634,2655114689","Predictive models are often employed to decide actions in interactive online systems. For example, ads are selectively served to users who are modeled as being inclined to purchase the product being advertised. News feed items are populated based on a model of the user's interests. A common consequence of these predictive models is the creation of a spurious correlation, or confounding, between the action and its desired outcome. In the above examples, the targeted users are likely to buy the product or find the news item regardless of the intervention. This presents a challenge for measuring the true impact of these systems. Here we present a novel framework for estimating causal effects that relies on neither randomized experiments nor adjusting for the potentially explosive number of variables used in predictive models. We propose the identification and instrumentation of events that mediate the effect of the action. When the effect of an action depends on a mediating event that is not subject to the same confounders, the problem of causal estimation is greatly simplified. We demonstrate this approach in display advertising using ad viewability as a natural experiment that mediates the impact of served ads. Approximately 45% of display ad impressions never make it into a viewable portion of the user's browser. We show that an analysis based on ad viewability can massively reduce the amount of bias in estimating campaign lift. We integrate the use of negative controls as well as the identification and adjustment for residual confounding to further reduce the bias in estimated lift to less than 10%. A system using these techniques is deployed to monitor the daily causal impact of many large-scale advertising campaigns.",2015,Knowledge Discovery and Data Mining,Fields of study: performance measurementnatural experimentdata miningsimulation
Estimating Local Intrinsic Dimensionality,Laurent Amsaleg (Centre national de la recherche scientifique)Oussama Chelly (National Institute of Informatics)Teddy Furon (French Institute for Research in Computer Science and Automation)Stéphane Girard (French Institute for Research in Computer Science and Automation)Michael E. Houle (National Institute of Informatics)Ken-ichi Kawarabayashi (National Institute of Informatics)Michael Nett (Google),"190871858,2226047698,2170021821,2168040728,2088104168,563105935,2632955242","This paper is concerned with the estimation of a local measure of intrinsic dimensionality (ID) recently proposed by Houle. The local model can be regarded as an extension of Karger and Ruhl's expansion dimension to a statistical setting in which the distribution of distances to a query point is modeled in terms of a continuous random variable. This form of intrinsic dimensionality can be particularly useful in search, classification, outlier detection, and other contexts in machine learning, databases, and data mining, as it has been shown to be equivalent to a measure of the discriminative power of similarity functions. Several estimators of local ID are proposed and analyzed based on extreme value theory, using maximum likelihood estimation (MLE), the method of moments (MoM), probability weighted moments (PWM), and regularly varying functions (RV). An experimental evaluation is also provided, using both real and artificial data.",2015,Knowledge Discovery and Data Mining,Fields of study: intrinsic dimensionnonlinear dimensionality reductionpattern recognitionmachine learningstatisticscomputer sciencemathematics
"Data-Driven Activity Prediction: Algorithms, Evaluation Methodology, and Applications",Bryan Minor (Washington State University)Janardhan Rao Doppa (Washington State University)Diane J. Cook (Washington State University),"2473070231,2074111126,2106796750","We consider a novel problem called Activity Prediction, where the goal is to predict the future activity occurrence times from sensor data. In this paper, we make three main contributions. First, we formulate and solve the activity prediction problem in the framework of imitation learning and reduce it to simple regression learning problem. This approach allows us to leverage powerful regression learners; is easy to implement; and can reason about the relational and temporal structure of the problem with negligible computational overhead. Second, we present several evaluation metrics to evaluate a given activity predictor, and discuss their pros and cons in the context of real-world applications. Third, we evaluate our approach using real sensor data collected from 24 smart home testbeds. We also embed the learned predictor into a mobile device based activity prompter and evaluate the app on multiple participants living in smart homes. Our experimental results indicate that the activity predictor learned with our approach performs better than the baseline methods, and offers a simple and reliable approach to prediction of activities from sensor data.",2015,Knowledge Discovery and Data Mining,Fields of study: smart environmentdata miningmachine learningsimulationcomputer science
Multi-Task Learning for Spatio-Temporal Event Forecasting,"Liang Zhao (Virginia Tech)Qian Sun (Arizona State University)Jieping Ye (University of Michigan)Feng Chen (University at Albany, SUNY)Chang Tien Lu (Virginia Tech)Naren Ramakrishnan (Virginia Tech)","2619584304,2620185474,2305258894,2601749234,2112878203,2199255697","Spatial event forecasting from social media is an important problem but encounters critical challenges, such as dynamic patterns of features (keywords) and geographic heterogeneity (e.g., spatial correlations, imbalanced samples, and different populations in different locations). Most existing approaches (e.g., LASSO regression, dynamic query expansion, and burst detection) are designed to address some of these challenges, but not all of them. This paper proposes a novel multi-task learning framework which aims to concurrently address all the challenges. Specifically, given a collection of locations (e.g., cities), we propose to build forecasting models for all locations simultaneously by extracting and utilizing appropriate shared information that effectively increases the sample size for each location, thus improving the forecasting performance. We combine both static features derived from a predefined vocabulary by domain experts and dynamic features generated from dynamic query expansion in a multi-task feature learning framework; we investigate different strategies to balance homogeneity and diversity between static and dynamic terms. Efficient algorithms based on Iterative Group Hard Thresholding are developed to achieve efficient and effective model training and prediction. Extensive experimental evaluations on Twitter data from four different countries in Latin America demonstrated the effectiveness of our proposed approach.",2015,Knowledge Discovery and Data Mining,Fields of study: multi task learninglassodata miningpattern recognitionmachine learningstatisticscomputer science
Real-Time Top-R Topic Detection on Twitter with Topic Hijack Filtering,Kohei Hayashi (National Institute of Informatics)Takanori Maehara (Shizuoka University)Masashi Toyoda (University of Tokyo)Ken-ichi Kawarabayashi (National Institute of Informatics),"2602971880,2134907451,2113072124,563105935","Twitter is a ""what's-happening-right-now"" tool that enables interested parties to follow thoughts and commentary of individual users in nearly real-time. While it is a valuable source of information for real-time topic detection and tracking, Twitter data are not clean because of noisy messages and users, which significantly diminish the reliability of obtained results. In this paper, we integrate both the extraction of meaningful topics and the filtering of messages over the Twitter stream. We develop a streaming algorithm for a sequence of document-frequency tables; our algorithm enables real-time monitoring of the top-10 topics from approximately 25% of all Twitter messages, while automatically filtering noisy and meaningless topics. We apply our proposed streaming algorithm to the Japanese Twitter stream and successfully demonstrate that, compared with other online nonnegative matrix factorization methods, our framework both tracks real-world events with high accuracy in terms of the perplexity and simultaneously eliminates irrelevant topics.",2015,Knowledge Discovery and Data Mining,Fields of study: streaming algorithmnon negative matrix factorizationfilterworld wide webinformation retrievaldata miningmachine learningcomputer science
A PCA-Based Change Detection Framework for Multidimensional Data Streams: Change Detection in Multidimensional Data Streams,Abdulhakim Ali Ali Qahtan (King Abdullah University of Science and Technology)Basma Mohammed Alharbi (King Abdullah University of Science and Technology)Suojin Wang (Texas A&M University)Xiangliang Zhang (King Abdullah University of Science and Technology),"2227459398,2222004373,2688482437,2129841492","Detecting changes in multidimensional data streams is an important and challenging task. In unsupervised change detection, changes are usually detected by comparing the distribution in a current (test) window with a reference window. It is thus essential to design divergence metrics and density estimators for comparing the data distributions, which are mostly done for univariate data. Detecting changes in multidimensional data streams brings difficulties to the density estimation and comparisons. In this paper, we propose a framework for detecting changes in multidimensional data streams based on principal component analysis, which is used for projecting data into a lower dimensional space, thus facilitating density estimation and change-score calculations. The proposed framework also has advantages over existing approaches by reducing computational costs with an efficient density estimator, promoting the change-score calculation by introducing effective divergence metrics, and by minimizing the efforts required from users on the threshold parameter setting by using the Page-Hinkley test. The evaluation results on synthetic and real data show that our framework outperforms two baseline methods in terms of both detection accuracy and computational costs.",2015,Knowledge Discovery and Data Mining,Fields of study: change detectiondensity estimationprincipal component analysisdata stream miningdata sciencedata miningmachine learningstatisticscomputer science
Click-through Prediction for Advertising in Twitter Timeline,Cheng Li (University of Michigan)Yue Lu (Twitter)Qiaozhu Mei (University of Michigan)Dong Wang (Twitter)Sandeep Pandey (Twitter),"2674810995,2652944969,2166036605,2720645064,2723427591","We present the problem of click-through prediction for advertising in Twitter timeline, which displays a stream of Tweets from accounts a user choose to follow. Traditional computational advertising usually appears in two forms: sponsored search that places ads onto the search result page when a query is issued to a search engine, and contextual advertising that places ads onto a regular, usually static Web page. Compared with these two paradigms, placing ads into a Tweet stream is particularly challenging given the nature of the data stream: the context into which an ad can be placed updates dynamically and never replicates. Every ad is therefore placed into a unique context. This makes the information available for training a machine learning model extremely sparse. In this study, we propose a learning-to-rank method which not only addresses the sparsity of training signals but also can be trained and updated online. The proposed method is evaluated using both offline experiments and online A/B tests, which involve very large collections of Twitter data and real Twitter users. Results of the experiments prove the effectiveness and efficiency of our solution, and its superiority over the current production model adopted by Twitter.",2015,Knowledge Discovery and Data Mining,Fields of study: contextual advertisingonline advertisinginternet privacyworld wide webdata miningcomputer science
Fast and Robust Parallel SGD Matrix Factorization,"Jinoh Oh (Pohang University of Science and Technology)Wook-Shin Han (Pohang University of Science and Technology)Hwanjo Yu (Pohang University of Science and Technology)Xiaoqian Jiang (University of California, San Diego)","2164587646,2307556295,2257366035,2132814769","Matrix factorization is one of the fundamental techniques for analyzing latent relationship between two entities. Especially, it is used for recommendation for its high accuracy. Efficient parallel SGD matrix factorization algorithms have been developed for large matrices to speed up the convergence of factorization. However, most of them are designed for a shared-memory environment thus fail to factorize a large matrix that is too big to fit in memory, and their performances are also unreliable when the matrix is skewed. This paper proposes a fast and robust parallel SGD matrix factorization algorithm, called MLGF-MF, which is robust to skewed matrices and runs efficiently on block-storage devices (e.g., SSD disks) as well as shared-memory. MLGF-MF uses Multi-Level Grid File (MLGF) for partitioning the matrix and minimizes the cost for scheduling parallel SGD updates on the partitioned regions by exploiting partial match queries processing}. Thereby, MLGF-MF produces reliable results efficiently even on skewed matrices. MLGF-MF is designed with asynchronous I/O permeated in the algorithm such that CPU keeps executing without waiting for I/O to complete. Thereby, MLGF-MF overlaps the CPU and I/O processing, which eventually offsets the I/O cost and maximizes the CPU utility. Recent flash SSD disks support high performance parallel I/O, thus are appropriate for executing the asynchronous I/O. From our extensive evaluations, MLGF-MF significantly outperforms (or converges faster than) the state-of-the-art algorithms in both shared-memory and block-storage environments. In addition, the outputs of MLGF-MF is significantly more robust to skewed matrices. Our implementation of MLGF-MF is available at http://dm.postech.ac.kr/MLGF-MF as executable files.",2015,Knowledge Discovery and Data Mining,Fields of study: stochastic gradient descentmatrix decompositiontheoretical computer scienceparallel computingdistributed computingmachine learningcomputer science
Facets: Fast Comprehensive Mining of Coevolving High-order Time Series,"Yongjie Cai (The Graduate Center, CUNY)Hanghang Tong (Arizona State University)Wei Fan (Baidu)Ping Ji (The Graduate Center, CUNY)Qing He (University at Buffalo)","2229854109,2667261544,2422054197,2722324186,2251990706","Mining time series data has been a very active research area in the past decade, exactly because of its prevalence in many high-impact applications, ranging from environmental monitoring, intelligent transportation systems, computer network forensics, to smart buildings and many more. It has posed many fascinating research questions. Among others, three prominent challenges shared by a variety of real applications are (a) high-order; (b) contextual constraints and (c) temporal smoothness. The state-of-the-art mining algorithms are rich in addressing each of these challenges, but relatively short of comprehensiveness in attacking the coexistence of multiple or even all of these three challenges. In this paper, we propose a comprehensive method, FACETS, to simultaneously model all these three challenges. We formulate it as an optimization problem from a dynamic graphical model perspective. The key idea is to use tensor factorization to address multi-aspect challenges, and perform careful regularizations to attack both contextual and temporal challenges. Based on that, we propose an effective and scalable algorithm to solve the problem. Our experimental evaluations on three real datasets demonstrate that our method (1) outperforms its competitors in two common data mining tasks (imputation and prediction); and (2) enjoys a linear scalability w.r.t. the length of time series.",2015,Knowledge Discovery and Data Mining,Fields of study: data sciencedata miningmachine learningmathematical optimizationstatisticscomputer sciencemathematics
Query Workloads for Data Series Indexes,Kostas Zoumpatianos (University of Trento)Yin Lou (LinkedIn)Themis Palpanas (Paris Descartes University)Johannes Gehrke (Microsoft),"2227802775,2308121590,2010554420,2083845045","Data series are a prevalent data type that has attracted lots of interest in recent years. Most of the research has focused on how to efficiently support similarity or nearest neighbor queries over large data series collections (an important data mining task), and several data series summarization and indexing methods have been proposed in order to solve this problem. Nevertheless, up to this point very little attention has been paid to properly evaluating such index structures, with most previous work relying solely on randomly selected data series to use as queries (with/without adding noise). In this work, we show that random workloads are inherently not suitable for the task at hand and we argue that there is a need for carefully generating a query workload. We define measures that capture the characteristics of queries, and we propose a method for generating workloads with the desired properties, that is, effectively evaluating and comparing data series summarizations and indexes. In our experimental evaluation, with carefully controlled query workloads, we shed light on key factors affecting the performance of nearest neighbor search in large data series collections.",2015,Knowledge Discovery and Data Mining,Fields of study: search engine indexinginformation retrievaldata miningdatabasecomputer science
Maximum Likelihood Postprocessing for Differential Privacy under Consistency Constraints,Jaewoo Lee (Pennsylvania State University)Yue Wang (Pennsylvania State University)Daniel Kifer (Pennsylvania State University),"2424114408,2579629656,2049563562","When analyzing data that has been perturbed for privacy reasons, one is often concerned about its usefulness. Recent research on differential privacy has shown that the accuracy of many data queries can be improved by post-processing the perturbed data to ensure consistency constraints that are known to hold for the original data. Most prior work converted this post-processing step into a least squares minimization problem with customized efficient solutions. While improving accuracy, this approach ignored the noise distribution in the perturbed data. In this paper, to further improve accuracy, we formulate this post-processing step as a constrained maximum likelihood estimation problem, which is equivalent to constrained L 1 minimization. Instead of relying on slow linear program solvers, we present a faster generic recipe (based on ADMM) that is suitable for a wide variety of applications including differentially private contingency tables, histograms, and the matrix mechanism (linear queries). An added benefit of our formulation is that it can often take direct advantage of algorithmic tricks used by the prior work on least-squares post-processing. An extensive set of experiments on various datasets demonstrates that this approach significantly improve accuracy over prior work.",2015,Knowledge Discovery and Data Mining,Fields of study: differential privacyvideo post processingdata miningmachine learningmathematical optimizationstatisticscomputer science
Warm Start for Parameter Selection of Linear Classifiers,Bo-Yu Chu (National Taiwan University)Chia-Hua Ho (National Taiwan University)Cheng-Hao Tsai (National Taiwan University)Chieh-Yen Lin (National Taiwan University)Chih-Jen Lin (National Taiwan University),"2222625391,2700953612,2141326772,2298736880,2168176072","In linear classification, a regularization term effectively remedies the overfitting problem, but selecting a good regularization parameter is usually time consuming. We consider cross validation for the selection process, so several optimization problems under different parameters must be solved. Our aim is to devise effective warm-start strategies to efficiently solve this sequence of optimization problems. We detailedly investigate the relationship between optimal solutions of logistic regression/linear SVM and regularization parameters. Based on the analysis, we develop an efficient tool to automatically find a suitable parameter for users with no related background knowledge.",2015,Knowledge Discovery and Data Mining,Fields of study: regularization perspectives on support vector machineslinear classifierregularizationpattern recognitionmachine learningmathematical optimizationcomputer sciencemathematics
Flexible and Robust Multi-Network Clustering,Jingchao Ni (Case Western Reserve University)Hanghang Tong (Arizona State University)Wei Fan (Baidu)Xiang Zhang (Case Western Reserve University),"2223717457,2667261544,2422054197,2553248206","Integrating multiple graphs (or networks) has been shown to be a promising approach to improve the graph clustering accuracy. Various multi-view and multi-domain graph clustering methods have recently been developed to integrate multiple networks. In these methods, a network is treated as a view or domain.The key assumption is that there is a common clustering structure shared across all views (domains), and different views (domains) provide compatible and complementary information on this underlying clustering structure. However, in many emerging real-life applications, different networks have different data distributions, where the assumption that all networks share a single common clustering structure does not hold. In this paper, we propose a flexible and robust framework that allows multiple underlying clustering structures across different networks. Our method models the domain similarity as a network, which can be utilized to regularize the clustering structures in different networks. We refer to such a data model as a network of networks (NoN). We develop NoNClus, a novel method based on non-negative matrix factorization (NMF), to cluster an NoN. We provide rigorous theoretical analysis of NoNClus in terms of its correctness, convergence and complexity. Extensive experimental results on synthetic and real-life datasets show the effectiveness of our method.",2015,Knowledge Discovery and Data Mining,Fields of study: hierarchical clustering of networksflame clusteringbrown clusteringcanopy clustering algorithmcorrelation clusteringhierarchical network modelconstrained clusteringdata stream clusteringcure data clustering algorithmclustering coefficientfuzzy clusteringclustering high dimensional datacommunity structurecomplex networkcluster analysisconsensus clusteringbiclusteringconceptual clusteringtheoretical computer sciencedata miningmachine learningcomputer sciencemathematics
Scaling Up Stochastic Dual Coordinate Ascent,Kenneth Tran (Microsoft)Saghar Hosseini (University of Washington)Lin Xiao (Microsoft)Thomas Finley (Microsoft)Mikhail Bilenko (Microsoft),"2558285753,2299914201,2700159705,2225993305,1963524793","Stochastic Dual Coordinate Ascent (SDCA) has recently emerged as a state-of-the-art method for solving large-scale supervised learning problems formulated as minimization of convex loss functions. It performs iterative, random-coordinate updates to maximize the dual objective. Due to the sequential nature of the iterations, it is typically implemented as a single-threaded algorithm limited to in-memory datasets. In this paper, we introduce an asynchronous parallel version of the algorithm, analyze its convergence properties, and propose a solution for primal-dual synchronization required to achieve convergence in practice. In addition, we describe a method for scaling the algorithm to out-of-memory datasets via multi-threaded deserialization of block-compressed data. This approach yields sufficient pseudo-randomness to provide the same convergence rate as random-order in-memory access. Empirical evaluation demonstrates the efficiency of the proposed methods and their ability to fully utilize computational resources and scale to out-of-memory datasets.",2015,Knowledge Discovery and Data Mining,Fields of study: social influencepricingdata miningmachine learningsimulationmathematical optimizationstatisticscomputer science
Predictive Modeling for Public Health: Preventing Childhood Lead Poisoning,Eric Potash (University of Chicago)Joe Brew (University of Florida)Alexander Loewi (Carnegie Mellon University)Subhabrata Majumdar (University of Minnesota)Andrew Reece (Harvard University)Joe Walsh (University of Chicago)Eric Rozier (University of Cincinnati)Emile Jorgenson (Chicago Department of Public Health)Raed Mansour (Chicago Department of Public Health)Rayid Ghani (University of Chicago),"2225304132,2162043170,2227715861,2105484418,2256244240,2680879499,2310643731,2227139402,2644928559,2655769349","Lead poisoning is a major public health problem that affects hundreds of thousands of children in the United States every year. A common approach to identifying lead hazards is to test all children for elevated blood lead levels and then investigate and remediate the homes of children with elevated tests. This can prevent exposure to lead of future residents, but only after a child has been poisoned. This paper describes joint work with the Chicago Department of Public Health (CDPH) in which we build a model that predicts the risk of a child to being poisoned so that an intervention can take place before that happens. Using two decades of blood lead level tests, home lead inspections, property value assessments, and census data, our model allows inspectors to prioritize houses on an intractably long list of potential hazards and identify children who are at the highest risk. This work has been described by CDPH as pioneering in the use of machine learning and predictive analytics in public health and has the potential to have a significant impact on both health and economic outcomes for communities across the US.",2015,Knowledge Discovery and Data Mining,Fields of study: common goodpublic policypublic healthmachine learningcomputer science
Adaptation Algorithm and Theory Based on Generalized Discrepancy,Corinna Cortes (Google)Mehryar Mohri (Courant Institute of Mathematical Sciences)Andrés Muñoz Medina (Courant Institute of Mathematical Sciences),"2134830209,2150814284,2168485313","We present a new algorithm for domain adaptation improving upon the discrepancy minimization algorithm (DM), which was previously shown to outperform a number of popular algorithms designed for this task. Unlike most previous approaches adopted for domain adaptation, our algorithm does not consist of a fixed reweighting of the losses over the training sample. Instead, it uses a reweighting that depends on the hypothesis considered and is based on the minimization of a new measure of generalized discrepancy. We give a detailed description of our algorithm and show that it can be formulated as a convex optimization problem. We also present a detailed theoretical analysis of its learning guarantees, which helps us select its parameters. Finally, we report the results of experiments demonstrating that it improves upon the DM algorithm in several tasks.",2015,Knowledge Discovery and Data Mining,Fields of study: learning theoryartificial intelligencemachine learningmathematical optimizationstatisticscomputer sciencemathematics
Accelerated Alternating Direction Method of Multipliers,Mojtaba Kadkhodaie (University of Minnesota)Konstantina Christakopoulou (University of Minnesota)Maziar Sanjabi (University of Minnesota)Arindam Banerjee (University of Minnesota),"2059276998,2223173307,2164522516,2037585042","Recent years have seen a revival of interest in the Alternating Direction Method of Multipliers (ADMM), due to its simplicity, versatility, and scalability. As a first order method for general convex problems, the rate of convergence of ADMM is O(1=k) [4, 25]. Given the scale of modern data mining problems, an algorithm with similar properties as ADMM but faster convergence rate can make a big difference in real world applications. In this paper, we introduce the Accelerated Alternating Direction Method of Multipliers (A2DM2) which solves problems with the same structure as ADMM. When the objective function is strongly convex, we show that A2DM2 has a O(1=k2) convergence rate. Unlike related existing literature on trying to accelerate ADMM, our analysis does not need any additional restricting assumptions. Through experiments, we show that A2DM2 converges faster than ADMM on a variety of problems. Further, we illustrate the versatility of the general A2DM2 on the problem of learning to rank, where it is shown to be competitive with the state-of-the-art specialized algorithms for the problem on both scalability and accuracy.",2015,Knowledge Discovery and Data Mining,Fields of study: combinatoricsmathematical optimizationalgorithmmathematics
FrauDetector: A Graph-Mining-based Framework for Fraudulent Phone Call Detection,Vincent S. Tseng (National Chiao Tung University)Jia-Ching Ying (National Chiao Tung University)Che-Wei Huang (National Cheng Kung University)Yimin KaoKuan-Ta Chen (Academia Sinica),"2662202843,2583099430,2230125761,2681061115,2149245865","In recent years, fraud is increasing rapidly with the development of modern technology and global communication. Although many literatures have addressed the fraud detection problem, these existing works focus only on formulating the fraud detection problem as a binary classification problem. Due to limitation of information provided by telecommunication records, such classifier-based approaches for fraudulent phone call detection normally do not work well. In this paper, we develop a graph-mining-based fraudulent phone call detection framework for a mobile application to automatically annotate fraudulent phone numbers with a ""fraud"" tag, which is a crucial prerequisite for distinguishing fraudulent phone calls from normal phone calls. Our detection approach performs a weighted HITS algorithm to learn the trust value of a remote phone number. Based on telecommunication records, we build two kinds of directed bipartite graph: i) CPG and ii) UPG to represent telecommunication behavior of users. To weight the edges of CPG and UPG, we extract features for each pair of user and remote phone number in two different yet complementary aspects: 1) duration relatedness (DR) between user and phone number; and 2) frequency relatedness (FR) between user and phone number. Upon weighted CPG and UPG, we determine a trust value for each remote phone number. Finally, we conduct a comprehensive experimental study based on a dataset collected through an anti-fraud mobile application, Whoscall. The results demonstrate the effectiveness of our weighted HITS-based approach and show the strength of taking both DR and FR into account in feature extraction.",2015,Knowledge Discovery and Data Mining,Fields of study: internet privacydata mining
Graph-Based User Behavior Modeling: From Prediction to Fraud Detection,Alex Beutel (Carnegie Mellon University)Leman Akoglu (Stony Brook University)Christos Faloutsos (Carnegie Mellon University),"2045447989,2288278917,2198983026","How can we model users' preferences? How do anomalies, fraud, and spam effect our models of normal users? How can we modify our models to catch fraudsters? In this tutorial we will answer these questions - connecting graph analysis tools for user behavior modeling to anomaly and fraud detection. In particular, we will focus on the application of subgraph analysis, label propagation, and latent factor models to static, evolving, and attributed graphs. For each of these techniques we will give a brief explanation of the algorithms and the intuition behind them. We will then give examples of recent research using the techniques to model, understand and predict normal behavior. With this intuition for how these methods are applied to graphs and user behavior, we will focus on state-of-the-art research showing how the outcomes of these methods are effected by fraud, and how they have been used to catch fraudsters.",2015,Knowledge Discovery and Data Mining,Fields of study: anomaly detectiondata sciencecomputer securitydata miningmachine learningcomputer science
Collective Spammer Detection in Evolving Multi-Relational Social Networks,"Shobeir Fakhraei (University of Maryland, College Park)James R. Foulds (University of California, Santa Cruz)Madhusudana V. S. Shashanka (Boston University)Lise Getoor (University of California, Santa Cruz)","2167285659,2170533113,2045336667,1984940772","Detecting unsolicited content and the spammers who create it is a long-standing challenge that affects all of us on a daily basis. The recent growth of richly-structured social networks has provided new challenges and opportunities in the spam detection landscape. Motivated by the Tagged.com social network, we develop methods to identify spammers in evolving multi-relational social networks. We model a social network as a time-stamped multi-relational graph where vertices represent users, and edges represent different activities between them. To identify spammer accounts, our approach makes use of structural features, sequence modelling, and collective reasoning. We leverage relational sequence information using k-gram features and probabilistic modelling with a mixture of Markov models. Furthermore, in order to perform collective reasoning and improve the predictive power of a noisy abuse reporting system, we develop a statistical relational model using hinge-loss Markov random fields (HL-MRFs), a class of probabilistic graphical models which are highly scalable. We use Graphlab Create and Probabilistic Soft Logic (PSL) to prototype and experimentally evaluate our solutions on internet-scale data from Tagged.com. Our experiments demonstrate the effectiveness of our approach, and show that models which incorporate the multi-relational nature of the social network significantly gain predictive performance over those that do not.",2015,Knowledge Discovery and Data Mining,Fields of study: social spamspamheterogeneous networksequential pattern miningsocial networkdata miningartificial intelligencemachine learningcomputer science
Cuckoo Linear Algebra,Li Zhou (Carnegie Mellon University)David G. Andersen (Carnegie Mellon University)Mu Li (Carnegie Mellon University)Alexander J. Smola (Carnegie Mellon University),"2698299682,2130397481,2658285018,1972291593","In this paper we present a novel data structure for sparse vectors based on Cuckoo hashing. It is highly memory efficient and allows for random access at near dense vector level rates. This allows us to solve sparse l 1 programming problems exactly and without preprocessing at a cost that is identical to dense linear algebra both in terms of memory and speed. Our approach provides a feasible alternative to the hash kernel and it excels whenever exact solutions are required, such as for feature selection.",2015,Knowledge Discovery and Data Mining,Fields of study: k independent hashinghopscotch hashingfeature hashingcuckoo hashinglinear hashinghash functionhash tablelinear modeltheoretical computer sciencemachine learningmathematical optimizationstatisticscomputer sciencemathematics
"Non-exhaustive, Overlapping Clustering via Low-Rank Semidefinite Programming",Yangyang Hou (Purdue University)Joyce Jiyoung Whang (University of Texas at Austin)David F. Gleich (Purdue University)Inderjit S. Dhillon (University of Texas at Austin),"2114601453,2086498169,2148810670,2033403132","Clustering is one of the most fundamental tasks in data mining. To analyze complex real-world data emerging in many data-centric applications, the problem of non-exhaustive, overlapping clustering has been studied where the goal is to find overlapping clusters and also detect outliers simultaneously. We propose a novel convex semidefinite program (SDP) as a relaxation of the non-exhaustive, overlapping clustering problem. Although the SDP formulation enjoys attractive theoretical properties with respect to global optimization, it is computationally intractable for large problem sizes. As an alternative, we optimize a low-rank factorization of the solution. The resulting problem is non-convex, but has a smaller number of solution variables. We construct an optimization solver using an augmented Lagrangian methodology that enables us to deal with problems with tens of thousands of data points. The new solver provides more accurate and reliable answers than other approaches. By exploiting the connection between graph clustering objective functions and a kernel k-means objective, our new low-rank solver can also compute overlapping communities of social networks with state-of-the-art accuracy.",2015,Knowledge Discovery and Data Mining,Fields of study: correlation clusteringdata stream clusteringcure data clustering algorithmcluster analysissemidefinite programmingcombinatoricsmachine learningmathematical optimizationcomputer sciencemathematics
Dynamic Topic Modeling for Monitoring Market Competition from Online Text and Image Data,Hao Zhang (Carnegie Mellon University)Gunhee Kim (Seoul National University)Eric P. Xing (Carnegie Mellon University),"2667793552,2210743192,351197510","We propose a dynamic topic model for monitoring temporal evolution of market competition by jointly leveraging tweets and their associated images. For a market of interest ( e.g. luxury goods ), we aim at automatically detecting the latent topics ( e.g. bags, clothes, luxurious ) that are competitively shared by multiple brands ( e.g. Burberry, Prada, and Chanel ), and tracking temporal evolution of the brands' stakes over the shared topics. One of key applications of our work is social media monitoring that can provide companies with temporal summaries of highly overlapped or discriminative topics with their major competitors. We design our model to correctly address three major challenges: multiview representation of text and images, modeling of competitiveness of multiple brands over shared topics, and tracking their temporal evolution. As far as we know, no previous model can satisfy all the three challenges. For evaluation, we analyze about 10 millions of tweets and 8 millions of associated images of the 23 brands in the two categories of luxury and beer. Through experiments, we show that the proposed approach is more successful than other candidate methods for the topic modeling of competition. We also quantitatively demonstrate the generalization power of the proposed method for three prediction tasks.",2015,Knowledge Discovery and Data Mining,Fields of study: dynamic topic modeldata miningmachine learningsimulation
One-Pass Ranking Models for Low-Latency Product Recommendations,Antonino Freno (University of Siena)Martin Saveski (Massachusetts Institute of Technology)Rodolphe Jenatton (Amazon.com)Cédric Archambeau (Amazon.com),"231058085,278134592,90224947,2006974593","Purchase logs collected in e-commerce platforms provide rich information about customer preferences. These logs can be leveraged to improve the quality of product recommendations by feeding them to machine-learned ranking models. However, a variety of deployment constraints limit the naive applicability of machine learning to this problem. First, the amount and the dimensionality of the data make in-memory learning simply not possible. Second, the drift of customers' preference over time require to retrain the ranking model regularly with freshly collected data. This limits the time that is available for training to prohibitively short intervals. Third, ranking in real-time is necessary whenever the query complexity prevents us from caching the predictions. This constraint requires to minimize prediction time (or equivalently maximize the data throughput), which in turn may prevent us from achieving the accuracy necessary in web-scale industrial applications. In this paper, we investigate how the practical challenges faced in this setting can be tackled via an online learning to rank approach. Sparse models will be the key to reduce prediction latency, whereas one-pass stochastic optimization will minimize the training time and restrict the memory footprint. Interestingly, and perhaps surprisingly, extensive experiments show that one-pass learning preserves most of the predictive performance. Additionally, we study a variety of online learning algorithms that enforce sparsity and provide insights to help the practitioner make an informed decision about which approach to pick. We report results on a massive purchase log dataset from the Amazon retail website, as well as on several benchmarks from the LETOR corpus.",2015,Knowledge Discovery and Data Mining,Fields of study: online machine learningrecommender systemdata sciencedata miningmachine learningstatisticscomputer science
Subspace Clustering Using Log-determinant Rank Approximation,Chong Peng (Southern Illinois University Carbondale)Zhao Kang (Southern Illinois University Carbondale)Huiqing Li (Southern Illinois University Carbondale)Qiang Cheng (Southern Illinois University Carbondale),"2104710458,2162933773,2239208029,2102340508","A number of machine learning and computer vision problems, such as matrix completion and subspace clustering, require a matrix to be of low-rank. To meet this requirement, most existing methods use the nuclear norm as a convex proxy of the rank function and minimize it. However, the nuclear norm simply adds all nonzero singular values together instead of treating them equally as the rank function does, which may not be a good rank approximation when some singular values are very large. To reduce this undesirable weighting effect, we use a log-determinant function as a non-convex rank approximation which reduces the contributions of large singular values while keeping those of small singular values close to zero. We apply the method of augmented Lagrangian multipliers to optimize this non-convex rank approximation-based objective function and obtain closed-form solutions for all subproblems of minimizing different variables alternatively. The log-determinant low-rank optimization method is used to solve subspace clustering problem, for which we construct an affinity matrix based on the angular information of the low-rank representation to enhance its separability property. Extensive experimental results on face clustering and motion segmentation data demonstrate the effectiveness of the proposed method.",2015,Knowledge Discovery and Data Mining,Fields of study: correlation clusteringmatrix normlow rank approximationrankdiscrete mathematicscombinatoricsmathematical optimizationmathematics
When-To-Post on Social Networks,Nemanja SpasojevicZhisheng LiAdithya RaoPrantik Bhattacharyya,"2225995881,2526240076,2226989131,2637719428","For many users on social networks, one of the goals when broadcasting content is to reach a large audience. The probability of receiving reactions to a message differs for each user and depends on various factors, such as location, daily and weekly behavior patterns and the visibility of the message. While previous work has focused on overall network dynamics and message flow cascades, the problem of recommending personalized posting times has remained an under-explored topic of research. In this study, we formulate a when-to-post problem, where the objective is to find the best times for a user to post on social networks in order to maximize the probability of audience responses. To understand the complexity of the problem, we examine user behavior in terms of post-to-reaction times, and compare cross-network and cross-city weekly reaction behavior for users in different cities, on both Twitter and Facebook. We perform this analysis on over a billion posted messages and observed reactions, and propose multiple approaches for generating personalized posting schedules. We empirically assess these schedules on a sampled user set of 0.5 million active users and more than 25 million messages observed over a 56 day period. We show that users see a reaction gain of up to 17% on Facebook and 4% on Twitter when the recommended posting times are used. We open the dataset used in this study, which includes timestamps for over 144 million posts and over 1.1 billion reactions. The personalized schedules derived here are used in a fully deployed production system to recommend posting times for millions of users every day.",2015,Knowledge Discovery and Data Mining,Fields of study: personalizationuser modelingmultimediaworld wide webcomputer science
Tornado Forecasting with Multiple Markov Boundaries,Kui Yu (Simon Fraser University)Dawei Wang (University of Massachusetts Boston)Wei Ding (University of Massachusetts Boston)Jian Pei (Simon Fraser University)David L. Small (Tufts University)Shafiqul Islam (Tufts University)Xindong Wu (Hefei University of Technology),"2567944696,2308533282,2116574971,2126330539,2422163516,2160149565,2123651450","Reliable tornado forecasting with a long-lead time can greatly support emergency response and is of vital importance for the economy and society. The large number of meteorological variables in spatiotemporal domains and the complex relationships among variables remain the top difficulties for a long-lead tornado forecasting. Standard data mining approaches to tackle high dimensionality are usually designed to discover a single set of features without alternating options for domain scientists to select more reliable and physical interpretable variables. In this work, we provide a new solution to use the concept of multiple Markov boundaries in local causal discovery to identify multiple sets of the precursors for tornado forecasting. Specifically, our algorithm first confines the extremely large feature spaces to a small core feature space, then it mines multiple sets of the precursors from the core feature space that may equally contribute to tornado forecasting. With the multiple sets of the precursors, we are able to report to domain scientists the predictive but practical set of precursors. An extensive empirical study is conducted on eight benchmark data sets and the historical tornado data near Oklahoma City, OK in the United States. Experimental results show that the tornado precursors we identified can help to improve the reliability of long-lead time catastrophic tornado forecasting.",2015,Knowledge Discovery and Data Mining,Fields of study: data sciencedata miningsimulation
Hierarchical Graph-Coupled HMMs for Heterogeneous Personalized Health Data,Kai Fan (Duke University)Marisa C. Eisenberg (University of Michigan)Alison Walsh (University of Michigan)Allison Aiello (University of North Carolina at Chapel Hill)Katherine A. Heller (Duke University),"2420008042,2125046471,2688659499,2115903723,2158760032","The purpose of this study is to leverage modern technology (mobile or web apps) to enrich epidemiology data and infer the transmission of disease. We develop hierarchical Graph-Coupled Hidden Markov Models (hGCHMMs) to simultaneously track the spread of infection in a small cell phone community and capture person-specific infection parameters by leveraging a link prior that incorporates additional covariates. In this paper we investigate two link functions, the beta-exponential link and sigmoid link, both of which allow the development of a principled Bayesian hierarchical framework for disease transmission. The results of our model allow us to predict the probability of infection for each persons on each day, and also to infer personal physical vulnerability and the relevant association with covariates. We demonstrate our approach theoretically and experimentally on both simulation data and real epidemiological records.",2015,Knowledge Discovery and Data Mining,Fields of study: social networkdata miningmachine learningsimulationstatistics
Predictive Approaches for Low-Cost Preventive Medicine Program in Developing Countries,Yukino Baba (National Institute of Informatics)Hisashi Kashima (Kyoto University)Yasunobu Nohara (Kyushu University)Eiko Kai (Kyushu University)Partha GhoshRafiqul IslamAshir Ahmed (Kyushu University)Masahiro Kuroda (National Institute of Information and Communications Technology)Sozo Inoue (Kyushu Institute of Technology)Tatsuo Hiramatsu (University of Tokyo)Michio Kimura (Hamamatsu University School of Medicine)Shuji Shimizu (Kyushu University)Kunihisa Kobayashi (Fukuoka University)Koji Tsuda (University of Tokyo)Masashi Sugiyama (University of Tokyo)Mathieu Blondel (Nippon Telegraph and Telephone)Naonori Ueda (Nippon Telegraph and Telephone)Masaru Kitsuregawa (University of Tokyo)Naoki Nakashima (Kyushu University),"2554033023,2126337623,1566097246,2142813047,2158745519,2306951007,2131841761,2171640188,2132842674,2230851409,2133930268,2111952877,2100313069,1858955830,2614228957,2199253878,2147735823,390930505,1966229521","Non-communicable diseases (NCDs) are no longer just a problem for high-income countries, but they are also a problem that affects developing countries. Preventive medicine is definitely the key to combat NCDs; however, the cost of preventive programs is a critical issue affecting the popularization of these medicine programs in developing countries. In this study, we investigate predictive modeling for providing a low-cost preventive medicine program. In our two-year-long field study in Bangladesh, we collected the health checkup results of 15,075 subjects, the data of 6,607 prescriptions, and the follow-up examination results of 2,109 subjects. We address three prediction problems, namely subject risk prediction, drug recommendation, and future risk prediction, by using machine learning techniques; our multiple-classifier approach successfully reduced the costs of health checkups, a multi-task learning method provided accurate recommendation for specific types of drugs, and an active learning method achieved an efficient assignment of healthcare workers for the follow-up care of subjects.",2015,Knowledge Discovery and Data Mining,Fields of study: health carepreventive healthcaredata miningcomputer science
Online Controlled Experiments: Lessons from Running A/B/n Tests for 12 Years,Ron Kohavi (Microsoft),73615348,"The Internet provides developers of connected software, including web sites, applications, and devices, an unprecedented opportunity to accelerate innovation by evaluating ideas quickly and accurately using trustworthy controlled experiments (e.g., A/B tests and their generalizations). From front-end user-interface changes to backend recommendation systems and relevance algorithms, from search engines (e.g., Google, Microsoft's Bing, Yahoo) to retailers (e.g., Amazon, eBay, Netflix, Etsy) to social networking services (e.g., Facebook, LinkedIn, Twitter) to Travel services (e.g., Expedia, Airbnb, Booking.com) to many startups, online controlled experiments are now utilized to make data-driven decisions at a wide range of companies. While the theory of a controlled experiment is simple, and dates back to Sir Ronald A. Fisher's experiments at the Rothamsted Agricultural Experimental Station in England in the 1920s, the deployment and mining of online controlled experiments at scale (e.g., hundreds of experiments run every day at Bing) and deployment of online controlled experiments across dozens of web sites and applications has taught us many lessons. We provide an introduction, share real examples, key lessons, and cultural challenges.",2015,Knowledge Discovery and Data Mining,Fields of study: world wide webdata miningsimulationcomputer science
Efficient Latent Link Recommendation in Signed Networks,"Dongjin Song (University of California, San Diego)David A. Meyer (University of California, San Diego)Dacheng Tao (University of Technology, Sydney)","2114426781,2176037993,2104129307","Signed networks, in which the relationship between two nodes can be either positive (indicating a relationship such as trust) or negative (indicating a relationship such as distrust), are becoming increasingly common. A plausible model for user behavior analytics in signed networks can be based upon the assumption that more extreme positive and negative relationships are explored and exploited before less extreme ones. Such a model implies that a personalized ranking list of latent links should place positive links on the top, negative links at the bottom, and unknown status links in between. Traditional ranking metrics, e.g., area under the receiver operating characteristic curve (AUC), are however not suitable for quantifying such a ranking list which includes positive, negative, and unknown status links. To address this issue, a generalized AUC (GAUC) which can measure both the head and tail of a ranking list has been introduced. Since GAUC weights each pairwise comparison equally and the calculation of GAUC requires quadratic time, we derive two lower bounds of GAUC which can be computed in linear time and put more emphasis on ranking positive links on the top and negative links at the bottom of a ranking list. Next, we develop two efficient latent link recommendation (ELLR) algorithms in order to recommend links by directly optimizing these two lower bounds, respectively. Finally, we compare these two ELLR algorithms with top-performing baseline methods over four benchmark datasets, among which the largest network has more than 100 thousand nodes and seven million entries. Thorough empirical studies demonstrate that the proposed ELLR algorithms outperform state-of-the-art approaches for link recommendation in signed networks at no cost in efficiency.",2015,Knowledge Discovery and Data Mining,Fields of study: recommender systemdata miningmachine learningstatisticscomputer sciencemathematics
A Clustering-Based Framework to Control Block Sizes for Entity Resolution,Jeffrey Fisher (Australian National University)Peter Christen (Australian National University)Qing Wang (Australian National University)Erhard Rahm (Leipzig University),"2148966539,2023765750,2287519903,2158742306","Entity resolution (ER) is a common data cleaning task that involves determining which records from one or more data sets refer to the same real-world entities. Because a pairwise comparison of all records scales quadratically with the number of records in the data sets to be matched, it is common to use blocking or indexing techniques to reduce the number of comparisons required. These techniques split the data sets into blocks and only records within blocks are compared with each other. Most existing blocking techniques do not provide control over the size of the generated blocks, despite this control being important in many practical applications of ER, such as privacy-preserving record linkage and real-time ER. We propose two novel hierarchical clustering approaches which can generate blocks within a specified size range, and we present a penalty function which allows control of the trade-off between block quality and block size in the clustering process. We evaluate our techniques on three real-world data sets and compare them against three baseline approaches. The results show our proposed techniques perform well on the measures of pairs completeness and reduction ratio compared to the baseline approaches, while also satisfying the block size restrictions.",2015,Knowledge Discovery and Data Mining,Fields of study: record linkageblockingsearch engine indexingtheoretical computer sciencedata miningdatabasestatisticscomputer science
Predicting Ambulance Demand: a Spatio-Temporal Kernel Approach,Zhengyi Zhou (Cornell University)David S. Matteson (Cornell University),"2111365928,2132804960","Predicting ambulance demand accurately at fine time and location scales is critical for ambulance fleet management and dynamic deployment. Large-scale datasets in this setting typically exhibit complex spatio-temporal dynamics and sparsity at high resolutions. We propose a predictive method using spatio-temporal kernel density estimation (stKDE) to address these challenges, and provide spatial density predictions for ambulance demand in Toronto, Canada as it varies over hourly intervals. Specifically, we weight the spatial kernel of each historical observation by its informativeness to the current predictive task. We construct spatio-temporal weight functions to incorporate various temporal and spatial patterns in ambulance demand, including location-specific seasonalities and short-term serial dependence. This allows us to draw out the most helpful historical data, and exploit spatio-temporal patterns in the data for accurate and fast predictions. We further provide efficient estimation and customizable prediction procedures. stKDE is easy to use and interpret by non-specialized personnel from the emergency medical service industry. It also has significantly higher statistical accuracy than the current industry practice, with a comparable amount of computational expense.",2015,Knowledge Discovery and Data Mining,Fields of study: kernel density estimationdata miningsimulationstatisticscomputer sciencemathematics
Smart Pacing for Effective Online Ad Campaign Optimization,Jian Xu (Yahoo!)Kuang-chih Lee (Yahoo!)Wentong Li (Yahoo!)Hang Qi (Yahoo!)Quan Lu (Yahoo!),"2430499991,2696605660,2170430983,2573513874,2238232786","In targeted online advertising, advertisers look for maximizing campaign performance under delivery constraint within budget schedule. Most of the advertisers typically prefer to impose the delivery constraint to spend budget smoothly over the time in order to reach a wider range of audiences and have a sustainable impact. Since lots of impressions are traded through public auctions for online advertising today, the liquidity makes price elasticity and bid landscape between demand and supply change quite dynamically. Therefore, it is challenging to perform smooth pacing control and maximize campaign performance simultaneously. In this paper, we propose a smart pacing approach in which the delivery pace of each campaign is learned from both offline and online data to achieve smooth delivery and optimal performance goals. The implementation of the proposed approach in a real DSP system is also presented. Experimental evaluations on both real online ad campaigns and offline simulations show that our approach can effectively improve campaign performance and achieve delivery goals.",2015,Knowledge Discovery and Data Mining,Fields of study: simulation
MOOCS: What Have We Learned?,Daphne Koller (Stanford University),2167404190,"It has been nearly four years since the first MOOCs (massive open online courses) were offered by Stanford University. MOOCs are now offered to tens of millions of learners worldwide, by hundreds of top universities. MOOCs are no longer an experiment - the learning, reach, and value they offer are now a reality. I will show how MOOCs provide opportunities for open-ended projects, intercultural learner interactions, and collaborative learning. I will discuss some of data that we are collecting from MOOCs, and what we are learning from these data about both courses and learners. Finally, I will discuss both data and examples of the kind of transformative impact that can be derived from providing millions of people with access to the world's best education.",2015,Knowledge Discovery and Data Mining,Fields of study: multimediaworld wide websimulationcomputer science
Efficient Algorithms for Public-Private Social Networks,Flavio Chierichetti (Sapienza University of Rome)Alessandro Epasto (Brown University)Ravi Kumar 0001 (Google)Silvio Lattanzi (Google)Vahab S. Mirrokni (Google),"2082432826,2303398780,2232709231,1989808900,2331823467","We introduce the public-private model of graphs. In this model, we have a public graph and each node in the public graph has an associated private graph. The motivation for studying this model stems from social networks, where the nodes are the users, the public graph is visible to everyone, and the private graph at each node is visible only to the user at the node. From each node's viewpoint, the graph is just a union of its private graph and the public graph. We consider the problem of efficiently computing various properties of the graphs from each node's point of view, with minimal amount of recomputation on the public graph. To illustrate the richness of our model, we explore two powerful computational paradigms for studying large graphs, namely, sketching and sampling, and focus on some key problems in social networks and show efficient algorithms in the public-private graph model. In the sketching model, we show how to efficiently approximate the neighborhood function, which in turn can be used to approximate various notions of centrality. In the sampling model, we focus on all-pair shortest path distances, node similarities, and correlation clustering.",2015,Knowledge Discovery and Data Mining,Fields of study: distance hereditary graphsimplex graphvoltage graphcomplement graphgraph bandwidthgraph powerlattice graphnull graphclique widthgraph propertygraphprivacysocial networktheoretical computer sciencecombinatoricsmachine learningcomputer sciencemathematics
Dense Subgraph Discovery: KDD 2015 tutorial,Aristides Gionis (Aalto University)Charalampos E. Tsourakakis (Harvard University),"737311942,750472553","Finding dense subgraphs is a fundamental graph-theoretic problem, that lies in the heart of numerous graph-mining applications, ranging from finding communities in social networks, to detecting regulatory motifs in DNA, and to identifying real-time stories in news. The problem of finding dense subgraphs has been studied extensively in theoretical computer science, and recently, due to the relevance of the problem in real-world applications, it has attracted considerable attention in the data-mining community. In this tutorial we aim to provide a comprehensive overview of (i) major algorithmic techniques for finding dense subgraphs in large graphs and (ii) graph mining applications that rely on dense subgraph extraction. We will present fundamental concepts and algorithms that date back to 80's, as well as the latest advances in the area, from theoretical and from practical point-of-view. We will motivate the problem of finding dense subgraphs by discussing how it can be used in real-world applications. We will discuss different density definitions and the complexity of the corresponding optimization problems. We will also present efficient algorithms for different density measures and under different computational models. Specifically, we will focus on scalable streaming, distributed and MapReduce algorithms. Finally we will discuss problem variants, extensions, and will provide pointers for future research directions.",2015,Knowledge Discovery and Data Mining,Fields of study: theoretical computer sciencebioinformaticsdata miningmachine learningcomputer science
An Effective Marketing Strategy for Revenue Maximization with a Quantity Constraint,Ya Wen Teng (National Taiwan University)Chih Hua Tai (National Taipei University)Philip S. Yu (University of Illinois at Chicago)Ming Syan Chen (National Taiwan University),"2138742337,2109327166,2125104194,2122365371","Recently the influence maximization problem has received much attention for its applications on viral marketing and product promotions. However, such influence maximization problems have not taken into account the monetary effect on the purchasing decision of individuals. To fulfill this gap, in this paper, we aim for maximizing the revenue by considering the quantity constraint on the promoted commodity. For this problem, we not only identify a proper small group of individuals as seeds for promotion but also determine the pricing of the commodity. To tackle the revenue maximization problem, we first introduce a strategic searching algorithm, referred to as Algorithm PRUB, which is able to derive the optimal solutions. After that, we further modify PRUB to propose a heuristic, Algorithm PRUB+IF, for obtaining feasible solutions more efficiently on larger instances. Experiments on real social networks with different valuation distributions demonstrate the effectiveness of PRUB and PRUB+IF.",2015,Knowledge Discovery and Data Mining,Fields of study: social influencepricing
Data Driven Science: SIGKDD Panel,Katharina MorikHugh Durrant-Whyte (University of Sydney)Gary Hill (University of Adelaide)Dietmar Müller (University of Sydney)Tanya Y. Berger-Wolf (University of Illinois at Chicago),"2696892498,2167801844,2509401935,2126035492,107058130","The panel session 'Data Driven Science' discusses application and use of knowledge discovery, machine learning and data analytics in science disciplines; in natural, physical, medical and social science; from physics to geology, and from neuroscience to population health. Knowledge discovery methods are finding broad application in all areas of scientific endeavor, to explore experimental data, to discover new models, to propose new scientific theories and ideas. In addition, the availability of ever larger scientific data sets is driving a new data-driven paradigm for modeling of complex phenomena in physical, natural and social sciences. The purpose of this panel is to bring together users of knowledge discovery, machine learning and data analytics methods across the science disciplines, to understand what tools and methods are proving effective in areas such as data exploration and modeling, to uncover common problems that can be addressed in the KDD community, and to explore the emerging data-driven paradigm in science.",2015,Knowledge Discovery and Data Mining,Fields of study: discovery scienceknowledge extractiondata sciencebioinformaticsdata miningcomputer science
Fast and Memory-Efficient Significant Pattern Mining via Permutation Testing,Felipe Llinares-López (ETH Zurich)Mahito Sugiyama (Osaka University)Laetitia Papaxanthos (ETH Zurich)Karsten M. Borgwardt (ETH Zurich),"657890086,2267106725,258061352,2634700092","We present a novel algorithm for significant pattern mining, Westfall-Young light. The target patterns are statistically significantly enriched in one of two classes of objects. Our method corrects for multiple hypothesis testing and correlations between patterns via the Westfall-Young permutation procedure, which empirically estimates the null distribution of pattern frequencies in each class via permutations. In our experiments, Westfall-Young light dramatically outperforms the current state-of-the-art approach, both in terms of runtime and memory efficiency on popular real-world benchmark datasets for pattern mining. The key to this efficiency is that, unlike all existing methods, our algorithm does not need to solve the underlying frequent pattern mining problem anew for each permutation and does not need to store the occurrence list of all frequent patterns. Westfall-Young light opens the door to significant pattern mining on large datasets that previously involved prohibitive runtime or memory costs. Our code is available from http://www.bsse.ethz.ch/mlcb/research/machine-learning/wylight.html",2015,Knowledge Discovery and Data Mining,Fields of study: p valuemultiple comparisons problemdata miningmachine learningstatisticsalgorithmmathematics
Towards Decision Support and Goal Achievement: Identifying Action-Outcome Relationships From Social Media,Emre Kıcıman (Microsoft)Matthew Richardson (Microsoft),"1994052019,2131647568","Every day, people take actions, trying to achieve their personal, high-order goals. People decide what actions to take based on their personal experience, knowledge and gut instinct. While this leads to positive outcomes for some people, many others do not have the necessary experience, knowledge and instinct to make good decisions. What if, rather than making decisions based solely on their own personal experience, people could take advantage of the reported experiences of hundreds of millions of other people? In this paper, we investigate the feasibility of mining the relationship between actions and their outcomes from the aggregated timelines of individuals posting experiential microblog reports. Our contributions include an architecture for extracting action-outcome relationships from social media data, techniques for identifying experiential social media messages and converting them to event timelines, and an analysis and evaluation of action-outcome extraction in case studies.",2015,Knowledge Discovery and Data Mining,Fields of study: social mediaknowledge baseknowledge managementdata miningcomputer science
An Evaluation of Parallel Eccentricity Estimation Algorithms on Undirected Real-World Graphs,Julian Shun (Carnegie Mellon University),2644778208,"This paper presents efficient shared-memory parallel implementations and the first comprehensive experimental study of graph eccentricity estimation algorithms in the literature. The implementations include (1) a simple algorithm based on executing two-pass breadth-first searches from a sample of vertices, (2) algorithms with sub-quadratic worst-case running time for sparse graphs and non-trivial approximation guarantees that execute breadth-first searches from a carefully chosen set of vertices, (3) algorithms based on probabilistic counters, and (4) a well-known 2-approximation algorithm that executes one breadth-first search per connected component. Our experiments on large undirected real-world graphs show that the algorithm based on two-pass breadth-first searches works surprisingly well, outperforming the other algorithms in terms of running time and/or accuracy by up to orders of magnitude. The high accuracy, efficiency, and parallelism of our best implementation allows the fast generation of eccentricity estimates for large graphs, which are useful in many applications arising in large-scale network analysis.",2015,Knowledge Discovery and Data Mining,Fields of study: implicit graphfolded cube graphindifference graphhopcroft karp algorithm1 planar graphgraph powercomparability graphblock graphclique problemlevel structurepseudoforestdistancegraph productmodular decompositionpathwidthline graphchordal graphgraphindependent setparallel algorithmapproximationestimationtheoretical computer sciencediscrete mathematicscombinatoricsstatisticsmathematics
Beyond Triangles: A Distributed Framework for Estimating 3-profiles of Large Graphs,Ethan R. Elenberg (University of Texas at Austin)Karthikeyan Shanmugam (University of Texas at Austin)Michael Borokhovich (University of Texas at Austin)Alexandros G. Dimakis (University of Texas at Austin),"2056322677,2467444355,47453084,1253426111","We study the problem of approximating the 3-profile of a large graph. 3-profiles are generalizations of triangle counts that specify the number of times a small graph appears as an induced subgraph of a large graph. Our algorithm uses the novel concept of 3-profile sparsifiers: sparse graphs that can be used to approximate the full 3-profile counts for a given large graph. Further, we study the problem of estimating local and ego 3-profiles, two graph quantities that characterize the local neighborhood of each vertex of a graph. Our algorithm is distributed and operates as a vertex program over the GraphLab PowerGraph framework. We introduce the concept of edge pivoting which allows us to collect 2-hop information without maintaining an explicit 2-hop neighborhood list at each vertex. This enables the computation of all the local 3-profiles in parallel with minimal communication. We test our implementation in several experiments scaling up to 640 cores on Amazon EC2. We find that our algorithm can estimate the 3-profile of a graph in approximately the same time as triangle counting. For the harder problem of ego 3-profiles, we introduce an algorithm that can estimate profiles of hundreds of thousands of vertices in parallel, in the timescale of minutes.",2015,Knowledge Discovery and Data Mining,Fields of study: factor critical graphdistance hereditary graphwindmill graphsimplex graphvoltage graphcomplement graphgraph bandwidthgraph powergraph factorizationbutterfly graphstring graphnull graphcirculant graphcubic graphcycle graphline graphvertexloopregular graphdegreetheoretical computer sciencediscrete mathematicscombinatoricsmachine learningmathematics
Unsupervised Feature Selection with Adaptive Structure Learning,Liang Du (Chinese Academy of Sciences)Yi-Dong Shen (Chinese Academy of Sciences),"1909139773,2129952813","The problem of feature selection has raised considerable interests in the past decade. Traditional unsupervised methods select the features which can faithfully preserve the intrinsic structures of data, where the intrinsic structures are estimated using all the input features of data. However, the estimated intrinsic structures are unreliable/inaccurate when the redundant and noisy features are not removed. Therefore, we face a dilemma here: one need the true structures of data to identify the informative features, and one need the informative features to accurately estimate the true structures of data. To address this, we propose a unified learning framework which performs structure learning and feature selection simultaneously. The structures are adaptively learned from the results of feature selection, and the informative features are reselected to preserve the refined structures of data. By leveraging the interactions between these two essential tasks, we are able to capture accurate structures and select more informative features. Experimental results on many benchmark data sets demonstrate that the proposed method outperforms many state of the art unsupervised feature selection methods.",2015,Knowledge Discovery and Data Mining,Fields of study: featuredata miningpattern recognitionmachine learningcomputer science
Probabilistic Community and Role Model for Social Networks,Yu Han (Tsinghua University)Jie Tang (Tsinghua University),"2669417999,2158012360","Numerous models have been proposed for modeling social networks to explore their structure or to address application problems, such as community detection and behavior prediction. However, the results are still far from satisfactory. One of the biggest challenges is how to capture all the information of a social network in a unified manner, such as links, communities, user attributes, roles and behaviors. In this paper, we propose a unified probabilistic framework, the Community Role Model (CRM), to model a social network. CRM incorporates all the information of nodes and edges that form a social network. We propose methods based on Gibbs sampling and an EM algorithm to estimate the model's parameters and fit our model to real social networks. Real data experiments show that CRM can be used not only to represent a social network, but also to handle various application problems with better performance than a baseline model, without any modification to the model, showing its great advantages.",2015,Knowledge Discovery and Data Mining,Fields of study: evolving networksdynamic network analysissocial networkcommunitydata sciencedata miningartificial intelligencemachine learning
Interpreting Advertiser Intent in Sponsored Search,"Bhanu Chandra Vattikonda (University of California, San Diego)Santhosh Kodipaka (Microsoft)Hongyan Zhou (Microsoft)Vacha Dave (Microsoft)Saikat Guha (Microsoft)Alex C. Snoeren (University of California, San Diego)","2050376626,2229589597,2632843007,2692960191,2264163384,2112598219","Search engines derive revenue by displaying sponsored results along with organic results in response to user queries. In general, search engines run a per-query, on-line auction amongst interested advertisers to select sponsored results to display. In doing so, they must carefully balance the revenue derived from sponsored results against potential degradation in user experience due to less-relevant results. Hence, major search engines attempt to analyze the relevance of potential sponsored results to the user's query using supervised learning algorithms. Past work has employed a bag-of-words approach using features extracted from both the query and potential sponsored result to train the ranker. We show that using features that capture the advertiser's intent can significantly improve the performance of relevance ranking. In particular, we consider the ad keyword the advertiser submits as part of the auction process as a direct expression of intent. We leverage the search engine itself to interpret the ad keyword by submitting the ad keyword as an independent query and incorporating the results as features when determining the relevance of the advertiser's sponsored result to the user's original query. We achieve 43.2% improvement in precision-recall AUC over the best previously published baseline and 2.7% improvement in the production system of a large search engine.",2015,Knowledge Discovery and Data Mining,Fields of study: online advertisingworld wide webinformation retrievaldata miningcomputer science
Simultaneous Discovery of Common and Discriminative Topics via Joint Nonnegative Matrix Factorization,Hannah Kim (Georgia Institute of Technology)Jaegul Choo (Korea University)Jingu Kim (Netflix)Chandan K. Reddy (Wayne State University)Haesun Park (Georgia Institute of Technology),"2128479451,2148380128,2171287728,2100435683,2123241397","Understanding large-scale document collections in an efficient manner is an important problem. Usually, document data are associated with other information (e.g., an author's gender, age, and location) and their links to other entities (e.g., co-authorship and citation networks). For the analysis of such data, we often have to reveal common as well as discriminative characteristics of documents with respect to their associated information, e.g., male- vs. female-authored documents, old vs. new documents, etc. To address such needs, this paper presents a novel topic modeling method based on joint nonnegative matrix factorization, which simultaneously discovers common as well as discriminative topics given multiple document sets. Our approach is based on a block-coordinate descent framework and is capable of utilizing only the most representative, thus meaningful, keywords in each topic through a novel pseudo-deflation approach. We perform both quantitative and qualitative evaluations using synthetic as well as real-world document data sets such as research paper collections and nonprofit micro-finance data. We show our method has a great potential for providing in-depth analyses by clearly identifying common and discriminative topics among multiple document sets.",2015,Knowledge Discovery and Data Mining,Fields of study: topic modelnon negative matrix factorizationinformation retrievaldata miningpattern recognitionmachine learningcomputer sciencemathematics
Integrating Vertex-centric Clustering with Edge-centric Clustering for Meta Path Graph Analysis,Yang Zhou (Georgia Institute of Technology)Ling Liu (Georgia Institute of Technology)David Buttler (Lawrence Livermore National Laboratory),"2625264668,2125988131,2057999953","Meta paths are good mechanisms to improve the quality of graph analysis on heterogeneous information networks. This paper presents a meta path graph clustering framework, VEPATHCLUSTER, that combines meta path vertex-centric clustering with meta path edge-centric clustering for improving the clustering quality of heterogeneous networks. First, we propose an edge-centric path graph model to capture the meta-path dependencies between pairwise path edges. We model a heterogeneous network containing M types of meta paths as M vertex-centric path graphs and M edge-centric path graphs. Second, we propose a clustering-based multigraph model to capture the fine-grained clustering-based relationships between pairwise vertices and between pairwise path edges. We perform clustering analysis on both a unified vertex-centric path graph and each edge-centric path graph to generate vertex clustering and edge clusterings of the original heterogeneous network respectively. Third, a reinforcement algorithm is provided to tightly integrate vertex-centric clustering and edge-centric clustering by mutually enhancing each other. Finally, an iterative learning strategy is presented to dynamically refine both vertex-centric clustering and edge-centric clustering by continuously learning the contributions and adjusting the weights of different path graphs.",2015,Knowledge Discovery and Data Mining,Fields of study: any angle path planningflame clusteringk medians clusteringbrown clusteringcanopy clustering algorithmwidest path problemcomplete linkage clusteringcorrelation clusteringconstrained clusteringdata stream clusteringpathcure data clustering algorithmsingle linkage clusteringlongest path problemclustering coefficientfuzzy clusteringclustering high dimensional datashortest path problemcluster analysisdiscrete mathematicscombinatoricsmachine learningmathematics
Instance Weighting for Patient-Specific Risk Stratification Models,Jen J. Gong (Massachusetts Institute of Technology)Thoralf M. Sundt (Harvard University)James D. Rawn (Brigham and Women's Hospital)John V. Guttag (Massachusetts Institute of Technology),"2722871259,2120612052,2051009429,120651226","Accurate risk models for adverse outcomes can provide important input to clinical decision-making. Surprisingly, one of the main challenges when using machine learning to build clinically useful risk models is the small amount of data available. Risk models need to be developed for specific patient populations, specific institutions, specific procedures, and specific outcomes. With each exclusion criterion, the amount of relevant training data decreases, until there is often an insufficient amount to learn an accurate model. This difficulty is compounded by the large class imbalance that is often present in medical applications. In this paper, we present an approach to address the problem of small data using transfer learning methods in the context of developing risk models for cardiac surgeries. We explore ways to build surgery-specific and hospital-specific models (the target task) using information from other kinds of surgeries and other hospitals (source tasks). We propose a novel method to weight examples based on their similarity to the target task training examples to take advantage of the useful examples while discounting less relevant ones. We show that incorporating appropriate source data in training can lead to improved performance over using only target task training data, and that our method of instance weighting can lead to further improvements. Applied to a surgical risk stratification task, our method, which used data from two institutions, performed comparably to the risk model published by the Society for Thoracic Surgeons, which was developed and tested on over one hundred thousand surgeries from hundreds of institutions.",2015,Knowledge Discovery and Data Mining,Fields of study: transfer of learningdata miningmachine learningsimulationstatisticscomputer science
A Learning-based Framework to Handle Multi-round Multi-party Influence Maximization on Social Networks,Su-Chen Lin (National Taiwan University)Shou-De Lin (National Taiwan University)Ming-Syan Chen (National Taiwan University),"2112185148,2114357324,2122365371","Considering nowadays companies providing similar products or services compete with each other for resources and customers, this work proposes a learning-based framework to tackle the multi-round competitive influence maximization problem on a social network. We propose a data-driven model leveraging the concept of meta-learning to maximize the expected influence in the long run. Our model considers not only the network information but also the opponent's strategy while making a decision. It maximizes the total influence in the end of the process instead of myopically pursuing short term gain. We propose solutions for scenarios when the opponent's strategy is known or unknown and available or unavailable for training. We also show how an effective framework can be trained without manually labeled data, and conduct several experiments to verify the effectiveness of the whole process.",2015,Knowledge Discovery and Data Mining,Fields of study: social networkreinforcement learningknowledge managementdata miningartificial intelligencemachine learningcomputer science
How Artificial Intelligence and Big Data Created Rocket Fuel: A Case Study,George John (Stanford University),2140179465,"In 2008, Rocket Fuel's founders saw a gap in the digital advertising market. None of the existing players were building autonomous systems based on big data and artificial intelligence, but instead they were offering fairly simple technology and relying on human campaign managers to drive success. Five years later in 2013, Rocket Fuel had the best technology IPO of the year on NASDAQ, reported $240 million in revenue, and was ranked by accounting firm Deloitte as the #1 fastest-growing technology company in North America. Along the way we learned that it's okay to be bold in our expectations of what is possible with fully autonomous systems, we learned that mainstream customers will buy advanced technology if it's delivered in a familiar way, and we also learned that it's incredibly difficult to debug the complex ""robot psychology"" when a number of complex autonomous systems interact. We also had excellent luck and timing: as we were building the company, real-time ad impression-level auctions with machine-to-machine buying and selling became commonplace, and marketers became increasingly focused on delivering better results for their company and delivering better personalized and relevant digital experiences for their customers. The case study presentation will present a fast-paced overview of the business and technology context for Rocket Fuel at inception and at present, key learnings and decisions, and the road ahead.",2015,Knowledge Discovery and Data Mining,Fields of study: real time biddingbig datadata miningsimulationcomputer science
Structural Graphical Lasso for Learning Mouse Brain Connectivity,"Sen Yang (Alibaba Group)Qian Sun (Arizona State University)Shuiwang Ji (Old Dominion University)Peter Wonka (King Abdullah University of Science and Technology)Ian Davidson (University of California, Davis)Jieping Ye (University of Michigan)","2617528270,2620185474,2149659377,729125449,2560595684,2305258894","Investigations into brain connectivity aim to recover networks of brain regions connected by anatomical tracts or by functional associations. The inference of brain networks has recently attracted much interest due to the increasing availability of high-resolution brain imaging data. Sparse inverse covariance estimation with lasso and group lasso penalty has been demonstrated to be a powerful approach to discover brain networks. Motivated by the hierarchical structure of the brain networks, we consider the problem of estimating a graphical model with tree-structural regularization in this paper. The regularization encourages the graphical model to exhibit a brain-like structure. Specifically, in this hierarchical structure, hundreds of thousands of voxels serve as the leaf nodes of the tree. A node in the intermediate layer represents a region formed by voxels in the subtree rooted at that node. The whole brain is considered as the root of the tree. We propose to apply the tree-structural regularized graphical model to estimate the mouse brain network. However, the dimensionality of whole-brain data, usually on the order of hundreds of thousands, poses significant computational challenges. Efficient algorithms that are capable of estimating networks from high-dimensional data are highly desired. To address the computational challenge, we develop a screening rule which can quickly identify many zero blocks in the estimated graphical model, thereby dramatically reducing the computational cost of solving the proposed model. It is based on a novel insight on the relationship between screening and the so-called proximal operator that we first establish in this paper. We perform experiments on both synthetic data and real data from the Allen Developing Mouse Brain Atlas; results demonstrate the effectiveness and efficiency of the proposed approach.",2015,Knowledge Discovery and Data Mining,Fields of study: electric field screeningdata miningartificial intelligencemachine learningstatisticscomputer science
Promoting Positive Post-Click Experience for In-Stream Yahoo Gemini Users,Mounia Lalmas (Yahoo!)Janette Lehmann (Free University of Berlin)Guy Shaked (Yahoo!)Fabrizio Silvestri (Yahoo!)Gabriele Tolomei (Yahoo!),"46148421,2164287212,2225433515,2134079936,2103603267","Click-through rate (CTR) is the most common metric used to assess the performance of an online advert; another performance of an online advert is the user post-click experience. In this paper, we describe the method we have implemented in Yahoo Gemini to measure the post-click experience on Yahoo mobile news streams via an automatic analysis of advert landing pages. We measure the post-click experience by means of two well-known metrics, dwell time and bounce rate. We show that these metrics can be used as proxy of an advert post-click experience, and that a negative post-click experience has a negative effect on user engagement and future ad clicks. We then put forward an approach that analyses advert landing pages, and show how these can affect dwell time and bounce rate. Finally, we develop a prediction model for advert quality based on dwell time, which was deployed on Yahoo mobile news stream app running on iOS. The results show that, using dwell time as a proxy of post-click experience, we can prioritise higher quality ads. We demonstrate the impact of this on users via A/B testing.",2015,Knowledge Discovery and Data Mining,Fields of study: native advertisingworld wide webdata miningsimulation
Scalable Blocking for Privacy Preserving Record Linkage,Alexandros Karakasidis (Hellenic Open University)Georgia Koloniari (University of Macedonia)Vassilios S. Verykios (Hellenic Open University),"2289470031,197949995,77221159","When dealing with sensitive and personal user data, the process of record linkage raises privacy issues. Thus, privacy preserving record linkage has emerged with the goal of identifying matching records across multiple data sources while preserving the privacy of the individuals they describe. The task is very resource demanding, considering the abundance of available data, which, in addition, are often dirty. Blocking techniques are deployed prior to matching to prune out unlikely to match candidate records so as to reduce processing time. However, when scaling to large datasets, such methods often result in quality loss. To this end, we propose Multi-Sampling Transitive Closure for Encrypted Fields (MS-TCEF), a novel privacy preserving blocking technique based on the use of reference sets. Our new method effectively prunes records based on redundant assignments to blocks, providing better fault-tolerance and maintaining result quality while scaling linearly with respect to the dataset size. We provide a theoretical analysis on the method's complexity and show how it outperforms state-of-the-art privacy preserving blocking techniques with respect to both recall and processing cost.",2015,Knowledge Discovery and Data Mining,Fields of study: performanceinternet privacyworld wide webdata miningcomputer science
Dynamic Poisson Autoregression for Influenza-Like-Illness Case Count Prediction,Zheng Wang (University of Michigan)Prithwish Chakraborty (Virginia Tech)Sumiko R. Mekaru (Boston Children's Hospital)John S. Brownstein (Boston Children's Hospital)Jieping Ye (University of Michigan)Naren Ramakrishnan (Virginia Tech),"2719260027,2121182072,1214141863,1915684033,2305258894,2199255697","Influenza-like-illness (ILI) is among of the most common diseases worldwide, and reliable forecasting of the same can have significant public health benefits. Recently, new forms of disease surveillance based upon digital data sources have been proposed and are continuing to attract attention over traditional surveillance methods. In this paper, we focus on short-term ILI case count prediction and develop a dynamic Poisson autoregressive model with exogenous inputs variables (DPARX) for flu forecasting. In this model, we allow the autoregressive model to change over time. In order to control the variation in the model, we construct a model similarity graph to specify the relationship between pairs of models at two time points and embed prior knowledge in terms of the structure of the graph. We formulate ILI case count forecasting as a convex optimization problem, whose objective balances the autoregressive loss and the model similarity regularization induced by the structure of the similarity graph. We then propose an efficient algorithm to solve this problem by block coordinate descent. We apply our model and the corresponding learning method on historical ILI records for 15 countries around the world using a variety of syndromic surveillance data sources. Our approach provides consistently better forecasting results than state-of-the-art models available for short-term ILI case count forecasting.",2015,Knowledge Discovery and Data Mining,Fields of study: star modelautoregressive modeleconometricsdata miningmachine learningstatistics
Graph Query Reformulation with Diversity,Davide Mottin (University of Trento)Francesco Bonchi (Yahoo!)Francesco Gullo (Yahoo!),"2111174164,2176652147,1979201319","We study a problem of graph-query reformulation enabling explorative query-driven discovery in graph databases. Given a query issued by the user, the system, apart from returning the result patterns, also proposes a number of specializations (i.e., supergraphs) of the original query to facilitate the exploration of the results. We formalize the problem of finding a set of reformulations of the input query by maximizing a linear combination of coverage (of the original query's answer set) and diversity among the specializations. We prove that our problem is hard, but also that a simple greedy algorithm achieves a (1/2)-approximation guarantee. The most challenging step of the greedy algorithm is the computation of the specialization that brings the maximum increment to the objective function. To efficiently solve this step, we show how to compute the objective-function increment of a specialization linearly in the number of its results and derive an upper bound that we exploit to devise an efficient search-space visiting strategy. An extensive evaluation on real and synthetic databases attests high efficiency and accuracy of our proposal.",2015,Knowledge Discovery and Data Mining,Fields of study: sargableboolean conjunctive querygraph databaseweb query classificationquery expansionquery optimizationtheoretical computer sciencedata miningmachine learningmathematical optimizationcomputer sciencemathematics
TimeMachine: Timeline Generation for Knowledge-Base Entities,Tim Althoff (Stanford University)Xin Luna Dong (Google)Kevin Murphy (Google)Safa Alai (Google)Van Dang (Google)Wei Zhang (Google),"2227326281,2150077005,2167731548,2225322651,2533981756,2646486594","We present a method called TIMEMACHINE to generate a timeline of events and relations for entities in a knowledge base. For example for an actor, such a timeline should show the most important professional and personal milestones and relationships such as works, awards, collaborations, and family relationships. We develop three orthogonal timeline quality criteria that an ideal timeline should satisfy: (1) it shows events that are relevant to the entity; (2) it shows events that are temporally diverse , so they distribute along the time axis, avoiding visual crowding and allowing for easy user interaction, such as zooming in and out; and (3) it shows events that are content diverse , so they contain many different types of events (e.g., for an actor, it should show movies and marriages and awards, not just movies). We present an algorithm to generate such timelines for a given time period and screen size, based on submodular optimization and web-co-occurrence statistics with provable performance guarantees. A series of user studies using Mechanical Turk shows that all three quality criteria are crucial to produce quality timelines and that our algorithm significantly outperforms various baseline and state-of-the-art methods.",2015,Knowledge Discovery and Data Mining,Fields of study: timelineknowledge basemultimediaworld wide webdata miningdatabasecomputer science
Trading Interpretability for Accuracy: Oblique Treed Sparse Additive Models,Jialei Wang (University of Chicago)Ryohei Fujimaki (NEC)Yosuke Motohashi (NEC),"2656372487,1124837368,2228419117","Model interpretability has been recognized to play a key role in practical data mining. Interpretable models provide significant insights on data and model behaviors and may convince end-users to employ certain models. In return for these advantages, however, there is generally a sacrifice in accuracy, i.e., flexibility of model representation (e.g., linear, rule-based, etc.) and model complexity needs to be restricted in order for users to be able to understand the results. This paper proposes oblique treed sparse additive models (OT-SpAMs). Our main focus is on developing a model which sacrifices a certain degree of interpretability for accuracy but achieves entirely sufficient accuracy with such fully non-linear models as kernel support vector machines (SVMs). OT-SpAMs are instances of region-specific predictive models. They divide feature spaces into regions with sparse oblique tree splitting and assign local sparse additive experts to individual regions. In order to maintain OT-SpAM interpretability, we have to keep the overall model structure simple, and this produces simultaneous model selection issues for sparse oblique region structures and sparse local experts. We address this problem by extending factorized asymptotic Bayesian inference. We demonstrate, on simulation, benchmark, and real world datasets that, in terms of accuracy, OT-SpAMs outperform state-of-the-art interpretable models and perform competitively with kernel SVMs, while still providing results that are highly understandable.",2015,Knowledge Discovery and Data Mining,Fields of study: model selectiondata miningpattern recognitionmachine learningstatisticsmathematics
State-Driven Dynamic Sensor Selection and Prediction with State-Stacked Sparseness,Guo-Jun Qi (University of Central Florida)Charu Aggarwal (IBM)Deepak S. Turaga (IBM)Daby M. Sow (IBM)Phil Anno (ConocoPhillips),"2237849324,2146335907,2303897886,2102328917,1996411167","An important problem in large-scale sensor mining is that of selecting relevant sensors for prediction purposes. Selecting small subsets of sensors, also referred to as active sensors, often leads to lower operational costs, and it reduces the noise and information overload for prediction. Existing sensor selection and prediction models either select a set of sensors a priori , or they use adaptive algorithms to determine the most relevant sensors for prediction. Sensor data sets often show dynamically varying patterns, because of which it is suboptimal to select a fixed subset of active sensors. To address this problem, we develop a novel dynamic prediction model that uses the notion of hidden system states to dynamically select a varying subset of sensors. These hidden system states are automatically learned by our model in a data-driven manner. The proposed algorithm can rapidly switch between different sets of active sensors when the model detects the (periodic or intermittent) change in the system state. We derive the dynamic sensor selection strategy by minimizing the error rates in tracking and predicting sensor readings over time. We introduce the notion of state-stacked sparseness to select a subset of the most critical sensors as a function of evolving system state. We present experimental results on two real sensor datasets, corresponding to oil drilling rig sensors and intensive care unit (ICU) sensors, and demonstrate the superiority of our approach with respect to other models.",2015,Knowledge Discovery and Data Mining,Fields of study: data miningartificial intelligencemachine learning
Performance Modeling and Scalability Optimization of Distributed Deep Learning Systems,Feng Yan 0001 (College of William & Mary)Olatunji Ruwase (Microsoft)Yuxiong He (Microsoft)Trishul M. Chilimbi (Microsoft),"2305502933,2597570656,2166872174,2077038638","Big deep neural network (DNN) models trained on large amounts of data have recently achieved the best accuracy on hard tasks, such as image and speech recognition. Training these DNNs using a cluster of commodity machines is a promising approach since training is time consuming and compute-intensive. To enable training of extremely large DNNs, models are partitioned across machines. To expedite training on very large data sets, multiple model replicas are trained in parallel on different subsets of the training examples with a global parameter server maintaining shared weights across these replicas. The correct choice for model and data partitioning and overall system provisioning is highly dependent on the DNN and distributed system hardware characteristics. These decisions currently require significant domain expertise and time consuming empirical state space exploration. This paper develops performance models that quantify the impact of these partitioning and provisioning decisions on overall distributed system performance and scalability. Also, we use these performance models to build a scalability optimizer that efficiently determines the optimal system configuration that minimizes DNN training time. We evaluate our performance models and scalability optimizer using a state-of-the-art distributed DNN training framework on two benchmark applications. The results show our performance models estimate DNN training time with high estimation accuracy and our scalability optimizer correctly chooses the best configurations, minimizing the training time of distributed DNNs.",2015,Knowledge Discovery and Data Mining,Fields of study: scalabilitydeep learningdata miningreal time computingmachine learningsimulationcomputer science
Learning a Hierarchical Monitoring System for Detecting and Diagnosing Service Issues,Vinod Nair (Microsoft)Ameya Raul (Microsoft)Shwetabh Khanduja (Microsoft)Vikas Bahirwani (Microsoft)Qihong Shao (Microsoft)Sundararajan Sellamanickam (Microsoft)Sathiya Keerthi (Microsoft)Steve Herbert (Microsoft)Sudheer Dhulipalla (Microsoft),"2307177701,2590794783,2224474251,2223702093,2610912427,2306535536,2649115054,2160520912,2611723962","We propose a machine learning based framework for building a hierarchical monitoring system to detect and diagnose service issues. We demonstrate its use for building a monitoring system for a distributed data storage and computing service consisting of tens of thousands of machines. Our solution has been deployed in production as an end-to-end system, starting from telemetry data collection from individual machines, to a visualization tool for service operators to examine the detection outputs. Evaluation results are presented on detecting 19 customer impacting issues in the past three months.",2015,Knowledge Discovery and Data Mining,Fields of study: unsupervised learningdata sciencedata miningmachine learningsimulationcomputer science
Unified and Contrasting Cuts in Multiple Graphs: Application to Medical Imaging Segmentation,"Chia-Tung Kuo (University of California, Davis)Xiang Wang (Google)Peter B. Walker (Naval Medical Research Center)Owen T. Carmichael (University of California, Davis)Jieping Ye (University of Michigan)Ian Davidson (University of California, Davis)","2224265742,2099725100,2136497257,311514008,2305258894,2560595684","The analysis of data represented as graphs is common having wide scale applications from social networks to medical imaging. A popular analysis is to cut the graph so that the disjoint subgraphs can represent communities (for social network) or background and foreground cognitive activity (for medical imaging). An emerging setting is when multiple data sets (graphs) exist which opens up the opportunity for many new questions. In this paper we study two such questions: i) For a collection of graphs find a single cut that is good for all the graphs and ii) For two collections of graphs find a single cut that is good for one collection but poor for the other. We show that existing formulations of multiview, consensus and alternative clustering cannot address these questions and instead we provide novel formulations in the spectral clustering framework. We evaluate our approaches on functional magnetic resonance imaging (fMRI) data to address questions such as: ""What common cognitive network does this group of individuals have?"" and ""What are the differences in the cognitive networks for these two groups?"" We obtain useful results without the need for strong domain knowledge.",2015,Knowledge Discovery and Data Mining,Fields of study: cutcombinatoricsdata miningmachine learningcomputer sciencemathematics
Transfer Learning for Bilingual Content Classification,Qian Sun (Arizona State University)Mohammad Shafkat Amin (LinkedIn)Baoshi Yan (LinkedIn)Craig Martell (LinkedIn)Vita Markman (LinkedIn)Anmol Bhasin (LinkedIn)Jieping Ye (University of Michigan),"2620185474,2124207833,2104629786,2505950623,1548460981,2157708525,2305258894","LinkedIn Groups provide a platform on which professionals with similar background, target and specialities can share content, take part in discussions and establish opinions on industry topics. As in most online social communities, spam content in LinkedIn Groups poses great challenges to the user experience and could eventually lead to substantial loss of active users. Building an intelligent and scalable spam detection system is highly desirable but faces difficulties such as lack of labeled training data, particularly for languages other than English. In this paper, we take the spam (Spanish) job posting detection as the target problem and build a generic machine learning pipeline for multi-lingual spam detection. The main components are feature generation and knowledge migration via transfer learning. Specifically, in the feature generation phase, a relatively large labeled data set is generated via machine translation. Together with a large set of unlabeled human written Spanish data, unigram features are generated based on the frequency. In the second phase, machine translated data are properly reweighted to capture the discrepancy from human written ones and classifiers can be built on top of them. To make effective use of a small portion of labeled data available in human written Spanish, an adaptive transfer learning algorithm is proposed to further improve the performance. We evaluate the proposed method on LinkedIn's production data and the promising results verify the efficacy of our proposed algorithm. The pipeline is ready for production.",2015,Knowledge Discovery and Data Mining,Fields of study: transfer of learningbiological classificationsemi supervised learningtext miningworld wide webdata miningmachine learningcomputer science
Gender and Interest Targeting for Sponsored Post Advertising at Tumblr,Mihajlo Grbovic (Yahoo!)Vladan Radosavljevic (Yahoo!)Nemanja Djuric (Yahoo!)Narayan Bhamidipati (Yahoo!)Ananth Nagarajan (Yahoo!),"2000240052,1981991050,2128657275,1964376778,2227432802","As one of the leading platforms for creative content, Tumblr offers advertisers a unique way of creating brand identity. Advertisers can tell their story through images, animation, text, music, video, and more, and can promote that content by sponsoring it to appear as an advertisement in the users' live feeds. In this paper, we present a framework that enabled two of the key targeted advertising components for Tumblr, gender and interest targeting. We describe the main challenges encountered during the development of the framework, which include the creation of a ground truth for training gender prediction models, as well as mapping Tumblr content to a predefined interest taxonomy. For purposes of inferring user interests, we propose a novel semi-supervised neural language model for categorization of Tumblr content (i.e., post tags and post keywords). The model was trained on a large-scale data set consisting of $6.8$ billion user posts, with a very limited amount of categorized keywords, and was shown to have superior performance over the baseline approaches. We successfully deployed gender and interest targeting capability in Yahoo production systems, delivering inference for users that covers more than 90% of daily activities on Tumblr. Online performance results indicate advantages of the proposed approach, where we observed 20% increase in user engagement with sponsored posts in comparison to untargeted campaigns.",2015,Knowledge Discovery and Data Mining,Fields of study: multimediaworld wide webdata miningartificial intelligencecomputer science
Stream Sampling for Frequency Cap Statistics,Edith Cohen (Google),2167811012,"Unaggregated data, in a streamed or distributed form, is prevalent and comes from diverse sources such as interactions of users with web services and IP traffic. Data elements have keys (cookies, users, queries) and elements with different keys interleave. Analytics on such data typically utilizes statistics expressed as a sum over keys in a specified segment of a function f applied to the frequency (the total number of occurrences) of the key. In particular, Distinct is the number of active keys in the segment, Sum is the sum of their frequencies, and both are special cases of frequency cap statistics, which cap the frequency by a parameter T . One important application of cap statistics is staging advertisement campaigns, where the cap parameter is the limit of the maximum number of impressions per user and we estimate the total number of qualifying impressions. The number of distinct active keys in the data can be very large, making exact computation of queries costly. Instead, we can estimate these statistics from a sample. An optimal sample for a given function f would include a key with frequency w with probability roughly proportional to f(w) . But while such a ""gold-standard"" sample can be easily computed over the aggregated data (the set of key-frequency pairs), exact aggregation itself is costly and slow. Ideally, we would like to compute and maintain a sample without aggregation. We present a sampling framework for unaggregated data that uses a single pass (for streams) or two passes (for distributed data) and state proportional to the desired sample size. Our design unifies classic solutions for Distinct and Sum. Specifically, our l-capped samples provide nonnegative unbiased estimates of any monotone non-decreasing frequency statistics, and close to gold-standard estimates for frequency cap statistics with T =Θ(l). Furthermore, our design facilitates multi-objective samples, which provide tight estimates for a specified set of statistics using a single smaller sample.",2015,Knowledge Discovery and Data Mining,Fields of study: samplingtheoretical computer scienceworld wide webdata miningstatistics
Locally Densest Subgraph Discovery,"Lu Qin (University of Technology, Sydney)Rong-Hua Li (Shenzhen University)Lijun Chang (University of New South Wales)Chengqi Zhang (University of Technology, Sydney)","2134649587,2097752033,2306187360,2166080598","Mining dense subgraphs from a large graph is a fundamental graph mining task and can be widely applied in a variety of application domains such as network science, biology, graph database, web mining, graph compression, and micro-blogging systems. Here a dense subgraph is defined as a subgraph with high density (#.edge / #.node). Existing studies of this problem either focus on finding the densest subgraph or identifying an optimal clique-like dense subgraph, and they adopt a simple greedy approach to find the top- k dense subgraphs. However, their identified subgraphs cannot be used to represent the dense regions of the graph. Intuitively, to represent a dense region, the subgraph identified should be the subgraph with highest density in its local region in the graph. However, it is non-trivial to formally model a locally densest subgraph. In this paper, we aim to discover top- k such representative locally densest subgraphs of a graph. We provide an elegant parameter-free definition of a locally densest subgraph. The definition not only fits well with the intuition, but is also associated with several nice structural properties. We show that the set of locally densest subgraphs in a graph can be computed in polynomial time. We further propose three novel pruning strategies to largely reduce the search space of the algorithm. In our experiments, we use several real datasets with various graph properties to evaluate the effectiveness of our model using four quality measures and a case study. We also test our algorithms on several real web-scale graphs, one of which contains 118.14 million nodes and 1.02 billion edges, to demonstrate the high efficiency of the proposed algorithms.",2015,Knowledge Discovery and Data Mining,Fields of study: implicit graphfactor critical graphdegeneracydistance hereditary graphrook s graphinduced subgraph isomorphism problemvoltage graphcomplement graphforbidden graph characterizationgraph powergraph factorizationbutterfly graphuniversal graphreconstruction conjecturenull graphcographclaw free graphcolor codingsubgraph isomorphism problemline graphgraphbig datatheoretical computer sciencediscrete mathematicscombinatoricsdata miningcomputer sciencemathematics
Statistical Arbitrage Mining for Display Advertising,Weinan Zhang (University College London)Jun Wang (University College London),"2527611484,2557836567","We study and formulate arbitrage in display advertising. Real-Time Bidding (RTB) mimics stock spot exchanges and utilises computers to algorithmically buy display ads per impression via a real-time auction. Despite the new automation, the ad markets are still informationally inefficient due to the heavily fragmented marketplaces. Two display impressions with similar or identical effectiveness (e.g., measured by conversion or click-through rates for a targeted audience) may sell for quite different prices at different market segments or pricing schemes. In this paper, we propose a novel data mining paradigm called Statistical Arbitrage Mining (SAM) focusing on mining and exploiting price discrepancies between two pricing schemes. In essence, our SAMer is a meta-bidder that hedges advertisers' risk between CPA (cost per action)-based campaigns and CPM (cost per mille impressions)-based ad inventories; it statistically assesses the potential profit and cost for an incoming CPM bid request against a portfolio of CPA campaigns based on the estimated conversion rate, bid landscape and other statistics learned from historical data. In SAM, (i) functional optimisation is utilised to seek for optimal bidding to maximise the expected arbitrage net profit, and (ii) a portfolio-based risk management solution is leveraged to reallocate bid volume and budget across the set of campaigns to make a risk and return trade-off. We propose to jointly optimise both components in an EM fashion with high efficiency to help the meta-bidder successfully catch the transient statistical arbitrage opportunities in RTB. Both the offline experiments on a real-world large-scale dataset and online A/B tests on a commercial platform demonstrate the effectiveness of our proposed solution in exploiting arbitrage in various model settings and market environments.",2015,Knowledge Discovery and Data Mining,Fields of study: index arbitragereal time biddingstatistical arbitragemathematical economicsadvertisingcommercemicroeconomicsmarketingeconomics
Monitoring Least Squares Models of Distributed Streams,Moshe Gabel (Technion – Israel Institute of Technology)Daniel Keren (University of Haifa)Assaf Schuster (Technion – Israel Institute of Technology),"2143143641,2061587065,2135728993","Least squares regression is widely used to understand and predict data behavior in many fields. As data evolves, regression models must be recomputed, and indeed much work has focused on quick, efficient and accurate computation of linear regression models. In distributed streaming settings, however, periodically recomputing the global model is wasteful: communicating new observations or model updates is required even when the model is, in practice, unchanged. This is prohibitive in many settings, such as in wireless sensor networks, or when the number of nodes is very large. The alternative, monitoring prediction accuracy, is not always sufficient: in some settings, for example, we are interested in the model's coefficients, rather than its predictions. We propose the first monitoring algorithm for multivariate regression models of distributed data streams that guarantees a bounded model error. It maintains an accurate estimate using a fraction of the communication by recomputing only when the precomputed model is sufficiently far from the (hypothetical) current global model. When the global model is stable, no communication is needed. Experiments on real and synthetic datasets show that our approach reduces communication by up to two orders of magnitude while providing an accurate estimate of the current global model in all nodes.",2015,Knowledge Discovery and Data Mining,Fields of study: regressionleast squaresdata miningmachine learningstatisticscomputer science
Dynamic Hierarchical Classification for Patient Risk-of-Readmission,Senjuti Basu Roy (University of Washington)Ankur Teredesai (University of Washington)Kiyana Zolfaghar (University of Washington)Rui Liu (University of Washington)David Hazel (University of Washington)Stacey Newman (University of Washington)Albert Marinez,"2097289406,387590160,1147347457,2721676454,2102999437,2230978778,2228157535","Congestive Heart Failure (CHF) is a serious chronic condition often leading to 50% mortality within 5 years. Improper treatment and post-discharge care of CHF patients leads to repeat frequent hospitalizations (i.e., readmissions). Accurately predicting patient's risk-of-readmission enables care-providers to plan resources, perform factor analysis, and improve patient quality of life. In this paper, we describe a supervised learning framework, Dynamic Hierarchical Classification (DHC) for patient's risk-of-readmission prediction. Learning the hierarchy of classifiers is often the most challenging component of such classification schemes. The novelty of our approach is to algorithmically generate various layers and combine them to predict overall 30-day risk-of-readmission. While the components of DHC are generic, in this work, we focus on congestive heart failure (CHF), a pressing chronic condition. Since healthcare data is diverse and rich and each source and feature-subset provides different insights into a complex problem, our DHC based prediction approach intelligently leverages each source and feature-subset to optimize different objectives (such as, Recall or AUC) for CHF risk-of-readmission. DHC's algorithmic layering capability is trained and tested over two real world datasets and is currently integrated into the clinical decision support tools at MultiCare Health System (MHS), a major provider of healthcare services in the northwestern US. It is integrated into a QlikView App (with EMR integration planned for Q2) and currently scores patients everyday, helping to mitigate readmissions and improve quality of care, leading to healthier outcomes and cost savings.",2015,Knowledge Discovery and Data Mining,Fields of study: health caredata miningsimulation
Machine Learning and Causal Inference for Policy Evaluation,Susan Athey (Stanford University),2281695842,"A large literature on causal inference in statistics, econometrics, biostatistics, and epidemiology (see, e.g., Imbens and Rubin [2015] for a recent survey) has focused on methods for statistical estimation and inference in a setting where the researcher wishes to answer a question about the (counterfactual) impact of a change in a policy, or ""treatment"" in the terminology of the literature. The policy change has not necessarily been observed before, or may have been observed only for a subset of the population; examples include a change in minimum wage law or a change in a firm's price. The goal is then to estimate the impact of small set of ""treatments"" using data from randomized experiments or, more commonly, ""observational"" studies (that is, non-experimental data). The literature identifies a variety of assumptions that, when satisfied, allow the researcher to draw the same types of conclusions that would be available from a randomized experiment. To estimate causal effects given non-random assignment of individuals to alternative policies in observational studies, popular techniques include propensity score weighting, matching, and regression analysis; all of these methods adjust for differences in observed attributes of individuals. Another strand of literature in econometrics, referred to as ""structural modeling,"" fully specifies the preferences of actors as well as a behavioral model, and estimates those parameters from data (for applications to auction-based electronic commerce, see Athey and Haile [2007] and Athey and Nekipelov [2012]). In both cases, parameter estimates are interpreted as ""causal,"" and they are used to make predictions about the effect of policy changes. In contrast, the supervised machine learning literature has traditionally focused on prediction, providing data-driven approaches to building rich models and relying on cross-validation as a powerful tool for model selection. These methods have been highly successful in practice. This talk will review several recent papers that attempt to bring the tools of supervised machine learning to bear on the problem of policy evaluation, where the papers are connected by three themes. The first theme is that it important for both estimation and inference to distinguish between parts of the model that relate to the causal question of interest, and ""attributes,"" that is, features or variables that describe attributes of individual units that are held fixed when policies change. Specifically, we propose to divide the features of a model into causal features, whose values may be manipulated in a counterfactual policy environment, and attributes. A second theme is that relative to conventional tools from the policy evaluation literature, tools from supervised machine learning can be particularly effective at modeling the association of outcomes with attributes, as well as in modeling how causal effects vary with attributes. A final theme is that modifications of existing methods may be required to deal with the ""fundamental problem of causal inference,"" namely, that no unit is observed in multiple counterfactual worlds at the same time: we do not see a patient at the same time with and without medication, and we do not see a consumer at the same moment exposed to two different prices. This creates a substantial challenge for cross-validation, as the ground truth for the causal effect is not observed for any individual.",2015,Knowledge Discovery and Data Mining,Fields of study: randomized experimentcausal inferencecross validationeconometricsdata miningmachine learningstatisticscomputer sciencemathematics
Algorithmic Cartography: Placing Points of Interest and Ads on Maps,Mohammad Mahdian (Google)Okke Schrijvers (Stanford University)Sergei Vassilvitskii (Google),"2161106313,2153540811,2156675704","We study the problem of selecting a set of points of interest (POIs) to show on a map. We begin with a formal model of the setting, noting that the utility of a POI may be discounted by (i) the presence of competing businesses nearby as well as (ii) its position in the set of establishments ordered by distance from the user. We present simple, approximately optimal selection algorithms, coupled with incentive compatible pricing schemes in case of advertiser supplied points of interest. Finally, we evaluate our algorithms on real data sets and show that they outperform simple baselines.",2015,Knowledge Discovery and Data Mining,Fields of study: externalitydata miningsimulationmathematics
Going In-Depth: Finding Longform on the Web,"Virginia Smith (University of California, Berkeley)Miriam Connor (Google)Isabelle Stanton (Google)","2130450629,2720443663,2514095475","tl;dr: Longform articles are extended, in-depth pieces that often serve as feature stories in newspapers and magazines. In this work, we develop a system to automatically identify longform content across the web. Our novel classifier is highly accurate despite huge variation within longform in terms of topic, voice, and editorial taste. It is also scalable and interpretable, requiring a surprisingly small set of features based only on language and parse structures, length, and document interest. We implement our system at scale and use it to identify a corpus of several million longform documents. Using this corpus, we provide the first web-scale study with quantifiable and measurable information on longform, giving new insight into questions posed by the media on the past and current state of this famed literary medium.",2015,Knowledge Discovery and Data Mining,Fields of study: featureweb miningdata scienceworld wide webdata miningmachine learningcomputer science
Exploiting Relevance Feedback in Knowledge Graph Search,"Yu Su (University of California, Santa Barbara)Shengqi Yang (University of California, Santa Barbara)Huan Sun (University of California, Santa Barbara)Mudhakar Srivatsa (IBM)Sue Kase (United States Army Research Laboratory)Michelle Vanni (United States Army Research Laboratory)Xifeng Yan (University of California, Santa Barbara)","2399811336,2226434982,2618514357,1449758934,1553375982,2106593583,2116657824","The big data era is witnessing a prevalent shift of data from homogeneous to heterogeneous, from isolated to linked. Exemplar outcomes of this shift are a wide range of graph data such as information, social, and knowledge graphs. The unique characteristics of graph data are challenging traditional search techniques like SQL and keyword search. Graph query is emerging as a promising complementary search form. In this paper, we study how to improve graph query by relevance feedback. Specifically, we focus on knowledge graph query, and formulate the graph relevance feedback (GRF) problem. We propose a general GRF framework that is able to (1) tune the original ranking function based on user feedback and (2) further enrich the query itself by mining new features from user feedback. As a consequence, a query-specific ranking function is generated, which is better aligned with the user search intent. Given a newly learned ranking function based on user feedback, we further investigate whether we shall re-rank the existing answers, or choose to search from scratch. We propose a strategy to train a binary classifier to predict which action will be more beneficial for a given query. The GRF framework is applied to searching DBpedia with graph queries derived from YAGO and Wikipedia. Experiment results show that GRF can improve the mean average precision by 80% to 100%.",2015,Knowledge Discovery and Data Mining,Fields of study: rankinggraph databasegraphinformation retrievaldata miningmachine learningcomputer science
Online Topic-based Social Influence Analysis for the Wimbledon Championships,Varun R. Embar (IBM)Indrajit Bhattacharya (IBM)Vinayaka Pandit (IBM)Roman Vaculin (IBM),"2225031596,2117917354,2139650730,1464331608","Various industries are turning to social media to identify key influencers on topics of interest. Following this trend, the All England Lawn Tennis and Croquet Club (AELTC) is keen to analyze the `social pulse' around the famous Wimbledon Championships. IBM developed and deployed social influence analysis capability for AELTC during the 2014 edition of the Championship. The design and implementation of influence analysis technology in the real world involves several challenges. In this paper, we define various functional and usability criteria that social influence scores should satisfy, and propose a multi-dimensional definition of influence that satisfies these criteria. We highlight the need to identify both all-time influencers and recent influencers, and track user influences over multiple time-scales for this purpose. We also stress the importance of aspect-specific influence analysis, and investigate an approach that uses an aspect hierarchy that annotates tweets with topics or aspects before analyzing them for influence. We also describe interesting insights discovered by our tool and the lessons that we learnt from this engagement.",2015,Knowledge Discovery and Data Mining,Fields of study: ibmoperations researchdata miningsimulationcomputer science
"Data, Knowledge and Discovery: Machine Learning meets Natural Science",Hugh Durrant-Whyte (University of Sydney),2167801844,"Increasingly it is data, vast amounts of data, that drives scientific discovery. At the heart of this so-called ""fourth paradigm of science"" is the rapid development of large scale statistical data fusion and machine learning methods. While these developments in ""big data"" methods are largely driven by commercial applications such as internet search or customer modelling, the opportunity for applying these to scientific discovery is huge. This talk will describe a number of applied machine learning projects addressing real-world inference problems in physical, life and social science areas. In particular, I will describe a major Science and Industry Endowment Fund (SIEF) project, in collaboration with the NICTA and Macquarie University, looking to apply machine learning techniques to discovery in the natural sciences. This talk will look at the key methods in machine learning that are being applied to the discovery process, especially in areas like geology, ecology and biological discovery.",2015,Knowledge Discovery and Data Mining,Fields of study: data sciencedata miningartificial intelligencemachine learningcomputer science
Linear Time Samplers for Supervised Topic Models using Compositional Proposals,Xun Zheng (Carnegie Mellon University)Yaoliang Yu (Carnegie Mellon University)Eric P. Xing (Carnegie Mellon University),"2132416899,2119954973,351197510","Topic models are effective probabilistic tools for processing large collections of unstructured data. With the exponential growth of modern industrial data, and consequentially also with our ambition to explore much bigger models, there is a real pressing need to significantly scale up topic modeling algorithms, which has been taken up in lots of previous works, culminating in the recent fast Markov chain Monte Carlo sampling algorithms in [10, 23] for the unsupervised latent Dirichlet allocation (LDA) formulations. In this work we extend the recent sampling advances for unsupervised LDA models to supervised tasks. We focus on the Gibbs MedLDA model [27] that is able to simultaneously discover latent structures and make accurate predictions. By combining a set of sampling techniques we are able to reduce the O ( K 3 + DK 2 + DNK complexity in [27] to O ( DK + DN ) when there are K topics and D documents with average length N . To our best knowledge, this is the first linear time sampling algorithm for supervised topic models. Our algorithm requires minimal modifications to incorporate most loss functions in a variety of supervised tasks, and we observe in our experiments an order of magnitude speedup over the current state-of-the-art implementation, while achieving similar prediction performances. The open-source C++ implementation of the proposed algorithm is available at https://github.com/xunzheng/light_medlda.",2015,Knowledge Discovery and Data Mining,Fields of study: topic modelmarkov chain monte carloregressiondata miningpattern recognitionmachine learningstatisticscomputer science
Utilizing Text Mining on Online Medical Forums to Predict Label Change due to Adverse Drug Reactions,Ronen Feldman (Hebrew University of Jerusalem)Oded Netzer (Columbia University)Aviv Peretz (Hebrew University of Jerusalem)Binyamin Rosenfeld (Bar-Ilan University),"2159469398,2154981325,2232469659,2108330698","We present an end-to-end text mining methodology for relation extraction of adverse drug reactions (ADRs) from medical forums on the Web. Our methodology is novel in that it combines three major characteristics: (i) an underlying concept of using a head-driven phrase structure grammar (HPSG) based parser; (ii) domain-specific relation patterns, the acquisition of which is done primarily using unsupervised methods applied to a large, unlabeled text corpus; and (iii) automated post-processing algorithms for enhancing the set of extracted relations. We empirically demonstrate the ability of our proposed approach to predict ADRs prior to their reporting by the Food and Drug Administration (FDA). Put differently, we put our approach to a predictive test by demonstrating that our methodology can credibly point to ADRs that were not uncovered in clinical trials for evaluating new drugs that come to market but were only reported later on by the FDA as a label change.",2015,Knowledge Discovery and Data Mining,Fields of study: head driven phrase structure grammartext miningdata scienceinformation retrievaldata miningcomputer science
Selective Hashing: Closing the Gap between Radius Search and k-NN Search,Jinyang Gao (National University of Singapore)H.V. Jagadish (University of Michigan)Beng Chin Ooi (National University of Singapore)Sheng Wang (National University of Singapore),"2107652876,360112113,2037466936,2670502447","Locality Sensitive Hashing (LSH) and its variants, are generally believed to be the most effective radius search methods in high-dimensional spaces. However, many applications involve finding the k nearest neighbors ( k -NN), where the k -NN distances of different query points may differ greatly and the performance of LSH suffers. We propose a novel indexing scheme called Selective Hashing , where a disjoint set of indices are built with different granularities and each point is only stored in the most effective index. Theoretically, we show that k -NN search using selective hashing can achieve the same recall as a fixed radius LSH search, using a radius equal to the distance of the c 1 k th nearest neighbor, with at most c 2 times overhead, where c 1 and c 2 are small constants. Selective hashing is also easy to build and update, and outperforms all the state-of-the-art algorithms such as DSH and IsoHash.",2015,Knowledge Discovery and Data Mining,Fields of study: hopscotch hashingk independent hashing2 choice hashingfeature hashinglocality preserving hashingdynamic perfect hashinglinear hashingopen addressinguniversal hashinglocality sensitive hashingconsistent hashinghash tabletheoretical computer scienceworld wide webdatabasemachine learningcomputer science
Discovery of Glaucoma Progressive Patterns Using Hierarchical MDL-Based Clustering,Shigeru Maya (University of Tokyo)Kai Morino (University of Tokyo)Hiroshi Murata (University of Tokyo)Ryo Asaoka (University of Tokyo)Kenji Yamanishi (University of Tokyo),"2228343014,2177082337,2130711118,2032585100,2302798453","In this paper, we propose a method to cluster the spacial patterns of the visual field in glaucoma patients to analyze the progression patterns of glaucoma. The degree of progression in the visual field of glaucoma patients can be divided into several regions by straight line boundaries, we call this specific structure Direct Product Structure in this paper. Since we can observe the direct product structure in the visual fields, we propose a bottom-up hierarchical clustering method to embed this structure into the clustering structure. In our method, according to the minimum description length (MDL) principle, we select the best cluster division so that the total code length required for encoding the data as well as the clustering structure is minimum. We can thereby select the clusters that are robust to the noise in the position of the direct product structure for clustering. We demonstrate the effectiveness of our method using an artificial dataset and a real glaucoma dataset. Our proposed method performed better than existing methods for both datasets. For the real glaucoma dataset in particular, our method discovered the characteristic progressive patterns of glaucoma as specific features of clusters. These patterns agree with clinical knowledge. Furthermore, we show that our clusters can be applied to improve the accuracy of predicting glaucoma progression. Thus, our clusters contain rich information of glaucoma, and hence can contribute to further development in glaucoma research.",2015,Knowledge Discovery and Data Mining,Fields of study: hierarchical clusteringbioinformaticsdata miningmachine learningstatisticscomputer sciencemathematics
Set Cover at Web Scale,Stergios Stergiou (Yahoo!)Kostas Tsioutsiouliklis (Yahoo!),"1979131420,53297445","The classic Set Cover problem requires selecting a minimum size subset A ⊆ F from a family of finite subsets F Of U such that the elements covered by A are the ones covered by F . It naturally occurs in many settings in web search, web mining and web advertising. The greedy algorithm that iteratively selects a set in F that covers the most uncovered elements, yields an optimum (1+ln | U |)-approximation but is inherently sequential. In this work we give the first MapReduce Set Cover algorithm that scales to problem sizes of ∼ 1 trillion elements and runs in log p Δ iterations for a nearly optimum approximation ratio of p ln Δ, where Δ is the cardinality of the largest set in F A web crawler is a system for bulk downloading of web pages. Given a set of seed URLs, the crawler downloads and extracts the hyperlinks embedded in them and schedules the crawling of the pages addressed by those hyperlinks for a subsequent iteration. While the average page out-degree is ∼ 50, the crawled corpus grows at a much smaller rate, implying a significant outlink overlap. Using our MapReduce Set Cover heuristic as a building block, we present the first large-scale seed generation algorithm that scales to ∼ 20 billion nodes and discovers new pages at a rate ∼ 4x faster than that obtained by prior art heuristics.",2015,Knowledge Discovery and Data Mining,Fields of study: set cover problemworld wide webdata miningmachine learningalgorithmcomputer science
BatchRank: A Novel Batch Mode Active Learning Framework for Hierarchical Classification,"Shayok Chakraborty (Carnegie Mellon University)Vineeth Nallure Balasubramanian (Indian Institute of Technology, Hyderabad)Adepu Ravi Sankar (Indian Institute of Technology, Hyderabad)Sethuraman Panchanathan (Arizona State University)Jieping Ye (University of Michigan)","2183505635,2167359976,2433251518,2055909567,2305258894","Active learning algorithms automatically identify the salient and exemplar instances from large amounts of unlabeled data and thus reduce human annotation effort in inducing a classification model. More recently, Batch Mode Active Learning (BMAL) techniques have been proposed, where a batch of data samples is selected simultaneously from an unlabeled set. Most active learning algorithms assume a flat label space, that is, they consider the class labels to be independent. However, in many applications, the set of class labels are organized in a hierarchical tree structure, with the leaf nodes as outputs and the internal nodes as clusters of outputs at multiple levels of granularity. In this paper, we propose a novel BMAL algorithm (BatchRank) for hierarchical classification. The sample selection is posed as an NP-hard integer quadratic programming problem and a convex relaxation (based on linear programming) is derived, whose solution is further improved by an iterative truncated power method. Finally, a deterministic bound is established on the quality of the solution. Our empirical results on several challenging, real-world datasets from multiple domains, corroborate the potential of the proposed framework for real-world hierarchical classification applications.",2015,Knowledge Discovery and Data Mining,Fields of study: active learningbig datadata miningpattern recognitionmachine learningstatisticscomputer science
Structured Hedging for Resource Allocations with Leverage,Nicholas Johnson (University of Minnesota)Arindam Banerjee (University of Minnesota),"2125834056,2037585042","Data mining algorithms for computing solutions to online resource allocation (ORA) problems have focused on budgeting resources currently in possession, e.g., investing in the stock market with cash on hand or assigning current employees to projects. In several settings, one can leverage borrowed resources with which tasks can be accomplished more efficiently and cheaply. Additionally, a variety of opposing allocation types or positions may be available with which one can hedge the allocation to alleviate risk from external changes. In this paper, we present a formulation for hedging online resource allocations with leverage and propose an efficient data mining algorithm (SHERAL). We pose the problem as a constrained online convex optimization problem. The key novel components of our formulation are (1) a loss function for general leveraging and opposing allocation positions and (2) a penalty function which hedges between structurally dependent allocation positions to control risk. We instantiate the problem in the context of portfolio selection and evaluate the effectiveness of the formulation through extensive experiments on five datasets in comparison with existing algorithms and several variants.",2015,Knowledge Discovery and Data Mining,Fields of study: structured predictionactuarial sciencedata miningmachine learningsimulationcomputer science
Entity Matching across Heterogeneous Sources,Yang Yang (Tsinghua University)Yizhou Sun (Northeastern University)Jie Tang (Tsinghua University)Bo Ma (Tsinghua University)Juanzi Li (Tsinghua University),"2673426936,2131539564,2158012360,2607880608,2129156004","Given an entity in a source domain, finding its matched entities from another (target) domain is an important task in many applications. Traditionally, the problem was usually addressed by first extracting major keywords corresponding to the source entity and then query relevant entities from the target domain using those keywords. However, the method would inevitably fails if the two domains have less or no overlapping in the content. An extreme case is that the source domain is in English and the target domain is in Chinese. In this paper, we formalize the problem as entity matching across heterogeneous sources and propose a probabilistic topic model to solve the problem. The model integrates the topic extraction and entity matching, two core subtasks for dealing with the problem, into a unified model. Specifically, for handling the text disjointing problem, we use a cross-sampling process in our model to extract topics with terms coming from all the sources, and leverage existing matching relations through latent topic layers instead of at text layers. Benefit from the proposed model, we can not only find the matched documents for a query entity, but also explain why these documents are related by showing the common topics they share. Our experiments in two real-world applications show that the proposed model can extensively improve the matching performance (+19.8% and +7.1% in two applications respectively) compared with several alternative methods.",2015,Knowledge Discovery and Data Mining,Fields of study: topic modelworld wide webinformation retrievaldata miningmachine learningcomputer science
Stochastic Divergence Minimization for Online Collapsed Variational Bayes Zero Inference of Latent Dirichlet Allocation,Issei Sato (University of Tokyo)Hiroshi Nakagawa (University of Tokyo),"2148390776,2094248864","The collapsed variational Bayes zero (CVB0) inference is a variational inference improved by marginalizing out parameters, the same as with the collapsed Gibbs sampler. A drawback of the CVB0 inference is the memory requirements. A probability vector must be maintained for latent topics for every token in a corpus. When the total number of tokens is N and the number of topics is K , the CVB0 inference requires Ο( NK ) memory. A stochastic approximation of the CVB0 (SCVB0) inference can reduce Ο( NK ) to Ο( VK ), where V denotes the vocabulary size. We reformulate the existing SCVB0 inference by using the stochastic divergence minimization algorithm, with which convergence can be analyzed in terms of Martingale convergence theory. We also reveal the property of the CVB0 inference in terms of the leave-one-out perplexity, which leads to the estimation algorithm of the Dirichlet distribution parameters. The predictive performance of the propose SCVB0 inference is better than that of the original SCVB0 inference in four datasets.",2015,Knowledge Discovery and Data Mining,Fields of study: variational message passingfiducial inferencefrequentist inferencetopic modellatent dirichlet allocationbayesian inferencestatistical inferencestochastic optimizationpattern recognitionmachine learningstatisticscomputer sciencemathematics
Efficient PageRank Tracking in Evolving Networks,Naoto Ohsaka (University of Tokyo)Takanori Maehara (Shizuoka University)Ken-ichi Kawarabayashi (National Institute of Informatics),"1892898320,2134907451,563105935","Real-world networks, such as the World Wide Web and online social networks, are very large and are evolving rapidly . Thus tracking personalized PageRank in such evolving networks is an important challenge in network analysis and graph mining. In this paper, we propose an efficient online algorithm for tracking personalized PageRank in an evolving network. The proposed algorithm tracks personalized PageRank accurately (i.e., within a given accuracy e > 0). Moreover it can update the personalized PageRank scores in amortized O(1/e) iterations for each graph modification. In addition, when m edges are randomly and sequentially inserted, the total number of iterations is expected to be O (log m /e). We evaluated our algorithm in real-world networks. In average case, for each edge insertion and deletion, our algorithm updated the personalized PageRank in 3us in a web graph with 105M vertices and 3.7B edges, and 20ms in a social network with 42M vertices and 1.5B edges. By comparing existing state-of-the-arts algorithms, our algorithm is 2--290 times faster with an equal accuracy.",2015,Knowledge Discovery and Data Mining,Fields of study: online algorithmtheoretical computer scienceworld wide webdata miningmachine learningcomputer science
Probabilistic Graphical Models of Dyslexia,Yair Lakretz (Tel Aviv University)Gal Chechik (Bar-Ilan University)Naama Friedmann (Tel Aviv University)Michal Rosen-Zvi (IBM),"2404476342,307548515,2139434295,725239793","Reading is a complex cognitive process, errors in which may assume diverse forms. In this study, introducing a novel approach, we use two families of probabilistic graphical models to analyze patterns of reading errors made by dyslexic people: an LDA-based model and two Naeve Bayes models which differ by their assumptions about the generation process of reading errors. The models are trained on a large corpus of reading errors. Results show that a Naeve Bayes model achieves highest accuracy compared to labels given by clinicians (AUC = 0.801 ± 0.05), thus providing the first automated and objective diagnosis tool for dyslexia which is solely based on reading errors data. Results also show that the LDA-based model best captures patterns of reading errors and could therefore contribute to the understanding of dyslexia and to future improvement of the diagnostic procedure. Finally, we draw on our results to shed light on a theoretical debate about the definition and heterogeneity of dyslexia. Our results support a model assuming multiple dyslexia subtypes, that of a heterogeneous view of dyslexia.",2015,Knowledge Discovery and Data Mining,Fields of study: dyslexialatent dirichlet allocationgraphical modelnatural language processingspeech recognitionmachine learningstatisticscomputer science
Simultaneous Modeling of Multiple Diseases for Mortality Prediction in Acute Hospital Care,Nozomi Nori (Kyoto University)Hisashi Kashima (Kyoto University)Kazuto Yamashita (Kyoto University)Hiroshi Ikai (Kyoto University)Yuichi Imanaka (Kyoto University),"1975772780,2126337623,2164856748,2042589429,2055260409","Acute hospital care as performed in the intensive care unit (ICU) is characterized by its frequent, but short-term interventions for patients who are severely ill. Because clinicians have to attend to more than one patient at a time and make decisions in a limited time in acute hospital care environments, the accurate prediction of the in-hospital mortality risk could assist them to pay more attention to patients with a higher in-hospital mortality risk, thereby improving the quality and efficiency of the care. One of the salient features of ICU is the diversity of patients: clinicians are faced by patients with a wide variety of diseases. However, mortality prediction for ICU patients has typically been conducted by building one common predictive model for all the diseases. In this paper, we incorporate disease-specific contexts into mortality modeling by formulating the mortality prediction problem as a multi-task learning problem in which a task corresponds to a disease. Our method effectively integrates medical domain knowledge relating to the similarity among diseases and the similarity among Electronic Health Records (EHRs) into a data-driven approach by incorporating graph Laplacians into the regularization term to encode these similarities. The experimental results on a real dataset from a hospital corroborate the effectiveness of the proposed method. The AUCs of several baselines were improved, including logistic regression without multi-task learning and several multi-task learning methods that do not incorporate the domain knowledge. In addition, we illustrate some interesting results pertaining to disease-specific predictive features, some of which are not only consistent with existing medical domain knowledge, but also contain suggestive hypotheses that could be validated by further investigations in the medical domain.",2015,Knowledge Discovery and Data Mining,Fields of study: multi task learninghealth caremachine learningcomputer science
Cinema Data Mining: The Smell of Fear,Jörg Wicker (University of Mainz)Nicolas Krauter (University of Mainz)Bettina Derstorff (Max Planck Society)Christof Stönner (Max Planck Society)Efstratios Bourtsoukidis (Max Planck Society)Thomas Klüpfel (Max Planck Society)Jonathan Williams (Max Planck Society)Stefan Kramer (University of Mainz),"2166053766,2229956027,2229772596,2228749554,2631270805,1979324151,2234095498,2272282606","While the physiological response of humans to emotional events or stimuli is well-investigated for many modalities (like EEG, skin resistance, ...), surprisingly little is known about the exhalation of so-called Volatile Organic Compounds (VOCs) at quite low concentrations in response to such stimuli. VOCs are molecules of relatively small mass that quickly evaporate or sublimate and can be detected in the air that surrounds us. The paper introduces a new field of application for data mining, where trace gas responses of people reacting on-line to films shown in cinemas (or movie theaters) are related to the semantic content of the films themselves. To do so, we measured the VOCs from a movie theater over a whole month in intervals of thirty seconds, and annotated the screened films by a controlled vocabulary compiled from multiple sources. To gain a better understanding of the data and to reveal unknown relationships, we have built prediction models for so-called forward prediction (the prediction of future VOCs from the past), backward prediction (the prediction of past scene labels from future VOCs), which is some form of abductive reasoning, and Granger causality. Experimental results show that some VOCs and some labels can be predicted with relatively low error, and that hint for causality with low p-values can be detected in the data. The data set is publicly available at: https://github.com/jorro/smelloffear.",2015,Knowledge Discovery and Data Mining,Fields of study: film analysisbreath gas analysisabductive reasoningcausalityatmospheric chemistrydata miningartificial intelligencemachine learningsimulationcomputer science
Non-transitive Hashing with Latent Similarity Components,Mingdong Ou (Tsinghua University)Peng Cui (Tsinghua University)Fei Wang (University of Connecticut)Jun Wang (Alibaba Group)Wenwu Zhu (Tsinghua University),"2157830877,2113115369,2465953593,2682315855,2111511002","Approximating the semantic similarity between entities in the learned Hamming space is the key for supervised hashing techniques. The semantic similarities between entities are often non-transitive since they could share different latent similarity components. For example, in social networks, we connect with people for various reasons, such as sharing common interests, working in the same company, being alumni and so on. Obviously, these social connections are non-transitive if people are connected due to different reasons. However, existing supervised hashing methods treat the pairwise similarity relationships in a simple and unified way and project data into a single Hamming space, while neglecting that the non-transitive property cannot be ade- quately captured by a single Hamming space. In this paper, we propose a non-transitive hashing method, namely Multi-Component Hashing (MuCH), to identify the latent similarity components to cope with the non-transitive similarity relationships. MuCH generates multiple hash tables with each hash table corresponding to a similarity component, and preserves the non-transitive similarities in different hash table respectively. Moreover, we propose a similarity measure, called Multi-Component Similarity, aggregating Hamming similarities in multiple hash tables to capture the non-transitive property of semantic similarity. We conduct extensive experiments on one synthetic dataset and two public real-world datasets (i.e. DBLP and NUS-WIDE). The results clearly demonstrate that the proposed MuCH method significantly outperforms the state-of-art hashing methods especially on search efficiency.",2015,Knowledge Discovery and Data Mining,Fields of study: k independent hashingfeature hashingsuhalocality preserving hashingdynamic perfect hashinguniversal hashinglocality sensitive hashingsemantic similarityhash functionhash tabledata miningpattern recognitionmachine learningcomputer sciencemathematics
Effective Audience Extension in Online Advertising,Jianqiang ShenSahin Cem GeyikAli Dasdan (Yahoo!),"2507557475,2704095071,4321789","In digital advertising, advertisers want to reach the right audience over media channels such as display, mobile, video, or social at the appropriate cost. The right audience for an advertiser consists of existing customers as well as valuable prospects, those that can potentially be turned into future customers. Identifying valuable prospects is called the audience extension problem because advertisers find new customers by extending the desirable criteria for their starting point, which is their existing audience or customers. The complexity of the audience extension problem stems from the difficulty of defining desirable criteria objectively, the number of desirable criteria (such as similarity, diversity, performance) to simultaneously satisfy, and the expected runtime (a few minutes) to find a solution over billions of cookie-based users. In this paper, we formally define the audience extension problem, propose an algorithm that extends a given audience set efficiently under multiple desirable criteria, and experimentally validate its performance. Instead of iterating over individual users, the algorithm takes in Boolean rules that define the seed audience and returns a new set of Boolean rules that corresponds to the extended audience that satisfy the multiple criteria.",2015,Knowledge Discovery and Data Mining,Fields of study: targetingonline advertisingmultimediacomputer science
Learning Tree Structure in Multi-Task Learning,Lei Han (Hong Kong Baptist University)Yu Zhang (Hong Kong Baptist University),"2531318857,2648094648","In multi-task learning (MTL), multiple related tasks are learned jointly by sharing information according to task relations. One promising approach is to utilize the given tree structure, which describes the hierarchical relations among tasks, to learn model parameters under the regularization framework. However, such a priori information is rarely available in most applications. To the best of our knowledge, there is no work to learn the tree structure among tasks and model parameters simultaneously under the regularization framework and in this paper, we develop a TAsk Tree (TAT) model for MTL to achieve this. By specifying the number of layers in the tree as H , the TAT method decomposes the parameter matrix into H component matrices, each of which corresponds to the model parameters in each layer of the tree. In order to learn the tree structure, we devise sequential constraints to make the distance between the parameters in the component matrices corresponding to each pair of tasks decrease over layers, and hence the component parameters will keep fused until the topmost layer, once they become fused in a layer. Moreover, to make the component parameters have chance to fuse in different layers, we develop a structural sparsity regularizer, which is the sum of the l 2 norm on the pairwise difference among the component parameters, to learn layer-specific task structure. In order to solve the resulting non-convex objective function, we use the general iterative shrinkage and thresholding (GIST) method. By using the alternating direction method of multipliers (ADMM) method, we decompose the proximal problem in the GIST method into three independent subproblems, where a key subproblem with the sequential constraints has an efficient solution as the other two subproblems do. We also provide some theoretical analysis for the TAT model. Experiments on both synthetic and real-world datasets show the effectiveness of the TAT model.",2015,Knowledge Discovery and Data Mining,Fields of study: multi task learningdata miningartificial intelligencemachine learningmathematical optimizationstatisticscomputer sciencemathematics
Web Personalization and Recommender Systems,Shlomo Berkovsky (Commonwealth Scientific and Industrial Research Organisation)Jill Freyne (Commonwealth Scientific and Industrial Research Organisation),"86410884,1613619093","The quantity of accessible information has been growing rapidly and far exceeded human processing capabilities. The sheer abundance of information often prevents users from discovering the desired information, or aggravates making informed and correct choices. This highlights the pressing need for intelligent personalized applications that simplify information access and discovery by taking into account users' preferences and needs. One type of personalized application that has recently become tremendously popular in research and industry is recommender systems. These provide to users personalized recommendations about information and products they may be interested to examine or purchase. Extensive research into recommender systems has yielded a variety of techniques, which have been published at a variety of conferences and adopted by numerous Web-sites. This tutorial will provide the participants with broad overview and thorough understanding of algorithms and practically deployed Web and mobile applications of personalized technologies.",2015,Knowledge Discovery and Data Mining,Fields of study: personalizationuser modelingrecommender systemmultimediaworld wide webdata miningmachine learningcomputer science
Adaptive Message Update for Fast Affinity Propagation,Yasuhiro Fujiwara (Nippon Telegraph and Telephone)Makoto Nakatsuji (Nippon Telegraph and Telephone)Hiroaki Shiokawa (Nippon Telegraph and Telephone)Yasutoshi Ida (Nippon Telegraph and Telephone)Machiko Toyoda (Nippon Telegraph and Telephone),"2157865440,2040281177,2139345634,2228300583,2118565305","Affinity Propagation is a clustering algorithm used in many applications. It iteratively updates messages between data points until convergence. The message updating process enables Affinity Propagation to have higher clustering quality compared with other approaches. However, its computation cost is high; it is quadratic in the number of data points. This is because it updates the messages of all data point pairs. This paper proposes an efficient algorithm that guarantees the same clustering results as the original algorithm. Our approach, F-AP , is based on two ideas: (1) it computes upper and lower estimates to limit the messages to be updated in each iteration, and (2) it dynamically detects converged messages to efficiently skip unneeded updates. Experiments show that F-AP is much faster than previous approaches with no loss in clustering performance.",2015,Knowledge Discovery and Data Mining,Fields of study: canopy clustering algorithmcorrelation clusteringconstrained clusteringdata stream clusteringcure data clustering algorithmaffinity propagationcluster analysistheoretical computer sciencedistributed computingdata miningmachine learningcomputer science
Leveraging Knowledge Bases for Contextual Entity Exploration,Joonseok Lee (Google)Ariel Fuxman (Google)Bo Zhao (LinkedIn)Yuanhua Lv (Microsoft),"2141663339,2665095167,2674375462,2132538679","Users today are constantly switching back and forth from applications where they consume or create content (such as e-books and productivity suites like Microsoft Office and Google Docs) to search engines where they satisfy their information needs. Unfortunately, though, this leads to a suboptimal user experience as the search engine lacks any knowledge about the content that the user is authoring or consuming in the application. As a result, productivity suites are starting to incorporate features that let the user ""explore while they work"". Existing work in the literature that can be applied to this problem takes a standard bag-of-words information retrieval approach, which consists of automatically creating a query that includes not only the target phrase or entity chosen by the user but also relevant terms from the context. While these approaches have been successful, they are inherently limited to returning results (documents) that have a syntactic match with the keywords in the query. We argue that the limitations of these approaches can be overcome by leveraging semantic signals from a knowledge graph built from knowledge bases such as Wikipedia. We present a system called Lewis for retrieving contextually relevant entity results leveraging a knowledge graph, and perform a large scale crowdsourcing experiment in the context of an e-reader scenario, which shows that Lewis can outperform the state-of-the-art contextual entity recommendation systems by more than 20% in terms of the MAP score.",2015,Knowledge Discovery and Data Mining,Fields of study: knowledge baseknowledge managementworld wide webdata miningartificial intelligencemachine learningcomputer science
Co-Clustering based Dual Prediction for Cargo Pricing Optimization,Yada Zhu (IBM)Hongxia Yang (Yahoo!)Jingrui He (Arizona State University),"2712115745,2701122602,2693123770","This paper targets the problem of cargo pricing optimization in the air cargo business. Given the features associated with a pair of origination and destination, how can we simultaneously predict both the optimal price for the bid stage and the outcome of the transaction (win rate) in the decision stage? In addition, it is often the case that the matrix representing pairs of originations and destinations has a block structure, i.e., the originations and destinations can be co-clustered such that the predictive models are similar within the same co-cluster, and exhibit significant variation among different co-clusters. How can we uncover the co-clusters of originations and destinations while constructing the dual predictive models for the two stages? We take the first step at addressing these problems. In particular, we propose a probabilistic framework to simultaneously construct dual predictive models and uncover the co-clusters of originations and destinations. It maximizes the conditional probability of observing the responses from both the quotation stage and the decision stage, given the features and the co-clusters. By introducing an auxiliary distribution based on the co-clustering assumption, such conditional probability can be converted into an objective function. To minimize the objective function, we propose the cocoa algorithm, which will generate both the suite of predictive models for all the pairs of originations and destinations, as well as the co-clusters consisting of similar pairs. Experimental results on both synthetic data and real data from cargo price bidding demonstrate the effectiveness and efficiency of the proposed algorithm.",2015,Knowledge Discovery and Data Mining,Fields of study: operations researchdata miningmachine learningsimulationcomputer science
The Child is Father of the Man: Foresee the Success at the Early Stage,Liangyue Li (Arizona State University)Hanghang Tong (Arizona State University),"2144246580,2676574370","Understanding the dynamic mechanisms that drive the high-impact scientific work (e.g., research papers, patents) is a long-debated research topic and has many important implications, ranging from personal career development and recruitment search, to the jurisdiction of research resources. Recent advances in characterizing and modeling scientific success have made it possible to forecast the long-term impact of scientific work, where data mining techniques, supervised learning in particular, play an essential role. Despite much progress, several key algorithmic challenges in relation to predicting long-term scientific impact have largely remained open. In this paper, we propose a joint predictive model to forecast the long-term scientific impact at the early stage, which simultaneously addresses a number of these open challenges, including the scholarly feature design, the non-linearity, the domain-heterogeneity and dynamics. In particular, we formulate it as a regularized optimization problem and propose effective and scalable algorithms to solve it. We perform extensive empirical evaluations on large, real scholarly data sets to validate the effectiveness and the efficiency of our method.",2015,Knowledge Discovery and Data Mining,Fields of study: operations researchdata miningmachine learningsimulationcomputer science
Analyzing Invariants in Cyber-Physical Systems using Latent Factor Regression,Marjan Momtazpour (Virginia Tech)Jinghe Zhang (Virginia Tech)Saifur Rahman (Virginia Tech)Ratnesh K. Sharma (NEC)Naren Ramakrishnan (Virginia Tech),"1429481628,2146925949,2601626281,2120320196,2199255697","The analysis of large scale data logged from complex cyber-physical systems, such as microgrids, often entails the discovery of invariants capturing functional as well as operational relationships underlying such large systems. We describe a latent factor approach to infer invariants underlying system variables and how we can leverage these relationships to monitor a cyber-physical system. In particular we illustrate how this approach helps rapidly identify outliers during system operation.",2015,Knowledge Discovery and Data Mining,Fields of study: regressionanomaly detectioneconometricsdata miningmachine learningstatisticscomputer sciencemathematics
Mining Administrative Data to Spur Urban Revitalization,Ben Green (Harvard University)Alejandra Caro (Carnegie Mellon University)Matthew Conway (University of Chicago)Robert Manduca (Harvard University)Tom Plagge (University of Chicago)Abby Miller,"2591994940,2225584860,2666088145,2607523582,809994977,2697136097","After decades of urban investment dominated by sprawl and outward growth, municipal governments in the United States are responsible for the upkeep of urban neighborhoods that have not received sufficient resources or maintenance in many years. One of city governments' biggest challenges is to revitalize decaying neighborhoods given only limited resources. In this paper, we apply data science techniques to administrative data to help the City of Memphis, Tennessee improve distressed neighborhoods. We develop new methods to efficiently identify homes in need of rehabilitation and to predict the impacts of potential investments on neighborhoods. Our analyses allow Memphis to design neighborhood-improvement strategies that generate greater impacts on communities. Since our work uses data that most US cities already collect, our models and methods are highly portable and inexpensive to implement. We also discuss the challenges we encountered while analyzing government data and deploying our tools, and highlight important steps to improve future data-driven efforts in urban policy.",2015,Knowledge Discovery and Data Mining,Fields of study: common gooddata mining
Robust Treecode Approximation for Kernel Machines,William B. March (University of Texas at Austin)Bo Xiao (University of Texas at Austin)Sameer Tharakan (University of Texas at Austin)Chenhan D. Yu (University of Texas at Austin)George Biros (University of Texas at Austin),"2155114799,2441062435,2225879173,2224177444,2056331068","Since exact evaluation of a kernel matrix requires O(N 2 ) work, scalable learning algorithms using kernels must approximate the kernel matrix. This approximation must be robust to the kernel parameters, for example the bandwidth for the Gaussian kernel. We consider two approximation methods: Nystrom and an algebraic treecode developed in our group. Nystrom methods construct a global low-rank approximation of the kernel matrix. Treecodes approximate just the off-diagonal blocks, typically using a hierarchical decomposition. We present a theoretical error analysis of our treecode and relate it to the error of Nystrom methods. Our analysis reveals how the block-rank structure of the kernel matrix controls the performance of the treecode. We evaluate our treecode by comparing it to the classical Nystrom method and a state-of-the-art fast approximate Nystrom method. We test the kernel matrix approximation accuracy for several different bandwidths and datasets. On the MNIST2M dataset (2M points in 784 dimensions) for a Gaussian kernel with bandwidth h=1 , the Nystrom methods' error is over 90% whereas our treecode delivers error less than 1%. We also test the performance of the three methods on binary classification using two models: a Bayes classifier and kernel ridge regression. Our evaluation reveals the existence of bandwidth values that should be examined in cross-validation but whose corresponding kernel matrices cannot be approximated well by Nystrom methods. In contrast, the treecode scheme performs much better for these values.",2015,Knowledge Discovery and Data Mining,Fields of study: kernel embedding of distributionsvariable kernel density estimationpolynomial kernelradial basis function kernelkernel principal component analysiskernel regressionkernel methodkernelpattern recognitionmachine learningmathematical optimizationcomputer sciencemathematics
LINKAGE: An Approach for Comprehensive Risk Prediction for Care Management,Zhaonan Sun (IBM)Fei Wang (University of Connecticut)Jianying Hu (IBM),"2309560616,2465953593,2098665416","Comprehensive risk assessment lies in the core of enabling proactive healthcare delivery systems. In recent years, data-driven predictive modeling approaches have been increasingly recognized as promising techniques to help enhance healthcare quality and reduce cost. In this paper, we propose a data-driven comprehensive risk prediction method, named LINKAGE, which can be used to jointly assess a set of associated risks in support of holistic care management. Our method can not only perform prediction but also discover the relationships among those risks. The advantages of the proposed model include: 1) It can leverage the relationship between risks and domains and achieve better risk prediction performance; 2) It provides a data-driven approach to understand relationship between risks; 3) It leverages the information between risk prediction and risk association learning to regulate the improvement on both parts; 4) It provides flexibility to incorporate domain knowledge in learning risk associations. We validate the effectiveness of the proposed model on synthetic data and a real-world healthcare survey data set.",2015,Knowledge Discovery and Data Mining,Fields of study: risk analysisgeneralized linear modelcovariance matrixdata sciencedata miningstatistics
Multi-View Incident Ticket Clustering for Optimal Ticket Dispatching,Mirela Madalina Botezatu (IBM)Jasmina Bogojeska (IBM)Ioana Giurgiu (IBM)Hagen Voelzer (IBM)Dorothea Wiesmann (IBM),"2229656828,317490117,2203074430,2224103501,2048997920","We present a novel technique that optimizes the dispatching of incident tickets to the agents in an IT Service Support Environment. Unlike the common skill-based dispatching, our approach also takes empirical evidence on the agent's speed from historical data into account. Our solution consists of two parts. First, a novel technique clusters historic tickets into incident categories that are discriminative in terms of agent's performance. Second, a dispatching policy selects, for an incoming ticket, the fastest available agent according to the target cluster. We show that, for ticket data collected from several Service Delivery Units, our new dispatching technique can reduce service time between $35\%$ and $44\%$.",2015,Knowledge Discovery and Data Mining,Fields of study: spectral clusteringfuzzy clusteringcutcomputer securitydata miningmachine learningsimulationcomputer science
The Effectiveness of Marketing Strategies in Social Media: Evidence from Promotional Events,Panagiotis Adamopoulos (New York University)Vilma Todri (New York University),"2048563838,336025091","This paper studies a novel social media venture and seeks to understand the effectiveness of marketing strategies in social media platforms by evaluating their impact on participating brands and organizations. We use a real-world data set and employ a promising research approach combining econometric with predictive modeling techniques in a causal estimation framework that allows for more accurate counterfactuals. Based on the results of the presented analysis and focusing on the long-term business value of marketing strategies in social media, we find that promotional events leveraging implicit or explicit advocacy in social media platforms result in significant abnormal returns for the participating brand, in terms of expanding the social media fan base of the firm. The effect is also economically significant as it corresponds to an increase of several thousand additional new followers per day for an average size brand. We also precisely quantify the impact of various promotion characteristics and demonstrate what types of promotions are more effective and for which brands, while suggesting specific tactical strategies. For instance, despite the competition for consumers' attention, brands and marketers should broadcast marketing messages on social networks during the time of peak usage in order to maximize their returns. Overall, we provide actionable insights with major implications for firms and social media platforms and contribute to the related literature as we discover new rich findings enabled by the employed causal estimation framework.",2015,Knowledge Discovery and Data Mining,Fields of study: business valuecounterfactual thinkingsocial mediaevent studycausal inferencebusiness analyticsdata miningcomputer science
"Diffusion in Social and Information Networks: Research Problems, Probabilistic Models and Machine Learning Methods",Manuel Gomez Rodriguez (Max Planck Society)Le Song (Georgia Institute of Technology),"2421855118,2113868374","In recent years, there has been an increasing effort on developing realistic models, and learning and inference algorithms to understand, predict, and influence diffusion over networks. This has been in part due to the increasing availability and granularity of large-scale diffusion data, which, in principle, allows for understanding and modeling not only macroscopic diffusion but also microscopic (node-level) diffusion. To this aim, a bottom-up approach has been typically considered, which starts by considering how particular ideas, pieces of information, products, or, more generally, contagions spread locally from node to node apparently at random to later produce global, macroscopic patterns at a network level.However, this bottom-up approach also raises significant modeling, algorithmic and computational challenges which require leveraging methods from machine learning, probabilistic modeling, temporal point processes and graph theory, as well as the nascent field of network science. In this tutorial, we will present several diffusion models designed for fine-grained large-scale diffusion and social event data, present some canonical research problem in the context of diffusion, and introduce state-of-the-art algorithms to solve some of these problems, in particular, network estimation, influence estimation and control, and rumor source identification.",2015,Knowledge Discovery and Data Mining,Fields of study: social influencesocial networktheoretical computer sciencedata miningartificial intelligencemachine learningstatisticscomputer sciencemathematics
Learning with Similarity Functions on Graphs using Matchings of Geometric Embeddings,Fredrik D. Johansson (Chalmers University of Technology)Devdatt P. Dubhashi (Chalmers University of Technology),"2138628933,729227961","We develop and apply the Balcan-Blum-Srebro (BBS) theory of classification via similarity functions (which are not necessarily kernels) to the problem of graph classification. First we place the BBS theory into the unifying framework of optimal transport theory. This also opens the way to exploit coupling methods for establishing properties required of a good similarity function as per their definition. Next, we use the approach to the problem of graph classification via geometric embeddings such as the Laplacian, pseudo-inverse Laplacian and the Lovasz orthogonal labellings. We consider the similarity function given by optimal and near--optimal matchings with respect to Euclidean distance of the corresponding embeddings of the graphs in high dimensions. We use optimal couplings to rigorously establish that this yields a ""good"" similarity measure in the BBS sense for two well known families of graphs. Further, we show that the similarity yields better classification accuracy in practice, on these families, than matchings of other well-known graph embeddings. Finally we perform an extensive empirical evaluation on benchmark data sets where we show that classifying graphs using matchings of geometric embeddings outperforms the previous state-of-the-art methods.",2015,Knowledge Discovery and Data Mining,Fields of study: topological graph theorygraphbiological classificationdiscrete mathematicscombinatoricstopologymathematics
Dimensionality Reduction Via Graph Structure Learning,Qi Mao (University at Buffalo)Li Wang (University of Victoria)Steve Goodison (Mayo Clinic)Yijun Sun (University at Buffalo),"2494340745,2570154708,1965542480,2110420596","We present a new dimensionality reduction setting for a large family of real-world problems. Unlike traditional methods, the new setting aims to explicitly represent and learn an intrinsic structure from data in a high-dimensional space, which can greatly facilitate data visualization and scientific discovery in downstream analysis. We propose a new dimensionality-reduction framework that involves the learning of a mapping function that projects data points in the original high-dimensional space to latent points in a low-dimensional space that are then used directly to construct a graph. Local geometric information of the projected data is naturally captured by the constructed graph. As a showcase, we develop a new method to obtain a discriminative and compact feature representation for clustering problems. In contrast to assumptions used in traditional clustering methods, we assume that centers of clusters should be close to each other if they are connected in a learned graph, and other cluster centers should be distant. Extensive experiments are performed that demonstrate that the proposed method is able to obtain discriminative feature representations yielding superior clustering performance, and correctly recover the intrinsic structures of various real-world datasets including curves, hierarchies and a cancer progression path.",2015,Knowledge Discovery and Data Mining,Fields of study: correlation clusteringdimensionality reductioncluster analysisunsupervised learningdata miningpattern recognitionmachine learningcomputer sciencemathematics
Traffic Measurement and Route Recommendation System for Mass Rapid Transit (MRT),Thomas HolleczekShanyang YinYunye JinSpiros Antonatos (Foundation for Research & Technology – Hellas)Han Leong GohSamantha LowAmy Shi-Nash,"2705948413,2230685310,2718824404,2037133037,2641849635,2716035187,2345700509","Understanding how people use public transport is important for the operation and future planning of the underlying transport networks. We have therefore developed and deployed a traffic measurement system for a key player in the transportation industry to gain insights into crowd behavior for planning purposes. The system has been in operation for several months and reports, at hourly intervals, (1) the crowdedness of subway stations, (2) the flows of people inside interchange stations, and (3) the expected travel time for each possible route in the subway network of Singapore. The core of our system is an efficient algorithm which detects individual subway trips from anonymized real-time data generated by the location based system of Singtel, the country's largest telecommunications company. To assess the accuracy of our system, we engaged an independent market research company to conduct a field study--a manual count of the number of passengers boarding and disembarking at a selected station on three separate days. A strong correlation between the calculations of our algorithm and the manual counts was found. One of our key findings is that travelers do not always choose the route with the shortest travel time in the subway network of Singapore. We have therefore also been developing a mobile app which allows users to plan their trips based on the average travel time between stations.",2015,Knowledge Discovery and Data Mining,Fields of study: public transportcellular networksimulationcomputer science
VC-Dimension and Rademacher Averages: From Statistical Learning Theory to Sampling Algorithms,Matteo Riondato (Brown University)Eli Upfal (Brown University),"1555209364,265232414","Rademacher Averages and the Vapnik-Chervonenkis dimension are fundamental concepts from statistical learning theory. They allow to study simultaneous deviation bounds of empirical averages from their expectations for classes of functions, by considering properties of the functions, of their domain (the dataset), and of the sampling process. In this tutorial, we survey the use of Rademacher Averages and the VC-dimension in sampling-based algorithms for graph analysis and pattern mining. We start from their theoretical foundations at the core of machine learning, then show a generic recipe for formulating data mining problems in a way that allows to use these concepts in efficient randomized algorithms for those problems. Finally, we show examples of the application of the recipe to graph problems (connectivity, shortest paths, betweenness centrality) and pattern mining. Our goal is to expose the usefulness of these techniques for the data mining researcher, and to encourage research in the area.",2015,Knowledge Discovery and Data Mining,Fields of study: betweenness centralityrandomized algorithmcombinatoricsdata miningmachine learningstatisticscomputer sciencemathematics
Model Multiple Heterogeneity via Hierarchical Multi-Latent Space Learning,Pei Yang (Arizona State University)Jingrui He (Arizona State University),"2309210939,2693123770","In many real world applications such as satellite image analysis, gene function prediction, and insider threat detection, the data collected from heterogeneous sources often exhibit multiple types of heterogeneity, such as task heterogeneity, view heterogeneity, and label heterogeneity. To address this problem, we propose a Hierarchical Multi-Latent Space (HiMLS) learning approach to jointly model the triple types of heterogeneity. The basic idea is to learn a hierarchical multi-latent space by which we can simultaneously leverage the task relatedness, view consistency and the label correlations to improve the learning performance. We first propose a multi-latent space framework to model the complex heterogeneity, which is used as a building block to stack up a multi-layer structure so as to learn the hierarchical multi-latent space. In such a way, we can gradually learn the more abstract concepts in the higher level. Then, a deep learning algorithm is proposed to solve the optimization problem. The experimental results on various data sets show the effectiveness of the proposed approach.",2015,Knowledge Discovery and Data Mining,Fields of study: multi task learningsemi supervised learningdata miningmachine learningsimulationcomputer science
Should You Trust Your Money to a Robot,Vasant Dhar (New York University),2184282108,"Computers are making more and more decisions for us, and increasingly so in areas that require human judgment. There is a palpable increase in machine intelligence across the touch points of our lives, driven by the proliferation of data feeding into intelligent algorithms capable of learning useful patterns and acting on them. A natural question to ask is how we should be thinking about the role of computers in managing our money. Should we trust our money to a robot? In an era of big data and machines to make sense of it all, do machines have an inherent advantage over humans? There is a surge of interest in Artificial Intelligence for financial prediction. Should we pay attention? Or is this an area where human judgment and input is always essential?",2015,Knowledge Discovery and Data Mining,Fields of study: quantitative investingcomputational financedata miningartificial intelligencemachine learningsimulationcomputer science
Big Data Analytics: Optimization and Randomization,Tianbao Yang (University of Iowa)Qihang Lin (University of Iowa)Rong Jin (Michigan State University),"2151859571,2668648025,2142724104","As the scale and dimensionality of data continue to grow in many applications of data analytics (e.g., bioinformatics, finance, computer vision, medical informatics), it becomes critical to develop efficient and effective algorithms to solve numerous machine learning and data mining problems. This tutorial will focus on simple yet practically effective techniques and algorithms for big data analytics. In the first part, we plan to present the state-of-the-art large-scale optimization algorithms, including various stochastic gradient descent methods, stochastic coordinate descent methods and distributed optimization algorithms, for solving various machine learning problems. In the second part, we will focus on randomized approximation algorithms for learning from large-scale data. We will discuss i) randomized algorithms for low-rank matrix approximation; ii) approximation techniques for solving kernel learning problems; iii) randomized reduction methods for addressing the high-dimensional challenge. Along with the description of algorithms, we will also present some empirical results to facilitate understanding of different algorithms and comparison between them.",2015,Knowledge Discovery and Data Mining,Fields of study: randomized algorithms as zero sum gamesonline machine learningtheoretical computer sciencedata miningmachine learningcomputer science
User Modeling in Telecommunications and Internet Industry,Qiang Yang (Hong Kong University of Science and Technology),2109031554,"It is extremely important in many application domains to have accurate models of user behavior. Data mining allows user models to be constructed based on vast available data automatically. User modeling has found applications in mobile APP recommendations, social networking, financial product marketing and customer service in telecommunications. Successful user modeling should be aware of several critical issues: who are the target users' How should the solutions be updated when new data come in? How should user feedback be handled? What are the ""pain"" points of users' In this talk, I will discuss my own experience on user modeling with big data. I will draw examples from telecommunications and the Internet industry, contrasting and highlighting some lessons learned in these industries.",2015,Knowledge Discovery and Data Mining,Fields of study: user journeycomputer user satisfactioninfranetbasic serviceuser interface designuser modelingthe internetknowledge managementworld wide webdata miningcomputer science
An Architecture for Agile Machine Learning in Real-Time Applications,Johann Schleier-Smith,2670928054,"Machine learning techniques have proved effective in recommender systems and other applications, yet teams working to deploy them lack many of the advantages that those in more established software disciplines today take for granted. The well-known Agile methodology advances projects in a chain of rapid development cycles, with subsequent steps often informed by production experiments. Support for such workflow in machine learning applications remains primitive. The platform developed at if(we) embodies a specific machine learning approach and a rigorous data architecture constraint, so allowing teams to work in rapid iterative cycles. We require models to consume data from a time-ordered event history, and we focus on facilitating creative feature engineering. We make it practical for data scientists to use the same model code in development and in production deployment, and make it practical for them to collaborate on complex models. We deliver real-time recommendations at scale, returning top results from among 10,000,000 candidates with sub-second response times and incorporating new updates in just a few seconds. Using the approach and architecture described here, our team can routinely go from ideas for new models to production-validated results within two weeks.",2015,Knowledge Discovery and Data Mining,Fields of study: agile software developmentrecommender systemknowledge managementdata miningmachine learningsimulationcomputer science
Stock Constrained Recommendation in Tmall,Wenliang Zhong (Alibaba Group)Rong Jin (Alibaba Group)Cheng Yang (Alibaba Group)Xiaowei Yan (Alibaba Group)Qi Zhang (Alibaba Group)Qiang Li (Alibaba Group),"2222329098,2718565268,2306458749,2282347060,2307585989,2304054576","A large number of recommender systems have been developed to serve users with interesting news, ads, products or other contents. One main limitation with the existing work is that they do not take into account the inventory size of of items to be recommended. As a result, popular items are likely to be out of stock soon as they have been recommended and sold to many users, significantly affecting the impact of recommendation and user experience. This observation motivates us to develop a novel aware recommender system. It jointly optimizes the recommended items for all users based on both user preference and inventory sizes of different items. It requires solving a non-smooth optimization involved estimating a matrix of nxn, where n is the number of items. With the proliferation of items, this approach can quickly become computationally infeasible. We address this challenge by developing a dual method that reduces the number of variables from n^2 to n, significantly improving the computational efficiency. We also extend this approach to the online setting, which is particularly important for big promotion events. Our empirical studies based on a real benchmark data with 100 millions of user visits from Tmall verify the effectiveness of the proposed approach.",2015,Knowledge Discovery and Data Mining,Fields of study: data miningcomputer science
On Estimating the Swapping Rate for Categorical Data,Daniel Kifer (Pennsylvania State University),2049563562,"When analyzing data, it is important to account for all sources of noise. Public use datasets, such as those provided by the Census Bureau, often undergo additional perturbations designed to protect confidentiality. This source of noise is generally ignored in data analysis because crucial parameters and details about its implementation are withheld. In this paper, we consider the problem of inferring such parameters from the data. Specifically, we target data swapping, a perturbation technique commonly used by the U.S. Census Bureau and which, barring practical breakthroughs in disclosure control, will be used in the foreseeable future. The vanilla version of data swapping selects pairs of records and exchanges some of their attribute values. The number of swapped records is kept secret even though it is needed for data analysis and investigations into the confidentiality protection of individual records. We propose algorithms for estimating the number of swapped records in categorical data, even when the true data distribution is unknown.",2015,Knowledge Discovery and Data Mining,Fields of study: data sciencecomputer securitydata miningstatisticscomputer science
Dynamic Matrix Factorization with Priors on Unknown Values,Robin Devooght (Université libre de Bruxelles)Nicolas Kourtellis (Telefónica)Amin Mantrach (Yahoo!),"2232492700,67437274,2027707398","Advanced and effective collaborative filtering methods based on explicit feedback assume that unknown ratings do not follow the same model as the observed ones not missing at random ). In this work, we build on this assumption, and introduce a novel dynamic matrix factorization framework that allows to set an explicit prior on unknown values. When new ratings, users, or items enter the system, we can update the factorization in time independent of the size of data (number of users, items and ratings). Hence, we can quickly recommend items even to very recent users. We test our methods on three large datasets, including two very sparse ones, in static and dynamic conditions. In each case, we outrank state-of-the-art matrix factorization methods that do not use a prior on unknown ratings.",2015,Knowledge Discovery and Data Mining,Fields of study: collaborative filteringmatrix decompositionrecommender systemtheoretical computer sciencedata miningmachine learningcomputer sciencemathematics
Online Outlier Exploration Over Large Datasets,Lei Cao (Worcester Polytechnic Institute)Mingrui Wei (Worcester Polytechnic Institute)Di Yang (Oracle Corporation)Elke A. Rundensteiner (Worcester Polytechnic Institute),"2114558166,2103758647,2558989459,2709761886","Traditional outlier detection systems process each individual outlier detection request instantiated with a particular parameter setting one at a time. This is not only prohibitively time-consuming for large datasets, but also tedious for analysts as they explore the data to hone in on the appropriate parameter setting and desired results. In this work, we present the first online outlier exploration platform, called ONION, that enables analysts to effectively explore anomalies even in large datasets. First, ONION features an innovative interactive anomaly exploration model that offers an ""outlier-centric panorama"" into big datasets along with rich classes of exploration operations. Second, to achieve this model ONION employs an online processing framework composed of a one time offline preprocessing phase followed by an online exploration phase that enables users to interactively explore the data. The preprocessing phase compresses raw big data into a knowledge-rich ONION abstraction that encodes critical interrelationships of outlier candidates so to support subsequent interactive outlier exploration. For the interactive exploration phase, our ONION framework provides several processing strategies that efficiently support the outlier exploration operations. Our user study with real data confirms the effectiveness of ONION in recognizing ""true"" outliers. Furthermore as demonstrated by our extensive experiments with large datasets, ONION supports all exploration operations within milliseconds response time.",2015,Knowledge Discovery and Data Mining,Fields of study: outlierdata sciencedata miningmachine learningstatistics
L∞ Error and Bandwidth Selection for Kernel Density Estimates of Large Data,Yan Zheng (University of Utah)Jeff M. Phillips (University of Utah),"2270987610,2162012049","Kernel density estimates are a robust way to reconstruct a continuous distribution from a discrete point set. Typically their effectiveness is measured either in L 1 or L 2 error. In this paper we investigate the challenges in using L ∞ (or worst case) error, a stronger measure than L 1 or L 2 . We present efficient solutions to two linked challenges: how to evaluate the L ∞ error between two kernel density estimates and how to choose the bandwidth parameter for a kernel density estimate built on a subsample of a large data set.",2015,Knowledge Discovery and Data Mining,Fields of study: kernel embedding of distributionsvariable kernel density estimationkernel density estimationkerneleconometricsmathematical optimizationstatisticsmathematics
Big Data System for Analyzing Risky Procurement Entities,Amit Dhurandhar (IBM)Bruce Graves (IBM)Rajesh Kumar Ravi (IBM)Gopikrishnan Maniachari (IBM)Markus Ettl (IBM),"2061123877,2106475288,2171459363,2227229003,2078921895","An accredited biennial 2014 study by the Association of Certified Fraud Examiners claims that on average 5% of a company's revenue is lost because of unchecked fraud every year. The reason for such heavy losses are that it takes around 18 months for a fraud to be caught and audits catch only 3% of the actual fraud. This begs the need for better tools and processes to be able to quickly and cheaply identify potential malefactors. In this paper, we describe a robust tool to identify procurement related fraud/risk, though the general design and the analytical components could be adapted to detecting fraud in other domains. Besides analyzing standard transactional data, our solution analyzes multiple public and private data sources leading to wider coverage of fraud types than what generally exists in the marketplace. Moreover, our approach is more principled in the sense that the learning component, which is based on investigation feedback has formal guarantees. Though such a tool is ever evolving, a deployment of this tool over the past 12 months has found many interesting cases from compliance risk and fraud point of view across more than 150 countries and 65000+ vendors, increasing the number of true positives found by over 80\% compared with other state-of-the-art tools that the domain experts were previously using.",2015,Knowledge Discovery and Data Mining,Fields of study: constructive fraudprocurementrisksocial networkbig datacomputer securitydata miningcomputer science
Discovering Valuable items from Massive Data,Hastagiri P. Vanchinathan (ETH Zurich)Andreas Marfurt (ETH Zurich)Charles-Antoine RobelinDonald Kossmann (ETH Zurich)Andreas Krause (ETH Zurich),"1586316923,2125483074,2721590600,2242617642,2165121431","Suppose there is a large collection of items, each with an associated cost and an inherent utility that is revealed only once we commit to selecting it. Given a budget on the cumulative cost of the selected items, how can we pick a subset of maximal value? This task generalizes several important problems such as multi-arm bandits, active search and the knapsack problem. We present an algorithm, GP-SELECT, which utilizes prior knowledge about similarity between items, expressed as a kernel function. GP-SELECT uses Gaussian process prediction to balance exploration (estimating the unknown value of items) and exploitation (selecting items of high value). We extend GP-SELECT to be able to discover sets that simultaneously have high utility and are diverse. Our preference for diversity can be specified as an arbitrary monotone submodular function that quantifies the diminishing returns obtained when selecting similar items. Furthermore, we exploit the structure of the model updates to achieve an order of magnitude (up to 40X) speedup in our experiments without resorting to approximations. We provide strong guarantees on the performance of GP-SELECT and apply it to three real-world case studies of industrial relevance: (1) Refreshing a repository of prices in a Global Distribution System for the travel industry, (2) Identifying diverse, binding-affine peptides in a vaccine design task and (3) Maximizing clicks in a web-scale recommender system by recommending items to users.",2015,Knowledge Discovery and Data Mining,Fields of study: active learningkernel methoddesign of experimentsrecommender systemdata miningmachine learningmathematical optimizationstatisticscomputer sciencemathematics
An Efficient Semi-Supervised Clustering Algorithm with Sequential Constraints,Jinfeng Yi (IBM)Lijun Zhang (Nanjing University)Tianbao Yang (University of Iowa)Wei Liu (IBM)Jun Wang (Alibaba Group),"2102191302,2276088550,2151859571,2182080890,2682315855","Semi-supervised clustering leverages side information such as pairwise constraints to guide clustering procedures. Despite promising progress, existing semi-supervised clustering approaches overlook the condition of side information being generated sequentially, which is a natural setting arising in numerous real-world applications such as social network and e-commerce system analysis. Given emerged new constraints, classical semi-supervised clustering algorithms need to re-optimize their objectives over all data samples and constraints in availability, which prevents them from efficiently updating the obtained data partitions. To address this challenge, we propose an efficient dynamic semi-supervised clustering framework that casts the clustering problem into a search problem over a feasible convex set, i.e. , a convex hull with its extreme points being an ensemble of m data partitions. According to the principle of ensemble clustering, the optimal partition lies in the convex hull, and can thus be uniquely represented by an m -dimensional probability simplex vector. As such, the dynamic semi-supervised clustering problem is simplified to the problem of updating a probability simplex vector subject to the newly received pairwise constraints. We then develop a computationally efficient updating procedure to update the probability simplex vector in O ( m 2 ) time, irrespective of the data size n . Our empirical studies on several real-world benchmark datasets show that the proposed algorithm outperforms the state-of-the-art semi-supervised clustering algorithms with visible performance gain and significantly reduced running time.",2015,Knowledge Discovery and Data Mining,Fields of study: k medians clusteringflame clusteringbrown clusteringcanopy clustering algorithmdbscancorrelation clusteringconstrained clusteringdata stream clusteringcure data clustering algorithmaffinity propagationfuzzy clusteringclustering high dimensional dataconvex hullcluster analysisconsensus clusteringbiclusteringdata miningmachine learningmathematical optimizationstatisticsmathematics
Turn Waste into Wealth: On Simultaneous Clustering and Cleaning over Dirty Data,Shaoxu Song (Tsinghua University)Chunping Li (Tsinghua University)Xiaoquan Zhang (Tsinghua University),"2649879278,2147389682,2492126484","Dirty data commonly exist. Simply discarding a large number of inaccurate points (as noises) could greatly affect clustering results. We argue that dirty data can be repaired and utilized as strong supports in clustering. To this end, we study a novel problem of clustering and repairing over dirty data at the same time. Referring to the minimum change principle in data repairing, the objective is to find a minimum modification of inaccurate points such that the large amount of dirty data can enhance the clustering. We show that the problem can be formulated as an integer linear programming (ILP) problem. Efficient approximation is then devised by a linear programming (LP) relaxation. In particular, we illustrate that an optimal solution of the LP problem can be directly obtained without calling a solver . A quadratic time approximation algorithm is developed based on the aforesaid LP solution. We further advance the algorithm to linear time cost, where a trade-off between effectiveness and efficiency is enabled. Empirical results demonstrate that both the clustering and cleaning accuracies can be improved by our approach of repairing and utilizing the dirty data in clustering.",2015,Knowledge Discovery and Data Mining,Fields of study: constrained clusteringdata stream clusteringdata cleansingdata miningmachine learningmathematical optimizationstatisticscomputer sciencemathematics
Scalable Machine Learning Approaches for Neighborhood Classification Using Very High Resolution Remote Sensing Imagery,Manu Sethi (University of Florida)Yupeng Yan (University of Florida)Anand Rangarajan (University of Florida)Ranga Raju Vatsavai (North Carolina State University)Sanjay Ranka (University of Florida),"2127541307,2139780352,2031572747,2121870003,2155894599","Urban neighborhood classification using very high resolution (VHR) remote sensing imagery is a challenging and {\em emerging} application. A semi-supervised learning approach for identifying neighborhoods is presented which employs superpixel tessellation representations of VHR imagery. The image representation utilizes homogeneous and irregularly shaped regions termed superpixels and derives novel features based on intensity histograms, geometry, corner and superpixel density and scale of tessellation. The semi-supervised learning approach uses a support vector machine (SVM) to obtain a preliminary classification which is then subsequently refined using graph Laplacian propagation. Several intermediate stages in the pipeline are presented to showcase the important features of this approach. We evaluated this approach on four different geographic settings with varying neighborhood types and compared it with the recent Gaussian Multiple Learning algorithm. This evaluation shows several advantages, including model building, accuracy, and efficiency which makes it a great choice for deployment in large scale applications like global human settlement mapping and population distribution (e.g., LandScan), and change detection.",2015,Knowledge Discovery and Data Mining,Fields of study: segmentationcomputer visionmachine learning
Social Media Anomaly Detection: Challenges and Solutions,Yan Liu (University of Southern California)Sanjay Chawla (Qatar Computing Research Institute),"2240541904,2201421368","Anomaly detection is of critical importance to prevent malicious activities such as bullying, terrorist attack planning, and fraud information dissemination. With the recent popularity of social media, new types of anomalous behaviors arise, causing concerns from various parties. While a large body of work haven been dedicated to traditional anomaly detection problems, we observe a surge of research interests in the new realm of social media anomaly detection. In this tutorial, we survey existing work on social media anomaly detection, focusing on the new anomalous phenomena in social media and most recent techniques to detect those special types of anomalies. We aim to provide a general overview of the problem domain, common formulations, existing methodologies and future directions.",2015,Knowledge Discovery and Data Mining,Fields of study: anomaly detectioninternet privacycomputer securitymachine learningcomputer science
Reducing the Unlabeled Sample Complexity of Semi-Supervised Multi-View Learning,Chao Lan (University of Kansas)Jun Huan (University of Kansas),"2125332459,2139058963","In semi-supervised multi-view learning, unlabeled sample complexity (u.s.c.) specifies the size of unlabeled training sample that guarantees a desired learning error. In this paper, we improve the state-of-art u.s.c. from O (1/e) to O (log 1/e) for small error e, under mild conditions. To obtain the improved result, as a primary step we prove a connection between the generalization error of a classifier and its incompatibility, which measures the fitness between the classifier and the sample distribution. We then prove that with a sufficiently large unlabeled sample, one is able to find classifiers with low incompatibility. Combining the two observations, we manage to prove a probably approximately correct (PAC) style learning bound for semi-supervised multi-view learning. We empirically verified our theory by designing two proof-of-concept multi-view learning algorithms, one based on active view sensing and the other based on online co-regularization, with real-world data sets.",2015,Knowledge Discovery and Data Mining,Fields of study: stabilityprobably approximately correct learninggeneralization erroractive learninglearning classifier systemsemi supervised learningunsupervised learningdata miningpattern recognitionmachine learningcomputer sciencemathematics
Website Optimization Problem and Its Solutions,Shuhei Iitsuka (University of Tokyo)Yutaka Matsuo (University of Tokyo),"2049774231,2717383051","Online controlled experiments are widely used to improve the performance of websites by comparison of user behavior related to different variations of the given website. Although such experiments might have an important effect on the key metrics to maximize, small-scale websites have difficulty applying this methodology because they have few users. Furthermore, the candidate variations increase exponentially with the number of elements that must be optimized. A testing method that finds a high-performing variation with a few samples must be devised to address these problems. As described herein, we formalize this problem as a website optimization problem and provide a basis to apply existing search algorithms to this problem. We further organize existing testing methods and extract devices to make the experiments more effective. By combining organized algorithms and devices, we propose a rapid testing method that detects high-performing variations with few users. We evaluated our proposed method using simulation experiments. Results show that it outperforms existing methods at any website scale. Moreover, we implemented our proposed method as an optimizer program and used it on an actual small-scale website. Results show that our proposed method can achieve 57% higher performance variation than that of the generally used A/B testing method. Therefore, our proposed method can optimize a website with fewer samples. The website optimization problem has broad application possibilities that are applicable not only to websites but also to manufactured goods.",2015,Knowledge Discovery and Data Mining,Fields of study: world wide webdata miningsimulationcomputer science
Deep Learning Architecture with Dynamically Programmed Layers for Brain Connectome Prediction,Vivek Veeriah (University of Central Florida)Rohit Durvasula (University of Central Florida)Guo-Jun Qi (University of Central Florida),"2653946760,2227803547,2237849324",This paper explores the idea of using deep neural network architecture with dynamically programmed layers for brain connectome prediction problem. Understanding the brain connectome structure is a very interesting and a challenging problem. It is critical in the research for epilepsy and other neuropathological diseases. We introduce a new deep learning architecture that exploits the spatial and temporal nature of the neuronal activation data. The architecture consists of a combination of Convolutional layer and a Recurrent layer for predicting the connectome of neurons based on their time-series of activation data. The key contribution of this paper is a dynamically programmed layer that is critical in determining the alignment between the neuronal activations of pair-wise combinations of neurons.,2015,Knowledge Discovery and Data Mining,Fields of study: deep learningartificial intelligencemachine learningcomputer science
Optimal Kernel Group Transformation for Exploratory Regression Analysis and Graphics,Pan Chao (Purdue University)Qiming Huang (Purdue University)Michael Zhu (Purdue University),"2628425722,2686404946,2576722380","The general goal of multivariate regression analysis is to infer about the relationship between a response variable Y and a predictor vector X . Many popularly used regression methods only focus on specific aspects of this relationship. Even though the conditional distribution P(Y|X) of Y given X fully characterizes the relationship between Y and X , estimation of the conditional density is challenging and becomes quickly infeasible when the dimension of X increases. In this article, we propose to use optimal group transformations as a general approach for exploring the relationship between Y and X . This approach can be considered an automatic procedure to identify the best characteristic of P(Y|X) under which the relationship between Y and X can be fully explored. The emphasis on using group transformations allows the approach to recover intrinsic group structures among the predictors. Furthermore, we develop kernel methods for estimating the optimal group transformations based on cross-covariance and conditional covariance operators. The statistical consistency of the estimates has been established. We refer to the proposed framework and approach as the Optimal Kernel Group Transformation (OKGT) method. Simulation study and real data applications show that the OKGT method is flexible and powerful for the general purpose of high dimensional regression analysis.",2015,Knowledge Discovery and Data Mining,Fields of study: polynomial regressionkernel regressionkernel methodeigendecomposition of a matrixregression analysiseconometricsdata miningmachine learningmathematical optimizationstatisticscomputer sciencemathematics
Spoken English Grading: Machine Learning with Crowd Intelligence,Vinay ShashidharNishant PandeyVarun Aggarwal,"2165579477,2101202866,2608931570","In this paper, we address the problem of grading spontaneous speech using a combination of machine learning and crowdsourcing. Traditional machine learning techniques solve the stated problem inadequately as automatic speaker-independent speech transcription is inaccurate. The features derived from it are also inaccurate and so is the machine learning model developed for speech evaluation. We propose a framework that combines machine learning with crowdsourcing. This entails identifying human intelligence tasks in the feature derivation step and using crowdsourcing to get them completed. We post the task of speech transcription to a large community of online workers (crowd). We also get spoken English grades from the crowd. We achieve 95% transcription accuracy by combining transcriptions from multiple crowd workers. Speech and prosody features are derived by force aligning the speech samples on these highly accurate transcriptions. Additionally, we derive surface and semantic level features directly from the transcription. We demonstrate the efficacy of our approach by predicting expert graded speech sample of 566 adult non-native speakers across two different countries - India and Philippines. Using the regression modeling technique, we are able achieve a Pearson correlation of 0.79 on the Philippines set and 0.74 on the Indian set with expert grades, an accuracy much higher than any previously reported machine learning approach. Our approach has an accuracy that rivals that of expert agreement. We show the value of the system through a case study in a real-world industrial deployment. This work is timely given the huge requirement of spoken English training and assessment.",2015,Knowledge Discovery and Data Mining,Fields of study: crowdsourcingnatural language processingspeech recognitiondata miningmachine learningcomputer science
Clouded Intelligence,Joseph Sirosh (Microsoft),2294877031,"Several exciting trends are driving the birth of the intelligent cloud. The vast majority of world's data is now connected data resident in the cloud. The majority of world's new software is now connected software, also resident in or using the cloud. New cloud based Machine Learning as a Service platforms help transform data into intelligence and build cloud-hosted intelligent APIs for connected software applications. Face analysis, computer vision, text analysis, speech recognition, and more traditional analytics such as churn prediction, recommendations, anomaly detection, forecasting, and clustering are all available now as cloud APIs, and far more are being created at a rapid pace. Cloud hosted marketplaces for crowdsourcing intelligent APIs have been launched. In this talk I will review what these trends mean for the future of data science and show examples of revolutionary applications that you can build using cloud platforms.",2015,Knowledge Discovery and Data Mining,Fields of study: cloud testingcloud computing securitycloud computingdata scienceworld wide webdata miningmachine learningcomputer science
Why It Happened: Identifying and Modeling the Reasons of the Happening of Social Events,Yu Rong (The Chinese University of Hong Kong)Hong Cheng (The Chinese University of Hong Kong)Zhiyu Mo (The Chinese University of Hong Kong),"2232298297,2161754280,2213490376","In nowadays social networks, a huge volume of content containing rich information, such as reviews, ratings, microblogs, etc., is being generated, consumed and diffused by users all the time. Given the temporal information, we can obtain the event cascade which indicates the time sequence of the arrival of information to users. Many models have been proposed to explain how information diffuses. However, most existing models cannot give a clear explanation why every specific event happens in the event cascade. Such explanation is essential for us to have a deeper understanding of information diffusion as well as a better prediction of future event cascade. In order to uncover the mechanism of the happening of social events, we analyze the rating event data crawled from Douban.com, a Chinese social network, from year 2006 to 2011. We distinguish three factors: social, external and intrinsic influence which can explain the emergence of every specific event. Then we use the mixed Poisson process to model event cascade generated by different factors respectively and integrate different Poisson processes with shared parameters. The proposed model, called Combinational Mixed Poisson Process (CMPP) model, can explain not only how information diffuses in social networks, but also why a specific event happens. This model can help us to understand information diffusion from both macroscopic and microscopic perspectives. We develop an efficient Classification EM algorithm to infer the model parameters. The explanatory and predictive power of the proposed model has been demonstrated by the experiments on large real data sets.",2015,Knowledge Discovery and Data Mining,Fields of study: event cascadeinformation cascadesocial influencepoisson processdata miningartificial intelligencestatistics
Distributed Personalization,Xu Miao (LinkedIn)Chun-Te Chu (Microsoft)Lijun Tang (LinkedIn)Yitong Zhou (LinkedIn)Joel Young (LinkedIn)Anmol Bhasin (LinkedIn),"2147399778,2652808727,2567120632,2294316712,2658387729,2157708525","Personalization is a long-standing problem in data mining and machine learning. Companies make personalized product recommendations to millions of users every second. In addition to the recommendation problem, with the emerging of personal devices, many conventional problems, e.g., recognition, need to be personalized as well. Moreover, as the number of users grows huge, solving personalization becomes quite challenging. In this paper, we formalize the generic personalization problem as an optimization problem. We propose several ADMM algorithms to solve this problem in a distributed way including a new Asynchronous ADMM that removes all synchronous barriers to maximize the training throughput. We provide a mathematical analysis to show that the proposed Asynchronous ADMM algorithm holds a linear convergence rate which is the best to our knowledge. The distributed personalization allows training to be performed in either a cluster or even on a user's device. This can improve the privacy protection as no personal data is uploaded, while personal models can still be shared with each other. We apply this approach to two industry problems, \emph{Facial Expression Recognition} and \emph{Job Recommendation}. Experiments demonstrate more than 30\% relative error reduction on both problems. Asynchronous ADMM allows faster training for problems with millions of users since it eliminates all network I/O waiting time to maximize the cluster CPU throughput. Experiments demonstrate 4 times faster than original synchronous ADMM algorithm.",2015,Knowledge Discovery and Data Mining,Fields of study: personalizationbig datatheoretical computer scienceworld wide webdata miningmachine learningcomputer science
Support Measure Data Description for group anomaly detection,Jorge Guevara (University of São Paulo)Stéphane Canu (Institut national des sciences appliquées de Rouen)R Hirata (University of São Paulo),"2322792061,816886801,2119941210","We address the problem of learning a data description model from a dataset containing probability measures as observations. We estimate the data description model by optimizing volume-sets of probability measures where each volume-set is defined as a set of probability measures whose representative functions in a reproducing kernel Hilbert space (RKHS) belong to an enclosing ball. We present three data description models, which are functions in a RKHS depending only on some probability measures, named support measures in analogy to support vectors. An advantage of the method is that we do not consider any particular form for the probability measures. We validate our method in the task of group anomaly detection, with artificial and real datasets.",2015,Knowledge Discovery and Data Mining,Fields of study: total variation distance of probability measuresone class classificationprobability measuredata miningpattern recognitionmachine learningstatisticscomputer sciencemathematics
Real Time Recommendations from Connoisseurs,Noriaki Kawamae (Tokyo Denki University),279243410,"The information overload problem remains serious for both consumers and service/content providers, leading to heightened demands for personalized recommendations. For recommender systems, updating user models is one of the most important tasks to keep up with their changing preferences and trends. Especially since new consumers and items emerge every day, which are promptly rated or reviewed, updating lists of items and rankings is crucial. In this paper, we set the goal of real time recommendation, to present these items instantly. Unlike standard collaborative filtering algorithms, our offline approach focuses only innovative consumers for these predictions, and then uses as few consumers as possible while keeping the same precision. Since innovators exist in many communities, and their opinions will spread and then stimulate their followers to adopt the same behavior, our approach is based on the hypothesis that a set of innova- tive consumers is sufficient to represent the most representative opinions in each community. Following this hypothesis, we derive a scalable method to detect both communities and innovative consumers in each community from a web- scale data from a behavior log. Our evaluation shows that our proposed weighting method can accurately sample given logs, and be compatible only with previous algorithms for real time recommendations.",2015,Knowledge Discovery and Data Mining,Fields of study: topic modelpersonalizationworld wide webdata miningmachine learningcomputer science
Data Science from the Lab to the Field to the Enterprise,Christopher White (Microsoft),2563939123,"DARPA has been investing in data science and building open source tools for applications ranging from counter threat finance, through radar operations and cancer research, to anti-human trafficking. This presentation will cover previous work at DARPA, experience building real-world applications for defense and law enforcement to analyze data, and the future of computer science as an enabler for content discovery, information extraction, relevance determination, and information visualization. The talk will be a mix of background, detailed examples, and software demonstration. It will cover the importance of anchoring in applications, minimization of design-to-testing time, development with users-in-the-loop, error tolerance of machine learning, design for diverse user populations, and the necessity of open source software integration. It will conclude by covering a few next directions for special projects at Microsoft.",2015,Knowledge Discovery and Data Mining,Fields of study: data sciencedata miningmachine learningsimulationcomputer science
Extreme States Distribution Decomposition Method for Search Engine Online Evaluation,Kirill Nikolaev (Yandex)Alexey Drutsa (Yandex)Ekaterina Gladkikh (Yandex)Alexander Ulianov (Yandex)Gleb Gusev (Yandex)Pavel Serdyukov (Yandex),"2583772917,2229408502,2228275753,2639363242,2005728791,2130450538","Nowadays, the development of most leading web services is controlled by online experiments that qualify and quantify the steady stream of their updates. The challenging problem is to define an appropriate online metric of user behavior, so-called Overall Evaluation Criterion (OEC), which is both interpretable and sensitive. The state-of-the-art approach is to choose a type of entities to observe in the behavior data, to define a key metric for these observations, and to estimate the average value of this metric over the observations in each of the system versions. A significant disadvantage of the OEC obtained in this way is that the average value of the key metric does not necessarily change, even if its distribution changes significantly. The reason is that the difference between the mean values of the key metric over the two variants of the system does not necessarily reflect the character of the change in the distribution. We develop a novel method of quantifying the change in the distribution of the key metric, which is (1) interpretable, (2) is based on the analysis of the two distributions as a whole, and, for this reason, is sensitive to more ways the two distributions may actually differ. We provide a thorough theoretical analysis of our approach and show experimentally that, other things being equal, it produces more sensitive OEC than the average.",2015,Knowledge Discovery and Data Mining,Fields of study: intrinsic metricdiscrete mathematicsdata miningmachine learningstatisticsmathematics
Discovering Collective Narratives of Theme Parks from Large Collections of Visitors' Photo Streams,Gunhee Kim (Seoul National University)Leonid Sigal (Disney Research),"2210743192,2407455571","We present an approach for generating pictorial storylines from large collections of online photo streams shared by visitors to theme parks (e.g. Disneyland), along with publicly available information such as visitor's maps. The story graph visualizes various events and activities recurring across visitors' photo sets, in the form of hierarchically branching narrative structure associated with attractions and districts in theme parks. We first estimate story elements of each photo stream, including the detection of faces and supporting objects, and attraction-based localization. We then create spatio-temporal story graphs via an inference of sparse time-varying directed graphs. Through quantitative evaluation and crowdsourcing-based user studies via Amazon Mechanical Turk, we show that the story graphs serve as a more convenient mid-level data structure to perform photo-based recommendation tasks than other alternatives. We also present storybook-like demo examples regarding exploration, recommendation, and temporal analysis, which may be most beneficial uses of the story graphs to visitors.",2015,Knowledge Discovery and Data Mining,Fields of study: user modelinginternet privacymultimediaworld wide webdata miningmachine learningcomputer science
On the Formation of Circles in Co-authorship Networks,Tanmoy Chakraborty (Indian Institute of Technology Kharagpur)Sikhar Patranabis (Indian Institute of Technology Kharagpur)Pawan Goyal (Indian Institute of Technology Kharagpur)Animesh Mukherjee (Indian Institute of Technology Kharagpur),"2471175502,2226423850,2556932677,2134540012","The availability of an overwhelmingly large amount of bibliographic information including citation and co-authorship data makes it imperative to have a systematic approach that will enable an author to organize her own personal academic network profitably. An effective method could be to have one's co-authorship network arranged into a set of ``circles'', which has been a recent practice for organizing relationships (e.g., friendship) in many online social networks. In this paper, we propose an unsupervised approach to automatically detect circles in an ego network such that each circle represents a densely knit community of researchers. Our model is an unsupervised method which combines a variety of node features and node similarity measures. The model is built from a rich co-authorship network data of more than 8 hundred thousand authors. In the first level of evaluation, our model achieves 13.33% improvement in terms of overlapping modularity compared to the best among four state-of-the-art community detection methods. Further, we conduct a task-based evaluation -- two basic frameworks for collaboration prediction are considered with the circle information (obtained from our model) included in the feature set. Experimental results show that including the circle information detected by our model improves the prediction performance by 9.87% and 15.25% on average in terms of AUC (Area under the ROC) and Prec@20 (Precision at Top 20) respectively compared to the case, where the circle information is not present.",2015,Knowledge Discovery and Data Mining,Fields of study: predictionworld wide webdata miningartificial intelligencemachine learningstatisticscomputer sciencemathematics
Building Discriminative User Profiles for Large-scale Content Recommendation,Erheng Zhong (Yahoo!)Nathan Liu (Yahoo!)Yue Shi (Yahoo!)Suju Rajan (Yahoo!),"2108094379,2230629529,2714713288,2229133469","Content recommendation systems are typically based on one of the following paradigms: user based customization, or recommendations based on either collaborative filtering or low rank matrix factorization methods, or with systems that impute user interest profiles based on content browsing behavior and retrieve items similar to the interest profiles. All of these systems have a distinct disadvantage, namely data sparsity and cold-start on items or users. Furthermore, very few content recommendation solutions explicitly model the wealth of information in implicit negative feedback from the users. In this paper, we propose a hybrid solution that makes use of a latent factor model to infer user interest vectors. The hybrid approach enables us to overcome both the data sparsity and cold-start problems. Our proposed method is learned purely on implicit user feedback, both positive and negative. Exploiting the information in the negative feedback allows the user profiles generated to be discriminative. We also provide a Map/Reduce framework based implementation that enables scaling our solution to real-world recommendation problems. We demonstrate the efficacy of our proposed approach with both offline experiments and A/B tests on live traffic on Yahoo properties.",2015,Knowledge Discovery and Data Mining,Fields of study: collaborative filteringmultimediaworld wide webinformation retrievaldata miningmachine learningcomputer science
Anatomical Annotations for Drosophila Gene Expression Patterns via Multi-Dimensional Visual Descriptors Integration: Multi-Dimensional Feature Learning,Hongchang Gao (University of Texas at Arlington)Lin Yan (Shanghai Jiao Tong University)Weidong Cai (University of Sydney)Heng Huang (University of Texas at Arlington),"2169884944,2223461991,2098283194,2137533801","In Drosophila gene expression pattern research, the in situ hybridization (ISH) image has become the standard technique to visualize and study the spatial distribution of RNA. To facilitate the search and comparison of Drosophila gene expression patterns during Drosophila embryogenesis, it is highly desirable to annotate the tissue-level anatomical ontology terms for ISH images. In ISH image annotations, the image content representation is crucial to achieve satisfactory results. However, existing methods mainly focus on improving the classification algorithms and only using simple visual descriptor. If we integrate the effective local and holistic visual descriptors via proper learning method, we can achieve more accurate image annotation results than using individual visual descriptor. We propose a novel structured sparsity-inducing norms based feature learning model to integrate the multi-dimensional visual descriptors for Drosophila gene expression patterns annotations. The new mixed norms are designed to learn the importance of different features from both local and global point of views. We successfully integrate six widely used visual descriptors to annotate the Drosophila gene expression patterns from the lateral, dorsal, and ventral views. The empirical results show that the proposed new method can effectively integrate different visual descriptors, and consistently outperforms related methods using the concatenated visual descriptors.",2015,Knowledge Discovery and Data Mining,Fields of study: bioinformaticscomputer visiondata mining
Mining for Causal Relationships: A Data-Driven Study of the Islamic State,Andrew Stanton (Arizona State University)Amanda Thart (Arizona State University)Ashish Jain (Arizona State University)Priyank Vyas (Arizona State University)Arpan Chatterjee (Arizona State University)Paulo Shakarian (Arizona State University),"2230250046,2222491500,2384357580,2224605451,2223482810,102302551","The Islamic State of Iraq and al-Sham (ISIS) is a dominant insurgent group operating in Iraq and Syria that rose to prominence when it took over Mosul in June, 2014. In this paper, we present a data-driven approach to analyzing this group using a dataset consisting of 2200 incidents of military activity surrounding ISIS and the forces that oppose it (including Iraqi, Syrian, and the American-led coalition). We combine ideas from logic programming and causal reasoning to mine for association rules for which we present evidence of causality. We present relationships that link ISIS vehicle-bourne improvised explosive device (VBIED) activity in Syria with military operations in Iraq, coalition air strikes, and ISIS IED activity, as well as rules that may serve as indicators of spikes in indirect fire, suicide attacks, and arrests.",2015,Knowledge Discovery and Data Mining,Fields of study: causalitylawoperations researchtelecommunicationsmanagementcomputer securityartificial intelligence
Improved Bounds on the Dot Product under Random Projection and Random Sign Projection,Ata Kaban (University of Birmingham),2082255270,"Dot product is a key building block in a number of data mining algorithms from classification, regression, correlation clustering, to information retrieval and many others. When data is high dimensional, the use of random projections may serve as a universal dimensionality reduction method that provides both low distortion guarantees and computational savings. Yet, contrary to the optimal guarantees that are known on the preservation of the Euclidean distance cf. the Johnson-Lindenstrauss lemma, the existing guarantees on the dot product under random projection are loose and incomplete in the current data mining and machine learning literature. Some recent literature even suggested that the dot product may not be preserved when the angle between the original vectors is obtuse. In this paper we provide improved bounds on the dot product under random projection that matches the optimal bounds on the Euclidean distance. As a corollary, we elucidate the impact of the angle between the original vectors on the relative distortion of the dot product under random projection, and we show that the obtuse vs. acute angles behave symmetrically in the same way. In a further corollary we make a link to sign random projection, where we generalise earlier results. Numerical simulations confirm our theoretical results. Finally we give an application of our results to bounding the generalisation error of compressive linear classifiers under the margin loss.",2015,Knowledge Discovery and Data Mining,Fields of study: discrete mathematicscombinatoricsdata miningmachine learningmathematical optimizationstatisticsmathematics
Discovering and Exploiting Deterministic Label Relationships in Multi-Label Learning,Christina Papagiannopoulou (Aristotle University of Thessaloniki)Grigorios Tsoumakas (Aristotle University of Thessaloniki)Ioannis Tsamardinos (University of Crete),"114354508,2012164496,2619192764","This work presents a probabilistic method for enforcing adherence of the marginal probabilities of a multi-label model to automatically discovered deterministic relationships among labels. In particular we focus on discovering two kinds of relationships among the labels. The first one concerns pairwise positive entailment: pairs of labels, where the presence of one implies the presence of the other in all instances of a dataset. The second concerns exclusion: sets of labels that do not coexist in the same instances of the dataset. These relationships are represented as a deterministic Bayesian network. Marginal probabilities are entered as soft evidence in the network and through probabilistic inference become consistent with the discovered knowledge. Our approach offers robust improvements in mean average precision compared to the standard binary relevance approach across all 12 datasets involved in our experiments. The discovery process helps interesting implicit knowledge to emerge, which could be useful in itself.",2015,Knowledge Discovery and Data Mining,Fields of study: bayesian networkknowledge extractiondata miningpattern recognitionmachine learningcomputer science
Early Identification of Violent Criminal Gang Members,Elham Shaabani (Arizona State University)Ashkan Aleali (Arizona State University)Paulo Shakarian (Arizona State University)John Bertetto (Chicago Police Department),"2070833751,2043509780,102302551,2222983607","Gang violence is a major problem in the United States accounting for a large fraction of homicides and other violent crime. In this paper, we study the problem of early identification of violent gang members. Our approach relies on modified centrality measures that take into account additional data of the individuals in the social network of co-arrestees which together with other arrest metadata provide a rich set of features for a classification algorithm. We show our approach obtains high precision and recall (0.89 and 0.78 respectively) in the case where the entire network is known and out-performs current approaches used by law-enforcement to the problem in the case where the network is discovered overtime by virtue of new arrests - mimicking real-world law-enforcement operations. Operational issues are also discussed as we are preparing to leverage this method in an operational environment.",2015,Knowledge Discovery and Data Mining,Fields of study: social network analysiscriminologysocial psychologysocial sciencecomputer securitycomputer science
Optimizing Marketing Impact through Data Driven Decisioning,Anil Kamath (Adobe Systems),2307516744,"Is my marketing working, how much is marketing helping the business and which campaigns and channels are effective? The key challenge for the Chief Marketing Officer is to tie investment in marketing to business results. In an increasingly complex marketing environment -- marketing organizations are being called upon to prove and optimize the return on marketing investment across different paid earned and owned marketing channels. In this talk we will show how data science and optimization techniques can be applied to cross channel data to attribute marketing effectiveness, drive media planning and real-time optimization of campaigns. Using terabytes of multi-channel data we answer questions such as what is the impact of different marketing campaigns on our business, how should we allocate our marketing dollars between different channels and when should I spend them and how do we execute our marketing campaigns based on the synergies of the different channels.",2015,Knowledge Discovery and Data Mining,Fields of study: quantitative marketing researchbusiness to governmentmarketing mix modelinginfluencer marketingreturn on marketing investmentmarketing effectivenessqualitative marketing researchagricultural marketingmarketing mixmarketing strategybusiness marketingintegrated marketing communicationsmarketing researchdigital marketingmarketing managementrelationship marketingmarketingcomputer science
Scaling Machine Learning and Statistics for Web Applications,Deepak Agarwal (LinkedIn),2116605949,"Scaling web applications like recommendation systems, search and computational advertising is challenging. Such systems have to make astronomical number of decisions every day on what to serve users when they are visiting the website and/or using the mobile app. Machine learning and statistical modeling approaches that can obtain insights by continuously processing large amounts of data emitted at very high frequency by these applications have emerged as the method of choice. However, there are three challenges to scale such methods : a) scientific b) infrastructure and c) organizational. I will provide an overview of these challenges and the strategies we have adopted at LinkedIn to address those. Throughout, I will illustrate with examples from real-world applications at LinkedIn.",2015,Knowledge Discovery and Data Mining,Fields of study: web applicationstatistical modeldata scienceworld wide webdata miningmachine learningcomputer science
Proof Protocol for a Machine Learning Technique Making Longitudinal Predictions in Dynamic Contexts,Kevin B. Pratt,2609115569,"We demonstrate a protocol for proving strongly that a black-box machine learning technique robustly predicts the future in dynamic, indefinite contexts. We propose necessary components of the proof protocol and demonstrate results visualizations to support evaluation of the proof components. Components include contemporaneously verifiable discrete predictions, deterministic computability of longitudinal predictions, imposition of realistic costs and domain constraints, exposure to diverse contexts, statistically significant excess benefits relative to a priori benchmarks and Monte Carlo trials, insignificant decay of excess benefits, pathology detection and an extended real-time trial ""in the wild."" We apply the protocol to a big data machine learning technique deployed since 2011 that finds persistent, exploitable opportunities in many of 41 segments of US financial markets, the existence of which opportunities substantially contradict the Efficient Market Hypothesis.",2015,Knowledge Discovery and Data Mining,Fields of study: black boxpredictionstatistical hypothesis testingdesign of experimentsbig datadata miningartificial intelligencemachine learningsimulationstatisticscomputer sciencemathematics
Real-Time Bid Prediction using Thompson Sampling-Based Expert Selection,Elena IkonomovskaSina JafarpourAli Dasdan (Yahoo!),"2632603940,2669908727,4321789","We study online meta-learners for real-time bid prediction that predict by selecting a single best predictor among several subordinate prediction algorithms, here called ""experts"". These predictors belong to the family of context-dependent past performance estimators that make a prediction only when the instance to be predicted falls within their areas of expertise. Within the advertising ecosystem, it is very common for the contextual information to be incomplete, hence, it is natural for some of the experts to abstain from making predictions on some of the instances. Experts' areas of expertise can overlap, which makes their predictions less suitable for merging; as such, they lend themselves better to the problem of best expert selection. In addition, their performance varies over time, which gives the expert selection problem a non-stochastic, adversarial flavor. In this paper we propose to use probability sampling (via Thompson Sampling) as a meta-learning algorithm that samples from the pool of experts for the purpose of bid prediction. We show performance results from the comparison of our approach to multiple state-of-the-art algorithms using exploration scavenging on a log file of over 300 million ad impressions, as well as comparison to a baseline rule-based model using production traffic from a leading DSP platform.",2015,Knowledge Discovery and Data Mining,Fields of study: online algorithmonline advertisingdata miningartificial intelligencemachine learningstatisticscomputer science
Exploiting Data Mining for Authenticity Assessment and Protection of High-Quality Italian Wines from Piedmont,Marco ArlorioJean Daniel CoissonGiorgio Leonardi (University of Pavia)Monica LocatelliLuigi Portinale (University of Eastern Piedmont),"190703158,2088669063,2109682370,2140713197,2271505588","This paper discusses the data mining approach followed in a project called TRAQUASwine, aimed at the definition of methods for data analytical assessment of the authenticity and protection, against fake versions, of some of the highest value Nebbiolo-based wines from Piedmont region in Italy. This is a big issue in the wine market, where commercial frauds related to such a kind of products are estimated to be worth millions of Euros. The objective is twofold: to show that the problem can be addressed without expensive and hyper-specialized wine analyses, and to demonstrate the actual usefulness of classification algorithms for data mining on the resulting chemical profiles. Following Wagstaff's proposal for practical exploitation of machine learning (and data mining) approaches, we describe how data have been collected and prepared for the production of different datasets, how suitable classification models have been identified and how the interpretation of the results suggests the emergence of an active role of classification techniques, based on standard chemical profiling, for the assesment of the authenticity of the wines target of the study.",2015,Knowledge Discovery and Data Mining,Fields of study: data sciencedata miningcomputer science
Gas Concentration Reconstruction for Coal-Fired Boilers Using Gaussian Process,Chao Yuan (Siemens)Matthias Behmann (Siemens Energy Sector)Bernhard Meerbeck (Siemens Energy Sector),"2676500187,1997736714,2228942015","The goal of combustion optimization of a coal-fired boiler is to improve its operating efficiency while reducing emissions at the same time. Being able to take measurements for key combustion ingredients, such as O2, CO, H2O is crucial for the feedback loop needed by this task. One state-of-the-art laser technique, namely, Tunable Diode Laser Absorption Spectroscopy (TDLAS) is able to measure the average value of gas concentration along a laser beam path. A active research direction in TDLAS is how to reconstruct gas concentration images based on these path averages. However, in reality the number of such paths is usually very limited, leading to an extremely under-constrained estimation problem. Another overlooked aspect of the problem is that how can we arrange paths such that the reconstructed image is more accurate? We propose a Bayesian approach based on Gaussian process (GP) to address both image reconstruction and path arrangement problems, simultaneously. Specifically, we use the GP posterior mean as the reconstructed image, and average posterior pixel variance as our objective function to optimize the path arrangement. Our algorithms have been integrated in Siemens SPPA-P3000 control system that provides real-time combustion optimization of boilers around the world.",2015,Knowledge Discovery and Data Mining,Fields of study: gaussian processsimulationmathematical optimizationstatistics
"More Constraints, Smaller Coresets: Constrained Matrix Approximation of Sparse Big Data",Dan Feldman (University of Haifa)Tamir Tassa (Open University),"2302839345,71704941","We suggest a generic data reduction technique with provable guarantees for computing the low rank approximation of a matrix under some $ell z error, and constrained factorizations, such as the Non-negative Matrix Factorization (NMF). Our main algorithm reduces a given n x d matrix into a small, e-dependent, weighted subset C of its rows (known as a coreset ), whose size is independent of both n and d . We then prove that applying existing algorithms on the resulting coreset can be turned into (1+e)-approximations for the original (large) input matrix. In particular, we provide the first linear time approximation scheme (LTAS) for the rank-one NMF. The coreset C can be computed in parallel and using only one pass over a possibly unbounded stream of row vectors. In this sense we improve the result in [4] (Best paper of STOC 2013). Moreover, since C is a subset of these rows, its construction time, as well as its sparsity (number of non-zeroes entries) and the sparsity of the resulting low rank approximation depend on the maximum sparsity of an input row, and not on the actual dimension d . In this sense, we improve the result of Libery [21](Best paper of KDD 2013) and answer affirmably, and in a more general setting, his open question of computing such a coreset. Source code is provided for reproducing the experiments and integration with existing and future algorithms.",2015,Knowledge Discovery and Data Mining,Fields of study: theoretical computer sciencedata miningmachine learningmathematical optimizationstatisticscomputer sciencemathematics
Reconstructing Textual Documents from n-grams,Matthias Gallé (Xerox)Matías Tealdi (National University of Cordoba),"2190102932,2116404588","We analyze the problem of reconstructing documents when we only have access to the n-grams (for n fixed) and their counts from the original documents. Formally, we are interested in recovering the longest contiguous substrings of whose presence in the original documents we are certain. We map this problem on a de Bruijn graph, where the n-grams form the edges and where every Eulerian cycles gives a plausible reconstruction. We define two rules that reduce this graph, preserving all possible reconstructions while at the same time increasing the length of the edge labels. From a theoretical perspective we prove that the iterative application of these rules gives an irreducible graph equivalent to the original one. We then apply this on the data from the Gutenberg project to measure the number and size of the obtained longest substrings. Moreoever, we analyze how the n-gram corpus could be noised to prevent reconstruction, showing empirically that removing low frequent n-grams has little impact. Instead, we propose another method consisting in adding strategically fictitious n-grams and show that a noised corpus like that is much harder to reconstruct while increasing only little the perplexity of a language model obtained through it.",2015,Knowledge Discovery and Data Mining,Fields of study: de bruijn graphtheoretical computer sciencecombinatoricsdata miningmachine learningstatisticsalgorithmcomputer sciencemathematics
Predicting Voice Elicited Emotions,Ying LiJose D. ContrerasLuis J. Salazar,"2706844710,2225462537,2486212268","We present the research, and product development and deployment, of Voice Analyzer' by Jobaline Inc. This is a patent pending technology that analyzes voice data and predicts human emotions elicited by the paralinguistic elements of a voice. Human voice characteristics, such as tone, complement the verbal communication. In several contexts of communication, ""how"" things are said is just as important as ""what"" is being said. This paper provides an overview of our deployed system, the raw data, the data processing steps, and the prediction algorithms we experimented with. A case study is included where, given a voice clip, our model predicts the degree in which a listener will find the voice ""engaging"". Our prediction results were verified through independent market research with 75% in agreement on how an average listener would feel. One application of Jobaline Voice Analyzer technology is for assisting companies to hire workers in the service industry where customers' emotional response to workers' voice may affect the service outcome. Jobaline Voice Analyzer is deployed in production as a product offer to our clients to help them identify workers who will better engage with their customers. We will also share some discoveries and lessons learned.",2015,Knowledge Discovery and Data Mining,Fields of study: voice of the customervoice analysispredictive analyticsmultimediaspeech recognitioncomputer science
Longitudinal LASSO: Jointly Learning Features and Temporal Contingency for Outcome Prediction,Tingyang Xu (University of Connecticut)Jiangwen Sun (University of Connecticut)Jinbo Bi (University of Connecticut),"2162091979,2096599005,2066975796","Longitudinal analysis is important in many disciplines, such as the study of behavioral transitions in social science. Only very recently, feature selection has drawn adequate attention in the context of longitudinal modeling. Standard techniques, such as generalized estimating equations, have been modified to select features by imposing sparsity-inducing regularizers. However, they do not explicitly model how a dependent variable relies on features measured at proximal time points. Recent graphical Granger modeling can select features in lagged time points but ignores the temporal correlations within an individual's repeated measurements. We propose an approach to automatically and simultaneously determine both the relevant features and the relevant temporal points that impact the current outcome of the dependent variable. Meanwhile, the proposed model takes into account the non- i.i.d nature of the data by estimating the within-individual correlations. This approach decomposes model parameters into a summation of two components and imposes separate block-wise LASSO penalties to each component when building a linear model in terms of the past τ measurements of features. One component is used to select features whereas the other is used to select temporal contingent points. An accelerated gradient descent algorithm is developed to efficiently solve the related optimization problem with detailed convergence analysis and asymptotic analysis. Computational results on both synthetic and real world problems demonstrate the superior performance of the proposed approach over existing techniques.",2015,Knowledge Discovery and Data Mining,Fields of study: regressioneconometricsmachine learningstatisticsmathematics
Data-Driven Product Innovation,Xin Fu (LinkedIn)Hernán Asorey (Salesforce.com),"2694575896,2496579673","Data Science is an increasingly popular area of Knowledge Discovery and Data Mining. Leading consumer Web companies such as Amazon, Facebook, eBay, Google and LinkedIn, as well as B2B companies like Salesforce, possess Petabytes of data. Through effective mining of this data, they create products and services that benefit millions of users and generate tremendous amount of business value. It is widely acknowledged that Data Scientists play key roles in the creation of these products, from pattern identification, idea generation and product prototyping to experiment design and launch decisions. Nonetheless, they also face common challenges, such as the gap between creating a prototype and turning it into a scalable product, or the frustration of generating innovative product ideas that do not get adopted. Organizers of this tutorial have many years of experience leading Data Science teams in some of the most successful consumer Web companies. In this tutorial, we introduce the framework that we created to nurture data-driven product innovations. The core of this framework is the focus on scale and impact - we take the audience through a discussion on how to balance between velocity and scale, between product innovation and product operation, and between theoretical research and practical impact. We also share some guidelines for successful data-driven product innovation with real examples from our experiences. We end the tutorial by discussing the organizational perspective of data-driven product innovation: how to structure Data Science teams so Data Scientists collaborate effectively with other functions, and how to hire and grow talents into Data Scientist roles.",2015,Knowledge Discovery and Data Mining,Fields of study: product innovationnew product developmentinnovationproduct engineeringinnovation managementknowledge managementdata miningsimulationcomputer science
Medical Mining: KDD 2015 Tutorial,Myra Spiliopoulou (Otto-von-Guericke University Magdeburg)Pedro Pereira Rodrigues (University of Porto)Ernestina Menasalvas (Technical University of Madrid),"192509020,2115274281,2219424376","In year 2015, we experience a proliferation of scientific publications, conferences and funding programs on KDD for medicine and healthcare. However, medical scholars and practitioners work differently from KDD researchers: their research is mostly hypothesis-driven, not data-driven. KDD researchers need to understand how medical researchers and practitioners work, what questions they have and what methods they use, and how mining methods can fit into their research frame and their everyday business. Purpose of this tutorial is to contribute to this learning process. We address medicine and healthcare; there the expertise of KDD scholars is needed and familiarity with medical research basics is a prerequisite. We aim to provide basics for (1) mining in epidemiology and (2) mining in the hospital. We also address, to a lesser extent, the subject of (3) preparing and annotating Electronic Health Records for mining.",2015,Knowledge Discovery and Data Mining,Fields of study: clinical decision support systemdata scienceknowledge managementdata miningmachine learningcomputer science
Powering Real-time Decision Engines in Finance and Healthcare using Open Source Software,Bassel Ojjeh,2223780538,"Financial services and healthcare companies could be the biggest beneficiaries of big data. Their real-time decision engines can be vastly improved by leveraging the latest advances in big data analytics. However, these companies are challenged in leveraging Open Software Systems (OSS). This presentation covers how, in collaboration with financial services and healthcare institutions, we built an OSS project to deliver a real-time decisioning engine for their respective applications. I will address two key issues. First, I will describe the strategy behind our hiring process to attract millennial big data developers and the results of this endeavor. Second, I will recount the collaboration effort that we had with our large clients and the various milestones we achieved during that process. I will explain the goals regarding big data analysis that our large clients presented to us and how we accomplished those goals. In particular, I will discuss how we leveraged open source to deliver a real-time decisioning software product called Fatafat to these institutions. An advantage of developing applications in Fatafat is that it is already integrated with Hadoop, Kafka for real-time data streaming, HBase and Cassandra for NoSQL data storage. I will talk about how these companies benefited from Fatafat and some of challenges we had in the design of this software. I will provide quantifiable improvements in key metrics driven by Fatafat and interesting, unsolved problems/challenges that need to be addressed for faster and wider adoption of OSS by these companies.",2015,Knowledge Discovery and Data Mining,Fields of study: financial servicesknowledge managementworld wide webdata miningcomputer science
Hadoop's Impact on the Future of Data Management,Amr Awadallah (Cloudera),2223754431,"As Hadoop and the surrounding projects & vendors mature, their impact on the data management sector is growing. Amr will talk about his views on how that impact will change over the next five years. How central will Hadoop be to the data center of 2020? What industries will benefit most? Which technologies are at risk of displacement or encroachment?",2015,Knowledge Discovery and Data Mining,Fields of study: data managementdata sciencedata miningdatabasecomputer science
Data Science at Visa,Waqar HasanMin Wang,"2654229981,2442090426",Visa is the payments technology that forms the backbone of the world's financial systems by handling more than 7 trillion dollars of payments annually and our data reflects how the world spends money. We will describe technical achievements we have made in the area of fraud and cover some open challenges in data science.,2015,Knowledge Discovery and Data Mining,Fields of study: operations researchcomputer science
Quick Sensitivity Analysis for Incremental Data Modification and Its Application to Leave-one-out CV in Linear Classification Problems,Shota Okumura (Nagoya Institute of Technology)Yoshiki Suzuki (Nagoya Institute of Technology)Ichiro Takeuchi (Nagoya Institute of Technology),"2223429567,2162605760,2619145315","We introduce a novel sensitivity analysis framework for large scale classification problems that can be used when a small number of instances are incrementally added or removed. For quickly updating the classifier in such a situation, incremental learning algorithms have been intensively studied in the literature. Although they are much more efficient than solving the optimization problem from scratch, their computational complexity yet depends on the entire training set size. It means that, if the original training set is large, completely solving an incremental learning problem might be still rather expensive. To circumvent this computational issue, we propose a novel framework that allows us to make an inference about the updated classifier without actually re-optimizing it. Specifically, the proposed framework can quickly provide a lower and an upper bounds of a quantity on the unknown updated classifier. The main advantage of the proposed framework is that the computational cost of computing these bounds depends only on the number of updated instances. This property is quite advantageous in a typical sensitivity analysis task where only a small number of instances are updated. In this paper we demonstrate that the proposed framework is applicable to various practical sensitivity analysis tasks, and the bounds provided by the framework are often sufficiently tight for making desired inferences.",2015,Knowledge Discovery and Data Mining,Fields of study: cross validationsensitivity analysisdata miningmachine learningstatisticsalgorithmcomputer sciencemathematics
Virus Propagation in Multiple Profile Networks,Angeliki Rapti (University of Patras)Spyros Sioutas (Ionian University)Kostas Tsichlas (Aristotle University of Thessaloniki)Giannis Tzimas (American Hotel & Lodging Educational Institute),"1975459573,2116472282,71753663,2227197689","Suppose we have a virus or one competing idea/product that propagates over a multiple profile (e.g., social) network. Can we predict what proportion of the network will actually get ""infected"" (e.g., spread the idea or buy the competing product), when the nodes of the network appear to have different sensitivity based on their profile? For example, if there are two profiles A and B in a network and the nodes of profile A and profile B are susceptible to a highly spreading virus with probabilities β A and β B respectively, what percentage of both profiles will actually get infected from the virus at the end? To reverse the question, what are the necessary conditions so that a predefined percentage of the network is infected? We assume that nodes of different profiles can infect one another and we prove that under realistic conditions, apart from the weak profile (great sensitivity), the stronger profile (low sensitivity) will get infected as well. First, we focus on cliques with the goal to provide exact theoretical results as well as to get some intuition as to how a virus affects such a multiple profile network. Then, we move to the theoretical analysis of arbitrary networks. We provide bounds on certain properties of the network based on the probabilities of infection of each node in it when it reaches the steady state. Finally, we provide extensive experimental results that verify our theoretical results and at the same time provide more insight on the problem.",2015,Knowledge Discovery and Data Mining,Fields of study: simulation
Safe Navigation in Urban Environments,Esther Galbrun (Boston University)Konstantinos Pelechrinis (University of Pittsburgh)Evimaria Terzi (Boston University),"2276244909,155875377,2110675235","Advancements in mobile technology and computing have fostered the collection of a large number of civic datasets that capture the pulse of urban life. Furthermore, the open government and data initiative has led many local authorities to make these datasets pub- licly available, hoping to drive innovation that will further improve the quality of life for the city-dwellers. In this paper, we develop a novel application that utilizes crime data to provide safe urban nav- igation. Specifically, using crime data from Chicago and Philadel- phia we develop a risk model for their street urban network which allows us to estimate the relative probability of a crime on any road segment. Given such model we define two variants of the SAFEPATHS problem where the goal is to find a short and low-risk path between a source and a destination locations. Since both the length and the risk of the path are equally important but cannot be combined into a single objective, we approach the urban-navigation problem as a biobjective shortest path problem. Our algorithms aim to output a small set of paths that provide tradeoffs between dis- tance and safety. Our experiments demonstrate the efficacy of our algorithms and their practical applicability.",2015,Knowledge Discovery and Data Mining,Fields of study: data miningsimulation
Patent Mining: A Survey,Longhui Zhang (Florida International University)Lei Li (Florida International University)Tao Li (Florida International University),"2147074165,2432045905,2472069284","Patent documents are important intellectual resources of protecting interests of individuals, organizations and companies. Different from general web documents, patent documents have a well-defined format including frontpage, description, nclaims, and figures. However, they are lengthy and rich in technical terms, which requires enormous human efforts for analysis. Hence, a new research area, called patent mining, emerges in recent years, aiming to assist patent analysts in investigating, processing, and analyzing patent documents. Despite the recent advances in patent mining, it is still far from being well explored in research communities. To help patent analysts and interested readers obtain a big picture of patent mining, we thus provide a systematic summary of existing research efforts along this direction. In this survey, we first present an overview of the technical trend in patent mining. We then investigate multiple research questions related to patent documents, including patent retrieval, patent classification, and patent visualization, and provide summaries and highlights for each question by delving into the corresponding research efforts.",2015,Knowledge Discovery and Data Mining,Fields of study: patent visualisationdata scienceinformation retrievaldata mining
A Social Formalism and Survey for Recommender Systems,Daniel Bernardes (Sorbonne)Mamadou Diaby (Sorbonne)Raphaël Fournier (Sorbonne)Françoise FogelmanSoulié (Sorbonne)Emmanuel Viennet (Sorbonne),"2581292151,2309473745,2602538212,2668363841,228387108","This paper presents a general formalism for Recommender Systems based on Social Network Analysis. After introducing the classical categories of recommender systems, we present our Social Filtering formalism and show that it extends association rules, classical Collaborative Filtering and Social Recommendation, while providing additional possibilities. This allows us to survey the literature and illustrate the versatility of our approach on various publicly available datasets, comparing our results with the literature.",2015,Knowledge Discovery and Data Mining,Fields of study: collaborative filteringsocial network analysisrecommender systemknowledge managementworld wide webdata miningmachine learningcomputer science
New Research Directions in Knowledge Discovery and Allied Spheres,Anisoara Nica (Sybase)Fabian M. Suchanek (Max Planck Society)Aparna S. Varde (Montclair State University),"1980026032,69603646,74820909","The realm of knowledge discovery extends across several allied spheres today. It encompasses database management areas such as data warehousing and schema versioning; information retrieval areas such as Web semantics and topic detection; and core data mining areas, e.g., knowledge based systems, uncertainty management, and time-series mining. This becomes particularly evident in the topics that Ph.D. students choose for their dissertation. As the grass roots of research, Ph.D. dissertations point out new avenues of research, and provide fresh viewpoints on combinations of known fields. In this article we overview some recently proposed developments in the domain of knowledge discovery and its related spheres. Our article is based on the topics presented at the doctoral workshop of the ACM Conference on Information and Knowledge Management, CIKM 2011.",2015,Knowledge Discovery and Data Mining,Fields of study: revision controlrankingsemantic webk nearest neighbors algorithmknowledge extractiontext miningdata sciencedata miningdatabasemachine learningcomputer science
The Data Problem in Data Mining,Albrecht Zimmermann (Institut national des sciences Appliquées de Lyon),2693365398,"Computer science is essentially an applied or engineering science, creating tools. In Data Mining, those tools are supposed to help humans understand large amounts of data. In this position paper, I argue that for all the progress that has been made in Data Mining, in particular Pattern Mining, we are lacking insight into three key aspects: 1) How pattern mining algorithms perform quantitatively, 2) How to choose parameter settings, and 3) How to relate found patterns to the processes that generated the data. I illustrate the issue by surveying existing work in light of these concerns and pointing to the (relatively few) papers that have attempted to fill in the gaps. I argue further that progress regarding those questions is held back by a lack of data with varying, controlled properties, and that this lack is unlikely to be remedied by the ever increasing collection of real-life data. Instead, I am convinced that we will need to make a science of digital data generation, and use it to develop guidance to data practitioners.",2015,Knowledge Discovery and Data Mining,Fields of study: collaborative filteringsocial network analysisrecommender systemdata scienceworld wide webdata miningmachine learningcomputer science
Learning Entry Profiles of Children with Autism from Multivariate Treatment Information Using Restricted Boltzmann Machines,Pratibha Vellanki (Deakin University)Dinh Q. Phung (Deakin University)Thi V. Duong (Deakin University)Svetha Venkatesh (Deakin University),"2278944905,2314522249,2159716571,2146461601","Entry profiles can be generated before children with Autism Spectrum Disorders ASD begini¾?to traverse an intervention program. They can help evaluate the progress of each child on the dedicated syllabus in addition to enabling narrowing down the best intervention course over time. However, the traits of ASD are expressed in different ways in every individual affected. The resulting spectrum nature of the disorder makes it challenging to discover profiles of children with ASD. Using data from 491 children, traversing the syllabus of a comprehensive intervention program on iPad called TOBY Playpad, we learn the entry profiles of the children based on their age, sex and performance on their first skills of the syllabus. Mixed-variate restricted Boltzmann machines allow us to integrate the heterogeneous data into one model making it a suitable technique. The data based discovery of entry profiles may assist in developing systems that can automatically suggest best suitable paths through the syllabus by clustering the children based on the characteristics they present at the beginning of the program. This may open the pathway for personalised intervention.",2015,Knowledge Discovery and Data Mining,Fields of study: interventiontechnologysimulationcomputer science
Web Site Audience Segmentation Using Hybrid Alignment Techniques,Vinh-Trung LuuGermain Forestier (University of Strasbourg)Frédéric Fondement (École Polytechnique Fédérale de Lausanne)Pierre-Alain Muller (French Institute for Research in Computer Science and Automation),"2497358092,2008768452,2204161815,2192537623","We are working on behavioral marketing in the Internet. On one hand we observe the behavior of visitors, and on the other hand we trigger in real-time stimulations intended to alter this behavior. Real-time and mass-customization are the two challenges that we have to address. In this paper, we present a hybrid approach for clustering visitor sessions, based on a combination of global and local sequence alignments, such as Needleman-Wunsch and Smith-Waterman. Our goal is to define very simple approaches able to address about 80i¾?% of visitor sessions to be segmented, and which can be easily turned into small pieces of program, to be run in parallel in thousands of web browsers.",2015,Knowledge Discovery and Data Mining,Fields of study: sequential pattern miningcluster analysisweb miningworld wide webdata miningmachine learningsimulationcomputer science
A Dynamic Feature Selection Based LDA Approach to Baseball Pitch Prediction,Phuong Hoang (North Carolina State University)Michael Hamilton (Columbia University)Joseph Murray (North Carolina State University)Corey Stafford (Columbia University)Hien Tran (North Carolina State University),"2293311659,2533855901,2515564782,2478401898,2235492663","Baseball, which is one of the most popular sports in the world, has a uniquely discrete gameplay structure. This stop-and-go style of play creates a natural ability for fans and observers to record information about the game in progress, resulting in a wealth of data that is available for analysis. Major League Baseball MLB, the professional baseball league in the US and Canada, uses a system known as PITCHf/x to record information about every individual pitch that is thrown in league play. We extend the classification to pitch prediction fastball or nonfastball by restricting our analysis to pre-pitch features. By performing significant feature analysis and introducing a novel approach for feature selection, moderate improvement over published results is achieved.",2015,Knowledge Discovery and Data Mining,Fields of study: statistical hypothesis testingfeature selectionspeech recognitionartificial intelligencemachine learningsimulationcomputer science
ProbitUCB: A Novel Method for Review Ranking,Wanying Ding (Drexel University)Yue Shang (Drexel University)Dae Hoon Park (University of Illinois at Urbana–Champaign)Lifan Guo (TCL Corporation)Xiaohua Hu (Drexel University),"2106552163,2669245976,2103089955,2139179961,2207916209","Online reviews play an important role in facilitating customers in making online purchase decisions. But with the dramatic increase in volume, it will cost customers hours going through all the reviews. This paper proposes a review ranking algorithm to present the most helpful reviews ahead, saving consumers' plenty of time in review hunting. Our ProbitUCB model implements a probabilistic kernel embedded UCB Upper Confident Bound ranking framework, and adopts a self-learning mechanism to distinguish out helpful reviews. Comparing to the current models, ProbitUCB's advantage is listing as follows: 1 it ranks under the exploit and explore mechanism, reducing the error brought from probability estimation inaccuracy; 2 it is training dataset free, saving users enormous amount of time in labeling data, which is required for most supervised methods; 3 it considers various potential features to rank, remedying the defect of only using word information in most unsupervised methods; 4 it adjusts the values of hyper parameters automatically, solving the intuitively value setting problem in many related work. Finally, we experiment our model on 6 datasets, and compare its performance with 10 other classical learn to rank algorithms, and the results show that our algorithm outperform all of them.",2015,Knowledge Discovery and Data Mining,Fields of study: restricted boltzmann machineranking svmmulti armed banditdata miningartificial intelligencemachine learningstatisticscomputer science
Modeling Vietnamese Speech Prosody: A Step-by-Step Approach Towards an Expressive Speech Synthesis System,Dang-Khoa Mac (Huazhong University of Science and Technology)Do-Dat Tran (Centre national de la recherche scientifique),"2543457848,2395262450","Attempts to add expressivity to synthesized speech is one of the main strategies in speech technologies. This paper summarizes our researches on modeling Vietnamese prosody, with the goal of improving naturalness of synthesized speech in Vietnamese, as well as integrating expressivities i.e. emotion/attitude. Based on the concept of ""rendez-vous"" between linguistic levels and prosodic functions, the prosody of utterance is proposed to be decomposed into several components. Therefore, each component is step by step modeled by an independent model: a dynamic linear segment model for tones, a relative registers model for F0 level of syllable, a rule-based approach for phrasing modeling and a F0 stylization modeling for the expressive function. All proposed models were integrated in speech Text-to-speech systems and also were evaluated by perception experiments.",2015,Knowledge Discovery and Data Mining,Fields of study: speech corpusvietnameseprosodyattitudechinese speech synthesisspeech synthesisspeech recognitioncomputer science
Analyzing User Behaviors Based on Temporal Patterns of Sequential Pattern Evaluation Indices on Twitter,Hidenao Abe (Bunkyo University),2618006434,"With social media sites, such as Twitter, providing a visual record of the daily interests and concerns of users in the form of tweets and tweeting behaviors, there is growing demand among users, such as corporations, to identify other interested users. However, accurately determining whether users who receive information such as tweets from enterprise users have a genuine interest in it can be difficult. In this study, the user behavior of resending information received on Twitter retweeting is analyzed with the aim of developing a method for constructing a model for predicting retweeting behavior using the content of past tweeting history via evaluation indices of words and phrases in the users' tweets. This paper analyzes the tweets sent by large online retail websites and by the followers who receive them, comparing the feature words obtained from the retweets with those in the tweets sent by the followers. This paper also discusses the feasibility of constructing a behavior prediction model by extracting temporal patterns of evaluation indices that are created from the usage frequencies of feature words and phrases obtained from followers' tweets.",2015,Knowledge Discovery and Data Mining,Fields of study: internet privacymultimediaworld wide webdata miningcomputer science
Mining Massive-Scale Spatiotemporal Trajectories in Parallel: A Survey,Pengtao Huang (Tsinghua University)Bo Yuan (Tsinghua University),"2225491498,2293770898","With the popularization of positioning devices such as GPS navigators and smart phones, large volumes of spatiotemporal trajectory data have been produced at unprecedented speed. For many trajectory mining problems, a number of computationally efficient approaches have been proposed. However, to more effectively tackle the challenge of big data, it is important to exploit various advanced parallel computing paradigms. In this paper, we present a comprehensive survey of the state-of-the-art techniques for mining massive-scale spatiotemporal trajectory data based on parallel computing platforms such as Graphics Processing Unit GPU, MapReduce and Field Programmable Gate Array FPGA. This survey covers essential topics including trajectory indexing and query, clustering, join, classification, pattern mining and applications. We also give an in-depth analysis of the related techniques and compare them according to their principles and performance.",2015,Knowledge Discovery and Data Mining,Fields of study: spatiotemporal databasedata sciencetheoretical computer sciencedata miningmachine learningcomputer science
Internal Clustering Evaluation of Data Streams,Marwan Hassani (RWTH Aachen University)Thomas Seidl (RWTH Aachen University),"2159472396,2140301036","Clustering validation is a crucial part of choosing a clustering algorithm which performs best for an input data. Internal clustering validation is efficient and realistic, whereas external validation requires a ground truth which is not provided in most applications. In this paper, we analyze the properties and performances of eleven internal clustering measures. In particular, as the importance of streaming data grows, we apply these measures to carefully synthesized stream scenarios to reveal how they react to clusterings on evolving data streams. A series of experimental results show that different from the case with static data, the Calinski-Harabasz index performs the best in coping with common aspects and errors of stream clustering.",2015,Knowledge Discovery and Data Mining,Fields of study: flame clusteringbrown clusteringcanopy clustering algorithmcorrelation clusteringconstrained clusteringdata stream clusteringcure data clustering algorithmaffinity propagationfuzzy clusteringclustering high dimensional datacluster analysisconsensus clusteringconceptual clusteringdata miningdatabasemachine learningstatisticscomputer science
Evaluation of Community Mining Algorithms in the Presence of Attributes,Reihaneh Rabbany (University of Alberta)Osmar R. Zaïane (University of Alberta),"1904584114,2308328903","Grouping data points is one of the fundamental tasks in data mining, commonly known as clustering. In the case of interrelated data, when data is represented in the form of nodes and their relationships, the grouping is referred to as community. A community is often defined based on the connectivity of nodes rather than their attributes or features. The variety of definitions and methods and its subjective nature, makes the evaluation of community mining methods non-trivial. In this paper we point out the critical issues in the common evaluation practices, and discuss the alternatives. In particular, we focus on the common practice of using attributes as the ground-truth communities in large real networks. We suggest to treat these attributes as another source of information, and to use them to refine the communities and tune parameters.",2015,Knowledge Discovery and Data Mining,Fields of study: management sciencedata sciencedata mining
Fast Dependency Parsing Using Distributed Word Representations,Phuong Le-Hong (Verenigde Nederlandse Uitgeverijen)Thi Minh Huyen Nguyen (Verenigde Nederlandse Uitgeverijen)Thi Luong NguyenMy Linh Ha (Verenigde Nederlandse Uitgeverijen),"2560823269,2618084774,2560629793,2560814793","In this work, we propose to use distributed word representations in a greedy, transition-based dependency parsing framework. Instead of using a very large number of sparse indicator features, the multinomial logistic regression classifier employed by the parser learns and uses a small number of dense features, therefore it can work very fast. The distributed word representations are produced by a continuous skip-gram model using a neural network architecture. Experiments on a Vietnamese dependency treebank show that the parser not only works faster but also achieves better accuracy in comparison to a conventional transition-based dependency parser.",2015,Knowledge Discovery and Data Mining,Fields of study: simple lr parserparser combinatortop down parsingnatural language processingspeech recognitionpattern recognitionmachine learningcomputer science
Feature Maximization Based Clustering Quality Evaluation: A Promising Approach,Jean-Charles Lamirel (University of Tartu)Shadi Al Shehabi (French Institute for Research in Computer Science and Automation),"2244012989,1990741520","Feature maximization is an alternative measure, as compared to usual distributional measures relying on entropy or on Chi-square metric or vector-based measures, like Euclidean distance or correlation distance. One of the key advantages of this measure is that it is operational in an incremental mode both on clustering and on traditional classification. In the classification framework, it does not presents the limitations of the aforementioned measures in the case of the processing of highly unbalanced, heterogeneous and highly multidimensional data. We present a new application of this measure in the clustering context for setting up new cluster quality indexes whose efficiency ranges for low to high dimensional data and that are tolerant to noise. We compare the behaviour of these new indexes with usual cluster quality indexes based on Euclidean distance on different kinds of test datasets for which ground truth is available. Proposed comparison clearly highlights the superior accuracy and stability of the new method.",2015,Knowledge Discovery and Data Mining,Fields of study: k medians clusteringfuzzy clusteringcluster analysisbig datadata miningpattern recognitionmachine learningstatisticscomputer sciencemathematics
High Dimensional Explicit Feature Biased Matrix Factorization Recommendation,Weibin Sun (Dalian University of Technology)Xianchao Zhang (Dalian University of Technology)Wenxin Liang (Dalian University of Technology)Zengyou He (Dalian University of Technology),"2537001457,2132650836,2103505816,2160623285","Collaborative Filtering method using latent factor model is one of the most popular approaches in personal recommending system. It is famous for its good performance by using only user-item rating matrix. The latent progress intelligently factorizes users' preference on different items through the rating matrix. However, the factorization progress is completely implicit. Thus, it is difficult to integrate new observed features, and it becomes more complicated when one feature has multiple values. In this paper, we propose a new algorithm based on Matrix Factorization to model explicit features besides rating values by adding high dimensional factors, which makes the factorized presentation explainable. The algorithm is generally applicable for such discrete features as type, genres, age and so on. Experimental results show that our approach outperforms the state-of-the-art methods using latent factor model.",2015,Knowledge Discovery and Data Mining,Fields of study: non negative matrix factorizationdata miningartificial intelligencemachine learningmathematics
Citation Enrichment Improves Deduplication of Primary Evidence,Miew Keen Choong (Macquarie University)Sarah Thorning (Birmingham City University Faculty of Health)Guy Tsafnat (Macquarie University),"2130082739,2527430888,2685198565","Objective: To automatically detect duplicate citations in a bibliographical database. Background: Citations retrieved from multiple search databases have different forms making manual and automatic detection of duplicates difficult. Existing methods rely on fuzzy-similarity measures which are error-prone. Methods: We analysed four pairs of original search results from MEDLINE and EMBASE that were used to create systematic reviews. An automatic tool deduplicated citations by first enriching citations with Digital Object Identifiers DOI, and/or other unique identifiers. Duplication of records was then determined by comparing these unique identifiers. We compared our method with the duplicate detection function of a popular citation management desktop application in several configurations. Results: Citation Enrichment identified 93i¾?% range 86i¾?%---100i¾?% of the duplicates indexed online and erroneously marked 3i¾?% range 0i¾?%---6i¾?% documents as duplicates. The citation management application found 68i¾?% range 64i¾?%---72i¾?% without error using default setting. When set for highest deduplication, the citation management application found 94i¾?% of duplicates range 77i¾?%---100i¾?% and 4i¾?% error range 0i¾?%---8i¾?%. Conclusion: Citation enrichment using unique identifiers enhances automatic deduplication. On its own, the approach seems slightly superior to tools that compare citations without enrichment. Methods that combine citation enrichment with existing fuzzy-matching may substantially reduce resource requirements of evidence synthesis.",2015,Knowledge Discovery and Data Mining,Fields of study: primary sourceworld wide webinformation retrievaldata miningcomputer science
A Research of Applying Association Rules and Decision Tree to Endometriosis,Lin Hui (Tamkang University)Huan-Chao Keh (Tamkang University)Nan-Ching Huang (National Yang-Ming University)Chiung-Tzu Chang (Tamkang University)Yi-Fan Yang (Tamkang University),"2235741280,2256599170,2655609855,2538710124,2533211966","With the medical technology getting developed day by day, medical quality has been playing a significant role in treatment achievement evaluation, recurrence evaluation and complication disease differentiation. However, the variety and complexity of medical data type makes it possible to cause some deviations of results by subjective judgments and affect the medical decision. So, it is necessary to choose the appropriate data analyzing methods. When it comes to strategy selection, we can find that categorized data type is superior to helping to make a decision. Through medical records, taking the method of data mining to analyze the association relations among certain diseases and then we can generalize the characteristics and dig out the hidden information. This research is to take use of association rules and decision tree to analyze the association relationship among endometriosis, barrenness and alcohol treatment. By analyzing the recorded medical associated data, we will find out the potential information as well as useful knowledge. That can provide references for physician when carry out the medical treatment.",2015,Knowledge Discovery and Data Mining,Fields of study: association rule learningdata sciencedata miningartificial intelligencecomputer science
Construction of a Prediction Model for Nephropathy Among Obese Patients Using Genetic and Clinical Features,Guan-Mau Huang (Yuan Ze University)Yi-Cheng Chen (Tamkang University)Julia Tzu-Ya Weng (Yuan Ze University),"2722431494,2711685426,2667381052","Obesity is a complex disease arising from an excessive accumulation of body fat which leads to various complications such as diabetes, hypertension, and renal diseases. The growing prevalence of obesity is also becoming a major risk factor for nephropathy. When patients are diagnosed with nephropathy, their progression towards renal failure is usually inevitable. Therefore, a prediction tool will help medical doctors identify patients with a higher risk of developing nephropathy and implement early treatment or prevention. In this study, we attempted to construct a diagnostic support system for nephropathy using clinical and genetic traits. Our results show that prediction models involving the use of both genetic and clinical features yielded the best classification performance. Our finding is in accordance with the complex nature of obesity-related nephropathy and support the notion of using genetic traits to design a personalized diagnostic model.",2015,Knowledge Discovery and Data Mining,Fields of study: obesitypredictionstatistics
Leveraging the Common Cause of Errors for Constraint-Based Data Cleansing,Ayako Hoshino (NEC)Hiroki Nakayama (NEC)Chihiro Ito (NEC)Kyota Kanno (NEC)Kenshi Nishimura (NEC),"2524397082,2431692734,2303315502,2129994481,2276909343","This study describes a statistically motivated approach to constraint-based data cleansing that derives the cause of errors from a distribution of conflicting tuples. In real-world dirty data, errors are often not randomly distributed. Rather, they often occur only under certain conditions, such as when the transaction is handled by a certain operator, or the weather is rainy. Leveraging such common conditions, or ""cause conditions"", the algorithm resolves multi-tuple conflicts with high speed, as well as high accuracy in realistic settings where the distribution of errors is skewed. We present complexity analyses of the problem, pointing out two subproblems that are NP-complete. We then introduce, for each subproblem, heuristics that work in sub-polynomial time. The algorithms are tested with three sets of data and rules. The experiments show that, compared to the state-of-the-art methods for Conditional Functional Dependencies CFD-based and FD-based data cleansing, the proposed algorithm scales better with respect to the data size, is the only method that outputs complete repairs, and is more accurate when the error distribution is skewed.",2015,Knowledge Discovery and Data Mining,Fields of study: data cleansingtheoretical computer sciencedata miningmachine learningstatisticscomputer science
A Simhash-Based Generalized Framework for Citation Matching in MapReduce,Pengsen Wang (Beijing University of Posts and Telecommunications)Bin Wu (Beijing University of Posts and Telecommunications)Xiaoming Li (Beijing University of Posts and Telecommunications)Lin Wang (Beijing University of Posts and Telecommunications)Bai Wang (Beijing University of Posts and Telecommunications),"2597873155,2464938123,2662043613,2653050357,2582794223","Citation matching is to find the cited papers according to only a small amount of information. There have been some works on citation matching. Most of the solutions require expensive model processing to achieve good results. However, when dealing with millions of citations in large digital libraries, these solutions may not be efficient enough. To address this problem, we propose a simhash-based generalized framework in MapReduce for citation matching. In the framework, we use title exact matching and distance-based short text similarity metrics to implement citation matching. Moreover, customizing citation fields, citation field weights and word segmentation weights are used for improving the accuracy. We also design a heuristic algorithm which can automatically calculate the weights of each citation field. For disposing the large-scale datasets, we implement the framework in Hadoop, a popular parallel computation platform. We do our experiments with real datasets from a Chinese Medicine Digital Library, and a comparative experiment with Cora corpus McCallum's citation matching test set. The results of experiments confirm the efficiency and effectiveness of our framework.",2015,Knowledge Discovery and Data Mining,Fields of study: data scienceworld wide webdata miningmachine learningcomputer science
Integrating Content Centric Networking and Web Content Mining: A Future Efficient Internet Architecture for Healthcare,Rabia Bashir (Federal Urdu University)Sajjad Akbar (Bournemouth University),"2554932432,2599452831","Healthcare Information Systems are indispensible and require more collaboration nowadays. Network Infrastructure for communication requires extreme bandwidth for doctors' diagnosis reports and medical images in order to interchange between various hospitals. The recent Internet architecture is a host-centric network that is less appropriate for healthcare applications in terms of patient monitoring and emergency services. With an emergent use of various interactive healthcare applications, the host-centric networking approach is supposed to be less proficient as it relies on the physical location. Therefore, a new approach to networking architecture called Content-Centric Network CCN is considered as the prospective Internet architecture for future. It uses name of data to access the contents rather than location. It reduces the network congestion and traffic by improving the delivery speed. CCN is still less efficient due to unavailability of appropriate content in Content Servers. For this purpose, Web Content Mining WCM techniques can facilitate to efficiently perform the data management of CCN. Therefore, this paper contributes: Firstly, by identifying the challenges of WCM such as language independency, structure flexibility, performance, dynamicity, redundancy handling, intelligence and relevant content retrieval. Secondly, it maps identified challenges on WCM techniques to adopt most suitable WCM approach for CCN. Finally, this paper introduces an innovative Internet architecture for healthcare by integrating CCN and WCM to deal the data management issues. From Information Centric Networks ICN, we have opted CCN for our proposed architecture. While from WCM, Agent-based approach is selected in order to locate most relevant healthcare data.",2015,Knowledge Discovery and Data Mining,Fields of study: health careweb miningmultimediaworld wide webdata miningcomputer science
Model Selection of Symbolic Regression to Improve the Accuracy of PM2.5 Concentration Prediction,Guangfei Yang (Dalian University of Technology)Jian Huang (Dalian University of Technology),"2128420767,2530318488","As one of the main components of haze, topics with respect to PM2.5 are coming into people's sight recently in China. In this paper, we try to predict PM2.5 concentrations in Dalian, China via symbolic regression SR based on genetic programming GP. During predicting, the key problem is how to select accurate models by proper interestingness measures. In addition to the commonly used measures, such as R-squared value, mean squared error, number of parameters, etc., we also study the effectiveness of a set of potentially useful measures, such as AIC, BIC, HQC, AICc and EDC. Besides, a new interestingness measure, namely Interestingness Elasticity IE, is proposed in this paper. From the experimental results, we find that the new measure gains the best performance on selecting candidate models and shows promising extrapolative capability.",2015,Knowledge Discovery and Data Mining,Fields of study: model selectiondata miningmachine learningstatisticsmathematics
Manifold Regularized Symmetric Joint Link Model for Overlapping Community Detection,Hao Chen (Dalian University of Technology)Xianchao Zhang (Dalian University of Technology)Wenxin Liang (Dalian University of Technology)Feng Ding (Dalian University of Technology),"2650462033,2132650836,2668970941,2658498593","Overlapping community detection is an important research topic in analyzing real-world networks. Among existing algorithms for detecting overlapping communities, generative models have shown their superiorities. However, previous generative models do not consider the intrinsic geometry of probability distribution manifold. To tackle this problem, we propose a Manifold Regularized Symmetric Joint Link Model MSJL, which utilizes the local geometrical structure of manifold to improve the performance of overlapping community detection. MSJL assumes that the community probability distribution lives on a submanifold, and adopts the manifold assumption which specifically requires two close nodes in an intrinsic geometry to have similar community distribution. The structure of the intrinsic manifold is modeled by a nearest neighbor graph, and MSJL incorporates the graph Laplacian as a manifold regularization into the maximum likelihood function of the standard SJL model. Experiments on synthetic benchmarks and real-world networks demonstrate that MSJL can significantly improve the performance compared with the state-of-the-art methods.",2015,Knowledge Discovery and Data Mining,Fields of study: manifold alignmentstatistical manifoldgenerative modellaplacian matrixpattern recognitionmachine learningmathematical optimizationcomputer sciencemathematics
A Cloud Based Type-2 Diabetes Mellitus Lifestyle Self-Management System,Shih-Hao Chang (Tamkang University)Chih-Ning Li (Tamkang University),"2708577339,2530702144","In this paper, we designed a patient-centric cloud based diabetes lifestyle management system. It is composed of three layers namely sensing, communication and user interface. The goal of this cloud based diabetes lifestyle management system is to provide Type-2 diabetes mellitus patients useful information to remind user's blood sugar level. The function of the sensor networks in this framework is to collect the data from human body and human activity as well as environmental information that may have effects on the healthy statement of the diabetes patients. The communication and central server part will handle the data exchange and data analysis that help to generate a final decision data and sent to user interface to remind the user of valuable information. Different from traditional e-health system, due to the diabetes patients are prone to effect by weather and environment varying. Therefore, the presented approach provide a rule algorithm which enables the rescue decision in the cloud server and transmit through the communication level, and finally provide a integrated user interface for diabetes users. An early warningi¾?user interface for diabetes patents has been designed and presented in this paper.",2015,Knowledge Discovery and Data Mining,Fields of study: cloud computinghuman computer interactionworld wide websimulationcomputer science
"Advances in Knowledge Discovery and Data Mining: 19th Pacific-Asia Conference, PAKDD 2015, Ho Chi Minh City, Vietnam, May 19-22, 2015, Proceedings, Part II","Tru Cao (Faculty of Information Technology, University Džemal Bijedić of Mostar)Ee-Peng Lim (Singapore Management University)Zhi-Hua Zhou (Nanjing University)Tu-Bao Ho (Japan Advanced Institute of Science and Technology)David Cheung (University of Hong Kong)Hiroshi Motoda (Osaka University)","2309770394,2657407642,2286237009,2135463138,1979772396,323689644","This two-volume set, LNAI 9077 + 9078, constitutes the refereed proceedings of the 19th Pacific-Asia Conference on Advances in Knowledge Discovery and Data Mining, PAKDD 2015, held in Ho Chi Minh City, Vietnam, in May 2015.The proceedings contain 117 paper carefully reviewed and selected from 405 submissions. They have been organized in topical sections named: social networks and social media; classification; machine learning; applications; novel methods and algorithms; opinion mining and sentiment analysis; clustering; outlier and anomaly detection; mining uncertain and imprecise data; mining temporal and spatial data; feature extraction and selection; mining heterogeneous, high-dimensional, and sequential data; entity resolution and topic-modeling; itemset and high-performance data mining; and recommendations.",2015,Knowledge Discovery and Data Mining,Fields of study: data sciencedata miningcomputer science
Modeling Location-Based User Rating Profiles for Personalized Recommendation,"Hongzhi Yin (University of Queensland)Bin Cui (Peking University)Ling Chen (University of Technology, Sydney)Zhiting Hu (Carnegie Mellon University)Chengqi Zhang (University of Technology, Sydney)","2145818752,2070862325,2633547329,2156033562,2166080598","This article proposes LA-LDA, a location-aware probabilistic generative model that exploits location-based ratings to model user profiles and produce recommendations. Most of the existing recommendation models do not consider the spatial information of users or items; however, LA-LDA supports three classes of location-based ratings, namely spatial user ratings for nonspatial items, nonspatial user ratings for spatial items, and spatial user ratings for spatial items. LA-LDA consists of two components, ULA-LDA and ILA-LDA, which are designed to take into account user and item location information, respectively. The component ULA-LDA explicitly incorporates and quantifies the influence from local public preferences to produce recommendations by considering user home locations, whereas the component ILA-LDA recommends items that are closer in both taste and travel distance to the querying users by capturing item co-occurrence patterns, as well as item location co-occurrence patterns. The two components of LA-LDA can be applied either separately or collectively, depending on the available types of location-based ratings. To demonstrate the applicability and flexibility of the LA-LDA model, we deploy it to both top- k recommendation and cold start recommendation scenarios. Experimental evidence on large-scale real-world data, including the data from Gowalla (a location-based social network), DoubanEvent (an event-based social network), and MovieLens (a movie recommendation system), reveal that LA-LDA models user profiles more accurately by outperforming existing recommendation models for top- k recommendation and the cold start problem.",2015,Knowledge Discovery and Data Mining,Fields of study: cold startlocation based servicerecommender systemworld wide webinformation retrievaldata miningmachine learningcomputer science
Use of Local Group Information to Identify Communities in Networks,Sucheta Soundarajan (Cornell University)John E. Hopcroft (Cornell University),"1846143951,2079447303","The recent interest in networks has inspired a broad range of work on algorithms and techniques to characterize, identify, and extract communities from networks. Such efforts are complicated by a lack of consensus on what a “community” truly is, and these disagreements have led to a wide variety of mathematical formulations for describing communities. Often, these mathematical formulations, such as modularity and conductance, have been founded in the general principle that communities, like a G ( n , p ) graph, are “round,” with connections throughout the entire community, and so algorithms were developed to optimize such mathematical measures. More recently, a variety of algorithms have been developed that, rather than expecting connectivity through the entire community, seek out very small groups of well-connected nodes and then connect these groups into larger communities. In this article, we examine seven real networks, each containing external annotation that allows us to identify “annotated communities.” A study of these annotated communities gives insight into why the second category of community detection algorithms may be more successful than the first category. We then present a flexible algorithm template that is based on the idea of joining together small sets of nodes. In this template, we first identify very small, tightly connected “subcommunities” of nodes, each corresponding to a single node’s “perception” of the network around it. We then create a new network in which each node represents such a subcommunity, and then identify communities in this new network. Because each node can appear in multiple subcommunities, this method allows us to detect overlapping communities. When evaluated on real data, we show that our template outperforms many other state-of-the-art algorithms.",2015,Knowledge Discovery and Data Mining,Fields of study: social networkbioinformaticsdata miningmachine learningcomputer science
Who Influenced You? Predicting Retweet via Social Influence Locality,Jing Zhang (Tsinghua University)Jie Tang (Tsinghua University)Juanzi Li (Tsinghua University)Yang Liu (Tsinghua University)Chunxiao Xing (Tsinghua University),"2635071740,2158012360,2129156004,2675443243,2648381266","Social influence occurs when one’s opinions, emotions, or behaviors are affected by others in a social network. However, social influence takes many forms, and its underlying mechanism is still unclear. For example, how is one’s behavior influenced by a group of friends who know each other and by the friends from different ego friend circlesq In this article, we study the social influence problem in a large microblogging network. Particularly, we consider users’ (re)tweet behaviors and focus on investigating how friends in one’s ego network influence retweet behaviors. We propose a novel notion of social influence locality and develop two instantiation functions based on pairwise influence and structural diversity. The defined influence locality functions have strong predictive power. Without any additional features, we can obtain an F1-score of 71.65p for predicting users’ retweet behaviors by training a logistic regression classifier based on the defined influence locality functions. We incorporate social influence locality into a factor graph model, which can further leverage the network-based correlation. Our experiments on the large microblogging network show that the model significantly improves the precision of retweet prediction. Our analysis also reveals several intriguing discoveries. For example, if you have six friends retweeting a microblog, the average likelihood that you will also retweet it strongly depends on the structure among the six friends: The likelihood will significantly drop (only f) when the six friends do not know each other, compared with the case when the six friends know each other.",2015,Knowledge Discovery and Data Mining,Fields of study: social influencesocial networkworld wide web
A Framework of Mining Trajectories from Untrustworthy Data in Cyber-Physical System,Lu An Tang (NEC)Xiao Yu (University of Illinois at Urbana–Champaign)Quanquan Gu (University of Illinois at Urbana–Champaign)Jiawei Han (University of Illinois at Urbana–Champaign)Guofei Jiang (NEC)Alice Leung (BBN Technologies)Thomas F. La Porta (Pennsylvania State University),"2143081449,2160715520,2167348148,2121939561,2168090285,2131880941,697457595","A cyber-physical system (CPS) integrates physical (i.e., sensor) devices with cyber (i.e., informational) components to form a context-sensitive system that responds intelligently to dynamic changes in real-world situations. The CPS has wide applications in scenarios such as environment monitoring, battlefield surveillance, and traffic control. One key research problem of CPS is called mining lines in the sand . With a large number of sensors (sand) deployed in a designated area, the CPS is required to discover all trajectories (lines) of passing intruders in real time. There are two crucial challenges that need to be addressed: (1) the collected sensor data are not trustworthy, and (2) the intruders do not send out any identification information. The system needs to distinguish multiple intruders and track their movements. This study proposes a method called LiSM (Line-in-the-Sand Miner) to discover trajectories from untrustworthy sensor data. LiSM constructs a watching network from sensor data and computes the locations of intruder appearances based on the link information of the network. The system retrieves a cone model from the historical trajectories to track multiple intruders. Finally, the system validates the mining results and updates sensors’ reliability scores in a feedback process. In addition, LoRM (Line-on-the-Road Miner) is proposed for trajectory discovery on road networks— mining lines on the roads . LoRM employs a filtering-and-refinement framework to reduce the distance computational overhead on road networks and uses a shortest-path-measure to track intruders. The proposed methods are evaluated with extensive experiments on big datasets. The experimental results show that the proposed methods achieve higher accuracy and efficiency in trajectory mining tasks.",2015,Knowledge Discovery and Data Mining,Fields of study: cyber physical systemtrajectorywireless sensor networkcomputer securitydata miningsimulationcomputer science
A Space-Efficient Streaming Algorithm for Estimating Transitivity and Triangle Counts Using the Birthday Paradox,Madhav Jha (Sandia National Laboratories)C. Seshadhri (Sandia National Laboratories)Ali Pinar (Sandia National Laboratories),"2645250562,1214261146,2104623905","We design a space-efficient algorithm that approximates the transitivity (global clustering coefficient) and total triangle count with only a single pass through a graph given as a stream of edges. Our procedure is based on the classic probabilistic result, the birthday paradox . When the transitivity is constant and there are more edges than wedges (common properties for social networks), we can prove that our algorithm requires O (√ n ) space ( n is the number of vertices) to provide accurate estimates. We run a detailed set of experiments on a variety of real graphs and demonstrate that the memory requirement of the algorithm is a tiny fraction of the graph. For example, even for a graph with 200 million edges, our algorithm stores just 40,000 edges to give accurate results. Being a single pass streaming algorithm, our procedure also maintains a real-time estimate of the transitivity/number of triangles of a graph by storing a minuscule fraction of edges.",2015,Knowledge Discovery and Data Mining,Fields of study: multiple edgesmixed graphbirthday problempathcycle graphstreaming algorithmtransitive relationclustering coefficientdiscrete mathematicscombinatoricsstatisticscomputer sciencemathematics
Universal and Distinct Properties of Communication Dynamics: How to Generate Realistic Inter-event Times,Pedro O. S. Vaz De Melo (Universidade Federal de Minas Gerais)Christos Faloutsos (Carnegie Mellon University)Renato Assunção (Universidade Federal de Minas Gerais)Rodrigo Alves (Universidade Federal de Minas Gerais)Antonio A. F. Loureiro (Universidade Federal de Minas Gerais),"2162838672,2198983026,2001700347,2168895552,2046347786","With the advancement of information systems, means of communications are becoming cheaper, faster, and more available. Today, millions of people carrying smartphones or tablets are able to communicate practically any time and anywhere they want. They can access their e-mails, comment on weblogs, watch and post videos and photos (as well as comment on them), and make phone calls or text messages almost ubiquitously. Given this scenario, in this article, we tackle a fundamental aspect of this new era of communication: How the time intervals between communication events behave for different technologies and means of communications. Are there universal patterns for the Inter-Event Time Distribution (IED)q How do inter-event times behave differently among particular technologiesq To answer these questions, we analyzed eight different datasets from real and modern communication data and found four well-defined patterns seen in all the eight datasets. Moreover, we propose the use of the Self-Feeding Process (SFP) to generate inter-event times between communications. The SFP is an extremely parsimonious point process that requires at most two parameters and is able to generate inter-event times with all the universal properties we observed in the data. We also show three potential applications of the SFP: as a framework to generate a synthetic dataset containing realistic communication events of any one of the analyzed means of communications, as a technique to detect anomalies, and as a building block for more specific models that aim to encompass the particularities seen in each of the analyzed systems.",2015,Knowledge Discovery and Data Mining,Fields of study: generative modeldata miningmachine learningsimulationcomputer science
Unbiased Characterization of Node Pairs over Large Graphs,Pinghui Wang (Huawei)Junzhou Zhao (Xi'an Jiaotong University)John C. S. Lui (The Chinese University of Hong Kong)Don Towsley (University of Massachusetts Amherst)Xiaohong Guan (Xi'an Jiaotong University),"2114181251,2128573971,2045404162,2177075905,2119048549","Characterizing user pair relationships is important for applications such as friend recommendation and interest targeting in online social networks (OSNs). Due to the large-scale nature of such networks, it is infeasible to enumerate all user pairs and thus sampling is used. In this article, we show that it is a great challenge for OSN service providers to characterize user pair relationships, even when they possess the complete graph topology. The reason is that when sampling techniques (i.e., uniform vertex sampling (UVS) and random walk (RW)) are naively applied, they can introduce large biases, particularly for estimating similarity distribution of user pairs with constraints like existence of mutual neighbors, which is important for applications such as identifying network homophily. Estimating statistics of user pairs is more challenging in the absence of the complete topology information, as an unbiased sampling technique like UVS is usually not allowed and exploring the OSN graph topology is expensive. To address these challenges, we present unbiased sampling methods to characterize user pair properties based on UVS and RW techniques. We carry out an evaluation of our methods to show their accuracy and efficiency. Finally, we apply our methods to three OSNs—Foursquare, Douban, and Xiami—and discover that significant homophily is present in these networks.",2015,Knowledge Discovery and Data Mining,Fields of study: random walksocial networkdata miningmachine learningstatisticsmathematics
Hierarchical Bayesian Inference and Recursive Regularization for Large-Scale Classification,Siddharth Gopal (Carnegie Mellon University)Yiming Yang (Carnegie Mellon University),"2128212913,2159253281","In this article, we address open challenges in large-scale classification, focusing on how to effectively leverage the dependency structures (hierarchical or graphical) among class labels, and how to make the inference scalable in jointly optimizing all model parameters. We propose two main approaches, namely the hierarchical Bayesian inference framework and the recursive regularization scheme. The key idea in both approaches is to reinforce the similarity among parameter across the nodes in a hierarchy or network based on the proximity and connectivity of the nodes. For scalability, we develop hierarchical variational inference algorithms and fast dual coordinate descent training procedures with parallelization. In our experiments for classification problems with hundreds of thousands of classes and millions of training instances with terabytes of parameters, the proposed methods show consistent and statistically significant improvements over other competing approaches, and the best results on multiple benchmark datasets for large-scale classification.",2015,Knowledge Discovery and Data Mining,Fields of study: bayesian probabilitydata miningpattern recognitionmachine learningstatisticscomputer science
Pairwised Specific Distance Learning from Physical Linkages,Juhua Hu (Nanjing University)De-Chuan Zhan (Nanjing University)Xintao Wu (University of Arkansas)Yuan Jiang (Nanjing University)Zhi-Hua Zhou (Nanjing University),"2160078699,2131836931,2623789330,2100145838,2286237009","In real tasks, usually a good classification performance can only be obtained when a good distance metric is obtained; therefore, distance metric learning has attracted significant attention in the past few years. Typical studies of distance metric learning evaluate how to construct an appropriate distance metric that is able to separate training data points from different classes or satisfy a set of constraints (e.g., must-links and/or cannot-links). It is noteworthy that this task becomes challenging when there are only limited labeled training data points and no constraints are given explicitly. Moreover, most existing approaches aim to construct a global distance metric that is applicable to all data points. However, different data points may have different properties and may require different distance metrics. We notice that data points in real tasks are often connected by physical links (e.g., people are linked with each other in social networks; personal webpages are often connected to other webpages, including nonpersonal webpages), but the linkage information has not been exploited in distance metric learning. In this article, we develop a pairwised specific distance (PSD) approach that exploits the structures of physical linkages and in particular captures the key observations that nonmetric and clique linkages imply the appearance of different or unique semantics, respectively. It is noteworthy that, rather than generating a global distance, PSD generates different distances for different pairs of data points; this property is desired in applications involving complicated data semantics. We mainly present PSD for multi-class learning and further extend it to multi-label learning. Experimental results validate the effectiveness of PSD, especially in the scenarios in which there are very limited labeled training data points and no explicit constraints are given.",2015,Knowledge Discovery and Data Mining,Fields of study: semi supervised learningdata miningartificial intelligencemachine learningmathematics
On Data Publishing with Clustering Preservation,Michail Vlachos (IBM)Johannes Schneider (IBM)Vassilios G. Vassiliadis (IBM),"2146138755,2306252458,2229061546","The emergence of cloud-based storage services is opening up new avenues in data exchange and data dissemination. This has amplified the interest in right-protection mechanisms to establish ownership in the event of data leakage. Current right-protection technologies, however, rarely provide strong guarantees on dataset utility after the protection process. This work presents techniques that explicitly address this topic and provably preserve the outcome of certain mining operations. In particular, we take special care to guarantee that the outcome of hierarchical clustering operations remains the same before and after right protection. Our approach considers all prevalent hierarchical clustering variants: single-, complete-, and average-linkage. We imprint the ownership in a dataset using watermarking principles, and we derive tight bounds on the expansion/contraction of distances incurred by the process. We leverage our analysis to design fast algorithms for right protection without exhaustively searching the vast design space. Finally, because the right-protection process introduces a user-tunable distortion on the dataset, we explore the possibility of using this mechanism for data obfuscation. We quantify the tradeoff between obfuscation and utility for spatiotemporal datasets and discover very favorable characteristics of the process. An additional advantage is that when one is interested in both right-protecting and obfuscating the original data values, the proposed mechanism can accomplish both tasks simultaneously.",2015,Knowledge Discovery and Data Mining,Fields of study: restricted isometry propertydigital watermarkingtheoretical computer sciencecomputer securitydata miningmachine learningstatisticscomputer science
Opinion Marks : A Human-Based Computation Approach to Instill Structure into Unstructured Text on the Web,Bum Chul Kwon (University of Konstanz)Jaegul Choo (Korea University)Sung-Hee Kim (University of British Columbia)Daniel Keim (University of Konstanz)Haesun Park (Georgia Institute of Technology)Ji Soo Yi (Purdue University),"2270870276,2148380128,2162789873,2147343253,2123241397,2137524282",-,2015,Knowledge Discovery and Data Mining,Fields of study: data scienceworld wide webdata miningcomputer science
The GOBIA method: Towards goal-oriented business intelligence architectures,David FeketeGottfried Vossen (University of Münster),"2518421265,721127700","Traditional Data Warehouse (DWH) architectures are chal- lenged by numerous novel Big Data products. These tools are typically presented as alternatives or extensions for one or more of the layers of a typical DWH reference architecture. Still, there is no established joint reference architecture for both DWH and Big Data that is inher- ently aligned with business goals as implied by Business Intelligence (BI) projects. In this paper, a work-in-progress approach towards such cus- tom BI architectures, the GOBIA method, is presented to address this gap, combining a BI reference architecture and a development process.",2015,Knowledge Discovery and Data Mining,Fields of study: data miningdatabasecomputer science
Creedo: Scalable and Repeatable Extrinsic Evaluation for pattern discovery systems by online user studies,Mario Boley (Fraunhofer Society)Maike Krause-TraudesBo Kang (Fraunhofer Society)Björn Jacobs,"2074964178,2678243573,2228844688,2708035514",-,2015,Knowledge Discovery and Data Mining,Fields of study: world wide webdata miningsimulationcomputer science
Open challenges for data stream mining research,Georg Krempl (University of Graz)Indre Žliobaite (Aalto University)Dariusz Brzeziński (Poznań University of Technology)Eyke Hüllermeier (University of Paderborn)Vincent LemaireTino Noack (MediaTech Institute)Ammar Shaker (University of Paderborn)Sonja SieviMyra Spiliopoulou (Humboldt University of Berlin)Jerzy Stefanowski (Poznań University of Technology),"2306518288,2690411833,2328495631,323026139,2306280139,2306769941,2139734814,2715439390,2672394148,142644238","Every day, huge volumes of sensory, transactional, and web data are continuously generated as streams, which need to be analyzed online as they arrive. Streaming data can be considered as one of the main sources of what is called big data. While predictive modeling for data streams and big data have received a lot of attention over the last decade, many research approaches are typically designed for well-behaved controlled problem settings, overlooking important challenges imposed by real-world applications. This article presents a discussion on eight open challenges for data stream mining. Our goal is to identify gaps between current research and meaningful applications, highlight open problems, and define new application-relevant research directions for data stream mining. The identified challenges cover the full cycle of knowledge discovery and involve such problems as: protecting data privacy, dealing with legacy systems, handling incomplete and delayed information, analysis of complex data, and evaluation of stream mining algorithms. The resulting analysis is illustrated by practical applications and provides general suggestions concerning lines of future research in data stream mining.",2014,Knowledge Discovery and Data Mining,Fields of study: data stream miningdata sciencedata miningdatabasecomputer science
What is Tumblr: a statistical overview and comparison,Yi Chang (Yahoo!)Lei Tang (Walmart Labs)Yoshiyuki Inagaki (Yahoo!)Yan Liu (University of Southern California),"2168000538,2648681023,2504698808,2240541904","Tumblr, as one of the most popular microblogging platforms, has gained momentum recently. It is reported to have 166.4 millions of users and 73.4 billions of posts by January 2014. While many articles about Tumblr have been published in major press, there is not much scholar work so far. In this paper, we provide some pioneer analysis on Tumblr from a variety of aspects. We study the social network structure among Tumblr users, analyze its user generated content, and describe reblogging patterns to analyze its user behavior. We aim to provide a comprehensive statistical overview of Tumblr and compare it with other popular social services, including blogosphere, Twitter and Facebook, in answering a couple of key questions: What is Tumblr? How is Tumblr different from other social media networks? In short, we find Tumblr has more rich content than other microblogging platforms, and it contains hybrid characteristics of social networking, traditional blogosphere, and social media. This work serves as an early snapshot of Tumblr that later work can leverage.",2014,Knowledge Discovery and Data Mining,Fields of study: microblogginginternet privacymultimediaworld wide webdata mining
Twitter analytics: a big data management perspective,Oshini Goonetilleke (RMIT University)Timos K. Sellis (RMIT University)Xiuzhen Zhang (RMIT University)Saket Sathe (IBM),"1993785969,2696661525,2126593655,2478550524","With the inception of the Twitter microblogging platform in 2006, a myriad of research efforts have emerged studying different aspects of the Twittersphere. Each study exploits its own tools and mechanisms to capture, store, query and analyze Twitter data. Inevitably, platforms have been developed to replace this ad-hoc exploration with a more structured and methodological form of analysis. Another body of literature focuses on developing languages for querying Tweets. This paper addresses issues around the big data nature of Twitter and emphasizes the need for new data management and query language frameworks that address limitations of existing systems. We review existing approaches that were developed to facilitate twitter analytics followed by a discussion on research issues and technical challenges in developing integrated solutions.",2014,Knowledge Discovery and Data Mining,Fields of study: data scienceworld wide webdata miningcomputer science
Contextual crowd intelligence,Beng Chin Ooi (National University of Singapore)Kian-Lee Tan (National University of Singapore)Quoc Trung Tran (National University of Singapore)James Wei Luen YipGang Chen 0001 (Zhejiang University)Zheng Jye LingThi Nguyen (National University of Singapore)Anthony K. H. Tung (National University of Singapore)Meihui Zhang (National University of Singapore),"2037466936,2142476784,2145857504,2695805245,2125776506,2273694601,2523331059,121171588,2150446338","Most data analytics applications are industry/domain specific, e.g., predicting patients at high risk of being admitted to intensive care unit in the healthcare sector or predicting malicious SMSs in the telecommunication sector. Existing solutions are based on ""best practices"", i.e., the systems' decisions are knowledge-driven and/or data-driven . However, there are rules and exceptional cases that can only be precisely formulated and identified by subject-matter experts (SMEs) who have accumulated many years of experience. This paper envisions a more intelligent database management system (DBMS) that captures such knowledge to effectively address the industry/domain specific applications. At the core, the system is a hybrid human-machine database engine where the machine interacts with the SMEs as part of a feedback loop to gather, infer, ascertain and enhance the database knowledge and processing. We discuss the challenges towards building such a system through examples in healthcare predictive analysis -- a popular area for big data analytics.",2014,Knowledge Discovery and Data Mining,Fields of study: data sciencedata miningmachine learningsimulationcomputer science
On power law distributions in large-scale taxonomies,Rohit Babbar (Centre national de la recherche scientifique)Cornelia Metzig (Centre national de la recherche scientifique)Ioannis Partalas (Centre national de la recherche scientifique)Eric Gaussier (Centre national de la recherche scientifique)Massih-Reza Amini (Centre national de la recherche scientifique),"2643210660,2713489539,2715815634,2663681485,2111378394","In many of the large-scale physical and social complex systems phenomena fat-tailed distributions occur, for which different generating mechanisms have been proposed. In this paper, we study models of generating power law distributions in the evolution of large-scale taxonomies such as Open Directory Project, which consist of websites assigned to one of tens of thousands of categories. The categories in such taxonomies are arranged in tree or DAG structured configurations having parent-child relations among them. We first quantitatively analyse the formation process of such taxonomies, which leads to power law distribution as the stationary distributions. In the context of designing classifiers for large-scale taxonomies, which automatically assign unseen documents to leaf-level categories, we highlight how the fat-tailed nature of these distributions can be leveraged to analytically study the space complexity of such classifiers. Empirical evaluation of the space complexity on publicly available datasets demonstrates the applicability of our approach.",2014,Knowledge Discovery and Data Mining,Fields of study: theoretical computer sciencedata miningmachine learning
Change detection in streaming data in the era of big data: models and issues,Dang-Hoan Tran (Vietnam Maritime University)Mohamed Medhat Gaber (Robert Gordon University)Kai-Uwe Sattler (Technische Universität Ilmenau),"2140605383,1976146371,2172089212","Big Data is identified by its three Vs, namely velocity, volume, and variety. The area of data stream processing has long dealt with the former two Vs velocity and volume. Over a decade of intensive research, the community has provided many important research discoveries in the area. The third V of Big Data has been the result of social media and the large unstructured data it generates. Streaming techniques have also been proposed recently addressing this emerging need. However, a hidden factor can represent an important fourth V, that is variability or change. Our world is changing rapidly, and accounting to variability is a crucial success factor. This paper provides a survey of change detection techniques as applied to streaming data. The review is timely with the rise of Big Data technologies, and the need to have this important aspect highlighted and its techniques categorized and detailed.",2014,Knowledge Discovery and Data Mining,Fields of study: data scienceworld wide webdata mining
"Interview: Michael Brodie, leading database researcher, industry leader, thinker",Gregory Piatetsky,2276955442,"We discuss the most important database research advances, industry developments, role of relational and NoSQL databases, Computing Reality, Data Curation, Cloud Computing, Tamr and Jisto startups, what he learned as a chief Scientist of Verizon, Knowledge Discovery, Privacy Issues, and more.",2014,Knowledge Discovery and Data Mining,Fields of study: data curationcloud computingprivacydata scienceworld wide webdata miningcomputer science
Knowledge vault: a web-scale approach to probabilistic knowledge fusion,Xin Luna Dong (Google)Evgeniy Gabrilovich (Google)Geremy Heitz (Google)Wilko Horn (Google)Ni Lao (Google)Kevin Murphy (Google)Thomas Strohmann (Google)Shaohua Sun (Google)Wei Zhang (Google),"2150077005,1804802447,2000496673,2136244735,2145221253,2167731548,2222397397,2124804123,2646486594","Recent years have witnessed a proliferation of large-scale knowledge bases, including Wikipedia, Freebase, YAGO, Microsoft's Satori, and Google's Knowledge Graph. To increase the scale even further, we need to explore automatic methods for constructing knowledge bases. Previous approaches have primarily focused on text-based extraction, which can be very noisy. Here we introduce Knowledge Vault, a Web-scale probabilistic knowledge base that combines extractions from Web content (obtained via analysis of text, tabular data, page structure, and human annotations) with prior knowledge derived from existing knowledge repositories. We employ supervised machine learning methods for fusing these distinct information sources. The Knowledge Vault is substantially bigger than any previously published structured knowledge repository, and features a probabilistic inference system that computes calibrated probabilities of fact correctness. We report the results of multiple studies that explore the relative utility of the different information sources and extraction methods.",2014,Knowledge Discovery and Data Mining,Fields of study: open knowledge base connectivityknowledge extractioninformation extractionknowledge baseknowledge based systemsdata sciencedata miningmachine learningcomputer science
DeepWalk: online learning of social representations,Bryan Perozzi (Stony Brook University)Rami Al-Rfou (Stony Brook University)Steven Skiena (Stony Brook University),"1983756286,73721942,2053543515","We present DeepWalk, a novel approach for learning latent representations of vertices in a network. These latent representations encode social relations in a continuous vector space, which is easily exploited by statistical models. DeepWalk generalizes recent advancements in language modeling and unsupervised feature learning (or deep learning ) from sequences of words to graphs. DeepWalk uses local information obtained from truncated random walks to learn latent representations by treating walks as the equivalent of sentences. We demonstrate DeepWalk's latent representations on several multi-label network classification tasks for social networks such as BlogCatalog, Flickr, and YouTube. Our results show that DeepWalk outperforms challenging baselines which are allowed a global view of the network, especially in the presence of missing information. DeepWalk's representations can provide F1 scores up to 10% higher than competing methods when labeled data is sparse. In some experiments, DeepWalk's representations are able to outperform all baseline methods while using 60% less training data. DeepWalk is also scalable. It is an online learning algorithm which builds useful incremental results, and is trivially parallelizable. These qualities make it suitable for a broad class of real world applications such as network classification, and anomaly detection.",2014,Knowledge Discovery and Data Mining,Fields of study: social networkdeep learningsemi supervised learningunsupervised learningcombinatoricssocial sciencepattern recognitionartificial intelligencemachine learningcomputer science
Open question answering over curated and extracted knowledge bases,Anthony Fader (University of Washington)Luke Zettlemoyer (University of Washington)Oren Etzioni (University of Washington),"2013751244,334758317,57747768","We consider the problem of open-domain question answering (Open QA) over massive knowledge bases (KBs). Existing approaches use either manually curated KBs like Freebase or KBs automatically extracted from unstructured text. In this paper, we present OQA, the first approach to leverage both curated and extracted KBs. A key technical challenge is designing systems that are robust to the high variability in both natural language questions and massive KBs. OQA achieves robustness by decomposing the full Open QA problem into smaller sub-problems including question paraphrasing and query reformulation. OQA solves these sub-problems by mining millions of rules from an unlabeled question corpus and across multiple KBs. OQA then learns to integrate these rules by performing discriminative training on question-answer pairs using a latent-variable structured perceptron algorithm. We evaluate OQA on three benchmark question sets and demonstrate that it achieves up to twice the precision and recall of a state-of-the-art Open QA system.",2014,Knowledge Discovery and Data Mining,Fields of study: information retrievaldata miningdatabasemachine learningcomputer science
Travel time estimation of a path using sparse trajectories,Yilun Wang (Microsoft)Yu Zheng (Microsoft)Yexiang Xue (Cornell University),"2703933034,2145115012,2154946902","In this paper, we propose a citywide and real-time model for estimating the travel time of any path (represented as a sequence of connected road segments) in real time in a city, based on the GPS trajectories of vehicles received in current time slots and over a period of history as well as map data sources. Though this is a strategically important task in many traffic monitoring and routing systems, the problem has not been well solved yet given the following three challenges. The first is the data sparsity problem, i.e., many road segments may not be traveled by any GPS-equipped vehicles in present time slot. In most cases, we cannot find a trajectory exactly traversing a query path either. Second, for the fragment of a path with trajectories, they are multiple ways of using (or combining) the trajectories to estimate the corresponding travel time. Finding an optimal combination is a challenging problem, subject to a tradeoff between the length of a path and the number of trajectories traversing the path (i.e., support). Third, we need to instantly answer users' queries which may occur in any part of a given city. This calls for an efficient, scalable and effective solution that can enable a citywide and real-time travel time estimation. To address these challenges, we model different drivers' travel times on different road segments in different time slots with a three dimension tensor. Combined with geospatial, temporal and historical contexts learned from trajectories and map data, we fill in the tensor's missing values through a context-aware tensor decomposition approach. We then devise and prove an object function to model the aforementioned tradeoff, with which we find the most optimal concatenation of trajectories for an estimate through a dynamic programming solution. In addition, we propose using frequent trajectory patterns (mined from historical trajectories) to scale down the candidates of concatenation and a suffix-tree-based index to manage the trajectories received in the present time slot. We evaluate our method based on extensive experiments, using GPS trajectories generated by more than 32,000 taxis over a period of two months. The results demonstrate the effectiveness, efficiency and scalability of our method beyond baseline approaches.",2014,Knowledge Discovery and Data Mining,Fields of study: data miningmachine learningsimulation
Inferring gas consumption and pollution emission of vehicles throughout a city,Jingbo Shang (Shanghai Jiao Tong University)Yu Zheng (Microsoft)Wenzhu Tong (Microsoft)Eric Chang (Microsoft)Yong Yu (Shanghai Jiao Tong University),"2223914299,2145115012,2620028873,2110283602,2119244895","This paper instantly infers the gas consumption and pollution emission of vehicles traveling on a city's road network in a current time slot, using GPS trajectories from a sample of vehicles (e.g., taxicabs). The knowledge can be used to suggest cost-efficient driving routes as well as identifying road segments where gas has been wasted significantly. The instant estimation of the emissions from vehicles can enable pollution alerts and help diagnose the root cause of air pollution in the long run. In our method, we first compute the travel speed of each road segment using the GPS trajectories received recently. As many road segments are not traversed by trajectories (i.e., data sparsity), we propose a Travel Speed Estimation (TSE) model based on a context-aware matrix factorization approach. TSE leverages features learned from other data sources, e.g., map data and historical trajectories, to deal with the data sparsity problem. We then propose a Traffic Volume Inference (TVI) model to infer the number of vehicles passing each road segment per minute. TVI is an unsupervised Bayesian Network that incorporates multiple factors, such as travel speed, weather conditions and geographical features of a road. Given the travel speed and traffic volume of a road segment, gas consumption and emissions can be calculated based on existing environmental theories. We evaluate our method based on extensive experiments using GPS trajectories generated by over 32,000 taxis in Beijing over a period of two months. The results demonstrate the advantages of our method over baselines, validating the contribution of its components and finding interesting discoveries for the benefit of society.",2014,Knowledge Discovery and Data Mining,Fields of study: simulation
GeoMF: joint geographical modeling and matrix factorization for point-of-interest recommendation,Defu Lian (University of Science and Technology of China)Cong Zhao (University of Science and Technology of China)Xing Xie (Microsoft)Guangzhong Sun (University of Science and Technology of China)Enhong Chen (University of Science and Technology of China)Yong Rui (Microsoft),"2110195189,2486997434,2125800575,2160951811,2136372366,2704008839","Point-of-Interest (POI) recommendation has become an important means to help people discover attractive locations. However, extreme sparsity of user-POI matrices creates a severe challenge. To cope with this challenge, viewing mobility records on location-based social networks (LBSNs) as implicit feedback for POI recommendation, we first propose to exploit weighted matrix factorization for this task since it usually serves collaborative filtering with implicit feedback better. Besides, researchers have recently discovered a spatial clustering phenomenon in human mobility behavior on the LBSNs, i.e., individual visiting locations tend to cluster together, and also demonstrated its effectiveness in POI recommendation, thus we incorporate it into the factorization model. Particularly, we augment users' and POIs' latent factors in the factorization model with activity area vectors of users and influence area vectors of POIs, respectively. Based on such an augmented model, we not only capture the spatial clustering phenomenon in terms of two-dimensional kernel density estimation, but we also explain why the introduction of such a phenomenon into matrix factorization helps to deal with the challenge from matrix sparsity. We then evaluate the proposed algorithm on a large-scale LBSN dataset. The results indicate that weighted matrix factorization is superior to other forms of factorization models and that incorporating the spatial clustering phenomenon into matrix factorization improves recommendation performance.",2014,Knowledge Discovery and Data Mining,Fields of study: kernel density estimationnon negative matrix factorizationdata miningmachine learningsimulationstatisticscomputer sciencemathematics
Optimal real-time bidding for display advertising,Weinan Zhang (University College London)Shuai Yuan (University College London)Jun Wang (University College London),"2527611484,2131459235,2557836567","In this paper we study bid optimisation for real-time bidding (RTB) based display advertising. RTB allows advertisers to bid on a display ad impression in real time when it is being generated. It goes beyond contextual advertising by motivating the bidding focused on user data and it is different from the sponsored search auction where the bid price is associated with keywords. For the demand side, a fundamental technical challenge is to automate the bidding process based on the budget, the campaign objective and various information gathered in runtime and in history. In this paper, the programmatic bidding is cast as a functional optimisation problem. Under certain dependency assumptions, we derive simple bidding functions that can be calculated in real time; our finding shows that the optimal bid has a non-linear relationship with the impression level evaluation such as the click-through rate and the conversion rate, which are estimated in real time from the impression level features. This is different from previous work that is mainly focused on a linear bidding function. Our mathematical derivation suggests that optimal bidding strategies should try to bid more impressions rather than focus on a small set of high valued impressions because according to the current RTB market data, compared to the higher evaluated impressions, the lower evaluated ones are more cost effective and the chances of winning them are relatively higher. Aside from the theoretical insights, offline experiments on a real dataset and online experiments on a production RTB system verify the effectiveness of our proposed optimal bidding strategies and the functional optimisation framework.",2014,Knowledge Discovery and Data Mining,Fields of study: ebiddingreal time biddingdata miningcomputer science
'Beating the news' with EMBERS: forecasting civil unrest using open source indicators,"Naren Ramakrishnan (Virginia Tech)Patrick Butler (Virginia Tech)Sathappan Muthiah (Virginia Tech)Nathan Self (Virginia Tech)Rupinder Paul Khandpur (Virginia Tech)Parang Saraf (Virginia Tech)Wei Wang 0064 (Virginia Tech)Jose Cadena (Virginia Tech)Anil Vullikanti (Virginia Tech)Gizem Korkmaz (Virginia Tech)Chris J. Kuhlman (Virginia Tech)Achla Marathe (Virginia Tech)Liang Zhao (Virginia Tech)Ting Hua (Virginia Tech)Feng Chen (University at Albany, SUNY)Chang-Tien Lu (Virginia Tech)Bert Huang (University of Maryland, College Park)Aravind Srinivasan (University of Maryland, College Park)Khoa Trinh (University of Maryland, College Park)Lise Getoor (University of California, Santa Cruz)Graham Katz (CACI)Andy Doyle (CACI)Chris Ackermann (CACI)Ilya Zavorin (CACI)Jim Ford (CACI)Kristen Maria Summers (CACI)Youssef FayedJaime Arredondo (University of California, San Diego)Dipak Gupta (San Diego State University)David Mares (University of California, San Diego)","2199255697,2293049679,1509108369,2093704914,2054069009,2160730744,2529750488,2153027628,393896382,2058784526,1966158927,2111312453,2619584304,2106233920,2601749234,2112878203,2121178673,2128242943,1984993552,1984940772,2104271048,2238455168,2112261622,1471712086,2556804610,2175894886,1811929682,2135686031,2142889206,2224701853","We describe the design, implementation, and evaluation of EMBERS, an automated, 24x7 continuous system for forecasting civil unrest across 10 countries of Latin America using open source indicators such as tweets, news sources, blogs, economic indicators, and other data sources. Unlike retrospective studies, EMBERS has been making forecasts into the future since Nov 2012 which have been (and continue to be) evaluated by an independent T&E team (MITRE). Of note, EMBERS has successfully forecast the June 2013 protests in Brazil and Feb 2014 violent protests in Venezuela. We outline the system architecture of EMBERS, individual models that leverage specific data sources, and a fusion and suppression engine that supports trading off specific evaluation criteria. EMBERS also provides an audit trail interface that enables the investigation of why specific predictions were made along with the data utilized for forecasting. Through numerous evaluations, we demonstrate the superiority of EMBERS over baserate methods and its capability to forecast significant societal happenings.",2014,Knowledge Discovery and Data Mining,Fields of study: operations researchworld wide websocial sciencesimulation
Inferring user demographics and social strategies in mobile social networks,Yuxiao Dong (University of Notre Dame)Yang Yang (Tsinghua University)Jie Tang (Tsinghua University)Nitesh V. Chawla (University of Notre Dame),"2157080782,2673426936,2158012360,1979796846","Demographics are widely used in marketing to characterize different types of customers. However, in practice, demographic information such as age, gender, and location is usually unavailable due to privacy and other reasons. In this paper, we aim to harness the power of big data to automatically infer users' demographics based on their daily mobile communication patterns. Our study is based on a real-world large mobile network of more than 7,000,000 users and over 1,000,000,000 communication records (CALL and SMS). We discover several interesting social strategies that mobile users frequently use to maintain their social connections. First, young people are very active in broadening their social circles, while seniors tend to keep close but more stable connections. Second, female users put more attention on cross-generation interactions than male users, though interactions between male and female users are frequent. Third, a persistent same-gender triadic pattern over one's lifetime is discovered for the first time, while more complex opposite-gender triadic patterns are only exhibited among young people. We further study to what extent users' demographics can be inferred from their mobile communications. As a special case, we formalize a problem of double dependent-variable prediction-inferring user gender and age simultaneously. We propose the WhoAmI method, a Double Dependent-Variable Factor Graph Model, to address this problem by considering not only the effects of features on gender/age, but also the interrelation between gender and age. Our experiments show that the proposed WhoAmI method significantly improves the prediction accuracy by up to 10% compared with several alternative methods.",2014,Knowledge Discovery and Data Mining,Fields of study: human communicationdata mining
"Jointly modeling aspects, ratings and sentiments for movie recommendation (JMARS)",Qiming Diao (Singapore Management University)Minghui Qiu (Singapore Management University)Chao-Yuan Wu (Carnegie Mellon University)Alexander J. Smola (Carnegie Mellon University)Jing Jiang (Singapore Management University)Chong Wang (Carnegie Mellon University),"2054432954,2158082241,2598151560,1972291593,2286546702,2221257390","Recommendation and review sites offer a wealth of information beyond ratings. For instance, on IMDb users leave reviews, commenting on different aspects of a movie (e.g. actors, plot, visual effects), and expressing their sentiments (positive or negative) on these aspects in their reviews. This suggests that uncovering aspects and sentiments will allow us to gain a better understanding of users, movies, and the process involved in generating ratings. The ability to answer questions such as ""Does this user care more about the plot or about the special effects?"" or ""What is the quality of the movie in terms of acting?"" helps us to understand why certain ratings are generated. This can be used to provide more meaningful recommendations. In this work we propose a probabilistic model based on collaborative filtering and topic modeling. It allows us to capture the interest distribution of users and the content distribution for movies; it provides a link between interest and relevance on a per-aspect basis and it allows us to differentiate between positive and negative sentiments on a per-aspect basis. Unlike prior work our approach is entirely unsupervised and does not require knowledge of the aspect specific ratings or genres for inference. We evaluate our model on a live copy crawled from IMDb. Our model offers superior performance by joint modeling. Moreover, we are able to address the cold start problem -- by utilizing the information inherent in reviews our model demonstrates improvement for new users and movies.",2014,Knowledge Discovery and Data Mining,Fields of study: topic modelcollaborative filteringsentiment analysismultimediaworld wide webdata miningmachine learningcomputer science
Practical Lessons from Predicting Clicks on Ads at Facebook,Xinran He (Facebook)Junfeng Pan (Facebook)Ou Jin (Facebook)Tianbing Xu (Facebook)Bo Liu (Facebook)Tao Xu (Facebook)Yanxin Shi (Facebook)Antoine Atallah (Facebook)Ralf Herbrich (Facebook)Stuart Bowers (Facebook)Joaquin Quiñonero Candela (Facebook),"2096901250,2227765068,2430705210,2340078064,2619626189,2309114299,2668938396,2129913044,2693067968,2222576984,2698755828","Online advertising allows advertisers to only bid and pay for measurable user responses, such as clicks on ads. As a consequence, click prediction systems are central to most online advertising systems. With over 750 million daily active users and over 1 million active advertisers, predicting clicks on Facebook ads is a challenging machine learning task. In this paper we introduce a model which combines decision trees with logistic regression, outperforming either of these methods on its own by over 3%, an improvement with significant impact to the overall system performance. We then explore how a number of fundamental parameters impact the final prediction performance of our system. Not surprisingly, the most important thing is to have the right features: those capturing historical information about the user or ad dominate other types of features. Once we have the right features and the right model (decisions trees plus logistic regression), other factors play small roles (though even small improvements are important at scale). Picking the optimal handling for data freshness, learning rate schema and data sampling improve the model slightly, though much less than adding a high-value feature, or picking the right model to begin with.",2014,Knowledge Discovery and Data Mining,Fields of study: logistic regressiononline advertisingworld wide webdata miningmachine learningsimulationcomputer science
Reducing the sampling complexity of topic models,Aaron Q. Li (Carnegie Mellon University)Amr Ahmed (Google)Sujith Ravi (Google)Alexander J. Smola (Carnegie Mellon University),"2500931535,2259645355,2590734359,1972291593","Inference in topic models typically involves a sampling step to associate latent variables with observations. Unfortunately the generative model loses sparsity as the amount of data increases, requiring O(k) operations per word for k topics. In this paper we propose an algorithm which scales linearly with the number of actually instantiated topics k d in the document. For large document collections and in structured hierarchical models k d ll k. This yields an order of magnitude speedup. Our method applies to a wide variety of statistical models such as PDP [16,4] and HDP [19]. At its core is the idea that dense, slowly changing distributions can be approximated efficiently by the combination of a Metropolis-Hastings step, use of sparsity, and amortized constant time sampling via Walker's alias method.",2014,Knowledge Discovery and Data Mining,Fields of study: topic modelscalabilitysamplingtheoretical computer sciencespeech recognitiondata miningmachine learningstatisticscomputer science
Mobile app recommendations with security and privacy awareness,Hengshu Zhu (University of Science and Technology of China)Hui Xiong (Rutgers–Newark)Yong Ge (University of North Carolina at Charlotte)Enhong Chen (University of Science and Technology of China),"2098414524,2153710278,2218492437,2136372366","With the rapid prevalence of smart mobile devices, the number of mobile Apps available has exploded over the past few years. To facilitate the choice of mobile Apps, existing mobile App recommender systems typically recommend popular mobile Apps to mobile users. However, mobile Apps are highly varied and often poorly understood, particularly for their activities and functions related to privacy and security. Therefore, more and more mobile users are reluctant to adopt mobile Apps due to the risk of privacy invasion and other security concerns. To fill this crucial void, in this paper, we propose to develop a mobile App recommender system with privacy and security awareness. The design goal is to equip the recommender system with the functionality which allows to automatically detect and evaluate the security risk of mobile Apps. Then, the recommender system can provide App recommendations by considering both the Apps' popularity and the users' security preferences. Specifically, a mobile App can lead to security risk because insecure data access permissions have been implemented in this App. Therefore, we first develop the techniques to automatically detect the potential security risk for each mobile App by exploiting the requested permissions. Then, we propose a flexible approach based on modern portfolio theory for recommending Apps by striking a balance between the Apps' popularity and the users' security concerns, and build an App hash tree to efficiently recommend Apps. Finally, we evaluate our approach with extensive experiments on a large-scale data set collected from Google Play. The experimental results clearly validate the effectiveness of our approach.",2014,Knowledge Discovery and Data Mining,Fields of study: mobile deep linkingmobile searchrecommender systemmobile computinginternet privacyworld wide webcomputer securitydata miningmachine learningcomputer science
Mining topics in documents: standing on the shoulders of big data,Zhiyuan Chen (University of Illinois at Chicago)Bing Liu (University of Illinois at Chicago),"2143860226,2244698799","Topic modeling has been widely used to mine topics from documents. However, a key weakness of topic modeling is that it needs a large amount of data (e.g., thousands of documents) to provide reliable statistics to generate coherent topics. However, in practice, many document collections do not have so many documents. Given a small number of documents, the classic topic model LDA generates very poor topics. Even with a large volume of data, unsupervised learning of topic models can still produce unsatisfactory results. In recently years, knowledge-based topic models have been proposed, which ask human users to provide some prior domain knowledge to guide the model to produce better topics. Our research takes a radically different approach. We propose to learn as humans do, i.e., retaining the results learned in the past and using them to help future learning. When faced with a new task, we first mine some reliable (prior) knowledge from the past learning/modeling results and then use it to guide the model inference to generate more coherent topics. This approach is possible because of the big data readily available on the Web. The proposed algorithm mines two forms of knowledge: must-link (meaning that two words should be in the same topic) and cannot-link (meaning that two words should not be in the same topic). It also deals with two problems of the automatically mined knowledge, i.e., wrong knowledge and knowledge transitivity. Experimental results using review documents from 100 product domains show that the proposed approach makes dramatic improvements over state-of-the-art baselines.",2014,Knowledge Discovery and Data Mining,Fields of study: topic modellifelong learningdata scienceinformation retrievaldata miningmachine learningcomputer science
Clustering and projected clustering with adaptive neighbors,Feiping Nie (University of Texas at Arlington)Xiaoqian Wang (University of Texas at Arlington)Heng Huang (University of Texas at Arlington),"2245267964,2149602155,2137533801","Many clustering methods partition the data groups based on the input data similarity matrix. Thus, the clustering results highly depend on the data similarity learning. Because the similarity measurement and data clustering are often conducted in two separated steps, the learned data similarity may not be the optimal one for data clustering and lead to the suboptimal results. In this paper, we propose a novel clustering model to learn the data similarity matrix and clustering structure simultaneously. Our new model learns the data similarity matrix by assigning the adaptive and optimal neighbors for each data point based on the local distances. Meanwhile, the new rank constraint is imposed to the Laplacian matrix of the data similarity matrix, such that the connected components in the resulted similarity matrix are exactly equal to the cluster number. We derive an efficient algorithm to optimize the proposed challenging problem, and show the theoretical analysis on the connections between our method and the K -means clustering, and spectral clustering. We also further extend the new clustering model for the projected clustering to handle the high-dimensional data. Extensive empirical results on both synthetic data and real-world benchmark data sets show that our new clustering methods consistently outperforms the related clustering approaches.",2014,Knowledge Discovery and Data Mining,Fields of study: k medians clusteringflame clusteringbrown clusteringcanopy clustering algorithmcomplete linkage clusteringdbscancorrelation clusteringconstrained clusteringdata stream clusteringcure data clustering algorithmsingle linkage clusteringaffinity propagationspectral clusteringfuzzy clusteringclustering high dimensional datahierarchical clusteringcluster analysisconsensus clusteringbiclusteringdata miningpattern recognitionmachine learningcomputer sciencemathematics
Efficient mini-batch training for stochastic optimization,Mu Li (Carnegie Mellon University)Tong Zhang (Baidu)Yuqiang Chen (Baidu)Alexander J. Smola (Carnegie Mellon University),"2658285018,2510858842,2309686674,1972291593","Stochastic gradient descent (SGD) is a popular technique for large-scale optimization problems in machine learning. In order to parallelize SGD, minibatch training needs to be employed to reduce the communication cost. However, an increase in minibatch size typically decreases the rate of convergence. This paper introduces a technique based on approximate optimization of a conservatively regularized objective function within each minibatch. We prove that the convergence rate does not decrease with increasing minibatch size. Experiments demonstrate that with suitable implementations of approximate optimization, the resulting algorithm can outperform standard SGD in many scenarios.",2014,Knowledge Discovery and Data Mining,Fields of study: stochastic gradient descentbig datatheoretical computer sciencemachine learningmathematical optimizationstatisticscomputer science
Log-based predictive maintenance,Ruben Sipos (Cornell University)Dmitriy Fradkin (Siemens)Fabian Moerchen (Amazon.com)Zhuang Wang (Temple University),"1980455668,2028188512,1991190249,2121236236","Success of manufacturing companies largely depends on reliability of their products. Scheduled maintenance is widely used to ensure that equipment is operating correctly so as to avoid unexpected breakdowns. Such maintenance is often carried out separately for every component, based on its usage or simply on some fixed schedule. However, scheduled maintenance is labor-intensive and ineffective in identifying problems that develop between technician's visits. Unforeseen failures still frequently occur. In contrast, predictive maintenance techniques help determine the condition of in-service equipment in order to predict when and what repairs should be performed. The main goal of predictive maintenance is to enable pro-active scheduling of corrective work, and thus prevent unexpected equipment failures.",2014,Knowledge Discovery and Data Mining,Fields of study: operational maintenancemaintenance testingcorrective maintenanceplanned maintenancepredictive maintenancedata miningreal time computingmachine learningcomputer science
Learning time-series shapelets,Josif Grabocka (University of Hildesheim)Nicolas Schilling (University of Hildesheim)Martin Wistuba (University of Hildesheim)Lars Schmidt-Thieme (University of Hildesheim),"2064655889,2019736652,2114698921,78243962","Shapelets are discriminative sub-sequences of time series that best predict the target variable. For this reason, shapelet discovery has recently attracted considerable interest within the time-series research community. Currently shapelets are found by evaluating the prediction qualities of numerous candidates extracted from the series segments. In contrast to the state-of-the-art, this paper proposes a novel perspective in terms of learning shapelets. A new mathematical formalization of the task via a classification objective function is proposed and a tailored stochastic gradient learning algorithm is applied. The proposed method enables learning near-to-optimal shapelets directly without the need to try out lots of candidates. Furthermore, our method can learn true top-K shapelets by capturing their interaction. Extensive experimentation demonstrates statistically significant improvement in terms of wins and ranks against 13 baselines over 28 time-series datasets.",2014,Knowledge Discovery and Data Mining,Fields of study: data miningpattern recognitionmachine learningstatisticscomputer science
"FastXML: a fast, accurate and stable tree-classifier for extreme multi-label learning",Yashoteja Prabhu (Indian Institute of Technology Delhi)Manik Varma (Microsoft),"1988058231,2130043413","The objective in extreme multi-label classification is to learn a classifier that can automatically tag a data point with the most relevant subset of labels from a large label set. Extreme multi-label classification is an important research problem since not only does it enable the tackling of applications with many labels but it also allows the reformulation of ranking problems with certain advantages over existing formulations. Our objective, in this paper, is to develop an extreme multi-label classifier that is faster to train and more accurate at prediction than the state-of-the-art Multi-label Random Forest (MLRF) algorithm [2] and the Label Partitioning for Sub-linear Ranking (LPSR) algorithm [35]. MLRF and LPSR learn a hierarchy to deal with the large number of labels but optimize task independent measures, such as the Gini index or clustering error, in order to learn the hierarchy. Our proposed FastXML algorithm achieves significantly higher accuracies by directly optimizing an nDCG based ranking loss function. We also develop an alternating minimization algorithm for efficiently optimizing the proposed formulation. Experiments reveal that FastXML can be trained on problems with more than a million labels on a standard desktop in eight hours using a single core and in an hour using multiple cores.",2014,Knowledge Discovery and Data Mining,Fields of study: rankingdata miningpattern recognitionmachine learningstatisticscomputer science
A dirichlet multinomial mixture model-based approach for short text clustering,Jianhua Yin (Tsinghua University)Jianyong Wang (Tsinghua University),"2145412373,2105625159","Short text clustering has become an increasingly important task with the popularity of social media like Twitter, Google+, and Facebook. It is a challenging problem due to its sparse, high-dimensional, and large-volume characteristics. In this paper, we proposed a collapsed Gibbs Sampling algorithm for the Dirichlet Multinomial Mixture model for short text clustering (abbr. to GSDMM). We found that GSDMM can infer the number of clusters automatically with a good balance between the completeness and homogeneity of the clustering results, and is fast to converge. GSDMM can also cope with the sparse and high-dimensional problem of short texts, and can obtain the representative words of each cluster. Our extensive experimental study shows that GSDMM can achieve significantly better performance than three other clustering models.",2014,Knowledge Discovery and Data Mining,Fields of study: k medians clusteringbrown clusteringcanopy clustering algorithmcorrelation clusteringconstrained clusteringdata stream clusteringcure data clustering algorithmfuzzy clusteringclustering high dimensional datagibbs samplingcluster analysisdata miningpattern recognitionmachine learningstatisticscomputer sciencemathematics
Meta-path based multi-network collective link prediction,Jiawei Zhang (University of Illinois at Chicago)Philip S. Yu (University of Illinois at Chicago)Zhi Hua Zhou (Nanjing University),"2305185572,2125104194,2286237009","Online social networks offering various services have become ubiquitous in our daily life. Meanwhile, users nowadays are usually involved in multiple online social networks simultaneously to enjoy specific services provided by different networks. Formally, social networks that share some common users are named as partially aligned networks. In this paper, we want to predict the formation of social links in multiple partially aligned social networks at the same time, which is formally defined as the multi-network link (formation) prediction problem. In multiple partially aligned social networks, users can be extensively correlated with each other by various connections. To categorize these diverse connections among users, 7 ""intra-network social meta paths"" and 4 categories of ""inter-network social meta paths"" are proposed in this paper. These ""social meta paths"" can cover a wide variety of connection information in the network, some of which can be helpful for solving the multi-network link prediction problem but some can be not. To utilize useful connection, a subset of the most informative ""social meta paths"" are picked, the process of which is formally defined as ""social meta path selection"" in this paper. An effective general link formation prediction framework, Mli (Multi-network Link Identifier), is proposed in this paper to solve the multi-network link (formation) prediction problem. Built with heterogenous topological features extracted based on the selected ""social meta paths"" in the multiple partially aligned social networks, Mli can help refine and disambiguate the prediction results reciprocally in all aligned networks. Extensive experiments conducted on real-world partially aligned heterogeneous networks, Foursquare and Twitter, demonstrate that Mli can solve the multi-network link prediction problem very well.",2014,Knowledge Discovery and Data Mining,Fields of study: transfer of learningbiological classificationdata miningartificial intelligencemachine learningcomputer sciencemathematics
Marble: high-throughput phenotyping from electronic health records via sparse nonnegative tensor factorization,Joyce C. Ho (University of Texas at Austin)Joydeep Ghosh (University of Texas at Austin)Jimeng Sun (Georgia Institute of Technology),"2125416977,2148168557,2110385854","The rapidly increasing availability of electronic health records (EHRs) from multiple heterogeneous sources has spearheaded the adoption of data-driven approaches for improved clinical research, decision making, prognosis, and patient management. Unfortunately, EHR data do not always directly and reliably map to phenotypes, or medical concepts, that clinical researchers need or use. Existing phenotyping approaches typically require labor intensive supervision from medical experts. We propose Marble, a novel sparse non-negative tensor factorization method to derive phenotype candidates with virtually no human supervision. Marble decomposes the observed tensor into two terms, a bias tensor and an interaction tensor. The bias tensor represents the baseline characteristics common amongst the overall population and the interaction tensor defines the phenotypes. We demonstrate the capability of our proposed model on both simulated and patient data from a publicly available clinical database. Our results show that Marble derived phenotypes provide at least a 42.8% reduction in the number of non-zero element and also retains predictive power for classification purposes. Furthermore, the resulting phenotypes and baseline characteristics from real EHR data are consistent with known characteristics of the patient population. Thus it can potentially be used to rapidly characterize, predict, and manage a large number of diseases, thereby promising a novel, data-driven solution that can benefit very large segments of the population.",2014,Knowledge Discovery and Data Mining,Fields of study: tensordimensionality reductiondata sciencebioinformaticsdata miningmachine learningcomputer science
A cost-effective recommender system for taxi drivers,Meng Qu (Rutgers–Newark)Hengshu Zhu (University of Science and Technology of China)Junming Liu (Rutgers–Newark)Guannan Liu (Tsinghua University)Hui Xiong (Rutgers–Newark),"2223048303,2098414524,2226988312,2273869953,2153710278","The GPS technology and new forms of urban geography have changed the paradigm for mobile services. As such, the abundant availability of GPS traces has enabled new ways of doing taxi business. Indeed, recent efforts have been made on developing mobile recommender systems for taxi drivers using Taxi GPS traces. These systems can recommend a sequence of pick-up points for the purpose of maximizing the probability of identifying a customer with the shortest driving distance. However, in the real world, the income of taxi drivers is strongly correlated with the effective driving hours. In other words, it is more critical for taxi drivers to know the actual driving routes to minimize the driving time before finding a customer. To this end, in this paper, we propose to develop a cost-effective recommender system for taxi drivers. The design goal is to maximize their profits when following the recommended routes for finding passengers. Specifically, we first design a net profit objective function for evaluating the potential profits of the driving routes. Then, we develop a graph representation of road networks by mining the historical taxi GPS traces and provide a Brute-Force strategy to generate optimal driving route for recommendation. However, a critical challenge along this line is the high computational cost of the graph based approach. Therefore, we develop a novel recursion strategy based on the special form of the net profit function for searching optimal candidate routes efficiently. Particularly, instead of recommending a sequence of pick-up points and letting the driver decide how to get to those points, our recommender system is capable of providing an entire driving route, and the drivers are able to find a customer for the largest potential profit by following the recommendations. This makes our recommender system more practical and profitable than other existing recommender systems. Finally, we carry out extensive experiments on a real-world data set collected from the San Francisco Bay area and the experimental results clearly validate the effectiveness of the proposed recommender system.",2014,Knowledge Discovery and Data Mining,Fields of study: computer securitydata miningsimulation
EARS (earthquake alert and report system): a real time decision support system for earthquake crisis management,Marco Avvenuti (University of Pisa)Stefano Cresci (National Research Council)Andrea Marchetti (National Research Council)Carlo Meletti (National Institute of Geophysics and Volcanology)Maurizio Tesconi (National Research Council),"268662966,2516129147,2421482707,2613845801,1838429848","Social sensing is based on the idea that communities or groups of people can provide a set of information similar to those obtainable from a sensor network. Emergency management is a candidate field of application for social sensing. In this work we describe the design, implementation and deployment of a decision support system for the detection and the damage assessment of earthquakes in Italy. Our system exploits the messages shared in real-time on Twitter, one of the most popular social networks in the world. Data mining and natural language processing techniques are employed to select meaningful and comprehensive sets of tweets. We then apply a burst detection algorithm in order to promptly identify outbreaking seismic events. Detected events are automatically broadcasted by our system via a dedicated Twitter account and by email notifications. In addition, we mine the content of the messages associated to an event to discover knowledge on its consequences. Finally we compare our results with official data provided by the National Institute of Geophysics and Volcanology (INGV), the authority responsible for monitoring seismic events in Italy. The INGV network detects shaking levels produced by the earthquake, but can only model the damage scenario by using empirical relationships. This scenario can be greatly improved with direct information site by site. Results show that the system has a great ability to detect events of a magnitude in the region of 3.5, with relatively low occurrences of false positives. Earthquake detection mostly occurs within seconds of the event and far earlier than the notifications shared by INGV or by other official channels. Thus, we are able to alert interested parties promptly. Information discovered by our system can be extremely useful to all the government agencies interested in mitigating the impact of earthquakes, as well as the news agencies looking for fresh information to publish.",2014,Knowledge Discovery and Data Mining,Fields of study: emergency managementdecision support systemworld wide webcomputer securitydata miningcomputer science
Methods for ordinal peer grading,Karthik Raman (Cornell University)Thorsten Joachims (Cornell University),"2289464261,245171893","Massive Online Open Courses have the potential to revolutionize higher education with their wide outreach and accessibility, but they require instructors to come up with scalable alternates to traditional student evaluation. Peer grading -- having students assess each other -- is a promising approach to tackling the problem of evaluation at scale, since the number of ""graders"" naturally scales with the number of students. However, students are not trained in grading, which means that one cannot expect the same level of grading skills as in traditional settings. Drawing on broad evidence that ordinal feedback is easier to provide and more reliable than cardinal feedback [5, 38, 29, 9], it is therefore desirable to allow peer graders to make ordinal statements (e.g. ""project X is better than project Y"") and not require them to make cardinal statements (e.g. ""project X is a B-""). Thus, in this paper we study the problem of automatically inferring student grades from ordinal peer feedback, as opposed to existing methods that require cardinal peer feedback. We formulate the ordinal peer grading problem as a type of rank aggregation problem, and explore several probabilistic models under which to estimate student grades and grader reliability. We study the applicability of these methods using peer grading data collected from a real class --- with instructor and TA grades as a baseline --- and demonstrate the efficacy of ordinal feedback techniques in comparison to existing cardinal peer grading methods. Finally, we compare these peer-grading techniques to traditional evaluation techniques.",2014,Knowledge Discovery and Data Mining,Fields of study: peer feedbackdata miningartificial intelligencesimulationmathematics
Seven rules of thumb for web site experimenters,Ron Kohavi (Microsoft)Alex Deng (Microsoft)Roger Longbotham (Microsoft)Ya Xu (LinkedIn),"73615348,2172042952,2067073079,2310280520","Web site owners, from small web sites to the largest properties that include Amazon, Facebook, Google, LinkedIn, Microsoft, and Yahoo, attempt to improve their web sites, optimizing for criteria ranging from repeat usage, time on site, to revenue. Having been involved in running thousands of controlled experiments at Amazon, Booking.com, LinkedIn, and multiple Microsoft properties, we share seven rules of thumb for experimenters, which we have generalized from these experiments and their results. These are principles that we believe have broad applicability in web optimization and analytics outside of controlled experiments, yet they are not provably correct, and in some cases exceptions are known. To support these rules of thumb, we share multiple real examples, most being shared in a public paper for the first time. Some rules of thumb have previously been stated, such as 'speed matters,' but we describe the assumptions in the experimental design and share additional experiments that improved our understanding of where speed matters more: certain areas of the web page are more critical. This paper serves two goals. First, it can guide experimenters with rules of thumb that can help them optimize their sites. Second, it provides the KDD community with new research challenges on the applicability, exceptions, and extensions to these, one of the goals for KDD's industrial track.",2014,Knowledge Discovery and Data Mining,Fields of study: randomized experimentworld wide webdata miningmachine learningsimulationstatistics
Balanced graph edge partition,Florian Bourse (École Normale Supérieure)Marc Lelarge (French Institute for Research in Computer Science and Automation)Milan Vojnovic (Microsoft),"80044717,2141456255,1992645465","Balanced edge partition has emerged as a new approach to partition an input graph data for the purpose of scaling out parallel computations, which is of interest for several modern data analytics computation platforms, including platforms for iterative computations, machine learning problems, and graph databases. This new approach stands in a stark contrast to the traditional approach of balanced vertex partition, where for given number of partitions, the problem is to minimize the number of edges cut subject to balancing the vertex cardinality of partitions. In this paper, we first characterize the expected costs of vertex and edge partitions with and without aggregation of messages, for the commonly deployed policy of placing a vertex or an edge uniformly at random to one of the partitions. We then obtain the first approximation algorithms for the balanced edge-partition problem which for the case of no aggregation matches the best known approximation ratio for the balanced vertex-partition problem, and show that this remains to hold for the case with aggregation up to factor that is equal to the maximum in-degree of a vertex. We report results of an extensive empirical evaluation on a set of real-world graphs, which quantifies the benefits of edge- vs. vertex-partition, and demonstrates efficiency of natural greedy online assignments for the balanced edge-partition problem with and with no aggregation.",2014,Knowledge Discovery and Data Mining,Fields of study: edge spacefrequency partition of a graphedge coveredge contractionfeedback vertex setvertex coververtexloopcutindependent setgraph partitionapproximation algorithmdiscrete mathematicscombinatoricsmathematical optimizationcomputer sciencemathematics
Unfolding physiological state: mortality modelling in intensive care units,Marzyeh Ghassemi (Massachusetts Institute of Technology)Tristan Naumann (Massachusetts Institute of Technology)Finale Doshi-Velez (Harvard University)Nicole Brimmer (Massachusetts Institute of Technology)Rohit Joshi (Massachusetts Institute of Technology)Anna Rumshisky (University of Massachusetts Lowell)Peter Szolovits (Massachusetts Institute of Technology),"2074001427,2134067980,2426295800,2229173847,2426526846,2324027522,29903417","Accurate knowledge of a patient's disease state and trajectory is critical in a clinical setting. Modern electronic healthcare records contain an increasingly large amount of data, and the ability to automatically identify the factors that influence patient outcomes stand to greatly improve the efficiency and quality of care. We examined the use of latent variable models (viz. Latent Dirichlet Allocation) to decompose free-text hospital notes into meaningful features, and the predictive power of these features for patient mortality. We considered three prediction regimes: (1) baseline prediction, (2) dynamic (time-varying) outcome prediction, and (3) retrospective outcome prediction. In each, our prediction task differs from the familiar time-varying situation whereby data accumulates; since fewer patients have long ICU stays, as we move forward in time fewer patients are available and the prediction task becomes increasingly difficult. We found that latent topic-derived features were effective in determining patient mortality under three timelines: in-hospital, 30 day post-discharge, and 1 year post-discharge mortality. Our results demonstrated that the latent topic features important in predicting hospital mortality are very different from those that are important in post-discharge mortality. In general, latent topic features were more predictive than structured features, and a combination of the two performed best. The time-varying models that combined latent topic features and baseline features had AUCs that reached 0.85, 0.80, and 0.77 for in-hospital, 30 day post-discharge and 1 year post-discharge mortality respectively. Our results agreed with other work suggesting that the first 24 hours of patient information are often the most predictive of hospital mortality. Retrospective models that used a combination of latent topic features and structured features achieved AUCs of 0.96, 0.82, and 0.81 for in-hospital, 30 day, and 1-year mortality prediction. Our work focuses on the dynamic (time-varying) setting because models from this regime could facilitate an on-going severity stratification system that helps direct care-staff resources and inform treatment strategies.",2014,Knowledge Discovery and Data Mining,Fields of study: support vector machinedata sciencedata miningmachine learningsimulationcomputer science
Non-parametric scan statistics for event detection and forecasting in heterogeneous social media graphs,"Feng Chen (University at Albany, SUNY)Daniel B. Neill (Carnegie Mellon University)","2601749234,2115216468","Event detection in social media is an important but challenging problem. Most existing approaches are based on burst detection, topic modeling, or clustering techniques, which cannot naturally model the implicit heterogeneous network structure in social media. As a result, only limited information, such as terms and geographic locations, can be used. This paper presents Non-Parametric Heterogeneous Graph Scan (NPHGS), a new approach that considers the entire heterogeneous network for event detection: we first model the network as a ""sensor"" network, in which each node senses its ""neighborhood environment"" and reports an empirical p-value measuring its current level of anomalousness for each time interval (e.g., hour or day). Then, we efficiently maximize a nonparametric scan statistic over connected subgraphs to identify the most anomalous network clusters. Finally, the event represented by each cluster is summarized with information such as type of event, geographical locations, time, and participants. As a case study, we consider two applications using Twitter data, civil unrest event detection and rare disease outbreak detection, and present empirical evaluations illustrating the effectiveness and efficiency of our proposed approach.",2014,Knowledge Discovery and Data Mining,Fields of study: social mediadata miningmachine learningstatisticscomputer science
Modeling human location data with mixtures of kernel densities,"Moshe Lichman (University of California, Irvine)Padhraic Smyth (University of California, Irvine)","2225533370,2137074633","Location-based data is increasingly prevalent with the rapid increase and adoption of mobile devices. In this paper we address the problem of learning spatial density models, focusing specifically on individual-level data. Modeling and predicting a spatial distribution for an individual is a challenging problem given both (a) the typical sparsity of data at the individual level and (b) the heterogeneity of spatial mobility patterns across individuals. We investigate the application of kernel density estimation (KDE) to this problem using a mixture model approach that can interpolate between an individual's data and broader patterns in the population as a whole. The mixture-KDE approach is evaluated on two large geolocation/check-in data sets, from Twitter and Gowalla, with comparisons to non-KDE baselines, using both log-likelihood and detection of simulated identity theft as evaluation metrics. Our experimental results indicate that the mixture-KDE method provides a useful and accurate methodology for capturing and predicting individual-level spatial patterns in the presence of noisy and sparse data.",2014,Knowledge Discovery and Data Mining,Fields of study: social mediakernel density estimationuser modelingprobabilistic methoddata miningpattern recognitionmachine learningstatisticscomputer science
ClusCite: effective citation recommendation by information network-based clustering,Xiang Ren (University of Illinois at Urbana–Champaign)Jialu Liu (University of Illinois at Urbana–Champaign)Xiao Yu (University of Illinois at Urbana–Champaign)Urvashi Khandelwal (University of Illinois at Urbana–Champaign)Quanquan Gu (University of Illinois at Urbana–Champaign)Lidan Wang (University of Illinois at Urbana–Champaign)Jiawei Han (University of Illinois at Urbana–Champaign),"2129405715,2095732305,2160715520,2223860396,2167348148,2710695387,2121939561","Citation recommendation is an interesting but challenging research problem. Most existing studies assume that all papers adopt the same criterion and follow the same behavioral pattern in deciding relevance and authority of a paper. However, in reality, papers have distinct citation behavioral patterns when looking for different references, depending on paper content, authors and target venues. In this study, we investigate the problem in the context of heterogeneous bibliographic networks and propose a novel cluster-based citation recommendation framework, called ClusCite, which explores the principle that citations tend to be softly clustered into interest groups based on multiple types of relationships in the network. Therefore, we predict each query's citations based on related interest groups, each having its own model for paper authority and relevance. Specifically, we learn group memberships for objects and the significance of relevance features for each interest group, while also propagating relative authority between objects, by solving a joint optimization problem. Experiments on both DBLP and PubMed datasets demonstrate the power of the proposed approach, with 17.68% improvement in Recall@50 and 9.57% growth in MRR over the best performing baseline.",2014,Knowledge Discovery and Data Mining,Fields of study: cluster analysisworld wide webinformation retrievaldata miningmachine learningcomputer science
Streaming submodular maximization: massive data summarization on the fly,Ashwinkumar Badanidiyuru (Cornell University)Baharan Mirzasoleiman (ETH Zurich)Amin Karbasi (ETH Zurich)Andreas Krause (ETH Zurich),"343171996,2508590060,2110010733,2165121431","How can one summarize a massive data set ""on the fly"", i.e., without even having seen it in its entirety? In this paper, we address the problem of extracting representative elements from a large stream of data. I.e., we would like to select a subset of say k data points from the stream that are most representative according to some objective function. Many natural notions of ""representativeness"" satisfy submodularity, an intuitive notion of diminishing returns. Thus, such problems can be reduced to maximizing a submodular set function subject to a cardinality constraint. Classical approaches to submodular maximization require full access to the data set. We develop the first efficient streaming algorithm with constant factor 1/2-e approximation guarantee to the optimum solution, requiring only a single pass through the data, and memory independent of data size. In our experiments, we extensively evaluate the effectiveness of our approach on several applications, including training large-scale kernel methods and exemplar-based clustering, on millions of data points. We observe that our streaming method, while achieving practically the same utility value, runs about 100 times faster than previous work.",2014,Knowledge Discovery and Data Mining,Fields of study: submodular set functionstreaming algorithmdata miningmachine learningmathematical optimizationcomputer sciencemathematics
CatchSync: catching synchronized behavior in large directed graphs,Meng Jiang (Tsinghua University)Peng Cui (Tsinghua University)Alex Beutel (Carnegie Mellon University)Christos Faloutsos (Carnegie Mellon University)Shiqiang Yang (Tsinghua University),"2115305989,2113115369,2045447989,2198983026,2127183023","Given a directed graph of millions of nodes, how can we automatically spot anomalous, suspicious nodes, judging only from their connectivity patterns? Suspicious graph patterns show up in many applications, from Twitter users who buy fake followers, manipulating the social network, to botnet members performing distributed denial of service attacks, disturbing the network traffic graph. We propose a fast and effective method, CatchSync, which exploits two of the tell-tale signs left in graphs by fraudsters: (a) synchronized behavior: suspicious nodes have extremely similar behavior pattern, because they are often required to perform some task together (such as follow the same user); and (b) rare behavior: their connectivity patterns are very different from the majority. We introduce novel measures to quantify both concepts (""synchronicity"" and ""normality"") and we propose a parameter-free algorithm that works on the resulting synchronicity-normality plots. Thanks to careful design, CatchSync has the following desirable properties: (a) it is scalable to large datasets, being linear on the graph size; (b) it is parameter free; and (c) it is side-information-oblivious: it can operate using only the topology, without needing labeled data, nor timing information, etc., while still capable of using side information, if available. We applied CatchSync on two large, real datasets 1-billion-edge Twitter social graph and 3-billion-edge Tencent Weibo social graph, and several synthetic ones; CatchSync consistently outperforms existing competitors, both in detection accuracy by 36% on Twitter and 20% on Tencent Weibo, as well as in speed.",2014,Knowledge Discovery and Data Mining,Fields of study: anomaly detectiondistributed computingcomputer securitydata miningmachine learningcomputer science
SigniTrend: scalable detection of emerging topics in textual streams by hashed significance thresholds,Erich Schubert (Ludwig Maximilian University of Munich)Michael Weiler (Ludwig Maximilian University of Munich)Hans-Peter Kriegel (Ludwig Maximilian University of Munich),"2011689237,2570757024,1919135125","Social media such as Twitter or weblogs are a popular source for live textual data. Much of this popularity is due to the fast rate at which this data arrives, and there are a number of global events - such as the Arab Spring - where Twitter is reported to have had a major influence. However, existing methods for emerging topic detection are often only able to detect events of a global magnitude such as natural disasters or celebrity deaths, and can monitor user-selected keywords or operate on a curated set of hashtags only. Interesting emerging topics may, however, be of much smaller magnitude and may involve the combination of two or more words that themselves are not unusually hot at that time. Our contributions to the detection of emerging trends are three-fold first of all, we propose a significance measure that can be used to detect emerging topics early, long before they become ""hot tags"", by drawing upon experience from outlier detection. Secondly, by using hash tables in a heavy-hitters type algorithm for establishing a noise baseline, we show how to track even all keyword pairs using only a fixed amount of memory. Finally, we aggregate the detected co-trends into larger topics using clustering approaches, as often as a single event will cause multiple word combinations to trend at the same time.",2014,Knowledge Discovery and Data Mining,Fields of study: hash functiontext miningdata scienceworld wide webdata miningmachine learningcomputer science
Guilt by association: large scale malware detection by mining file-relation graphs,Acar Tamersoy (Georgia Institute of Technology)Kevin A. Roundy (Symantec)Duen Horng Chau (Georgia Institute of Technology),"187710220,2260592415,2024561599","The increasing sophistication of malicious software calls for new defensive techniques that are harder to evade, and are capable of protecting users against novel threats. We present AESOP, a scalable algorithm that identifies malicious executable files by applying Aesop's moral that ""a man is known by the company he keeps."" We use a large dataset voluntarily contributed by the members of Norton Community Watch, consisting of partial lists of the files that exist on their machines, to identify close relationships between files that often appear together on machines. AESOP leverages locality-sensitive hashing to measure the strength of these inter-file relationships to construct a graph, on which it performs large scale inference by propagating information from the labeled files (as benign or malicious) to the preponderance of unlabeled files. AESOP attained early labeling of 99% of benign files and 79% of malicious files, over a week before they are labeled by the state-of-the-art techniques, with a 0.9961 true positive rate at flagging malware, at 0.0001 false positive rate.",2014,Knowledge Discovery and Data Mining,Fields of study: locality sensitive hashingbelief propagationworld wide webcomputer securitydata miningmachine learningcomputer science
From micro to macro: data driven phenotyping by densification of longitudinal electronic medical records,Jiayu Zhou (Arizona State University)Fei Wang (IBM)Jianying Hu (IBM)Jieping Ye (Arizona State University),"2097879502,2465953593,2098665416,2305258894","Inferring phenotypic patterns from population-scale clinical data is a core computational task in the development of personalized medicine. One important source of data on which to conduct this type of research is patient Electronic Medical Records (EMR). However, the patient EMRs are typically sparse and noisy, which creates significant challenges if we use them directly to represent patient phenotypes. In this paper, we propose a data driven phenotyping framework called Pacifier (PAtient reCord densIFIER), where we interpret the longitudinal EMR data of each patient as a sparse matrix with a feature dimension and a time dimension, and derive more robust patient phenotypes by exploring the latent structure of those matrices. Specifically, we assume that each derived phenotype is composed of a subset of the medical features contained in original patient EMR, whose value evolves smoothly over time. We propose two formulations to achieve such goal. One is Individual Basis Approach (IBA), which assumes the phenotypes are different for every patient. The other is Shared Basis Approach (SBA), which assumes the patient population shares a common set of phenotypes. We develop an efficient optimization algorithm that is capable of resolving both problems efficiently. Finally we validate Pacifier on two real world EMR cohorts for the tasks of early prediction of Congestive Heart Failure (CHF) and End Stage Renal Disease (ESRD). Our results show that the predictive performance in both tasks can be improved significantly by the proposed algorithms (average AUC score improved from 0.689 to 0.816 on CHF, and from 0.756 to 0.838 on ESRD respectively, on diagnosis group granularity). We also illustrate some interesting phenotypes derived from our data.",2014,Knowledge Discovery and Data Mining,Fields of study: urban densityphenotypehealth informaticsdata sciencebioinformaticsdata miningcomputer science
Active semi-supervised learning using sampling theory for graph signals,Akshay Gadde (University of Southern California)Aamir Anis (University of Southern California)Antonio Ortega (University of Southern California),"1943879374,2146888950,2118429164","We consider the problem of offline, pool-based active semi-supervised learning on graphs. This problem is important when the labeled data is scarce and expensive whereas unlabeled data is easily available. The data points are represented by the vertices of an undirected graph with the similarity between them captured by the edge weights. Given a target number of nodes to label, the goal is to choose those nodes that are most informative and then predict the unknown labels. We propose a novel framework for this problem based on our recent results on sampling theory for graph signals. A graph signal is a real-valued function defined on each node of the graph. A notion of frequency for such signals can be defined using the spectrum of the graph Laplacian matrix. The sampling theory for graph signals aims to extend the traditional Nyquist-Shannon sampling theory by allowing us to identify the class of graph signals that can be reconstructed from their values on a subset of vertices. This approach allows us to define a criterion for active learning based on sampling set selection which aims at maximizing the frequency of the signals that can be reconstructed from their samples on the set. Experiments show the effectiveness of our method.",2014,Knowledge Discovery and Data Mining,Fields of study: strength of a graphvoltage graphcomplement graphgraph bandwidthgraph powerlevel structurenull graphgraph labelingclique widthgraph propertyrandom geometric graphgraphdirected graphcombinatoricspattern recognitionmachine learningstatisticsmathematics
Graph sample and hold: a framework for big-graph analytics,Nesreen K. Ahmed (Purdue University)Nick G. Duffield (Rutgers University)Jennifer Neville (Purdue University)Ramana Rao Kompella (Purdue University),"2119838086,2017920553,2124572662,229044414","Sampling is a standard approach in big-graph analytics; the goal is to efficiently estimate the graph properties by consulting a sample of the whole population. A perfect sample is assumed to mirror every property of the whole population. Unfortunately, such a perfect sample is hard to collect in complex populations such as graphs (e.g. web graphs, social networks), where an underlying network connects the units of the population. Therefore, a good sample will be representative in the sense that graph properties of interest can be estimated with a known degree of accuracy. While previous work focused particularly on sampling schemes to estimate certain graph properties (e.g. triangle count), much less is known for the case when we need to estimate various graph properties with the same sampling scheme. In this paper, we pro- pose a generic stream sampling framework for big-graph analytics, called Graph Sample and Hold (gSH), which samples from massive graphs sequentially in a single pass, one edge at a time, while maintaining a small state in memory. We use a Horvitz-Thompson construction in conjunction with a scheme that samples arriving edges without adjacencies to previously sampled edges with probability p and holds edges with adjacencies with probability q . Our sample and hold framework facilitates the accurate estimation of subgraph patterns by enabling the dependence of the sampling process to vary based on previous history. Within our framework, we show how to produce statistically unbiased estimators for various graph properties from the sample. Given that the graph analytics will run on a sample instead of the whole population, the runtime complexity is kept under control. Moreover, given that the estimators are unbiased, the approximation error is also kept under control. Finally, we test the performance of the proposed framework (gSH) on various types of graphs, showing that from a sample with -- 40K edges, it produces estimates with relative errors",2014,Knowledge Discovery and Data Mining,Fields of study: strength of a graphgraph bandwidthnull modelcombinatoricsmachine learningstatisticsmathematics
People on drugs: credibility of user statements in health communities,Subhabrata Mukherjee (Max Planck Society)Gerhard Weikum (Max Planck Society)Cristian Danescu-Niculescu-Mizil (Max Planck Society),"2301124665,514836396,160157097","Online health communities are a valuable source of information for patients and physicians. However, such user-generated resources are often plagued by inaccuracies and misinformation. In this work we propose a method for automatically establishing the credibility of user-generated medical statements and the trustworthiness of their authors by exploiting linguistic cues and distant supervision from expert sources. To this end we introduce a probabilistic graphical model that jointly learns user trustworthiness, statement credibility, and language objectivity. We apply this methodology to the task of extracting rare or unknown side-effects of medical drugs --- this being one of the problems where large scale non-expert data has the potential to complement expert medical knowledge. We show that our method can reliably extract side-effects and filter out false statements, while identifying trustworthy users that are likely to contribute valuable medical information.",2014,Knowledge Discovery and Data Mining,Fields of study: objectivitytrustworthinessgraphical modelworld wide webdata miningmachine learningcomputer science
Large-scale high-precision topic modeling on twitter,Shuang-Hong Yang (Twitter)Alek Kolcz (Twitter)Andy Schlaikjer (Twitter)Pankaj Gupta (Twitter),"2721211097,2223511781,2338891889,2303536639","We are interested in organizing a continuous stream of sparse and noisy texts, known as ""tweets"", in real time into an ontology of hundreds of topics with measurable and stringently high precision. This inference is performed over a full-scale stream of Twitter data, whose statistical distribution evolves rapidly over time. The implementation in an industrial setting with the potential of affecting and being visible to real users made it necessary to overcome a host of practical challenges. We present a spectrum of topic modeling techniques that contribute to a deployed system. These include non-topical tweet detection, automatic labeled data acquisition, evaluation with human computation, diagnostic and corrective learning and, most importantly, high-precision topic inference. The latter represents a novel two-stage training algorithm for tweet text classification and a close-loop inference mechanism for combining texts with additional sources of information. The resulting system achieves 93% precision at substantial overall coverage.",2014,Knowledge Discovery and Data Mining,Fields of study: topic modelsocial mediadata sciencedata miningmachine learningcomputer science
Box drawings for learning with imbalanced data,Siong Thye Goh (Massachusetts Institute of Technology)Cynthia Rudin (Massachusetts Institute of Technology),"2156533279,2141705163","The vast majority of real world classification problems are imbalanced, meaning there are far fewer data from the class of interest (the positive class) than from other classes. We propose two machine learning algorithms to handle highly imbalanced classification problems. The classifiers are disjunctions of conjunctions, and are created as unions of parallel axis rectangles around the positive examples, and thus have the benefit of being interpretable. The first algorithm uses mixed integer programming to optimize a weighted balance between positive and negative class accuracies. Regularization is introduced to improve generalization performance. The second method uses an approximation in order to assist with scalability. Specifically, it follows a \textit{characterize then discriminate} approach, where the positive class is characterized first by boxes, and then each box boundary becomes a separate discriminative classifier. This method has the computational advantages that it can be easily parallelized, and considers only the relevant regions of feature space.",2014,Knowledge Discovery and Data Mining,Fields of study: decision treebiological classificationdata miningpattern recognitionmachine learningcomputer sciencemathematics
Exploiting geographic dependencies for real estate appraisal: a mutual perspective of ranking and clustering,Yanjie Fu (Rutgers–Newark)Hui Xiong (Rutgers–Newark)Yong Ge (University of North Carolina at Charlotte)Zijun Yao (Rutgers–Newark)Yu Zheng (Microsoft)Zhi-Hua Zhou (Nanjing University),"2168873515,2153710278,2218492437,2229271911,2145115012,2286237009","It is traditionally a challenge for home buyers to understand, compare and contrast the investment values of real estates. While a number of estate appraisal methods have been developed to value real property, the performances of these methods have been limited by the traditional data sources for estate appraisal. However, with the development of new ways of collecting estate-related mobile data, there is a potential to leverage geographic dependencies of estates for enhancing estate appraisal. Indeed, the geographic dependencies of the value of an estate can be from the characteristics of its own neighborhood (individual), the values of its nearby estates (peer), and the prosperity of the affiliated latent business area (zone). To this end, in this paper, we propose a geographic method, named ClusRanking, for estate appraisal by leveraging the mutual enforcement of ranking and clustering power. ClusRanking is able to exploit geographic individual, peer, and zone dependencies in a probabilistic ranking model. Specifically, we first extract the geographic utility of estates from geography data, estimate the neighborhood popularity of estates by mining taxicab trajectory data, and model the influence of latent business areas via ClusRanking. Also, we use a linear model to fuse these three influential factors and predict estate investment values. Moreover, we simultaneously consider individual, peer and zone dependencies, and derive an estate-specific ranking likelihood as the objective function. Finally, we conduct a comprehensive evaluation with real-world estate related data, and the experimental results demonstrate the effectiveness of our method.",2014,Knowledge Discovery and Data Mining,Fields of study: cost approachdata mining
Unsupervised learning of disease progression models,Xiang Wang (IBM)David Sontag (New York University)Fei Wang (IBM),"2099725100,2138531792,2465953593","Chronic diseases, such as Alzheimer's Disease, Diabetes, and Chronic Obstructive Pulmonary Disease, usually progress slowly over a long period of time, causing increasing burden to the patients, their families, and the healthcare system. A better understanding of their progression is instrumental in early diagnosis and personalized care. Modeling disease progression based on real-world evidence is a very challenging task due to the incompleteness and irregularity of the observations, as well as the heterogeneity of the patient conditions. In this paper, we propose a probabilistic disease progression model that address these challenges. As compared to existing disease progression models, the advantage of our model is three-fold: 1) it learns a continuous-time progression model from discrete-time observations with non-equal intervals; 2) it learns the full progression trajectory from a set of incomplete records that only cover short segments of the progression; 3) it learns a compact set of medical concepts as the bridge between the hidden progression process and the observed medical evidence, which are usually extremely sparse and noisy. We demonstrate the capabilities of our model by applying it to a real-world COPD patient cohort and deriving some interesting clinical insights.",2014,Knowledge Discovery and Data Mining,Fields of study: bayesian networkhealth informaticsdata sciencedata miningmachine learningsimulationcomputer science
Who to follow and why: link prediction with explanations,Nicola Barbieri (Yahoo!)Francesco Bonchi (Yahoo!)Giuseppe Manco (Indian Council of Agricultural Research),"2155070167,2176652147,2093732677","User recommender systems are a key component in any on-line social networking platform: they help the users growing their network faster, thus driving engagement and loyalty. In this paper we study link prediction with explanations for user recommendation in social networks. For this problem we propose WTFW (""Who to Follow and Why""), a stochastic topic model for link prediction over directed and nodes-attributed graphs. Our model not only predicts links, but for each predicted link it decides whether it is a ""topical"" or a ""social"" link, and depending on this decision it produces a different type of explanation. A topical link is recommended between a user interested in a topic and a user authoritative in that topic: the explanation in this case is a set of binary features describing the topic responsible of the link creation. A social link is recommended between users which share a large social neighborhood: in this case the explanation is the set of neighbors which are more likely to be responsible for the link creation. Our experimental assessment on real-world data confirms the accuracy of WTFW in the link prediction and the quality of the associated explanations.",2014,Knowledge Discovery and Data Mining,Fields of study: social networkdata miningartificial intelligencemachine learningcomputer science
Prediction of human emergency behavior and their mobility following large-scale disaster,Xuan Song (University of Tokyo)Quanshi Zhang (University of Tokyo)Yoshihide Sekimoto (University of Tokyo)Ryosuke Shibasaki (University of Tokyo),"2127257590,2162175003,2086893366,291396532","The frequency and intensity of natural disasters has significantly increased over the past decades and this trend is predicted to continue. Facing these possible and unexpected disasters, accurately predicting human emergency behavior and their mobility will become the critical issue for planning effective humanitarian relief, disaster management, and long-term societal reconstruction. In this paper, we build up a large human mobility database (GPS records of 1.6 million users over one year) and several different datasets to capture and analyze human emergency behavior and their mobility following the Great East Japan Earthquake and Fukushima nuclear accident. Based on our empirical analysis through these data, we find that human behavior and their mobility following large-scale disaster sometimes correlate with their mobility patterns during normal times, and are also highly impacted by their social relationship, intensity of disaster, damage level, government appointed shelters, news reporting, large population flow and etc. On the basis of these findings, we develop a model of human behavior that takes into account these factors for accurately predicting human emergency behavior and their mobility following large-scale disaster. The experimental results and validations demonstrate the efficiency of our behavior model, and suggest that human behavior and their movements during disasters may be significantly more predictable than previously thought.",2014,Knowledge Discovery and Data Mining,Fields of study: operations researchcomputer securitysimulation
Community membership identification from small seed sets,Isabel M. Kloumann (Cornell University)Jon M. Kleinberg (Cornell University),"2483265514,2261367123","In many applications we have a social network of people and would like to identify the members of an interesting but unlabeled group or community. We start with a small number of exemplar group members -- they may be followers of a political ideology or fans of a music genre -- and need to use those examples to discover the additional members. This problem gives rise to the seed expansion problem in community detection: given example community members, how can the social graph be used to predict the identities of remaining, hidden community members? In contrast with global community detection (graph partitioning or covering), seed expansion is best suited for identifying communities locally concentrated around nodes of interest. A growing body of work has used seed expansion as a scalable means of detecting overlapping communities. Yet despite growing interest in seed expansion, there are divergent approaches in the literature and there still isn't a systematic understanding of which approaches work best in different domains. Here we evaluate several variants and uncover subtle trade-offs between different approaches. We explore which properties of the seed set can improve performance, focusing on heuristics that one can control in practice. As a consequence of this systematic understanding we have found several opportunities for performance gains. We also consider an adaptive version in which requests are made for additional membership labels of particular nodes, such as one finds in field studies of social communities. This leads to interesting connections and contrasts with active learning and the trade-offs of exploration and exploitation. Finally, we explore topological properties of communities and seed sets that correlate with algorithm performance, and explain these empirical observations with theoretical ones. We evaluate our methods across multiple domains, using publicly available datasets with labeled, ground-truth communities.",2014,Knowledge Discovery and Data Mining,Fields of study: data miningmachine learningcomputer science
FUNNEL: automatic mining of spatially coevolving epidemics,Yasuko Matsubara (Kumamoto University)Yasushi Sakurai (Kumamoto University)Willem G. van Panhuis (University of Pittsburgh)Christos Faloutsos (Carnegie Mellon University),"2106416586,1989929707,1272417158,2198983026","Given a large collection of epidemiological data consisting of the count of d contagious diseases for l locations of duration n, how can we find patterns, rules and outliers? For example, the Project Tycho provides open access to the count infections for U.S. states from 1888 to 2013, for 56 contagious diseases (e.g., measles, influenza), which include missing values, possible recording errors, sudden spikes (or dives) of infections, etc. So how can we find a combined model, for all these diseases, locations, and time-ticks? In this paper, we present FUNNEL, a unifying analytical model for large scale epidemiological data, as well as a novel fitting algorithm, FUNNELFIT, which solves the above problem. Our method has the following properties: (a) Sense-making: it detects important patterns of epidemics, such as periodicities, the appearance of vaccines, external shock events, and more; (b) Parameter-free: our modeling framework frees the user from providing parameter values; (c) Scalable: FUNNELFIT is carefully designed to be linear on the input size; (d) General: our model is general and practical, which can be applied to various types of epidemics, including computer-virus propagation, as well as human diseases. Extensive experiments on real data demonstrate that FUNNELFIT does indeed discover important properties of epidemics: (P1) disease seasonality, e.g., influenza spikes in January, Lyme disease spikes in July and the absence of yearly periodicity for gonorrhea; (P2) disease reduction effect, e.g., the appearance of vaccines; (P3) local/state-level sensitivity, e.g., many measles cases in NY; (P4) external shock events, e.g., historical flu pandemics; (P5) detect incongruous values, i.e., data reporting errors.",2014,Knowledge Discovery and Data Mining,Fields of study: time seriesdata miningsimulationstatistics
Heat kernel based community detection,Kyle Kloster (Purdue University)David F. Gleich (Purdue University),"2045755628,2148810670","The heat kernel is a type of graph diffusion that, like the much-used personalized PageRank diffusion, is useful in identifying a community nearby a starting seed node. We present the first deterministic, local algorithm to compute this diffusion and use that algorithm to study the communities that it produces. Our algorithm is formally a relaxation method for solving a linear system to estimate the matrix exponential in a degree-weighted norm. We prove that this algorithm stays localized in a large graph and has a worst-case constant runtime that depends only on the parameters of the diffusion, not the size of the graph. On large graphs, our experiments indicate that the communities produced by this method have better conductance than those produced by PageRank, although they take slightly longer to compute. On a real-world community identification task, the heat kernel communities perform better than those from the PageRank diffusion.",2014,Knowledge Discovery and Data Mining,Fields of study: variable kernel density estimationheat kernelcombinatoricsmachine learningmathematical optimizationcomputer sciencemathematics
Knock it off: profiling the online storefronts of counterfeit merchandise,"Matthew F. Der (University of California, San Diego)Lawrence K. Saul (University of California, San Diego)Stefan Savage (University of California, San Diego)Geoffrey M. Voelker (University of California, San Diego)","2140611830,2143047568,2171030022,241629895","We describe an automated system for the large-scale monitoring of Web sites that serve as online storefronts for spam-advertised goods. Our system is developed from an extensive crawl of black-market Web sites that deal in illegal pharmaceuticals, replica luxury goods, and counterfeit software. The operational goal of the system is to identify the affiliate programs of online merchants behind these Web sites; the system itself is part of a larger effort to improve the tracking and targeting of these affiliate programs. There are two main challenges in this domain. The first is that appearances can be deceiving: Web pages that render very differently are often linked to the same affiliate program of merchants. The second is the difficulty of acquiring training data: the manual labeling of Web pages, though necessary to some degree, is a laborious and time-consuming process. Our approach in this paper is to extract features that reveal when Web pages linked to the same affiliate program share a similar underlying structure. Using these features, which are mined from a small initial seed of labeled data, we are able to profile the Web sites of forty-four distinct affiliate programs that account, collectively, for hundreds of millions of dollars in illicit e-commerce. Our work also highlights several broad challenges that arise in the large-scale, empirical study of malicious activity on the Web.",2014,Knowledge Discovery and Data Mining,Fields of study: web developmentinternet privacyworld wide webdata miningcomputer science
COM: a generative model for group recommendation,Quan Yuan (Nanyang Technological University)Gao Cong (Nanyang Technological University)Chin-Yew Lin (Microsoft),"2163879794,2295915604,2159460278","With the rapid development of online social networks, a growing number of people are willing to share their group activities, e.g. having dinners with colleagues, and watching movies with spouses. This motivates the studies on group recommendation, which aims to recommend items for a group of users. Group recommendation is a challenging problem because different group members have different preferences, and how to make a trade-off among their preferences for recommendation is still an open problem. In this paper, we propose a probabilistic model named COM (COnsensus Model) to model the generative process of group activities, and make group recommendations. Intuitively, users in a group may have different influences, and those who are expert in topics relevant to the group are usually more influential. In addition, users in a group may behave differently as group members from as individuals. COM is designed based on these intuitions, and is able to incorporate both users' selection history and personal considerations of content factors. When making recommendations, COM estimates the preference of a group to an item by aggregating the preferences of the group members with different weights. We conduct extensive experiments on four datasets, and the results show that the proposed model is effective in making group recommendations, and outperforms baseline methods significantly.",2014,Knowledge Discovery and Data Mining,Fields of study: topic modelgroup decision makingcollaborative filteringknowledge managementworld wide webdata miningmachine learningcomputer science
Predicting student risks through longitudinal analysis,Ashay Tamhane (IBM)Shajith Ikbal (IBM)Bikram Sengupta (IBM)Mayuri Duggirala (Tata Research Development and Design Centre)James Appleton,"2067374820,2038118144,2113952558,2223488594,2661232338","Poor academic performance in K-12 is often a precursor to unsatisfactory educational outcomes such as dropout, which are associated with significant personal and social costs. Hence, it is important to be able to predict students at risk of poor performance, so that the right personalized intervention plans can be initiated. In this paper, we report on a large-scale study to identify students at risk of not meeting acceptable levels of performance in one state-level and one national standardized assessment in Grade 8 of a major US school district. An important highlight of our study is its scale - both in terms of the number of students included, the number of years and the number of features, which provide a very solid grounding to the research. We report on our experience with handling the scale and complexity of data, and on the relative performance of various machine learning techniques we used for building predictive models. Our results demonstrate that it is possible to predict students at-risk of poor assessment performance with a high degree of accuracy, and to do so well in advance. These insights can be used to pro-actively initiate personalized intervention programs and improve the chances of student success.",2014,Knowledge Discovery and Data Mining,Fields of study: riskpredictionanalysisdata sciencedata miningsimulationstatistics
Focused clustering and outlier detection in large attributed graphs,Bryan Perozzi (Stony Brook University)Leman Akoglu (Stony Brook University)Patricia Iglesias Sánchez (Karlsruhe Institute of Technology)Emmanuel Müller (Karlsruhe Institute of Technology),"1983756286,2288278917,2229277874,2112493600","Graph clustering and graph outlier detection have been studied extensively on plain graphs, with various applications. Recently, algorithms have been extended to graphs with attributes as often observed in the real-world. However, all of these techniques fail to incorporate the user preference into graph mining, and thus, lack the ability to steer algorithms to more interesting parts of the attributed graph. In this work, we overcome this limitation and introduce a novel user-oriented approach for mining attributed graphs. The key aspect of our approach is to infer user preference by the so-called focus attributes through a set of user-provided exemplar nodes. In this new problem setting, clusters and outliers are then simultaneously mined according to this user preference. Specifically, our FocusCO algorithm identifies the focus, extracts focused clusters and detects outliers. Moreover, FocusCO scales well with graph size, since we perform a local clustering of interest to the user rather than global partitioning of the entire graph. We show the effectiveness and scalability of our method on synthetic and real-world graphs, as compared to both existing graph clustering and outlier detection approaches.",2014,Knowledge Discovery and Data Mining,Fields of study: null modelcluster analysisdata miningpattern recognitionmachine learningcomputer sciencemathematics
Stability of influence maximization,Xinran He (University of Southern California)David Kempe (University of Southern California),"2096901250,2137221145","The present article serves as an erratum to our paper of the same title, which was presented and published in the KDD 2014 conference. In that article, we claimed falsely that the objective function defined in Section 1.4 is non-monotone submodular. We are deeply indebted to Debmalya Mandal, Jean Pouget-Abadie and Yaron Singer for bringing to our attention a counter-example to that claim. Subsequent to becoming aware of the counter-example, we have shown that the objective function is in fact NP-hard to approximate to within a factor of O(n 1-e ) for any e > 0. In an attempt to fix the record, the present article combines the problem motivation, models, and experimental results sections from the original incorrect article with the new hardness result. We would like readers to only cite and use this version (which will remain an unpublished note) instead of the incorrect conference version.",2014,Knowledge Discovery and Data Mining,Fields of study: noiseuncertaintyrobust optimizationmathematical economicscombinatoricsartificial intelligencemachine learningstatisticsalgorithmcomputer sciencemathematics
MMRate: inferring multi-aspect diffusion networks with multi-pattern cascades,Senzhang Wang (Beihang University)Xia Hu (Arizona State University)Philip S. Yu (University of Illinois at Chicago)Zhoujun Li (Beihang University),"2108200212,2161448330,2125104194,2133880114","Inferring diffusion networks from traces of cascades has been extensively studied to better understand information diffusion in many domains. A widely used assumption in previous work is that the diffusion network is homogenous and diffusion processes of cascades follow the same pattern. However, in social media, users may have various interests and the connections among them are usually multi-faceted. In addition, different cascades normally diffuse at different speeds and spread to diverse scales, and hence show various diffusion patterns. It is challenging for traditional models to capture the heterogeneous user interactions and diverse patterns of cascades in social media. In this paper, we investigate a novel problem of inferring multi-aspect diffusion networks with multi-pattern cascades. In particular, we study the effects of various diffusion patterns on the information diffusion process by analyzing users' retweeting behavior on a microblogging dataset. By incorporating aspect-level user interactions and various diffusion patterns, a new model for inferring Multi-aspect transmission Rates between users using Multi-pattern cascades (MMRate) is proposed. We also provide an Expectation Maximization algorithm to effectively estimate the parameters. Experimental results on both synthetic and microblogging datasets demonstrate the superior performance of our approach over the state-of-the-art methods in inferring multi-aspect diffusion networks.",2014,Knowledge Discovery and Data Mining,Fields of study: social networkdata miningmachine learningsimulationcomputer science
Automated hypothesis generation based on mining scientific literature,W. Scott Spangler (IBM)Angela D. Wilkins (Baylor College of Medicine)Benjamin J. Bachman (Baylor College of Medicine)Meena Nagarajan (IBM)Tajhal Dayaram (Baylor College of Medicine)Peter J. Haas (IBM)Sam Regenbogen (Baylor College of Medicine)Curtis R. Pickering (University of Texas at Austin)Austin Comer (University of Texas at Austin)Jeffrey N. Myers (University of Texas at Austin)Ioana Stanoi (IBM)Linda Kato (IBM)Ana Lelescu (IBM)Jacques J. Labrie (IBM)Neha Parikh (Baylor College of Medicine)Andreas Martin Lisewski (Baylor College of Medicine)Lawrence A. Donehower (Baylor College of Medicine)Ying Chen (IBM)Olivier Lichtarge (Baylor College of Medicine),"2106251501,1999041300,2164310169,2225273551,2003203090,2118460571,2041939495,1967103034,2096283413,2152867474,1965523549,2156620596,70079055,2114099301,2546617393,1968450065,2268387923,2527554124,256939357","Keeping up with the ever-expanding flow of data and publications is untenable and poses a fundamental bottleneck to scientific progress. Current search technologies typically find many relevant documents, but they do not extract and organize the information content of these documents or suggest new scientific hypotheses based on this organized content. We present an initial case study on KnIT, a prototype system that mines the information contained in the scientific literature, represents it explicitly in a queriable network, and then further reasons upon these data to generate novel and experimentally testable hypotheses. KnIT combines entity detection with neighbor-text feature analysis and with graph-based diffusion of information to identify potential new properties of entities that are strongly implied by existing relationships. We discuss a successful application of our approach that mines the published literature to identify new protein kinases that phosphorylate the protein tumor suppressor p53. Retrospective analysis demonstrates the accuracy of this approach and ongoing laboratory experiments suggest that kinases identified by our system may indeed phosphorylate p53. These results establish proof of principle for automated hypothesis generation and discovery based on text mining of the scientific literature.",2014,Knowledge Discovery and Data Mining,Fields of study: text miningdata sciencebioinformaticsdata miningmachine learningstatisticscomputer science
Predicting employee expertise for talent management in the enterprise,Kush R. Varshney (IBM)Vijil Chenthamarakshan (IBM)Scott W. Fancher (IBM)Jun Wang (IBM)Dongping Fang (IBM)Aleksandra Mojsilović (IBM),"241780900,2221080166,2225592770,2567112495,2293216060,2116316968","Strategic planning and talent management in large enterprises composed of knowledge workers requires complete, accurate, and up-to-date representation of the expertise of employees in a form that integrates with business processes. Like other similar organizations operating in dynamic environments, the IBM Corporation strives to maintain such current and correct information, specifically assessments of employees against job roles and skill sets from its expertise taxonomy. In this work, we deploy an analytics-driven solution that infers the expertise of employees through the mining of enterprise and social data that is not specifically generated and collected for expertise inference. We consider job role and specialty prediction and pose them as supervised classification problems. We evaluate a large number of feature sets, predictive models and postprocessing algorithms, and choose a combination for deployment. This expertise analytics system has been deployed for key employee population segments, yielding large reductions in manual effort and the ability to continually and consistently serve up-to-date and accurate data for several business functions. This expertise management system is in the process of being deployed throughout the corporation.",2014,Knowledge Discovery and Data Mining,Fields of study: workforce planninghuman resourcesmanagement scienceknowledge managementdata miningmachine learning
Effective global approaches for mutual information based feature selection,Xuan Vinh Nguyen (University of Melbourne)Jeffrey Chan (University of Melbourne)Simone Romano (University of Melbourne)James Bailey (University of Melbourne),"2229342320,2408228308,2165099692,2131557737","Most current mutual information (MI) based feature selection techniques are greedy in nature thus are prone to sub-optimal decisions. Potential performance improvements could be gained by systematically posing MI-based feature selection as a global optimization problem. A rare attempt at providing a global solution for the MI-based feature selection is the recently proposed Quadratic Programming Feature Selection (QPFS) approach. We point out that the QPFS formulation faces several non-trivial issues, in particular, how to properly treat feature `self-redundancy' while ensuring the convexity of the objective function. In this paper, we take a systematic approach to the problem of global MI-based feature selection. We show how the resulting NP-hard global optimization problem could be efficiently approximately solved via spectral relaxation and semi-definite programming techniques. We experimentally demonstrate the efficiency and effectiveness of these novel feature selection frameworks.",2014,Knowledge Discovery and Data Mining,Fields of study: mutual informationfeatureglobal optimizationfeature selectiondata miningpattern recognitionmachine learningmathematical optimizationcomputer sciencemathematics
On social event organization,Keqian Li (University of British Columbia)Wei Lu (University of British Columbia)Smriti Bhagat (Technicolor)Laks V.S. Lakshmanan (University of British Columbia)Cong Yu (Google),"2102298416,2596596230,2291721935,2289816208,2687370976","Online platforms, such as Meetup and Plancast, have recently become popular for planning gatherings and event organization. However, there is a surprising lack of studies on how to effectively and efficiently organize social events for a large group of people through such platforms. In this paper, we study the key computational problem involved in organization of social events, to our best knowledge, for the first time. We propose the Social Event Organization (SEO) problem as one of assigning a set of events for a group of users to attend, where the users are socially connected with each other and have innate levels of interest in those events. As a first step toward Social Event Organization, we introduce a formal definition of a restricted version of the problem and show that it is NP-hard and is hard to approximate. We propose efficient heuristic algorithms that improve upon simple greedy algorithms by incorporating the notion of phantom events and by using look-ahead estimation. Using synthetic datasets and three real datasets including those from the platforms Meetup and Plancast, we experimentally demonstrate that our greedy heuristics are scalable and furthermore outperform the baseline algorithms significantly in terms of achieving superior social welfare.",2014,Knowledge Discovery and Data Mining,Fields of study: social networkdata miningartificial intelligencemachine learningcomputer science
Activity ranking in LinkedIn feed,Deepak Agarwal (LinkedIn)Bee-Chung Chen (LinkedIn)Rupesh Gupta (LinkedIn)Joshua Hartman (LinkedIn)Qi He (LinkedIn)Anand Iyer (LinkedIn)Sumanth Kolar (LinkedIn)Yiming Ma (LinkedIn)Pannagadatta Shivaswamy (LinkedIn)Ajit Singh (LinkedIn)Liang Zhang (LinkedIn),"2116605949,2152441490,2299393997,2521112563,2658691334,2497422388,2343471319,2237551533,2227780436,2498138001,2306956169","Users on an online social network site generate a large number of heterogeneous activities, ranging from connecting with other users, to sharing content, to updating their profiles. The set of activities within a user's network neighborhood forms a stream of updates for the user's consumption. In this paper, we report our experience with the problem of ranking activities in the LinkedIn homepage feed. In particular, we provide a taxonomy of social network activities, describe a system architecture (with a number of key components open-sourced) that supports fast iteration in model development, demonstrate a number of key factors for effective ranking, and report experimental results from extensive online bucket tests.",2014,Knowledge Discovery and Data Mining,Fields of study: relevanceworld wide webdata miningmachine learningcomputer science
Minimizing seed set selection with probabilistic coverage guarantee in a social network,Peng Zhang (Purdue University)Wei Chen (Microsoft)Xiaoming Sun (Chinese Academy of Sciences)Yajun Wang (Microsoft)Jialin Zhang (Chinese Academy of Sciences),"2228075735,2527738285,2717639794,2567737653,2130509011","A topic propagating in a social network reaches its tipping point if the number of users discussing it in the network exceeds a critical threshold such that a wide cascade on the topic is likely to occur. In this paper, we consider the task of selecting initial seed users of a topic with minimum size so that {\em with a guaranteed probability} the number of users discussing the topic would reach a given threshold. We formulate the task as an optimization problem called {\em seed minimization with probabilistic coverage guarantee (SM-PCG)}. This problem departs from the previous studies on social influence maximization or seed minimization because it considers influence coverage with {\em probabilistic} guarantees instead of guarantees on {\em expected} influence coverage. We show that the problem is not submodular, and thus is harder than previously studied problems based on submodular function optimization. We provide an approximation algorithm and show that it approximates the optimal solution with both a multiplicative ratio and an additive error. The multiplicative ratio is tight while the additive error would be small if influence coverage distributions of certain seed sets are well concentrated. For one-way bipartite graphs we analytically prove the concentration condition and obtain an approximation algorithm with an $O(\log n)$ multiplicative ratio and an $O(\sqrt{n})$ additive error, where $n$ is the total number of nodes in the social graph. Moreover, we empirically verify the concentration condition in real-world networks and experimentally demonstrate the effectiveness of our proposed algorithm comparing to commonly adopted benchmark algorithms.",2014,Knowledge Discovery and Data Mining,Fields of study: social networkcombinatoricssocial sciencemachine learningmathematical optimizationstatisticsmathematics
Gradient boosted feature selection,Zhixiang Eddie Xu (Washington University in St. Louis)Gao Huang (Tsinghua University)Kilian Q. Weinberger (Washington University in St. Louis)Alice X. Zheng (Microsoft),"2118732703,2114281204,2003907699,2005707679","A feature selection algorithm should ideally satisfy four conditions: reliably extract relevant features; be able to identify non-linear feature interactions; scale linearly with the number of features and dimensions; allow the incorporation of known sparsity structure. In this work we propose a novel feature selection algorithm, Gradient Boosted Feature Selection (GBFS), which satisfies all four of these requirements. The algorithm is flexible, scalable, and surprisingly straight-forward to implement as it is based on a modification of Gradient Boosted Trees. We evaluate GBFS on several real world data sets and show that it matches or outperforms other state of the art feature selection algorithms. Yet it scales to larger data set sizes and naturally allows for domain-specific side information.",2014,Knowledge Discovery and Data Mining,Fields of study: minimum redundancy feature selectiongradient boostingfeaturefeature selectiondata miningpattern recognitionmachine learningcomputer sciencemathematics
Modeling professional similarity by mining professional career trajectories,Ye Xu (Dartmouth College)Zang Li (LinkedIn)Abhishek Gupta (LinkedIn)Ahmet Bugdayci (LinkedIn)Anmol Bhasin (LinkedIn),"2678874673,2167376257,2101578767,841859240,2157708525","For decades large corporations as well as labor placement services have maintained extensive yet static resume databanks. Online professional networks like LinkedIn have taken these resume databanks to a dynamic, constantly updated and massive scale professional profile dataset spanning career records from hundreds of industries, millions of companies and hundreds of millions of people worldwide. Using this professional profile dataset, this paper attempts to model profiles of individuals as a sequence of positions held by them as a time-series of nodes, each of which represents one particular position or job experience in the individual's career trajectory. These career trajectory models can be employed in various utility applications including career trajectory planning for students in schools & universities using knowledge inferred from real world career outcomes. They can also be employed for decoding sequences to uncover paths leading to certain professional milestones from a user's current professional status. We deploy the proposed technique to ascertain professional similarity between two individuals by developing a similarity measure SimCareers (Similar Career Paths). The measure employs sequence alignment between two career trajectories to quantify professional similarity between career paths. To the best of our knowledge, SimCareers is the first framework to model professional similarity between two people taking account their career trajectory information. We posit, that using the temporal and structural features of a career trajectory for modeling profile similarity is a far more superior approach than using similarity measures on semi-structured attribute representation of a profile for this application. We validate our hypothesis by extensive quantitative evaluations on a gold dataset of similar profiles generated from recruiting activity logs from actual recruiters using LinkedIn. In addition, we show significant improvements in engagement by running an A/B test on a real-world application called Similar Profiles on LinkedIn, world's largest online professional network.",2014,Knowledge Discovery and Data Mining,Fields of study: similaritysocial networkknowledge managementdata miningsimulation
TCS: efficient topic discovery over crowd-oriented service data,Yongxin Tong (Hong Kong University of Science and Technology)Caleb Chen Cao (Hong Kong University of Science and Technology)Lei Chen (Hong Kong University of Science and Technology),"2114386387,2136041333,2318776917","In recent years, with the widespread usage of Web 2.0 techniques, crowdsourcing plays an important role in offering human intelligence in various service websites, such as Yahoo! Answer and Quora. With the increasing amount of crowd-oriented service data, an important task is to analyze latest hot topics and track topic evolution over time. However, the existing techniques in text mining cannot effectively work due to the unique structure of crowd-oriented service data, task-response pairs, which consists of the task and its corresponding responses. In particular, existing approaches become ineffective with the ever-increasing crowd-oriented service data that accumulate along the time. In this paper, we first study the problem of discovering topics over crowd-oriented service data. Then we propose a new probabilistic topic model, the Topic Crowd Service Model (TCS model), to effectively discover latent topics from massive crowd-oriented service data. In particular, in order to train TCS efficiently, we design a novel parameter inference algorithm, the Bucket Parameter Estimation (BPE), which utilizes belief propagation and a new sketching technique, called Pairwise Sketch (pSketch). Finally, we conduct extensive experiments to verify the effectiveness and efficiency of the TCS model and the BPE algorithm.",2014,Knowledge Discovery and Data Mining,Fields of study: crowdsourcingdata scienceworld wide webdata miningmachine learningcomputer science
Detecting anomalies in dynamic rating data: a robust probabilistic model for rating evolution,Stephan Günnemann (Carnegie Mellon University)Nikou Günnemann (Carnegie Mellon University)Christos Faloutsos (Carnegie Mellon University),"316694267,177157178,2198983026","Rating data is ubiquitous on websites such as Amazon, TripAdvisor, or Yelp. Since ratings are not static but given at various points in time, a temporal analysis of rating data provides deeper insights into the evolution of a product's quality. In this work, we tackle the following question: Given the time stamped rating data for a product or service, how can we detect the general rating behavior of users as well as time intervals where the ratings behave anomalous? We propose a Bayesian model that represents the rating data as sequence of categorical mixture models. In contrast to existing methods, our method does not require any aggregation of the input but it operates on the original time stamped data. To capture the dynamic effects of the ratings, the categorical mixtures are temporally constrained: Anomalies can occur in specific time intervals only and the general rating behavior should evolve smoothly over time. Our method automatically determines the intervals where anomalies occur, and it captures the temporal effects of the general behavior by using a state space model on the natural parameters of the categorical distributions. For learning our model, we propose an efficient algorithm combining principles from variational inference and dynamic programming. In our experimental study we show the effectiveness of our method and we present interesting discoveries on multiple real world datasets.",2014,Knowledge Discovery and Data Mining,Fields of study: anomaly detectiondata sciencedata miningmachine learningstatisticscomputer science
FAST-PPR: scaling personalized pagerank estimation for large graphs,Peter A. Lofgren (Stanford University)Siddhartha Banerjee (Stanford University)Ashish Goel (Stanford University)C. Seshadhri (Sandia National Laboratories),"2503222904,2146436969,2112067655,1214261146","We propose a new algorithm, FAST-PPR, for computing personalized PageRank: given start node s and target node t in a directed graph, and given a threshold δ, it computes the Personalized PageRank π_s(t) from s to t , guaranteeing that the relative error is small as long π s ( t ) > δ. Existing algorithms for this problem have a running-time of Ω(1/δ in comparison, FAST-PPR has a provable average running-time guarantee of O (√ d /δ) (where d is the average in-degree of the graph). This is a significant improvement, since δ is often O (1/ n ) (where n is the number of nodes) for applications. We also complement the algorithm with an Ω(1/√δ) lower bound for PageRank estimation, showing that the dependence on δ cannot be improved. We perform a detailed empirical study on numerous massive graphs, showing that FAST-PPR dramatically outperforms existing algorithms. For example, on the 2010 Twitter graph with 1.5 billion edges, for target nodes sampled by popularity, FAST-PPR has a 20 factor speedup over the state of the art. Furthermore, an enhanced version of FAST-PPR has a 160 factor speedup on the Twitter graph, and is at least 20 times faster on all our candidate graphs.",2014,Knowledge Discovery and Data Mining,Fields of study: theoretical computer scienceworld wide webdistributed computingcomputer science
Identifying and labeling search tasks via query-based hawkes processes,Liangda Li (Georgia Institute of Technology)Hongbo Deng (Yahoo!)Anlei Dong (Yahoo!)Yi Chang (Yahoo!)Hongyuan Zha (East China Normal University),"2148021991,2682826105,2102564942,2168000538,2099091510","We consider a search task as a set of queries that serve the same user information need. Analyzing search tasks from user query streams plays an important role in building a set of modern tools to improve search engine performance. In this paper, we propose a probabilistic method for identifying and labeling search tasks based on the following intuitive observations: queries that are issued temporally close by users in many sequences of queries are likely to belong to the same search task, meanwhile, different users having the same information needs tend to submit topically coherent search queries. To capture the above intuitions, we directly model query temporal patterns using a special class of point processes called Hawkes processes, and combine topic models with Hawkes processes for simultaneously identifying and labeling search tasks. Essentially, Hawkes processes utilize their self-exciting properties to identify search tasks if influence exists among a sequence of queries for individual users, while the topic model exploits query co-occurrence across different users to discover the latent information needed for labeling search tasks. More importantly, there is mutual reinforcement between Hawkes processes and the topic model in the unified model that enhances the performance of both. We evaluate our method based on both synthetic data and real-world query log data. In addition, we also apply our model to query clustering and search task identification. By comparing with state-of-the-art methods, the results demonstrate that the improvement in our proposed approach is consistent and promising.",2014,Knowledge Discovery and Data Mining,Fields of study: search analyticsphrase searchbeam searchweb search queryweb query classificationquery expansionprobabilistic methodlatent dirichlet allocationsearch engineinformation retrievaldata miningmachine learningcomputer science
Top-k frequent itemsets via differentially private FP-trees,Jaewoo Lee (Purdue University)Christopher W. Clifton (Purdue University),"2319471130,2158488542","Frequent itemset mining is a core data mining task and has been studied extensively. Although by their nature, frequent itemsets are aggregates over many individuals and would not seem to pose a privacy threat, an attacker with strong background information can learn private individual information from frequent itemsets. This has lead to differentially private frequent itemset mining, which protects privacy by giving inexact answers. We give an approach that first identifies top- k frequent itemsets, then uses them to construct a compact, differentially private FP-tree. Once the noisy FP-tree is built, the (privatized) support of all frequent itemsets can be derived from it without access to the original data. Experimental results show that the proposed algorithm gives substantially better results than prior approaches, especially for high levels of privacy.",2014,Knowledge Discovery and Data Mining,Fields of study: differential privacyworld wide webdata miningdatabasemachine learningcomputer science
Scalable hands-free transfer learning for online advertising,Brian DalessandroDaizhuo ChenTroy Raeder (University of Notre Dame)Claudia Perlich (IBM)Melinda Han WilliamsFoster J. Provost (New York University),"1876520739,2222883865,1999864269,164824025,2507887416,2158932634","Internet display advertising is a critical revenue source for publishers and online content providers, and is supported by massive amounts of user and publisher data. Targeting display ads can be improved substantially with machine learning methods, but building many models on massive data becomes prohibitively expensive computationally. This paper presents a combination of strategies, deployed by the online advertising firm Dstillery, for learning many models from extremely high-dimensional data efficiently and without human intervention. This combination includes: (i)~A method for simple-yet-effective transfer learning where a model learned from data that is relatively abundant and cheap is taken as a prior for Bayesian logistic regression trained with stochastic gradient descent (SGD) from the more expensive target data. (ii)~A new update rule for automatic learning rate adaptation, to support learning from sparse, high-dimensional data, as well as the integration with adaptive regularization. We present an experimental analysis across 100 different ad campaigns, showing that the transfer learning indeed improves performance across a large number of them, especially at the start of the campaigns. The combined ""hands-free"" method needs no fiddling with the SGD learning rate, and we show that it is just as effective as using expensive grid search to set the regularization parameter for each campaign.",2014,Knowledge Discovery and Data Mining,Fields of study: online machine learningmulti task learningstochastic gradient descenttransfer of learningactive learningonline advertisingsemi supervised learningdata miningmachine learningsimulationstatisticscomputer science
Crowdsourced time-sync video tagging using temporal and personalized topic modeling,Bin Wu (Hong Kong University of Science and Technology)Erheng Zhong (Hong Kong University of Science and Technology)Ben Tan (Hong Kong University of Science and Technology)Andrew Horner (Hong Kong University of Science and Technology)Qiang Yang (Hong Kong University of Science and Technology),"2697906172,2108094379,2102551043,2139157468,2109031554","Time-sync video tagging aims to automatically generate tags for each video shot. It can improve the user's experience in previewing a video's timeline structure compared to traditional schemes that tag an entire video clip. In this paper, we propose a new application which extracts time-sync video tags by automatically exploiting crowdsourced comments from video websites such as Nico Nico Douga, where videos are commented on by online crowd users in a time-sync manner. The challenge of the proposed application is that users with bias interact with one another frequently and bring noise into the data, while the comments are too sparse to compensate for the noise. Previous techniques are unable to handle this task well as they consider video semantics independently, which may overfit the sparse comments in each shot and thus fail to provide accurate modeling. To resolve these issues, we propose a novel temporal and personalized topic model that jointly considers temporal dependencies between video semantics, users' interaction in commenting, and users' preferences as prior knowledge. Our proposed model shares knowledge across video shots via users to enrich the short comments, and peels off user interaction and user bias to solve the noisy-comment problem. Log-likelihood analyses and user studies on large datasets show that the proposed model outperforms several state-of-the-art baselines in video tagging quality. Case studies also demonstrate our model's capability of extracting tags from the crowdsourced short and noisy comments.",2014,Knowledge Discovery and Data Mining,Fields of study: crowdsourcingtopic modelvideo trackingmultimediaworld wide webinformation retrievaldata miningmachine learningcomputer science
"Improving management of aquatic invasions by integrating shipping network, ecological, and environmental data: data mining for social good",Jian Xu (University of Notre Dame)Thanuka L. Wickramarathne (University of Notre Dame)Nitesh V. Chawla (University of Notre Dame)Erin K. Grey (University of Notre Dame)Karsten Steinhaeuser (University of Minnesota)Reuben P. Keller (Loyola University Chicago)John M. Drake (University of Georgia)David M. Lodge (University of Notre Dame),"2422058014,2674681908,1979796846,2136003642,258494835,2557536933,2166122367,1982916981","The unintentional transport of invasive species (i.e., non-native and harmful species that adversely affect habitats and native species) through the Global Shipping Network (GSN) causes substantial losses to social and economic welfare (e.g., annual losses due to ship-borne invasions in the Laurentian Great Lakes is estimated to be as high as USD 800 million). Despite the huge negative impacts, management of such invasions remains challenging because of the complex processes that lead to species transport and establishment. Numerous difficulties associated with quantitative risk assessments (e.g., inadequate characterizations of invasion processes, lack of crucial data, large uncertainties associated with available data, etc.) have hampered the usefulness of such estimates in the task of supporting the authorities who are battling to manage invasions with limited resources. We present here an approach for addressing the problem at hand via creative use of computational techniques and multiple data sources, thus illustrating how data mining can be used for solving crucial, yet very complex problems towards social good. By modeling implicit species exchanges as a network that we refer to as the Species Flow Network (SFN), large-scale species flow dynamics are studied via a graph clustering approach that decomposes the SFN into clusters of ports and inter-cluster connections. We then exploit this decomposition to discover crucial knowledge on how patterns in GSN affect aquatic invasions, and then illustrate how such knowledge can be used to devise effective and economical invasive species management strategies. By experimenting on actual GSN traffic data for years 1997-2006, we have discovered crucial knowledge that can significantly aid the management authorities.",2014,Knowledge Discovery and Data Mining,Fields of study: invasive speciesrisk assessmentcluster analysisdata miningmachine learningcomputer science
Event detection in activity networks,Polina Rozenshtein (Aalto University)Aris Anagnostopoulos (Sapienza University of Rome)Aristides Gionis (Aalto University)Nikolaj Tatti (Aalto University),"285511381,2136686850,737311942,1367500519","With the fast growth of smart devices and social networks, a lot of computing systems collect data that record different types of activities. An important computational challenge is to analyze these data, extract patterns, and understand activity trends. We consider the problem of mining activity networks to identify interesting events, such as a big concert or a demonstration in a city, or a trending keyword in a user community in a social network. We define an event to be a subset of nodes in the network that are close to each other and have high activity levels. We formalize the problem of event detection using two graph-theoretic formulations. The first one captures the compactness of an event using the sum of distances among all pairs of the event nodes. We show that this formulation can be mapped to the maxcut problem, and thus, it can be solved by applying standard semidefinite programming techniques. The second formulation captures compactness using a minimum-distance tree. This formulation leads to the prize-collecting Steiner-tree problem, which we solve by adapting existing approximation algorithms. For the two problems we introduce, we also propose efficient and effective greedy approaches and we prove performance guarantees for one of them. We experiment with the proposed algorithms on real datasets from a public bicycling system and a geolocation-enabled social network dataset collected from twitter. The results show that our methods are able to detect meaningful events.",2014,Knowledge Discovery and Data Mining,Fields of study: maximum cutdata miningmachine learningmathematical optimizationstatisticsmathematics
FoodSIS: a text mining system to improve the state of food safety in singapore,Kiran Kate (IBM)Sneha Chaudhari (Carnegie Mellon University)Andy Prapanca (IBM)Jayant Kalagnanam (IBM),"2231207944,2128294140,2230607073,1129663990","Food safety is an important health issue in Singapore as the number of food poisoning cases have increased significantly over the past few decades. The National Environment Agency of Singapore (NEA) is the primary government agency responsible for monitoring and mitigating the food safety risks. In an effort to pro-actively monitor emerging food safety issues and to stay abreast with developments related to food safety in the world, NEA tracks the World Wide Web as a source of news feeds to identify food safety related articles. However, such information gathering is a difficult and time consuming process due to information overload. In this paper, we present FoodSIS, a system for end-to-end web information gathering for food safety. FoodSIS improves efficiency of such focused information gathering process with the use of machine learning techniques to identify and rank relevant content. We discuss the challenges in building such a system and describe how thoughtful system design and recent advances in machine learning provide a framework that synthesizes interactive learning with classification to provide a system that is used in daily operations. We conduct experiments and demonstrate that our classification approach results in improving the efficiency by average 35% compared to a conventional approach and the ranking approach leads to average 16% improvement in elevating the ranks of relevant articles.",2014,Knowledge Discovery and Data Mining,Fields of study: rankingdata sciencedata miningmachine learningsimulationcomputer science
On the permanence of vertices in network communities,Tanmoy Chakraborty (Indian Institute of Technology Kharagpur)Sriram Srinivasan (University of Nebraska Omaha)Niloy Ganguly (Indian Institute of Technology Kharagpur)Animesh Mukherjee (Indian Institute of Technology Kharagpur)Sanjukta Bhowmick (University of Nebraska Omaha),"2471175502,2162236797,2097625090,2134540012,2137502878","Despite the prevalence of community detection algorithms, relatively less work has been done on understanding whether a network is indeed modular and how resilient the community structure is under perturbations. To address this issue, we propose a new vertex-based metric called ""permanence"", that can quantitatively give an estimate of the community- like structure of the network. The central idea of permanence is based on the observation that the strength of membership of a vertex to a community depends upon the following two factors: (i) the distribution of external connectivity of the vertex to individual communities and not the total external connectivity, and (ii) the strength of its internal connectivity and not just the total internal edges. In this paper, we demonstrate that compared to other metrics, permanence provides (i) a more accurate estimate of a derived community structure to the ground-truth community and (ii) is more sensitive to perturbations in the network. As a by-product of this study, we have also developed a community detection algorithm based on maximizing permanence. For a modular network structure, the results of our algorithm match well with ground-truth communities.",2014,Knowledge Discovery and Data Mining,Fields of study: object permanencemodularitycombinatoricsartificial intelligencecomputer sciencemathematics
Towards scalable critical alert mining,"Bo Zong (University of California, Santa Barbara)Yinghui Wu (University of California, Santa Barbara)Jie SongAmbuj K. Singh (University of California, Santa Barbara)Hasan Cam (United States Army Research Laboratory)Jiawei Han (University of Illinois at Urbana–Champaign)Xifeng Yan (University of California, Santa Barbara)","2251840456,2134127457,2308330850,2099219664,2701033434,2121939561,2116657824","Performance monitor software for data centers typically generates a great number of alert sequences. These alert sequences indicate abnormal network events. Given a set of observed alert sequences, it is important to identify the most critical alerts that are potentially the causes of others. While the need for mining critical alerts over large scale alert sequences is evident, most alert analysis techniques stop at modeling and mining the causal relations among the alerts. This paper studies the critical alert mining problem: Given a set of alert sequences, we aim to find a set of k critical alerts such that the number of alerts potentially triggered by them is maximized. We show that the problem is intractable; therefore, we resort to approximation and heuristic algorithms. First, we develop an approximation algorithm that obtains a near-optimal alert set in quadratic time, and propose pruning techniques to improve its runtime performance. Moreover, we show a faster approximation exists, when the alerts follow certain causal structure. Second, we propose two fast heuristic algorithms based on tree sampling techniques. On real-life data, these algorithms identify a critical alert from up to 270,000 mined causal relations in 5 seconds; meanwhile, they preserve more than 80% of solution quality, and are up to 5,000 times faster than their approximation counterparts.",2014,Knowledge Discovery and Data Mining,Fields of study: root cause analysisdata scienceworld wide webdata miningcomputer science
Time-varying learning and content analytics via sparse factor analysis,Andrew S. Lan (Rice University)Christoph Studer (Cornell University)Richard G. Baraniuk (Rice University),"2110067709,2153121925,2040939885","We propose SPARFA-Trace, a new machine learning-based framework for time-varying learning and content analytics for educational applications. We develop a novel message passing-based, blind, approximate Kalman filter for sparse factor analysis (SPARFA) that jointly traces learner concept knowledge over time, analyzes learner concept knowledge state transitions (induced by interacting with learning resources, such as textbook sections, lecture videos, etc., or the forgetting effect), and estimates the content organization and difficulty of the questions in assessments. These quantities are estimated solely from binary-valued (correct/incorrect) graded learner response data and the specific actions each learner performs (e.g., answering a question or studying a learning resource) at each time instant. Experimental results on two online course datasets demonstrate that SPARFA-Trace is capable of tracing each learner's concept knowledge evolution over time, analyzing the quality and content organization of learning resources, and estimating the question--concept associations and the question difficulties. Moreover, we show that SPARFA-Trace achieves comparable or better performance in predicting unobserved learner responses compared to existing collaborative filtering and knowledge tracing methods.",2014,Knowledge Discovery and Data Mining,Fields of study: kalman filterexpectation maximization algorithmpersonalized learningdata sciencedata miningmachine learningstatisticscomputer science
Correlating events with time series for incident diagnosis,Chen Luo (Jilin University)Jian-Guang Lou (Microsoft)Qingwei Lin (Microsoft)Qiang Fu (Microsoft)Rui Ding (Microsoft)Dongmei Zhang (Microsoft)Zhe Wang (Jilin University),"2282028488,2147362817,2045251856,2526604622,2247650633,2161363617,2656835010","As online services have more and more popular, incident diagnosis has emerged as a critical task in minimizing the service downtime and ensuring high quality of the services provided. For most online services, incident diagnosis is mainly conducted by analyzing a large amount of telemetry data collected from the services at runtime. Time series data and event sequence data are two major types of telemetry data. Techniques of correlation analysis are important tools that are widely used by engineers for data-driven incident diagnosis. Despite their importance, there has been little previous work addressing the correlation between two types of heterogeneous data for incident diagnosis: continuous time series data and temporal event data. In this paper, we propose an approach to evaluate the correlation between time series data and event data. Our approach is capable of discovering three important aspects of event-timeseries correlation in the context of incident diagnosis: existence of correlation, temporal order, and monotonic effect. Our experimental results on simulation data sets and two real data sets demonstrate the effectiveness of the algorithm.",2014,Knowledge Discovery and Data Mining,Fields of study: correlationdata sciencedata miningreal time computingcomputer science
Modeling delayed feedback in display advertising,Olivier Chapelle (Yahoo!),2049499784,"In performance display advertising a key metric of a campaign effectiveness is its conversion rate -- the proportion of users who take a predefined action on the advertiser website, such as a purchase. Predicting this conversion rate is thus essential for estimating the value of an impression and can be achieved via machine learning. One difficulty however is that the conversions can take place long after the impression -- up to a month -- and this delayed feedback hinders the conversion modeling. We tackle this issue by introducing an additional model that captures the conversion delay. Intuitively, this probabilistic model helps determining whether a user that has not converted should be treated as a negative sample -- when the elapsed time is larger than the predicted delay -- or should be discarded from the training set -- when it is too early to tell. We provide experimental results on real traffic logs that demonstrate the effectiveness of the proposed model.",2014,Knowledge Discovery and Data Mining,Fields of study: multimediamachine learningsimulationcomputer science
We know what you want to buy: a demographic-based system for product recommendation on microblogs,Xin Wayne Zhao (Renmin University of China)Yanwei Guo (Peking University)Yulan He (Aston University)Han Jiang (Peking University)Yuexin WuXiaoming Li (Peking University),"2191882568,2227545697,2124193838,2109387073,2533174134,2633873560","Product recommender systems are often deployed by e-commerce websites to improve user experience and increase sales. However, recommendation is limited by the product information hosted in those e-commerce sites and is only triggered when users are performing e-commerce activities. In this paper, we develop a novel product recommender system called METIS, a MErchanT Intelligence recommender System, which detects users' purchase intents from their microblogs in near real-time and makes product recommendation based on matching the users' demographic information extracted from their public profiles with product demographics learned from microblogs and online reviews. METIS distinguishes itself from traditional product recommender systems in the following aspects: 1) METIS was developed based on a microblogging service platform. As such, it is not limited by the information available in any specific e-commerce website. In addition, METIS is able to track users' purchase intents in near real-time and make recommendations accordingly. 2) In METIS, product recommendation is framed as a learning to rank problem. Users' characteristics extracted from their public profiles in microblogs and products' demographics learned from both online product reviews and microblogs are fed into learning to rank algorithms for product recommendation. We have evaluated our system in a large dataset crawled from Sina Weibo. The experimental results have verified the feasibility and effectiveness of our system. We have also made a demo version of our system publicly available and have implemented a live system which allows registered users to receive recommendations in real time.",2014,Knowledge Discovery and Data Mining,Fields of study: microblogginge commerceworld wide webdata miningcomputer science
Incremental and decremental training for linear classification,Cheng-Hao Tsai (National Taiwan University)Chieh-Yen Lin (National Taiwan University)Chih-Jen Lin (National Taiwan University),"2141326772,2298736880,2168176072","In classification, if a small number of instances is added or removed, incremental and decremental techniques can be applied to quickly update the model. However, the design of incremental and decremental algorithms involves many considerations. In this paper, we focus on linear classifiers including logistic regression and linear SVM because of their simplicity over kernel or other methods. By applying a warm start strategy, we investigate issues such as using primal or dual formulation, choosing optimization methods, and creating practical implementations. Through theoretical analysis and practical experiments, we conclude that a warm start setting on a high-order optimization method for primal formulations is more suitable than others for incremental and decremental learning of linear classification.",2014,Knowledge Discovery and Data Mining,Fields of study: linear classifierpattern recognitionmachine learningalgorithmcomputer science
Experiments with non-parametric topic models,Wray L. Buntine (Monash University)Swapnil Mishra (Australian National University),"2282891647,2615983930","In topic modelling, various alternative priors have been developed, for instance asymmetric and symmetric priors for the document-topic and topic-word matrices respectively, the hierarchical Dirichlet process prior for the document-topic matrix and the hierarchical Pitman-Yor process prior for the topic-word matrix. For information retrieval, language models exhibiting word burstiness are important. Indeed, this burstiness effect has been show to help topic models as well, and this requires additional word probability vectors for each document. Here we show how to combine these ideas to develop high-performing non-parametric topic models exhibiting burstiness based on standard Gibbs sampling. Experiments are done to explore the behavior of the models under different conditions and to compare the algorithms with previously published. The full non-parametric topic models with burstiness are only a small factor slower than standard Gibbs sampling for LDA and require double the memory, making them very competitive. We look at the comparative behaviour of different models and present some experimental insights.",2014,Knowledge Discovery and Data Mining,Fields of study: topic modelspeech recognitiondata miningpattern recognitionmachine learningstatisticscomputer science
Fast flux discriminant for large-scale sparse nonlinear classification,Wenlin Chen (Washington University in St. Louis)Yixin Chen (Washington University in St. Louis)Kilian Q. Weinberger (Washington University in St. Louis),"2231133452,2295009377,2003907699","In this paper, we propose a novel supervised learning method, Fast Flux Discriminant (FFD), for large-scale nonlinear classification. Compared with other existing methods, FFD has unmatched advantages, as it attains the efficiency and interpretability of linear models as well as the accuracy of nonlinear models. It is also sparse and naturally handles mixed data types. It works by decomposing the kernel density estimation in the entire feature space into selected low-dimensional subspaces. Since there are many possible subspaces, we propose a submodular optimization framework for subspace selection. The selected subspace predictions are then transformed to new features on which a linear model can be learned. Besides, since the transformed features naturally expect non-negative weights, we only require smooth optimization even with the L1 regularization. Unlike other nonlinear models such as kernel methods, the FFD model is interpretable as it gives importance weights on the original features. Its training and testing are also much faster than traditional kernel models. We carry out extensive empirical studies on real-world datasets and show that the proposed model achieves state-of-the-art classification results with sparsity, interpretability, and exceptional scalability. Our model can be learned in minutes on datasets with millions of samples, for which most existing nonlinear methods will be prohibitively expensive in space and time.",2014,Knowledge Discovery and Data Mining,Fields of study: sparsity of effects principlebiological classificationdata miningpattern recognitionmachine learningstatisticsmathematics
Large scale visual recommendations from street fashion images,Vignesh Jagadeesh (eBay)Robinson Piramuthu (eBay)Anurag Bhardwaj (eBay)Wei Di (eBay)Neel Sundaresan (eBay),"1214313910,866809645,2152063300,2311154374,1981173961","We describe a completely automated large scale visual recommendation system for fashion. Our focus is to efficiently harness the availability of large quantities of online fashion images and their rich meta-data. Specifically, we propose two classes of data driven models in the Deterministic Fashion Recommenders (DFR) and Stochastic Fashion Recommenders (SFR) for solving this problem. We analyze relative merits and pitfalls of these algorithms through extensive experimentation on a large-scale data set and baseline them against existing ideas from color science. We also illustrate key fashion insights learned through these experiments and show how they can be employed to design better recommendation systems. The industrial applicability of proposed models is in the context of mobile fashion shopping. Finally, we also outline a large-scale annotated data set of fashion images Fashion-136K ) that can be exploited for future research in data driven visual fashion.",2014,Knowledge Discovery and Data Mining,Fields of study: e commercemultimediacomputer science
Dynamics of news events and social media reaction,Mikalai Tsytsarau (University of Trento)Themis Palpanas (Paris Descartes University)Malu Castellanos (Hewlett-Packard),"18896700,2010554420,2148997595","The analysis of social sentiment expressed on the Web is becoming increasingly relevant to a variety of applications, and it is important to understand the underlying mechanisms which drive the evolution of sentiments in one way or another, in order to be able to predict these changes in the future. In this paper, we study the dynamics of news events and their relation to changes of sentiment expressed on relevant topics. We propose a novel framework, which models the behavior of news and social media in response to events as a convolution between event's importance and media response function, specific to media and event type. This framework is suitable for detecting time and duration of events, as well as their impact and dynamics, from time series of publication volume. These data can greatly enhance events analysis; for instance, they can help distinguish important events from unimportant, or predict sentiment and stock market shifts. As an example of such application, we extracted news events for a variety of topics and then correlated this data with the corresponding sentiment time series, revealing the connection between sentiment shifts and event dynamics.",2014,Knowledge Discovery and Data Mining,Fields of study: social mediasentiment analysisinternet privacyworld wide webdata miningcomputer science
LASTA: large scale topic assignment on multiple social networks,Nemanja SpasojevicJinyun YanAdithya RaoPrantik Bhattacharyya,"2225995881,2601997146,2226989131,2637719428","Millions of people use social networks everyday to talk about a variety of subjects, publish opinions and share information. Understanding this data to infer user's topical interests is a challenging problem with applications in various data-powered products. In this paper, we present 'LASTA' (Large Scale Topic Assignment), a full production system used at Klout, Inc., which mines topical interests from five social networks and assigns over 10,000 topics to hundreds of millions of users on a daily basis. The system continuously collects streams of user data and is reactive to fresh information, updating topics for users as interests shift. LASTA generates over 50 distinct features derived from signals such as user generated posts and profiles, user reactions such as comments and retweets, user attributions such as lists, tags and endorsements, as well as signals based on social graph connections. We show that using this diverse set of features leads to a better representation of a user's topical interests as compared to using only generated text or only graph based features. We also show that using cross-network information for a user leads to a more complete and accurate understanding of the user's topics, as compared to using any single network. We evaluate LASTA's topic assignment system on an internal labeled corpus of 32,264 user-topic labels generated from real users.",2014,Knowledge Discovery and Data Mining,Fields of study: user modelingworld wide webinformation retrievaldata miningmachine learningcomputer science
Active learning for sparse bayesian multilabel classification,Deepak Vasisht (Massachusetts Institute of Technology)Andreas C. Damianou (University of Sheffield)Manik Varma (Microsoft)Ashish Kapoor (Microsoft),"227480808,2056376007,2130043413,2161328394","We study the problem of active learning for multilabel classification. We focus on the real-world scenario where the average number of positive (relevant) labels per data point is small leading to positive label sparsity. Carrying out mutual information based near-optimal active learning in this setting is a challenging task since the computational complexity involved is exponential in the total number of labels. We propose a novel inference algorithm for the sparse Bayesian multilabel model of [17]. The benefit of this alternate inference scheme is that it enables a natural approximation of the mutual information objective. We prove that the approximation leads to an identical solution to the exact optimization problem but at a fraction of the optimization cost. This allows us to carry out efficient, non-myopic, and near-optimal active learning for sparse multilabel classification. Extensive experiments reveal the effectiveness of the method.",2014,Knowledge Discovery and Data Mining,Fields of study: active learningmutual informationdata miningpattern recognitionmachine learningstatisticsmathematics
"Bringing structure to text: mining phrases, entities, topics, and hierarchies",Jiawei Han (University of Illinois at Urbana–Champaign)Chi Wang (University of Illinois at Urbana–Champaign)Ahmed El-Kishky (University of Illinois at Urbana–Champaign),"2121939561,2461963590,2231529517","Mining phrases, entity concepts, topics, and hierarchies from massive text corpus is an essential problem in the age of big data. Text data in electronic forms are ubiquitous, ranging from scientific articles to social networks, enterprise logs, news articles, social media and general web pages. It is highly desirable but challenging to bring structure to unstructured text data, uncover underlying hierarchies, relationships, patterns and trends, and gain knowledge from such data. In this tutorial, we provide a comprehensive survey on the state-of-the art of data-driven methods that automatically mine phrases, extract and infer latent structures from text corpus, and construct multi-granularity topical groupings and hierarchies of the underlying themes. We study their principles, methodologies, algorithms and applications using several real datasets including research papers and news articles and demonstrate how these methods work and how the uncovered latent entity structures may help text understanding, knowledge discovery and management.",2014,Knowledge Discovery and Data Mining,Fields of study: co occurrence networkstopic modeltext miningdata scienceinformation retrievaldata miningmachine learningcomputer science
A hazard based approach to user return time prediction,Komal Kapoor (University of Minnesota)Mingxuan Sun (Georgia Institute of Technology)Jaideep Srivastava (University of Minnesota)Tao Ye,"2280298947,2306158810,2192802387,2484564270","In the competitive environment of the internet, retaining and growing one's user base is of major concern to most web services. Furthermore, the economic model of many web services is allowing free access to most content, and generating revenue through advertising. This unique model requires securing user time on a site rather than the purchase of good which makes it crucially important to create new kinds of metrics and solutions for growth and retention efforts for web services. In this work, we address this problem by proposing a new retention metric for web services by concentrating on the rate of user return. We further apply predictive analysis to the proposed retention metric on a service, as a means for characterizing lost customers. Finally, we set up a simple yet effective framework to evaluate a multitude of factors that contribute to user return. Specifically, we define the problem of return time prediction for free web services. Our solution is based on the Cox's proportional hazard model from survival analysis. The hazard based approach offers several benefits including the ability to work with censored data, to model the dynamics in user return rates, and to easily incorporate different types of covariates in the model. We compare the performance of our hazard based model in predicting the user return time and in categorizing users into buckets based on their predicted return time, against several baseline regression and classification methods and find the hazard based approach to be superior.",2014,Knowledge Discovery and Data Mining,Fields of study: customer relationship managementdata miningsimulationcomputer science
Matching users and items across domains to improve the recommendation quality,Chung-Yi Li (National Taiwan University)Shou-De Lin (National Taiwan University),"2251662783,2114357324","Given two homogeneous rating matrices with some overlapped users/items whose mappings are unknown, this paper aims at answering two questions. First, can we identify the unknown mapping between the users and/or items? Second, can we further utilize the identified mappings to improve the quality of recommendation in either domain? Our solution integrates a latent space matching procedure and a refining process based on the optimization of prediction to identify the matching. Then, we further design a transfer-based method to improve the recommendation performance. Using both synthetic and real data, we have done extensive experiments given different real life scenarios to verify the effectiveness of our models. The code and other materials are available at http://www.csie.ntu.edu.tw/~r00922051/matching/",2014,Knowledge Discovery and Data Mining,Fields of study: transfer of learningcollaborative filteringmatrix decompositionworld wide webinformation retrievaldata miningmachine learningcomputer science
GLAD: group anomaly detection in social media analysis,Rose Yu (University of Southern California)Xinran He (University of Southern California)Yan Liu (University of Southern California),"2107161032,2096901250,2240541904","Traditional anomaly detection on social media mostly focuses on individual point anomalies while anomalous phenomena usually occur in groups. Therefore it is valuable to study the collective behavior of individuals and detect group anomalies. Existing group anomaly detection approaches rely on the assumption that the groups are known, which can hardly be true in real world social media applications. In this paper, we take a generative approach by proposing a hierarchical Bayes model: Group Latent Anomaly Detection (GLAD) model. GLAD takes both pair-wise and point-wise data as input, automatically infers the groups and detects group anomalies simultaneously. To account for the dynamic properties of the social media data, we further generalize GLAD to its dynamic extension d-GLAD. We conduct extensive experiments to evaluate our models on both synthetic and real world datasets. The empirical results demonstrate that our approach is effective and robust in discovering latent groups and detecting group anomalies.",2014,Knowledge Discovery and Data Mining,Fields of study: anomaly detectiondata miningartificial intelligencemachine learningcomputer science
Scalable diffusion-aware optimization of network topology,Elias Boutros Khalil (Georgia Institute of Technology)Bistra N. Dilkina (Georgia Institute of Technology)Le Song (Georgia Institute of Technology),"2137038384,2252452800,2113868374","How can we optimize the topology of a networked system to bring a flu under control, propel a video to popularity, or stifle a network malware in its infancy? Previous work on information diffusion has focused on modeling the diffusion dynamics and selecting nodes to maximize/minimize influence. Only a paucity of recent studies have attempted to address the network modification problems, where the goal is to either facilitate desirable spreads or curtail undesirable ones by adding or deleting a small subset of network nodes or edges. In this paper, we focus on the widely studied linear threshold diffusion model, and prove, for the first time, that the network modification problems under this model have supermodular objective functions. This surprising property allows us to design efficient data structures and scalable algorithms with provable approximation guarantees, despite the hardness of the problems in question. Both the time and space complexities of our algorithms are linear in the size of the network, which allows us to experiment with millions of nodes and edges. We show that our algorithms outperform an array of heuristics in terms of their effectiveness in controlling diffusion processes, often beating the next best by a significant margin.",2014,Knowledge Discovery and Data Mining,Fields of study: network formationapproximationtheoretical computer sciencedata miningmachine learningsimulationmathematical optimizationcomputer sciencemathematics
An empirical study of reserve price optimisation in real-time bidding,Shuai Yuan (University College London)Jun Wang (University College London)Bowei Chen (University College London)Peter MasonSam Seljan,"2131459235,2557836567,2224779522,2236107786,2222677293","In this paper, we report the first empirical study and live test of the reserve price optimisation problem in the context of Real-Time Bidding (RTB) display advertising from an operational environment. A reserve price is the minimum that the auctioneer would accept from bidders in auctions, and in a second price auction it could potentially uplift the auctioneer's revenue by charging winners the reserve price instead of the second highest bids. As such it has been used for sponsored search and been well studied in that context. However, comparing with sponsored search and contextual advertising, this problem in the RTB context is less understood yet more critical for publishers because 1) bidders have to submit a bid for each individual impression, which mostly is associated with user data that is subject to change over time. This, coupled with practical constraints such as the budget, campaigns' life time, etc. makes the theoretical result from optimal auction theory not necessarily applicable and a further empirical study is required to confirm its optimality from the real-world system; 2) in RTB an advertiser is facing nearly unlimited supply and the auction is almost done in ""last second"", which encourages spending less on the high cost ad placements. This could imply the loss of bid volume over time if a correct reserve price is not in place. In this paper we empirically examine several commonly adopted algorithms for setting up a reserve price. We report our results of a large scale online experiment in a production platform. The results suggest the our proposed game theory based OneShot algorithm performed the best and the superiority is significant in most cases.",2014,Knowledge Discovery and Data Mining,Fields of study: mid pricereal time biddingdutch auctionreservation pricerevenue equivalenceenglish auctiononline advertisingdata mining
"Temporal skeletonization on sequential data: patterns, categorization, and visualization",Chuanren Liu (Rutgers University)Kai Zhang (NEC)Hui Xiong (Rutgers University)Guofei Jiang (NEC)Qiang Yang (Hong Kong University of Science and Technology),"2169554947,2600447970,2153710278,2168090285,2109031554","Sequential pattern analysis targets on finding statistically relevant temporal structures where the values are delivered in a sequence. With the growing complexity of real-world dynamic scenarios, more and more symbols are often needed to encode a meaningful sequence. This is so-called 'curse of cardinality', which can impose significant challenges to the design of sequential analysis methods in terms of computational efficiency and practical use. Indeed, given the overwhelming scale and the heterogeneous nature of the sequential data, new visions and strategies are needed to face the challenges. To this end, in this paper, we propose a 'temporal skeletonization' approach to proactively reduce the representation of sequences to uncover significant, hidden temporal structures. The key idea is to summarize the temporal correlations in an undirected graph. Then, the 'skeleton' of the graph serves as a higher granularity on which hidden temporal patterns are more likely to be identified. In the meantime, the embedding topology of the graph allows us to translate the rich temporal content into a metric space. This opens up new possibilities to explore, quantify, and visualize sequential data. Our approach has shown to greatly alleviate the curse of cardinality in challenging tasks of sequential pattern mining and clustering. Evaluation on a Business-to-Business (B2B) marketing application demonstrates that our approach can effectively discover critical buying paths from noisy customer event data.",2014,Knowledge Discovery and Data Mining,Fields of study: sequential pattern miningdata miningpattern recognitionmachine learningcomputer sciencemathematics
"Good-enough brain model: challenges, algorithms and discoveries in multi-subject experiments",Evangelos E. Papalexakis (Carnegie Mellon University)Alona Fyshe (Carnegie Mellon University)Nicholas D. Sidiropoulos (University of Minnesota)Partha Pratim Talukdar (Carnegie Mellon University)Tom M. Mitchell (Carnegie Mellon University)Christos Faloutsos (Carnegie Mellon University),"1418764031,212974769,2268379917,2407474466,2151014374,2198983026","Given a simple noun such as {\em apple}, and a question such as ""is it edible?"", what processes take place in the human brain? More specifically, given the stimulus, what are the interactions between (groups of) neurons (also known as functional connectivity) and how can we automatically infer those interactions, given measurements of the brain activity? Furthermore, how does this connectivity differ across different human subjects? In this work we present a simple, novel good-enough brain model, or GeBM in short, and a novel algorithm Sparse-SysId, which are able to effectively model the dynamics of the neuron interactions and infer the functional connectivity. Moreover, GeBM is able to simulate basic psychological phenomena such as habituation and priming (whose definition we provide in the main text). We evaluate GeBM by using both synthetic and real brain data. Using the real data, GeBM produces brain activity patterns that are strikingly similar to the real ones, and the inferred functional connectivity is able to provide neuroscientific insights towards a better understanding of the way that neurons interact with each other, as well as detect regularities and outliers in multi-subject brain activity measurements.",2014,Knowledge Discovery and Data Mining,Fields of study: system identificationartificial intelligence
Differentially private network data release via structural inference,Qian Xiao (National University of Singapore)Rui Chen (Hong Kong Baptist University)Kian-Lee Tan (National University of Singapore),"2149315091,2632013297,2142476784","Information networks, such as social media and email networks, often contain sensitive information. Releasing such network data could seriously jeopardize individual privacy. Therefore, we need to sanitize network data before the release. In this paper, we present a novel data sanitization solution that infers a network's structure in a differentially private manner. We observe that, by estimating the connection probabilities between vertices instead of considering the observed edges directly, the noise scale enforced by differential privacy can be greatly reduced. Our proposed method infers the network structure by using a statistical hierarchical random graph (HRG) model. The guarantee of differential privacy is achieved by sampling possible HRG structures in the model space via Markov chain Monte Carlo (MCMC). We theoretically prove that the sensitivity of such inference is only O(log n), where n is the number of vertices in a network. This bound implies less noise to be injected than those of existing works. We experimentally evaluate our approach on four real-life network datasets and show that our solution effectively preserves essential network structural properties like degree distribution, shortest path length distribution and influential nodes.",2014,Knowledge Discovery and Data Mining,Fields of study: differential privacytheoretical computer sciencedata miningmachine learningstatisticscomputer science
A case study: privacy preserving release of spatio-temporal density in paris,Gergely Acs (French Institute for Research in Computer Science and Automation)Claude Castelluccia (French Institute for Research in Computer Science and Automation),"2017223069,227365023","With billions of handsets in use worldwide, the quantity of mobility data is gigantic. When aggregated they can help understand complex processes, such as the spread viruses, and built better transportation systems, prevent traffic congestion. While the benefits provided by these datasets are indisputable, they unfortunately pose a considerable threat to location privacy. In this paper, we present a new anonymization scheme to release the spatio-temporal density of Paris, in France, i.e., the number of individuals in 989 different areas of the city released every hour over a whole week. The density is computed from a call-data-record (CDR) dataset, provided by the French Telecom operator Orange, containing the CDR of roughly 2 million users over one week. Our scheme is differential private, and hence, provides provable privacy guarantee to each individual in the dataset. Our main goal with this case study is to show that, even with large dimensional sensitive data, differential privacy can provide practical utility with meaningful privacy guarantee, if the anonymization scheme is carefully designed. This work is part of the national project XData (http://xdata.fr) that aims at combining large (anonymized) datasets provided by different service providers (telecom, electricity, water management, postal service, etc.).",2014,Knowledge Discovery and Data Mining,Fields of study: differential privacyinternet privacycomputer securitydata miningmachine learningcomputer science
Personalized search result diversification via structured learning,Shangsong Liang (University of Amsterdam)Zhaochun Ren (University of Amsterdam)Maarten de Rijke (University of Amsterdam),"2131113258,2158267603,401833296","This paper is concerned with the problem of personalized diversification of search results, with the goal of enhancing the performance of both plain diversification and plain personalization algorithms. In previous work, the problem has mainly been tackled by means of unsupervised learning. To further enhance the performance, we propose a supervised learning strategy. Specifically, we set up a structured learning framework for conducting supervised personalized diversification, in which we add features extracted directly from the tokens of documents and those utilized by unsupervised personalized diversification algorithms, and, importantly, those generated from our proposed user-interest latent Dirichlet topic model. Based on our proposed topic model whether a document can cater to a user's interest can be estimated in our learning strategy. We also define two constraints in our structured learning framework to ensure that search results are both diversified and consistent with a user's interest. We conduct experiments on an open personalized diversification dataset and find that our supervised learning strategy outperforms unsupervised personalized diversification methods as well as other plain personalization and plain diversification methods.",2014,Knowledge Discovery and Data Mining,Fields of study: personalizationunsupervised learningworld wide webdata miningmachine learning
"Open-domain quantity queries on web tables: annotation, response, and consensus models",Sunita Sarawagi (Indian Institute of Technology Bombay)Soumen Chakrabarti (Indian Institute of Technology Bombay),"156875573,2103349674","Over 40% of columns in hundreds of millions of Web tables contain numeric quantities. Tables are a richer source of structured knowledge than free text. We harness Web tables to answer queries whose target is a quantity with natural variation, such as net worth of zuckerburg, battery life of ipad, half life of plutonium, and calories in pizza. Our goal is to respond to such queries with a ranked list of quantity distributions, suitably represented. Apart from the challenges of informal schema and noisy extractions, which have been known since tables were used for non-quantity information extraction, we face additional problems of noisy number formats, as well as unit specifications that are often contextual and ambiguous. Early ""hardening"" of extraction decisions at a table level leads to poor accuracy. Instead, we use a probabilistic context free grammar (PCFG) based unit extractor on the tables, and retain several top-scoring extractions of quantity and numerals. Then we inject these into a new collective inference framework that makes global decisions about the relevance of candidate table snippets, the interpretation of the query's target quantity type, the value distributions to be ranked and presented, and the degree of consensus that can be built to support the proposed quantity distributions. Experiments with over 25 million Web tables and 350 diverse queries show robust, large benefits from our quantity catalog, unit extractor, and collective inference.",2014,Knowledge Discovery and Data Mining,Fields of study: tabledata sciencedata miningdatabasemachine learningstatisticscomputer sciencemathematics
FEMA: flexible evolutionary multi-faceted analysis for dynamic behavioral pattern discovery,Meng Jiang (Tsinghua University)Peng Cui (Tsinghua University)Fei Wang (IBM)Xinran Xu (Tsinghua University)Wenwu Zhu (Tsinghua University)Shiqiang Yang (Tsinghua University),"2115305989,2113115369,2465953593,2574153509,2111511002,2127183023","Behavioral pattern discovery is increasingly being studied to understand human behavior and the discovered patterns can be used in many real world applications such as web search, recommender system and advertisement targeting. Traditional methods usually consider the behaviors as simple user and item connections, or represent them with a static model. In real world, however, human behaviors are actually complex and dynamic: they include correlations between user and multiple types of objects and also continuously evolve along time. These characteristics cause severe data sparsity and computational complexity problem, which pose great challenge to human behavioral analysis and prediction. In this paper, we propose a Flexible Evolutionary Multi-faceted Analysis (FEMA) framework for both behavior prediction and pattern mining. FEMA utilizes a flexible and dynamic factorization scheme for analyzing human behavioral data sequences, which can incorporate various knowledge embedded in different object domains to alleviate the sparsity problem. We give approximation algorithms for efficiency, where the bound of approximation loss is theoretically proved. We extensively evaluate the proposed method in two real datasets. For the prediction of human behaviors, the proposed FEMA significantly outperforms other state-of-the-art baseline methods by 17.4%. Moreover, FEMA is able to discover quite a number of interesting multi-faceted temporal patterns on human behaviors with good interpretability. More importantly, it can reduce the run time from hours to minutes, which is significant for industry to serve real-time applications.",2014,Knowledge Discovery and Data Mining,Fields of study: behavioral patternbehavioral modelingdata miningartificial intelligencemachine learningsimulationcomputer science
Style in the long tail: discovering unique interests with latent variable models in large scale social E-commerce,Diane J. HuRob Hall (Carnegie Mellon University)Josh Attenberg (Polytechnic Institute of New York University),"2680646177,2102454835,2065581641","Purchasing decisions in many product categories are heavily influenced by the shopper's aesthetic preferences. It's insufficient to simply match a shopper with popular items from the category in question; a successful shopping experience also identifies products that match those aesthetics. The challenge of capturing shoppers' styles becomes more difficult as the size and diversity of the marketplace increases. At Etsy, an online marketplace for handmade and vintage goods with over 30 million diverse listings, the problem of capturing taste is particularly important -- users come to the site specifically to find items that match their eclectic styles. In this paper, we describe our methods and experiments for deploying two new style-based recommender systems on the Etsy site. We use Latent Dirichlet Allocation (LDA) to discover trending categories and styles on Etsy, which are then used to describe a user's ""interest"" profile. We also explore hashing methods to perform fast nearest neighbor search on a map-reduce framework, in order to efficiently obtain recommendations. These techniques have been implemented successfully at very large scale, substantially improving many key business metrics.",2014,Knowledge Discovery and Data Mining,Fields of study: topic modelcollaborative filteringrecommender systemdata miningmachine learningcomputer science
Utilizing temporal patterns for estimating uncertainty in interpretable early decision making,Mohamed F. Ghalwash (Temple University)Vladan Radosavljevic (Yahoo!)Zoran Obradovic (Temple University),"312459519,1981991050,2029694244","Early classification of time series is prevalent in many time-sensitive applications such as, but not limited to, early warning of disease outcome and early warning of crisis in stock market. \textcolor{black}{ For example,} early diagnosis allows physicians to design appropriate therapeutic strategies at early stages of diseases. However, practical adaptation of early classification of time series requires an easy to understand explanation (interpretability) and a measure of confidence of the prediction results (uncertainty estimates). These two aspects were not jointly addressed in previous time series early classification studies, such that a difficult choice of selecting one of these aspects is required. In this study, we propose a simple and yet effective method to provide uncertainty estimates for an interpretable early classification method. The question we address here is "" how to provide estimates of uncertainty in regard to interpretable early prediction ."" In our extensive evaluation on twenty time series datasets we showed that the proposed method has several advantages over the state-of-the-art method that provides reliability estimates in early classification. Namely, the proposed method is more effective than the state-of-the-art method, is simple to implement, and provides interpretable results.",2014,Knowledge Discovery and Data Mining,Fields of study: time seriesuncertaintyreliabilityeconometricsdata miningmachine learningstatisticscomputer science
Scaling up deep learning,Yoshua Bengio (Université de Montréal),161269817,"Deep learning has rapidly moved from a marginal approach in the machine learning community less than ten years ago to one that has strong industrial impact, in particular for high-dimensional perceptual data such as speech and images, but also natural language. The demand for experts in deep learning is growing very fast (faster than we can graduate PhDs), thereby considerably increasing their market value. Deep learning is based on the idea of learning multiple levels of representation, with higher levels computed as a function of lower levels, and corresponding to more abstract concepts automatically discovered by the learner. Deep learning arose out of research on artificial neural networks and graphical models and the literature on that subject has considerably grown in recent years, culminating in the creation of a dedicated conference (ICLR). The tutorial will introduce some of the basic algorithms, both on the supervised and unsupervised sides, as well as discuss some of the guidelines for successfully using them in practice. Finally, it will introduce current research questions regarding the challenge of scaling up deep learning to much larger models that can successfully extract information from huge datasets.",2014,Knowledge Discovery and Data Mining,Fields of study: stabilitycompetitive learningactive learningalgorithmic learning theorydeep learningbig datainstance based learningdata sciencedata miningartificial intelligencemachine learningcomputer science
Distance metric learning using dropout: a structured regularization approach,Qi Qian (Michigan State University)Juhua Hu (Simon Fraser University)Rong Jin (Michigan State University)Jian Pei (Simon Fraser University)Shenghuo Zhu (NEC),"2163753803,2160078699,2142724104,2126330539,2096537073","Distance metric learning (DML) aims to learn a distance metric better than Euclidean distance. It has been successfully applied to various tasks, e.g., classification, clustering and information retrieval. Many DML algorithms suffer from the over-fitting problem because of a large number of parameters to be determined in DML. In this paper, we exploit the dropout technique, which has been successfully applied in deep learning to alleviate the over-fitting problem, for DML. Different from the previous studies that only apply dropout to training data, we apply dropout to both the learned metrics and the training data. We illustrate that application of dropout to DML is essentially equivalent to matrix norm based regularization. Compared with the standard regularization scheme in DML, dropout is advantageous in simulating the structured regularizers which have shown consistently better performance than non structured regularizers. We verify, both empirically and theoretically, that dropout is effective in regulating the learned metric to avoid the over-fitting problem. Last, we examine the idea of wrapping the dropout technique in the state-of-art DML methods and observe that the dropout technique can significantly improve the performance of the original DML methods.",2014,Knowledge Discovery and Data Mining,Fields of study: econometricsmachine learningmathematical optimizationcomputer sciencemathematics
Class-distribution regularized consensus maximization for alleviating overfitting in model combination,Sihong Xie (University of Illinois at Chicago)Jing Gao (University at Buffalo)Wei Fan (Huawei)Deepak S. Turaga (IBM)Philip S. Yu (University of Illinois at Chicago),"2106011892,2096731881,2422054197,2303897886,2125104194","In data mining applications such as crowdsourcing and privacy-preserving data mining, one may wish to obtain consolidated predictions out of multiple models without access to features of the data. Besides, multiple models usually carry complementary predictive information, model combination can potentially provide more robust and accurate predictions by correcting independent errors from individual models. Various methods have been proposed to combine predictions such that the final predictions are maximally agreed upon by multiple base models. Though this maximum consensus principle has been shown to be successful, simply maximizing consensus can lead to less discriminative predictions and overfit the inevitable noise due to imperfect base models. We argue that proper regularization for model combination approaches is needed to alleviate such overfitting effect. Specifically, we analyze the hypothesis spaces of several model combination methods and identify the trade-off between model consensus and generalization ability. We propose a novel model called Regularized Consensus Maximization (RCM), which is formulated as an optimization problem to combine the maximum consensus and large margin principles. We theoretically show that RCM has a smaller upper bound on generalization error compared to the version without regularization. Experiments show that the proposed algorithm outperforms a wide spectrum of state-of-the-art model combination methods on 11 tasks.",2014,Knowledge Discovery and Data Mining,Fields of study: generalization errorensembldata miningmachine learningstatisticscomputer sciencemathematics
Quantifying herding effects in crowd wisdom,Ting Wang (IBM)Dashun Wang (IBM)Fei Wang (IBM),"2568094062,2098143673,2465953593","In many diverse settings, aggregated opinions of others play an increasingly dominant role in shaping individual decision making. One key prerequisite of harnessing the ""crowd wisdom"" is the independency of individuals' opinions, yet in real settings collective opinions are rarely simple aggregations of independent minds. Recent experimental studies document that disclosing prior collective opinions distorts individuals' decision making as well as their perceptions of quality and value, highlighting a fundamental disconnect from current modeling efforts: How to model social influence and its impact on systems that are constantly evolving? In this paper, we develop a mechanistic framework to model social influence of prior collective opinions (e.g., online product ratings) on subsequent individual decision making. We find our method successfully captures the dynamics of rating growth, helping us separate social influence bias from inherent values. Using large-scale longitudinal customer rating datasets, we demonstrate that our model not only effectively assesses social influence bias, but also accurately predicts long-term cumulative growth of ratings solely based on early rating trajectories. We believe our framework will play an increasingly important role as our understanding of social processes deepens. It promotes strategies to untangle manipulations and social biases and provides insights towards a more reliable and effective design of social platforms.",2014,Knowledge Discovery and Data Mining,Fields of study: social influencemanagement sciencedata mining
Correlation clustering in MapReduce,Flavio Chierichetti (Sapienza University of Rome)Nilesh N. Dalvi (Facebook)Ravi Kumar 0001 (Google),"2082432826,1986584529,2232709231","Correlation clustering is a basic primitive in data miner's toolkit with applications ranging from entity matching to social network analysis. The goal in correlation clustering is, given a graph with signed edges, partition the nodes into clusters to minimize the number of disagreements. In this paper we obtain a new algorithm for correlation clustering. Our algorithm is easily implementable in computational models such as MapReduce and streaming, and runs in a small number of rounds. In addition, we show that our algorithm obtains an almost 3-approximation to the optimal correlation clustering. Experiments on huge graphs demonstrate the scalability of our algorithm and its applicability to data mining problems.",2014,Knowledge Discovery and Data Mining,Fields of study: k medians clusteringflame clusteringhierarchical clustering of networksbrown clusteringcanopy clustering algorithmdetermining the number of clusters in a data setdbscancorrelation clusteringconstrained clusteringdata stream clusteringcure data clustering algorithmsingle linkage clusteringaffinity propagationfuzzy clusteringclustering high dimensional datacluster analysisconsensus clusteringbiclusteringtheoretical computer sciencedata miningmachine learningcomputer science
"The interplay between dynamics and networks: centrality, communities, and cheeger inequality",Rumi Ghosh (Bosch)Shang-hua Teng (University of Southern California)Kristina Lerman (University of Southern California)Xiaoran Yan (University of Southern California),"2111901679,2103844955,2149625712,2114830456","We study the interplay between a dynamic process and the structure of the network on which it is defined. Specifically, we examine the impact of this interaction on the quality-measure of network clusters and node centrality. This enables us to effectively identify network communities and important nodes participating in the dynamics. As the first step towards this objective, we introduce an umbrella framework for defining and characterizing an ensemble of dynamic processes on a network. This framework generalizes the traditional Laplacian framework to continuous-time biased random walks and also allows us to model some epidemic processes over a network. For each dynamic process in our framework, we can define a function that measures the quality of every subset of nodes as a potential cluster (or community) with respect to this process on a given network. This subset-quality function generalizes the traditional conductance measure for graph partitioning. We partially justify our choice of the quality function by showing that the classic Cheeger's inequality, which relates the conductance of the best cluster in a network with a spectral quantity of its Laplacian matrix, can be extended from the Laplacian-conductance setting to this more general setting.",2014,Knowledge Discovery and Data Mining,Fields of study: alpha centralitycentralitynetwork scienceconductancespectral clusteringgraph theorydiscrete mathematicscombinatoricsmachine learningmathematical optimizationmathematics
Core decomposition of uncertain graphs,Francesco Bonchi (Yahoo!)Francesco Gullo (Yahoo!)Andreas Kaltenbrunner (Pompeu Fabra University)Yana Volkovich (University of Twente),"2176652147,1979201319,1685235157,1986349404","Core decomposition has proven to be a useful primitive for a wide range of graph analyses. One of its most appealing features is that, unlike other notions of dense subgraphs, it can be computed linearly in the size of the input graph. In this paper we provide an analogous tool for uncertain graphs, i.e., graphs whose edges are assigned a probability of existence. The fact that core decomposition can be computed efficiently in deterministic graphs does not guarantee efficiency in uncertain graphs, where even the simplest graph operations may become computationally intensive. Here we show that core decomposition of uncertain graphs can be carried out efficiently as well. We extensively evaluate our definitions and methods on a number of real-world datasets and applications, such as influence maximization and task-driven team formation.",2014,Knowledge Discovery and Data Mining,Fields of study: partial k treeindifference graphforbidden graph characterization1 planar graphtree depthgraph operationsuniversal graphblock graphcographgraph productsplit graphdense graphpermutation graphmodular decompositionmaximal independent setpathwidthlongest path problemtreewidthchordal graphdiscrete mathematicscombinatoricsmathematical optimizationmathematics
Analyzing expert behaviors in collaborative networks,"Huan Sun (University of California, Santa Barbara)Mudhakar Srivatsa (IBM)Shulong Tan (University of California, Santa Barbara)Yang Li (University of California, Santa Barbara)Lance M. Kaplan (United States Army Research Laboratory)Shu Tao (IBM)Xifeng Yan (University of California, Santa Barbara)","2618514357,1449758934,2161400417,2294602198,2119398415,2509613010,2116657824","Collaborative networks are composed of experts who cooperate with each other to complete specific tasks, such as resolving problems reported by customers. A task is posted and subsequently routed in the network from an expert to another until being resolved. When an expert cannot solve a task, his routing decision ( i.e. , where to transfer a task) is critical since it can significantly affect the completion time of a task. In this work, we attempt to deduce the cognitive process of task routing, and model the decision making of experts as a generative process where a routing decision is made based on mixed routing patterns. In particular, we observe an interesting phenomenon that an expert tends to transfer a task to someone whose knowledge is neither too similar to nor too different from his own. Based on this observation, an expertise difference based routing pattern is developed. We formalize multiple routing patterns by taking into account both rational and random analysis of tasks, and present a generative model to combine them. For a held-out set of tasks, our model not only explains their real routing sequences very well, but also accurately predicts their completion time. Under three different quality measures, our method significantly outperforms all the alternatives with more than 75% accuracy gain. In practice, with the help of our model, hypotheses on how to improve a collaborative network can be tested quickly and reliably, thereby significantly easing performance improvement of collaborative networks.",2014,Knowledge Discovery and Data Mining,Fields of study: generative modeluser modelingdata miningmachine learningsimulationcomputer science
Applying data mining techniques to address critical process optimization needs in advanced manufacturing,Li Zheng (Florida International University)Chunqiu Zeng (Florida International University)Lei Li (Florida International University)Yexi Jiang (Florida International University)Wei Xue (Florida International University)Jingxuan Li (Florida International University)Chao Shen (Florida International University)Wubai Zhou (Florida International University)Hongtai Li (Florida International University)Liang Tang (Florida International University)Tao Li (Florida International University)Bing Duan (College of the Canyons)Ming Lei (College of the Canyons)Pengnian Wang (College of the Canyons),"2112178922,2155150411,2432045905,2402026574,2232451325,2164615415,2171136085,2227176090,2154980435,2128176332,2472069284,2230928869,2308153979,2223530132","Advanced manufacturing such as aerospace, semi-conductor, and flat display device often involves complex production processes, and generates large volume of production data. In general, the production data comes from products with different levels of quality, assembly line with complex flows and equipments, and processing craft with massive controlling parameters. The scale and complexity of data is beyond the analytic power of traditional IT infrastructures. To achieve better manufacturing performance, it is imperative to explore the underlying dependencies of the production data and exploit analytic insights to improve the production process. However, few research and industrial efforts have been reported on providing manufacturers with integrated data analytical solutions to reveal potentials and optimize the production process from data-driven perspectives. In this paper, we design, implement and deploy an integrated solution, named PDP-Miner, which is a data analytics platform customized for process optimization in Plasma Display Panel (PDP) manufacturing. The system utilizes the latest advances in data mining technologies and Big Data infrastructures to create a complete analytical solution. Besides, our proposed system is capable of supporting automatically configuring and scheduling analysis tasks, and balancing heterogeneous computing resources. The system and the analytic strategies can be applied to other advanced manufacturing fields to enable complex data analysis tasks. Since 2013, PDP-Miner has been deployed as the data analysis platform of ChangHong COC. By taking the advantages of our system, the overall PDP yield rate has increased from 91% to 94%. The monthly production is boosted by 10,000 panels, which brings more than 117 million RMB of revenue improvement per year.",2014,Knowledge Discovery and Data Mining,Fields of study: advanced manufacturingprocess optimizationbig datadata miningsimulationcomputer science
Integrating spreadsheet data via accurate and low-effort extraction,Zhe Chen (University of Michigan)Michael J. Cafarella (University of Michigan),"2302085265,2893888","Spreadsheets contain valuable data on many topics. However, spreadsheets are difficult to integrate with other data sources. Converting spreadsheet data to the relational model would allow data analysts to use relational integration tools. We propose a two-phase semiautomatic system that extracts accurate relational metadata while minimizing user effort. Based on an undirected graphical model, our system enables downstream spreadsheet integration applications. First, the automatic extractor uses hints from spreadsheets' graphical style and recovered metadata to extract the spreadsheet data as accurately as possible. Second, the interactive repair identifies similar regions in distinct spreadsheets scattered across large spreadsheet corpora, allowing a user's single manual repair to be amortized over many possible extraction errors. Our experiments show that a human can obtain the accurate extraction with just 31% of the manual operations required by a standard classification based technique on two real-world datasets.",2014,Knowledge Discovery and Data Mining,Fields of study: graphical modelinformation extractiondata sciencedata miningdatabasemachine learningcomputer science
Active collaborative permutation learning,Jialei Wang (University of Chicago)Nathan Srebro (Toyota Technological Institute at Chicago)James Evans (University of Chicago),"2656372487,1531155176,2125360209","We consider the problem of Collaborative Permutation Recovery, i.e. recovering multiple permutations over objects (e.g. preference rankings over different options) from limited pairwise comparisons. We tackle both the problem of how to recover multiple related permutations from limited observations, and the active learning problem of which pairwise comparison queries to ask so as to allow better recovery. There has been much work on recovering single permutations from pairwise comparisons, but we show that considering several related permutations jointly we can leverage their relatedness so as to reduce the number of comparisons needed compared to reconstructing each permutation separately. To do so, we take a collaborative filtering / matrix completion approach and use a trace-norm or max-norm regularized matrix learning model. Our approach can also be seen as a collaborative learning version of Jamieson and Nowak's recent work on constrained permutation recovery, where instead of basing the recovery on known features, we learn the best features de novo.",2014,Knowledge Discovery and Data Mining,Fields of study: active learningmatrix decompositioncombinatoricspattern recognitionmachine learningcomputer sciencemathematics
Large margin distribution machine,Teng Zhang (Nanjing University)Zhi-Hua Zhou (Nanjing University),"2657676441,2286237009","Support vector machine (SVM) has been one of the most popular learning algorithms, with the central idea of maximizing the minimum margin, i.e., the smallest distance from the instances to the classification boundary. Recent theoretical results, however, disclosed that maximizing the minimum margin does not necessarily lead to better generalization performances, and instead, the margin distribution has been proven to be more crucial. In this paper, we propose the Large margin Distribution Machine (LDM), which tries to achieve a better generalization performance by optimizing the margin distribution. We characterize the margin distribution by the first- and second-order statistics, i.e., the margin mean and variance. The LDM is a general learning approach which can be used in any place where SVM can be applied, and its superiority is verified both theoretically and empirically in this paper.",2014,Knowledge Discovery and Data Mining,Fields of study: margin classifiermarginbiological classificationpattern recognitionmachine learningmathematical optimizationmathematics
Clinical risk prediction with multilinear sparse logistic regression,Fei Wang (IBM)Ping Zhang (IBM)Buyue Qian (IBM)Xiang Wang (IBM)Ian Davidson (IBM),"2465953593,2621458975,2306481588,2099725100,2661463464","Logistic regression is one core predictive modeling technique that has been used extensively in health and biomedical problems. Recently a lot of research has been focusing on enforcing sparsity on the learned model to enhance its effectiveness and interpretability, which results in sparse logistic regression model. However, no matter the original or sparse logistic regression, they require the inputs to be in vector form. This limits the applicability of logistic regression in the problems when the data cannot be naturally represented vectors (e.g., functional magnetic resonance imaging and electroencephalography signals). To handle the cases when the data are in the form of multi-dimensional arrays, we propose MulSLR : Multilinear Sparse Logistic Regression. MulSLR can be viewed as a high order extension of sparse logistic regression. Instead of solving one classification vector as in conventional logistic regression, we solve for K classification vectors in MulSLR ( K is the number of modes in the data). We propose a block proximal descent approach to solve the problem and prove its convergence. The convergence rate of the proposed algorithm is also analyzed. Finally we validate the efficiency and effectiveness of MulSLR on predicting the onset risk of patients with Alzheimer's disease and heart failure.",2014,Knowledge Discovery and Data Mining,Fields of study: logistic model treemultilinear maplogistic distributionmultinomial logistic regressionpolynomial regressionlogistic regressioneconometricsdata miningmachine learningstatisticscomputer sciencemathematics
Grouping students in educational settings,Rakesh Agrawal (Microsoft)Behzad Golshan (Boston University)Evimaria Terzi (Boston University),"2537924216,2056540169,2110675235","Given a class of large number of students, each exhibiting a different ability level, how can we group them into sections so that the overall gain for students is maximized? This question has been a topic of central concern and debate amongst social scientists and policy makers for a long time. We propose a framework for rigorously studying this question, taking a computational perspective. We present a formal definition of the grouping problem and investigate some of its variants. Such variants are determined by the desired number of groups as well as the definition of the gain for each student in the group. We focus on two natural instantiations of the gain function and we show that for both of them the problem of identifying a single group of students that maximizes the gain among its members can be solved in polynomial time. The corresponding partitioning problem, where the goal is to partition the students into non-overlapping groups appear to be much harder. However, the algorithms for the single-group version can be leveraged for solving the more complex partitioning problem. Our experiments with generated data coming from different distributions demonstrate that our algorithm is significantly better than the current strategies in vogue for dividing students in a class into sections.",2014,Knowledge Discovery and Data Mining,Fields of study: cluster analysismathematics educationdata miningartificial intelligencemachine learningsimulationcomputer sciencemathematics
A system to grade computer programming skills using machine learning,Shashank SrikantVarun Aggarwal,"2226147892,2608931570","The automatic evaluation of computer programs is a nascent area of research with a potential for large-scale impact. Extant program assessment systems score mostly based on the number of test-cases passed, providing no insight into the competency of the programmer. In this paper, we present a system to grade computer programs automatically. In addition to grading a program on its programming practices and complexity, the key kernel of the system is a machine-learning based algorithm which determines closeness of the logic of the given program to a correct program. This algorithm uses a set of highly-informative features, derived from the abstract representations of a given program, that capture the program's functionality. These features are then used to learn a model to grade the programs, which are built against evaluations done by experts. We show that the regression models provide much better grading than the ubiquitous test-case-pass based grading and rivals the grading accuracy of other open-response problems such as essay grading . We also show that our novel features add significant value over and above basic keyword/expression count features. In addition to this, we propose a novel way of posing computer-program grading as a one-class modeling problem and report encouraging preliminary results. We show the value of the system through a case study in a real-world industrial deployment. To the best of the authors' knowledge, this is the first time a system using machine learning has been developed and used for grading programs. The work is timely with regard to the recent boom in Massively Online Open Courseware (MOOCs), which promises to produce a significant amount of hand-graded digitized data.",2014,Knowledge Discovery and Data Mining,Fields of study: featuresupervised learningdata miningartificial intelligencemachine learningsimulationcomputer science
Algorithms for interpretable machine learning,Cynthia Rudin (Massachusetts Institute of Technology),2141705163,"It is extremely important in many application domains to have transparency in predictive modeling. Domain experts do not tend to prefer ""black box"" predictive model models. They would like to understand how predictions are made, and possibly, prefer models that emulate the way a human expert might make a decision, with a few important variables, and a clear convincing reason to make a particular prediction. I will discuss recent work on interpretable predictive modeling with decision lists and sparse integer linear models. I will describe several approaches, including an algorithm based on discrete optimization, and an algorithm based on Bayesian analysis. I will show examples of interpretable models for stroke prediction in medical patients and prediction of violent crime in young people raised in out-of-home care. Collaborators are Ben Letham, Berk Ustun, Stefano Traca, Siong Thye Goh, Tyler McCormick, and David Madigan.",2014,Knowledge Discovery and Data Mining,Fields of study: sparsity of effects principledata miningartificial intelligencemachine learningstatisticscomputer sciencemathematics
Corporate residence fraud detection,Enric Junqué de Fortuny (University of Antwerp)Marija Stankova (University of Antwerp)Julie Moeyersoms (University of Antwerp)Bart Minnaert (Ghent University)Foster J. Provost (New York University)David Martens (University of Antwerp),"691374638,2149205662,301938878,1949701817,2158932634,2605386389","With the globalisation of the world's economies and ever-evolving financial structures, fraud has become one of the main dissipaters of government wealth and perhaps even a major contributor in the slowing down of economies in general. Although corporate residence fraud is known to be a major factor, data availability and high sensitivity have caused this domain to be largely untouched by academia. The current Belgian government has pledged to tackle this issue at large by using a variety of in-house approaches and cooperations with institutions such as academia, the ultimate goal being a fair and efficient taxation system. This is the first data mining application specifically aimed at finding corporate residence fraud, where we show the predictive value of using both structured and fine-grained invoicing data. We further describe the problems involved in building such a fraud detection system, which are mainly data-related (e.g. data asymmetry, quality, volume, variety and velocity) and deployment-related (e.g. the need for explanations of the predictions made).",2014,Knowledge Discovery and Data Mining,Fields of study: transaction datadata modelactuarial sciencedata miningcomputer science
Targeting direct cash transfers to the extremely poor,Brian AbelsonKush R. Varshney (IBM)Joy Sun,"2011844313,241780900,2313497777","Unconditional cash transfers to the extreme poor via mobile telephony represent a radical, new approach to giving. GiveDirectly is a non-governmental organization (NGO) at the vanguard of delivering this proven and effective approach to reducing poverty. In this work, we streamline an important step in the operations of the NGO by developing and deploying a data-driven system for locating villages with extreme poverty in Kenya and Uganda. Using the type of roof of a home, thatched or metal, as a proxy for poverty, we develop a new remote sensing approach for selecting extremely poor villages to target for cash transfers. We develop an analytics algorithm that estimates housing quality and density in patches of publicly-available satellite imagery by learning a predictive model with sieves of template matching results combined with color histograms as features. We develop and deploy a crowdsourcing interface to obtain labeled training data. We deploy the predictive model to construct a fine-scale heat map of poverty and integrate this discovered knowledge into the processes of GiveDirectly's operations. Aggregating estimates at the village level, we produce a ranked list from which top villages are included in GiveDirectly's planned distribution of cash transfers. The automated approach increases village selection efficiency significantly.",2014,Knowledge Discovery and Data Mining,Fields of study: common gooddata miningsimulation
Using strong triadic closure to characterize ties in social networks,Stavros Sintos (University of Ioannina)Panayiotis Tsaparas (University of Ioannina),"2232139926,2234654910","In the past few years there has been an explosion of social networks in the online world. Users flock these networks, creating profiles and linking themselves to other individuals. Connecting online has a small cost compared to the physical world, leading to a proliferation of connections, many of which carry little value or importance. Understanding the strength and nature of these relationships is paramount to anyone interesting in making use of the online social network data. In this paper, we use the principle of Strong Triadic Closure to characterize the strength of relationships in social networks. The Strong Triadic Closure principle stipulates that it is not possible for two individuals to have a strong relationship with a common friend and not know each other. We consider the problem of labeling the ties of a social network as strong or weak so as to enforce the Strong Triadic Closure property. We formulate the problem as a novel combinatorial optimization problem, and we study it theoretically. Although the problem is NP-hard, we are able to identify cases where there exist efficient algorithms with provable approximation guarantees. We perform experiments on real data, and we show that there is a correlation between the labeling we obtain and empirical metrics of tie strength, and that weak edges act as bridges between different communities in the network. Finally, we study extensions and variations of our problem both theoretically and experimentally.",2014,Knowledge Discovery and Data Mining,Fields of study: triadic closuresocial networkapproximation algorithmartificial intelligencemachine learningcomputer sciencemathematics
Scalable heterogeneous translated hashing,Ying Wei (Hong Kong University of Science and Technology)Yangqiu Song (University of Illinois at Urbana–Champaign)Yi Zhen (Duke University)Bo Liu (Hong Kong University of Science and Technology)Qiang Yang (Hong Kong University of Science and Technology),"2235263654,2099747503,2706898368,2423441494,2109031554","Hashing has enjoyed a great success in large-scale similarity search. Recently, researchers have studied the multi-modal hashing to meet the need of similarity search across different types of media. However, most of the existing methods are applied to search across multi-views among which explicit bridge information is provided. Given a heterogeneous media search task, we observe that abundant multi-view data can be found on the Web which can serve as an auxiliary bridge. In this paper, we propose a Heterogeneous Translated Hashing (HTH) method with such auxiliary bridge incorporated not only to improve current multi-view search but also to enable similarity search across heterogeneous media which have no direct correspondence. HTH simultaneously learns hash functions embedding heterogeneous media into different Hamming spaces, and translators aligning these spaces. Unlike almost all existing methods that map heterogeneous data in a common Hamming space, mapping to different spaces provides more flexible and discriminative ability. We empirically verify the effectiveness and efficiency of our algorithm on two real world large datasets, one publicly available dataset of Flickr and the other MIRFLICKR-Yahoo Answers dataset.",2014,Knowledge Discovery and Data Mining,Fields of study: k independent hashingfeature hashinglocality preserving hashingdouble hashingdynamic perfect hashinguniversal hashinglocality sensitive hashinghash tablescalabilitytheoretical computer scienceworld wide webdata miningmachine learningcomputer science
New algorithms for parking demand management and a city-scale deployment,Onno Zoeter (Xerox)Christopher R. Dance (Xerox)Stéphane Clinchant (Xerox)Jean-Marc Andreoli (Xerox),"2641092198,1991661564,288365595,2004261629","On-street parking, just as any publicly owned utility, is used inefficiently if access is free or priced very far from market rates. This paper introduces a novel demand management solution: using data from dedicated occupancy sensors an iteration scheme updates parking rates to better match demand. The new rates encourage parkers to avoid peak hours and peak locations and reduce congestion and underuse. The solution is deliberately simple so that it is easy to understand, easily seen to be fair and leads to parking policies that are easy to remember and act upon. We study the convergence properties of the iteration scheme and prove that it converges to a reasonable distribution for a very large class of models. The algorithm is in use to change parking rates in over 6000 spaces in downtown Los Angeles since June 2012 as part of the LA Express Park project. Initial results are encouraging with a reduction of congestion and underuse, while in more locations rates were decreased than increased.",2014,Knowledge Discovery and Data Mining,Fields of study: demand managementtransportsimulationcomputer science
Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining,"Sofus Macskassy (Facebook)Claudia Perlich (IBM)Jure Leskovec (Stanford University)Wei Wang (University of California, Los Angeles)Rayid Ghani (University of Chicago)","2620516491,164824025,1878631932,2315689540,2655769349","It is our great pleasure to welcome you to the 20th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD). The annual ACM SIGKDD conference is the premier international forum for data science, data mining, knowledge discovery and big data. It brings together researchers and practitioners from academia, industry, and government to share their ideas, research results and experiences. KDD-2014 features plenary presentations, paper presentations, poster sessions, workshops, tutorials, exhibits, and the KDD Cup competition. We are happy to announce that this year we are partnering with Bloomberg to emphasize our theme of Data Science for Social Good. To this end, part of our workshop and tutorial program will be held at the Bloomberg facilities together with Bloomberg-specific events, all focusing on issues pertaining to social good. Today, you hear a lot about data science, big data and data intensive computing. The core of this work is extracting knowledge and useful information from data, which for science leads to beautiful insights, and for applications leads to actions, alerts and decisions. The KDD community has always been at the center of this activity and it is clear from this conference that it will continue to drive this broader field of data science. This year we had a record number of submissions. There were 1036 submissions to the Research Track, and 151 papers were accepted. There were 197 submissions to the Industry and Government Track, and 44 papers were accepted. KDD also has a history of inviting talks that are of broad interest to the KDD community. This year we chose to have 4 plenary talks. A program committee also selected 8 talks to present at the Industry and Government track. A strength of the KDD conference is the number of workshops and tutorials that are co-located with it. This year there were 9 full-day workshops, 16 half-day workshops, and 12 tutorials. As part of our partnership with Bloomberg on the theme of social good, Bloomberg will have 3 workshops jointly located with our workshops at their New York Office. Our community is a unique blend of industry and academia, ranging from people starting their career to leaders in their respective fields. This year, we are piloting programs to facilitate networking amongst these groups. Specifically, we have a networking lounge for industry and job-seekers to meet and we helped find good matches. We also have a networking event focused on defining what a data science career looks like and have senior members meet young people to help them understand the skills needed and what a job in this discipline might entail.",2014,Knowledge Discovery and Data Mining,Fields of study: data scienceoperations researchdata miningcomputer science
FBLG: a simple and effective approach for temporal dependence discovery from time series data,Dehua Cheng (University of Southern California)Mohammad Taha Bahadori (University of Southern California)Yan Liu (University of Southern California),"2166044507,2032867848,2240541904","Discovering temporal dependence structure from multivariate time series has established its importance in many applications. We observe that when we look in reversed order of time, the temporal dependence structure of the time series is usually preserved after switching the roles of cause and effect. Inspired by this observation, we create a new time series by reversing the time stamps of original time series and combine both time series to improve the performance of temporal dependence recovery. We also provide theoretical justification for the proposed algorithm for several existing time series models. We test our approach on both synthetic and real world datasets. The experimental results confirm that this surprisingly simple approach is indeed effective under various circumstances.",2014,Knowledge Discovery and Data Mining,Fields of study: granger causalitygeneralized linear modeltime serieseconometricsmachine learningstatisticscomputer sciencemathematics
Probabilistic latent network visualization: inferring and embedding diffusion networks,Takeshi Kurashima (Nippon Telegraph and Telephone)Tomoharu Iwata (Nippon Telegraph and Telephone)Noriko Takaya (Nippon Telegraph and Telephone)Hiroshi Sawada (Nippon Telegraph and Telephone),"2090365413,2108993706,2584052289,2099875912","The diffusion of information, rumors, and diseases are assumed to be probabilistic processes over some network structure. An event starts at one node of the network, and then spreads to the edges of the network. In most cases, the underlying network structure that generates the diffusion process is unobserved, and we only observe the times at which each node is altered/influenced by the process. This paper proposes a probabilistic model for inferring the diffusion network, which we call Probabilistic Latent Network Visualization (PLNV); it is based on cascade data, a record of observed times of node influence. An important characteristic of our approach is to infer the network by embedding it into a low-dimensional visualization space. We assume that each node in the network has latent coordinates in the visualization space, and diffusion is more likely to occur between nodes that are placed close together. Our model uses maximum a posteriori estimation to learn the latent coordinates of nodes that best explain the observed cascade data. The latent coordinates of nodes in the visualization space can 1) enable the system to suggest network layouts most suitable for browsing, and 2) lead to high accuracy in inferring the underlying network when analyzing the diffusion process of new or rare information, rumors, and disease.",2014,Knowledge Discovery and Data Mining,Fields of study: dynamic network analysisnetwork formationgraph drawingnetwork simulationsurvival analysistheoretical computer sciencedata miningmachine learningstatisticscomputer science
Community detection in graphs through correlation,Lian Duan (New Jersey Institute of Technology)Willian Nick Street (University of Iowa)Yanchi Liu (New Jersey Institute of Technology)Haibing Lu (Santa Clara University),"2301862080,2625178443,2703695699,2123828596","Community detection is an important task for social networks, which helps us understand the functional modules on the whole network. Among different community detection methods based on graph structures, modularity-based methods are very popular recently, but suffer a well-known resolution limit problem. This paper connects modularity-based methods with correlation analysis by subtly reformatting their math formulas and investigates how to fully make use of correlation analysis to change the objective function of modularity-based methods, which provides a more natural and effective way to solve the resolution limit problem. In addition, a novel theoretical analysis on the upper bound of different objective functions helps us understand their bias to different community sizes, and experiments are conducted on both real life and simulated data to validate our findings.",2014,Knowledge Discovery and Data Mining,Fields of study: modularityleveragemodularitylikelihood ratio testdata miningmachine learningstatisticscomputer sciencemathematics
Activity-edge centric multi-label classification for mining heterogeneous information networks,Yang Zhou (Georgia Institute of Technology)Ling Liu (Georgia Institute of Technology),"2625264668,2125988131","Multi-label classification of heterogeneous information networks has received renewed attention in social network analysis. In this paper, we present an activity-edge centric multi-label classification framework for analyzing heterogeneous information networks with three unique features. First, we model a heterogeneous information network in terms of a collaboration graph and multiple associated activity graphs. We introduce a novel concept of vertex-edge homophily in terms of both vertex labels and edge labels and transform a general collaboration graph into an activity-based collaboration multigraph by augmenting its edges with class labels from each activity graph through activity-based edge classification. Second, we utilize the label vicinity to capture the pairwise vertex closeness based on the labeling on the activity-based collaboration multigraph. We incorporate both the structure affinity and the label vicinity into a unified classifier to speed up the classification convergence. Third, we design an iterative learning algorithm, AEClass, to dynamically refine the classification result by continuously adjusting the weights on different activity-based edge classification schemes from multiple activity graphs, while constantly learning the contribution of the structure affinity and the label vicinity in the unified classifier. Extensive evaluation on real datasets demonstrates that AEClass outperforms existing representative methods in terms of both effectiveness and efficiency.",2014,Knowledge Discovery and Data Mining,Fields of study: heterogeneous networkdata miningpattern recognitionmachine learningcomputer science
Simultaneous feature and feature group selection through hard thresholding,Shuo Xiang (Arizona State University)Tao Yang (Arizona State University)Jieping Ye (Arizona State University),"2121591154,2688435411,2305258894","Selecting an informative subset of features has important applications in many data mining tasks especially for high-dimensional data. Recently, simultaneous selection of features and feature groups (a.k.a bi-level selection) becomes increasingly popular since it not only reduces the number of features but also unveils the underlying grouping effect in the data, which is a valuable functionality in many applications such as bioinformatics and web data mining. One major challenge of bi-level selection (or even feature selection only) is that computing a globally optimal solution requires a prohibitive computational cost. To overcome such a challenge, current research mainly falls into two categories. The first one focuses on finding suitable continuous computational surrogates for the discrete functions and this leads to various convex and nonconvex optimization models. Although efficient, convex models usually deliver sub-optimal performance while nonconvex models on the other hand require significantly more computational effort. Another direction is to use greedy algorithms to solve the discrete optimization directly. However, existing algorithms are proposed to handle single-level selection only and it remains challenging to extend these methods to handle bi-level selection. In this paper, we fulfill this gap by introducing an efficient sparse group hard thresholding algorithm. Our main contributions are: (1) we propose a novel bi-level selection model and show that the key combinatorial problem admits a globally optimal solution using dynamic programming; (2) we provide an error bound between our solution and the globally optimal under the RIP (Restricted Isometry Property) theoretical framework. Our experiments on synthetic and real data demonstrate that the proposed algorithm produces encouraging performance while keeping comparable computational efficiency to convex relaxation models.",2014,Knowledge Discovery and Data Mining,Fields of study: dynamic programmingsupervised learningfeature selectiondata miningpattern recognitionmachine learningmathematical optimizationstatisticscomputer science
Sampling for big data: a tutorial,Graham Cormode (University of Warwick)Nick G. Duffield (Rutgers University),"2190072679,2017920553","One response to the proliferation of large datasets has been to develop ingenious ways to throw resources at the problem, using massive fault tolerant storage architectures, parallel and graphical computation models such as MapReduce, Pregel and Giraph. However, not all environments can support this scale of resources, and not all queries need an exact response. This motivates the use of sampling to generate summary datasets that support rapid queries, and prolong the useful life of the data in storage. To be effective, sampling must mediate the tensions between resource constraints, data characteristics, and the required query accuracy. The state-of-the-art in sampling goes far beyond simple uniform selection of elements, to maximize the usefulness of the resulting sample. This tutorial reviews progress in sample design for large datasets, including streaming and graph-structured data. Applications are discussed to sampling network traffic and social networks.",2014,Knowledge Discovery and Data Mining,Fields of study: samplingdata sciencetheoretical computer sciencedata miningmachine learningstatisticscomputer science
Spatially embedded co-offence prediction using supervised learning,Mohammad A. Tayebi (Simon Fraser University)Martin Ester (Simon Fraser University)Uwe Glässer (Simon Fraser University)Patricia L. Brantingham (Simon Fraser University),"2167566403,2067196623,205901731,1659196009","Crime reduction and prevention strategies are essential to increase public safety and reduce the crime costs to society. Law enforcement agencies have long realized the importance of analyzing co-offending networks---networks of offenders who have committed crimes together---for this purpose. Although network structure can contribute significantly to co-offence prediction, research in this area is very limited. Here we address this important problem by proposing a framework for co-offence prediction using supervised learning. Considering the available information about offenders, we introduce social, geographic, geo-social and similarity feature sets which are used for classifying potential negative and positive pairs of offenders. Similar to other social networks, co-offending networks also suffer from a highly skewed distribution of positive and negative pairs. To address the class imbalance problem, we identify three types of criminal cooperation opportunities which help to reduce the class imbalance ratio significantly, while keeping half of the co-offences. The proposed framework is evaluated on a large crime dataset for the Province of British Columbia, Canada. Our experimental evaluation of four different feature sets show that the novel geo-social features are the best predictors. Overall, we experimentally show the high effectiveness of the proposed co-offence prediction framework. We believe that our framework will not only allow law enforcement agencies to improve their crime reduction and prevention strategies, but also offers new criminological insights into criminal link formation between offenders.",2014,Knowledge Discovery and Data Mining,Fields of study: social networkcomputer securitydata miningmachine learning
Constructing and mining web-scale knowledge graphs: KDD 2014 tutorial,Antoine Bordes (Facebook)Evgeniy Gabrilovich (Google),"2691862885,1804802447","Recent years have witnessed a proliferation of large-scale knowledge graphs, such as Freebase, YAGO, Google's Knowledge Graph, and Microsoft's Satori. Whereas there is a large body of research on mining homogeneous graphs, this new generation of information networks are highly heterogeneous, with thousands of entity and relation types and billions of instances of vertices and edges. In this tutorial, we will present the state of the art in constructing, mining, and growing knowledge graphs. The purpose of the tutorial is to equip newcomers to this exciting field with an understanding of the basic concepts, tools and methodologies, available datasets, and open research challenges. A publicly available knowledge base (Freebase) will be used throughout the tutorial to exemplify the different techniques.",2014,Knowledge Discovery and Data Mining,Fields of study: data sciencetheoretical computer sciencedata miningmachine learningcomputer science
Topic-factorized ideal point estimation model for legislative voting network,Yupeng Gu (Northeastern University)Yizhou Sun (Northeastern University)Ning Jiang (University of Illinois at Urbana–Champaign)Bingyu Wang (Northeastern University)Ting Chen (Northeastern University),"2223659785,2131539564,2346080499,2222008448,2625853772","Ideal point estimation that estimates legislators' ideological positions and understands their voting behavior has attracted studies from political science and computer science. Typically, a legislator is assigned a global ideal point based on her voting or other social behavior. However, it is quite normal that people may have different positions on different policy dimensions. For example, some people may be more liberal on economic issues while more conservative on cultural issues. In this paper, we propose a novel topic-factorized ideal point estimation model for a legislative voting network in a unified framework. First, we model the ideal points of legislators and bills for each topic instead of assigning them to a global one. Second, the generation of topics are guided by the voting matrix in addition to the text information contained in bills. A unified model that combines voting behavior modeling and topic modeling is presented, and an iterative learning algorithm is proposed to learn the topics of bills as well as the topic-factorized ideal points of legislators and bills. By comparing with the state-of-the-art ideal point estimation models, our method has a much better explanation power in terms of held-out log-likelihood and other measures. Besides, case studies show that the topic-factorized ideal points coincide with human intuition. Finally, we illustrate how to use these topic-factorized ideal points to predict voting results for unseen bills.",2014,Knowledge Discovery and Data Mining,Fields of study: cardinal voting systemsdisapproval votinganti plurality votingcalculus of votingtopic modeldata miningmachine learningcomputer science
Networked bandits with disjoint linear payoffs,"Meng Fang (University of Technology, Sydney)Dacheng Tao (University of Technology, Sydney)","2152013896,2104129307","In this paper, we study `networked bandits', a new bandit problem where a set of interrelated arms varies over time and, given the contextual information that selects one arm, invokes other correlated arms. This problem remains under-investigated, in spite of its applicability to many practical problems. For instance, in social networks, an arm can obtain payoffs from both the selected user and its relations since they often share the content through the network. We examine whether it is possible to obtain multiple payoffs from several correlated arms based on the relationships. In particular, we formalize the networked bandit problem and propose an algorithm that considers not only the selected arm, but also the relationships between arms. Our algorithm is `optimism in face of uncertainty' style, in that it decides an arm depending on integrated confidence sets constructed from historical data. We analyze the performance in simulation experiments and on two real-world offline datasets. The experimental results demonstrate our algorithm's effectiveness in the networked bandit setting.",2014,Knowledge Discovery and Data Mining,Fields of study: social networkdistributed computingartificial intelligencemachine learningsimulationcomputer science
Modeling mass protest adoption in social network communities using geometric brownian motion,"Fang Jin (Virginia Tech)Rupinder Paul Khandpur (Virginia Tech)Nathan Self (Virginia Tech)Edward R. Dougherty (Virginia Tech)Sheng Guo (LinkedIn)Feng Chen (University at Albany, SUNY)B. Aditya Prakash (Virginia Tech)Naren Ramakrishnan (Virginia Tech)","2187881468,2054069009,2093704914,1963520773,2140742556,2601749234,2124002246,2199255697","Modeling the movement of information within social media outlets, like Twitter, is key to understanding to how ideas spread but quantifying such movement runs into several difficulties. Two specific areas that elude a clear characterization are (i) the intrinsic random nature of individuals to potentially adopt and subsequently broadcast a Twitter topic, and (ii) the dissemination of information via non-Twitter sources, such as news outlets and word of mouth, and its impact on Twitter propagation. These distinct yet inter-connected areas must be incorporated to generate a comprehensive model of information diffusion. We propose a bispace model to capture propagation in the union of (exclusively) Twitter and non-Twitter environments. To quantify the stochastic nature of Twitter topic propagation, we combine principles of geometric Brownian motion and traditional network graph theory. We apply Poisson process functions to model information diffusion outside of the Twitter mentions network. We discuss techniques to unify the two sub-models to accurately model information dissemination. We demonstrate the novel application of these techniques on real Twitter datasets related to mass protest adoption in social communities.",2014,Knowledge Discovery and Data Mining,Fields of study: geometric brownian motionsocial networkdata miningmachine learningsimulation
Up next: retrieval methods for large scale related video suggestion,Michael Bendersky (Google)Lluis Garcia-Pueyo (Google)Jeremiah Harmsen (Google)Vanja Josifovski (Google)Dima Lepikhin (Google),"2345232400,1975913520,2275660371,344688379,2224679871","The explosive growth in sharing and consumption of the video content on the web creates a unique opportunity for scientific advances in video retrieval, recommendation and discovery. In this paper, we focus on the task of video suggestion, commonly found in many online applications. The current state-of-the-art video suggestion techniques are based on the collaborative filtering analysis, and suggest videos that are likely to be co-viewed with the watched video. In this paper, we propose augmenting the collaborative filtering analysis with the topical representation of the video content to suggest related videos. We propose two novel methods for topical video representation. The first method uses information retrieval heuristics such as tf-idf, while the second method learns the optimal topical representations based on the implicit user feedback available in the online scenario. We conduct a large scale live experiment on YouTube traffic, and demonstrate that augmenting collaborative filtering with topical representations significantly improves the quality of the related video suggestions in a live setting, especially for categories with fresh and topically-rich video content such as news videos. In addition, we show that employing user feedback for learning the optimal topical video representations can increase the user engagement by more than 80% over the standard information retrieval representation, when compared to the collaborative filtering baseline.",2014,Knowledge Discovery and Data Mining,Fields of study: subjective video qualityvideo trackingmultimediaworld wide webinformation retrievalcomputer science
A bayesian framework for estimating properties of network diffusions,Varun R. Embar (IBM)Rama Kumar Pasumarthi (IBM)Indrajit Bhattacharya (IBM),"2225031596,2222739194,2117917354","The analysis of network connections, diffusion processes and cascades requires evaluating properties of the diffusion network. Properties of interest often involve variables that are not explicitly observed in real world diffusions. Connection strengths in the network and diffusion paths of infections over the network are examples of such hidden variables. These hidden variables therefore need to be estimated for these properties to be evaluated. In this paper, we propose and study this novel problem in a Bayesian framework by capturing the posterior distribution of these hidden variables given the observed cascades, and computing the expectation of these properties under this posterior distribution. We identify and characterize interesting network diffusion properties whose expectations can be computed exactly and efficiently, either wholly or in part. For properties that are not `nice' in this sense, we propose a Gibbs Sampling framework for Monte Carlo integration. In detailed experiments using various network diffusion properties over multiple synthetic and real datasets, we demonstrate that the proposed approach is significantly more accurate than a frequentist plug-in baseline. We also propose a map-reduce implementation of our framework and demonstrate that this can analyze cascades with millions of infections in minutes.",2014,Knowledge Discovery and Data Mining,Fields of study: information cascadegibbs samplingbayesian probabilityeconometricsmachine learningstatisticscomputer sciencemathematics
Sentiment expression conditioned by affective transitions and social forces,Moritz Sudhof (Stanford University)Andrés Goméz Emilsson (Stanford University)Andrew L. Maas (Stanford University)Christopher Potts (Stanford University),"2059246351,2230781588,2162130455,2114426036",-,2014,Knowledge Discovery and Data Mining,-
Fast influence-based coarsening for large networks,"Manish Purohit (University of Maryland, College Park)B. Aditya Prakash (Virginia Tech)Chanhyun Kang (University of Maryland, College Park)Yao Zhang (Virginia Tech)V.S. Subrahmanian (University of Maryland, College Park)","2234545523,2124002246,2102885882,2293580633,2261167843","Given a social network, can we quickly 'zoom-out' of the graph? Is there a smaller equivalent representation of the graph that preserves its propagation characteristics? Can we group nodes together based on their influence properties? These are important problems with applications to influence analysis, epidemiology and viral marketing applications. In this paper, we first formulate a novel Graph Coarsening Problem to find a succinct representation of any graph while preserving key characteristics for diffusion processes on that graph. We then provide a fast and effective near-linear-time (in nodes and edges) algorithm COARSENET for the same. Using extensive experiments on multiple real datasets, we demonstrate the quality and scalability of COARSENET, enabling us to reduce the graph by 90% in some cases without much loss of information. Finally we also show how our method can help in diverse applications like influence maximization and detecting patterns of propagation at the level of automatically created groups on real cascade data.",2014,Knowledge Discovery and Data Mining,Fields of study: moral graphrandom geometric graphdiffusiontheoretical computer sciencediscrete mathematicscombinatoricsmachine learningcomputer science
Prototype-based learning on concept-drifting data streams,Junming Shao (University of Mainz)Zahra Ahmadi (University of Mainz)Stefan Kramer (University of Mainz),"2109747452,2505307014,2272282606","Data stream mining has gained growing attentions due to its wide emerging applications such as target marketing, email filtering and network intrusion detection. In this paper, we propose a prototype-based classification model for evolving data streams, called SyncStream, which dynamically models time-changing concepts and makes predictions in a local fashion. Instead of learning a single model on a sliding window or ensemble learning, SyncStream captures evolving concepts by dynamically maintaining a set of prototypes in a new data structure called the P-tree. The prototypes are obtained by error-driven representativeness learning and synchronization-inspired constrained clustering. To identify abrupt concept drift in data streams, PCA and statistics based heuristic approaches are employed. SyncStream has several attractive benefits: (a) It is capable of dynamically modeling evolving concepts from even a small set of prototypes and is robust against noisy examples. (b) Owing to synchronization-based constrained clustering and the P-Tree, it supports an efficient and effective data representation and maintenance. (c) Gradual and abrupt concept drift can be effectively detected. Empirical results shows that our method achieves good predictive performance compared to state-of-the-art algorithms and that it requires much less time than another instance-based stream mining algorithm.",2014,Knowledge Discovery and Data Mining,Fields of study: concept driftsynchronizationbiological classificationdata stream miningdata sciencedata miningmachine learningcomputer science
Identifying tourists from public transport commuters,"Mingqiang Xue (Agency for Science, Technology and Research)Huayu Wu (Agency for Science, Technology and Research)Wei Chen (Agency for Science, Technology and Research)Wee Siong Ng (Agency for Science, Technology and Research)Gin Howe Goh (Land Transport Authority)","2303542671,2704714878,2666362508,2136598690,2223734089","Tourism industry has become a key economic driver for Singapore. Understanding the behaviors of tourists is very important for the government and private sectors, e.g., restaurants, hotels and advertising companies, to improve their existing services or create new business opportunities. In this joint work with Singapore's Land Transport Authority (LTA), we innovatively apply machine learning techniques to identity the tourists among public commuters using the public transportation data provided by LTA. On successful identification, the travelling patterns of tourists are then revealed and thus allow further analyses to be carried out such as on their favorite destinations, region of stay, etc. Technically, we model the tourists identification as a classification problem, and design an iterative learning algorithm to perform inference with limited prior knowledge and labeled data. We show the superiority of our algorithm with performance evaluation and comparison with other state-of-the-art learning algorithms. Further, we build an interactive web-based system for answering queries regarding the moving patterns of the tourists, which can be used by stakeholders to gain insight into tourists' travelling behaviors in Singapore.",2014,Knowledge Discovery and Data Mining,Fields of study: public transportdata analysissimulation
Representative clustering of uncertain data,Andreas Züfle (Ludwig Maximilian University of Munich)Tobias Emrich (Ludwig Maximilian University of Munich)Klaus Arthur Schmid (Ludwig Maximilian University of Munich)Nikos Mamoulis (University of Hong Kong)Arthur Zimek (Ludwig Maximilian University of Munich)Matthias Renz (Ludwig Maximilian University of Munich),"79808299,2056229408,2191395951,18851973,242745652,2150497105","This paper targets the problem of computing meaningful clusterings from uncertain data sets. Existing methods for clustering uncertain data compute a single clustering without any indication of its quality and reliability; thus, decisions based on their results are questionable. In this paper, we describe a framework, based on possible-worlds semantics; when applied on an uncertain dataset, it computes a set of representative clusterings, each of which has a probabilistic guarantee not to exceed some maximum distance to the ground truth clustering, i.e., the clustering of the actual (but unknown) data. Our framework can be combined with any existing clustering algorithm and it is the first to provide quality guarantees about its result. In addition, our experimental evaluation shows that our representative clusterings have a much smaller deviation from the ground truth clustering than existing approaches, thus reducing the effect of uncertainty.",2014,Knowledge Discovery and Data Mining,Fields of study: k medians clusteringflame clusteringbrown clusteringcanopy clustering algorithmdbscancorrelation clusteringconstrained clusteringdata stream clusteringcure data clustering algorithmaffinity propagationfuzzy clusteringpossible worldclustering high dimensional datacluster analysisconsensus clusteringbiclusteringconceptual clusteringdata miningpattern recognitionmachine learningcomputer sciencemathematics
Recommendation in social media: recent advances and new frontiers,Jiliang Tang (Arizona State University)Jie Tang (Tsinghua University)Huan Liu (Arizona State University),"2147392410,2158012360,2122391114","The pervasive use of social media generates massive data in an unprecedented rate and the information overload problem becomes increasingly severe for social media users. Recommendation has been proven to be effective in mitigating the information overload problem, demonstrated its strength in improving the quality of user experience, and positively impacted the success of social media. New types of data introduced by social media not only provide more information to advance traditional recommender systems but also manifest new research possibilities for recommendation. In this tutorial, we aim to provide a comprehensive overview of various recommendation tasks in social media, especially their recent advances and new frontiers. We introduce basic concepts, review state-of-the-art algorithms, and deliberate the emerging challenges and opportunities. Finally we summarize the tutorial with discussions on open issues and challenges about recommendation in social media. Updated information about the tutorial can be found at \url{http://www.public.asu.edu/~jtang20/Recommendation.htm}.",2014,Knowledge Discovery and Data Mining,Fields of study: multimediaworld wide webdata miningcomputer science
Multi-Touch Attribution Based Budget Allocation in Online Advertising,Sahin Cem GeyikAbhishek SaxenaAli Dasdan (Yahoo!),"2704095071,2028060928,4321789","Budget allocation in online advertising deals with distributing the campaign (insertion order) level budgets to different sub-campaigns which employ different targeting criteria and may perform differently in terms of return-on-investment (ROI). In this paper, we present the efforts at Turn on how to best allocate campaign budget so that the advertiser or campaign-level ROI is maximized. To do this, it is crucial to be able to correctly determine the performance of sub-campaigns. This determination is highly related to the action-attribution problem, i.e. to be able to find out the set of ads, and hence the sub-campaigns that provided them to a user, that an action should be attributed to. For this purpose, we employ both last-touch (last ad gets all credit) and multi-touch (many ads share the credit) attribution methodologies. We present the algorithms deployed at Turn for the attribution problem, as well as their parallel implementation on the large advertiser performance datasets. We conclude the paper with our empirical comparison of last-touch and multi-touch attribution-based budget allocation in a real online advertising setting.",2014,Knowledge Discovery and Data Mining,Fields of study: online advertisingsimulationcomputer science
"Optimal recommendations under attraction, aversion, and social influence",Wei Lu (University of British Columbia)Stratis Ioannidis (Technicolor)Smriti Bhagat (Technicolor)Laks V.S. Lakshmanan (University of British Columbia),"2596596230,2098025727,2291721935,2289816208","People's interests are dynamically evolving, often affected by external factors such as trends promoted by the media or adopted by their friends. In this work, we model interest evolution through dynamic interest cascades: we consider a scenario where a user's interests may be affected by (a) the interests of other users in her social circle, as well as (b) suggestions she receives from a recommender system. In the latter case, we model user reactions through either attraction or aversion towards past suggestions. We study this interest evolution process, and the utility accrued by recommendations, as a function of the system's recommendation strategy. We show that, in steady state, the optimal strategy can be computed as the solution of a semi-definite program (SDP). Using datasets of user ratings, we provide evidence for the existence of aversion and attraction in real-life data, and show that our optimal strategy can lead to significantly improved recommendations over systems that ignore aversion and attraction.",2014,Knowledge Discovery and Data Mining,Fields of study: attractivenessrecommender systemcomputer science
Mining text snippets for images on the web,Anitha Kannan (Microsoft)Simon Baker (Microsoft)Krishnan Ramnath (Microsoft)Juliet Fiss (University of Washington)Dahua Lin (Massachusetts Institute of Technology)Lucy Vanderwende (Microsoft)Rizwan Ansary (Microsoft)Ashish Kapoor (Microsoft)Qifa Ke (Microsoft)Matt Uyttendaele (Microsoft)Xin-Jing Wang (Microsoft)Lei Zhang (Microsoft),"2697361100,2105774983,2170464529,2223605241,2154770419,2006711911,2225733881,2161328394,2685828387,2500613571,2123395615,2560368284","Images are often used to convey many different concepts or illustrate many different stories. We propose an algorithm to mine multiple diverse, relevant, and interesting text snippets for images on the web. Our algorithm scales to all images on the web. For each image, all webpages that contain it are considered. The top-K text snippet selection problem is posed as combinatorial subset selection with the goal of choosing an optimal set of snippets that maximizes a combination of relevancy, interestingness, and diversity. The relevancy and interestingness are scored by machine learned models. Our algorithm is run at scale on the entire image index of a major search engine resulting in the construction of a database of images with their corresponding text snippets. We validate the quality of the database through a large-scale comparative study. We showcase the utility of the database through two web-scale applications: (a) augmentation of images on the web as webpages are browsed and (b)~an image browsing experience (similar in spirit to web browsing) that is enabled by interconnecting semantically related images (which may not be visually related) through shared concepts in their corresponding text snippets.",2014,Knowledge Discovery and Data Mining,Fields of study: relevanceworld wide webinformation retrievaldata miningcomputer science
Almost linear-time algorithms for adaptive betweenness centrality using hypergraph sketches,Yuichi Yoshida,2427933268,"Betweenness centrality measures the importance of a vertex by quantifying the number of times it acts as a midpoint of the shortest paths between other vertices. This measure is widely used in network analysis. In many applications, we wish to choose the k vertices with the maximum adaptive betweenness centrality, which is the betweenness centrality without considering the shortest paths that have been taken into account by already-chosen vertices. All previous methods are designed to compute the betweenness centrality in a fixed graph. Thus, to solve the above task, we have to run these methods $k$ times. In this paper, we present a method that directly solves the task, with an almost linear runtime no matter how large the value of k. Our method first constructs a hypergraph that encodes the betweenness centrality, and then computes the adaptive betweenness centrality by examining this graph. Our technique can be utilized to handle other centrality measures. We theoretically prove that our method is very accurate, and experimentally confirm that it is three orders of magnitude faster than previous methods. Relying on the scalability of our method, we experimentally demonstrate that strategies based on adaptive betweenness centrality are effective in important applications studied in the network science and database communities.",2014,Knowledge Discovery and Data Mining,Fields of study: random walk closeness centralityalpha centralitykatz centralitynetwork controllabilitycentralitynetwork sciencebetweenness centralityrandomized algorithmnetwork theorydiscrete mathematicscombinatoricsmachine learningcomputer sciencemathematics
Exponential random graph estimation under differential privacy,Wentian Lu (University of Massachusetts Amherst)Gerome Miklau (University of Massachusetts Amherst),"2100370277,2302554261","The effective analysis of social networks and graph-structured data is often limited by the privacy concerns of individuals whose data make up these networks. Differential privacy offers individuals a rigorous and appealing guarantee of privacy. But while differentially private algorithms for computing basic graph properties have been proposed, most graph modeling tasks common in the data mining community cannot yet be carried out privately. In this work we propose algorithms for privately estimating the parameters of exponential random graph models (ERGMs). We break the estimation problem into two steps: computing private sufficient statistics, then using them to estimate the model parameters. We consider specific alternating statistics that are in common use for ERGM models and describe a method for estimating them privately by adding noise proportional to a high-confidence bound on their local sensitivity. In addition, we propose an estimation algorithm that considers the noise distribution of the private statistics and offers better accuracy than performing standard parameter estimation using the private statistics.",2014,Knowledge Discovery and Data Mining,Fields of study: differential privacyexponential random graph modelsdata miningmachine learningstatisticscomputer science
Scalable near real-time failure localization of data center networks,Herodotos Herodotou (Microsoft)Bolin Ding (Microsoft)Shobana Balakrishnan (Microsoft)Geoff Outhred (Microsoft)Percy Fitter (Microsoft),"2304238510,2642048689,2154190106,1965751879,2230277177","Large-scale data center networks are complex---comprising several thousand network devices and several hundred thousand links---and form the critical infrastructure upon which all higher-level services depend on. Despite the built-in redundancy in data center networks, performance issues and device or link failures in the network can lead to user-perceived service interruptions. Therefore, determining and localizing user-impacting availability and performance issues in the network in near real time is crucial. Traditionally, both passive and active monitoring approaches have been used for failure localization. However, data from passive monitoring is often too noisy and does not effectively capture silent or gray failures, whereas active monitoring is potent in detecting faults but limited in its ability to isolate the exact fault location depending on its scale and granularity. Our key idea is to use statistical data mining techniques on large-scale active monitoring data to determine a ranked list of suspect causes, which we refine with passive monitoring signals. In particular, we compute a failure probability for devices and links in near real time using data from active monitoring, and look for statistically significant increases in the failure probability. We also correlate the probabilistic output with other failure signals from passive monitoring to increase the confidence of the probabilistic analysis. We have implemented our approach in the Windows Azure production environment and have validated its effectiveness in terms of localization accuracy, precision, and time to localization using known network incidents over the past three months. The correlated ranked list of devices and links is surfaced as a report that is used by network operators to investigate current issues and identify probable root causes.",2014,Knowledge Discovery and Data Mining,Fields of study: computer securitydata miningreal time computingmachine learningstatisticscomputer science
Efficient multi-task feature learning with calibration,Pinghua Gong (Arizona State University)Jiayu Zhou (Arizona State University)Wei Fan (Huawei)Jieping Ye (Arizona State University),"2152377557,2097879502,2422054197,2305258894","Multi-task feature learning has been proposed to improve the generalization performance by learning the shared features among multiple related tasks and it has been successfully applied to many real-world problems in machine learning, data mining, computer vision and bioinformatics. Most existing multi-task feature learning models simply assume a common noise level for all tasks, which may not be the case in real applications. Recently, a Calibrated Multivariate Regression (CMR) model has been proposed, which calibrates different tasks with respect to their noise levels and achieves superior prediction performance over the non-calibrated one. A major challenge is how to solve the CMR model efficiently as it is formulated as a composite optimization problem consisting of two non-smooth terms. In this paper, we propose a variant of the calibrated multi-task feature learning formulation by including a squared norm regularizer. We show that the dual problem of the proposed formulation is a smooth optimization problem with a piecewise sphere constraint. The simplicity of the dual problem enables us to develop fast dual optimization algorithms with low per-iteration cost. We also provide a detailed convergence analysis for the proposed dual optimization algorithm. Empirical studies demonstrate that, the dual optimization algorithm quickly converges and it is much more efficient than the primal optimization algorithm. Moreover, the calibrated multi-task feature learning algorithms with and without the squared norm regularizer achieve similar prediction performance and both outperform the non-calibrated ones. Thus, the proposed variant not only enables us to develop fast optimization algorithms, but also keeps the superior prediction performance of the calibrated multi-task feature learning over the non-calibrated one.",2014,Knowledge Discovery and Data Mining,Fields of study: multi task learninggeneralization errordualitycalibrationfeaturefeature learningfeature selectionartificial intelligencemachine learningmathematical optimizationcomputer sciencemathematics
The setwise stream classification problem,Charu C. Aggarwal (IBM),2146335907,"In many applications, classification labels may not be associated with a single instance of records, but may be associated with a data set of records. The class behavior may not be possible to infer effectively from a single record, but may be only be inferred by an aggregate set of records. Therefore, in this problem, the class label is associated with a set of instances both in the training and test data . Therefore, the problem may be understood to be that of classifying a set of data sets . Typically, the classification behavior may only be inferred from the overall patterns of data distribution, and very little information is embedded in any given record for classification purposes. We refer to this problem as the setwise classification problem. The problem can be extremely challenging in scenarios where the data is received in the form of a stream, and the records within any particular data set may not necessarily be received contiguously. In this paper, we present a first approach for real time and streaming classification of such data. We present experimental results illustrating the effectiveness of the approach.",2014,Knowledge Discovery and Data Mining,Fields of study: one class classificationdata miningdatabasepattern recognitionmachine learningcomputer science
LUDIA: an aggregate-constrained low-rank reconstruction algorithm to leverage publicly released health data,Yubin Park (University of Texas at Austin)Joydeep Ghosh (University of Texas at Austin),"2161098790,2148168557","In the past few years, the government and other agencies have publicly released a prodigious amount of data that can be potentially mined to benefit the society at large. However, data such as health records are typically only provided at aggregated levels (e.g. per State, per Hospital Referral Region, etc.) to protect privacy. Unfortunately aggregation can severely diminish the utility of such data when modeling or analysis is desired at a per-individual basis. So, not surprisingly, despite the increasing abundance of aggregate data, there have been very few successful attempts in exploiting them for individual-level analyses. This paper introduces LUDIA, a novel low-rank approximation algorithm that utilizes aggregation constraints in addition to auxiliary information in order to estimate or ""reconstruct"" the original individual-level values from aggregate data. If the reconstructed data are statistically similar to the original individual-level data, off-the-shelf individual-level models can be readily and reliably applied for subsequent predictive or descriptive analytics. LUDIA is more robust to nonlinear estimates and random effects than other reconstruction algorithms. It solves a Sylvester equation and leverages multi-level (also known as hierarchical or mixed-effect) modeling approaches efficiently. A novel graphical model is also introduced to provide a probabilistic viewpoint of LUDIA. Experimental results using a Texas inpatient dataset show that individual-level data can be reasonably reconstructed from county-, hospital-, and zip code-level aggregate data. Several factors affecting the reconstruction quality are discussed, along with the implications of this work for current aggregation guidelines.",2014,Knowledge Discovery and Data Mining,Fields of study: low rank approximationdata aggregatordata scienceeconometricsdata miningmachine learningstatisticscomputer science
Online chinese restaurant process,Chien-Liang Liu (Industrial Technology Research Institute)Tsung-Hsun Tsai (National Chiao Tung University)Chia-Hoang Lee (National Chiao Tung University),"2658412127,2631425500,2649609037","Processing large volumes of streaming data in near-real-time is becoming increasingly important as the Internet, sensor networks and network traffic grow. Online machine learning is a typical means of dealing with streaming data, since it allows the classification model to learn one instance of data at a time. Although many online learning methods have been developed since the development of the Perceptron algorithm, existing online methods assume that the number of classes is available in advance of classification process. However, this assumption is unrealistic for large scale or streaming data sets. This work proposes an online Chinese restaurant process (CRP) algorithm, which is an online and nonparametric algorithm, to tackle this problem. This work proposes a relaxing function as part of the prior and updates the parameters with the likelihood function in terms of the consistency between the true label information and predicted result. This work presents two Gibbs sampling algorithms to perform posterior inference. In the experiments, the online CRP is applied to three massive data sets, and compared with several online learning and batch learning algorithms. One of the data sets is obtained from Wikipedia, which comprises approximately two million documents. The experimental results reveal that the proposed online CRP performs well and efficiently on massive data sets. Finally, this work proposes two methods to update the hyperparameter $\alpha$ of the online CRP. The first method is based on the posterior distribution of $\alpha$, and the second exploits the property of online learning, namely adapting to change, to adjust $\alpha$ dynamically.",2014,Knowledge Discovery and Data Mining,Fields of study: online machine learningchinese restaurant processnonparametric statisticsadaptive learningdata sciencedata miningmachine learningstatisticscomputer science
Entity profiling with varying source reliabilities,Furong Li (National University of Singapore)Mong Li Lee (National University of Singapore)Wynne Hsu (National University of Singapore),"2265697202,2159408573,2123778117","The rapid growth of information sources on the Web has intensified the problem of data quality. In particular, the same real world entity may be described by different sources in various ways with overlapping information, and possibly conflicting or even erroneous values. In order to obtain a more complete and accurate picture for a real world entity, we need to collate the data records that refer to the entity, as well as correct any erroneous values. We observe that these two tasks are often tightly coupled: rectifying erroneous values will facilitate data collation, while linking similar records provides us with a clearer view of the data and additional evidence for error correction. In this paper, we present a framework called Comet that interleaves record linkage with error correction, taking into consideration the source reliabilities on various attributes. The proposed framework first utilizes confidence based matching to discriminate records in terms of ambiguity and source reliability. Then it performs adaptive matching to reduce the impact of erroneous values. Experiment results demonstrate that Comet outperforms the state-of-the-art techniques and is able to build complete and accurate profiles for real world entities.",2014,Knowledge Discovery and Data Mining,Fields of study: record linkageinformation retrievaldata miningdatabasestatisticscomputer science
Online multiple kernel regression,Doyen Sahoo (Singapore Management University)Steven C.H. Hoi (Singapore Management University)Bin Li (Wuhan University),"2160809552,108406206,2640472960","Kernel-based regression represents an important family of learning techniques for solving challenging regression tasks with non-linear patterns. Despite being studied extensively, most of the existing work suffers from two major drawbacks: (i) they are often designed for solving regression tasks in a batch learning setting, making them not only computationally inefficient and but also poorly scalable in real-world applications where data arrives sequentially; and (ii) they usually assume a fixed kernel function is given prior to the learning task, which could result in poor performance if the chosen kernel is inappropriate. To overcome these drawbacks, this paper presents a novel scheme of Online Multiple Kernel Regression (OMKR), which sequentially learns the kernel-based regressor in an online and scalable fashion, and dynamically explore a pool of multiple diverse kernels to avoid suffering from a single fixed poor kernel so as to remedy the drawback of manual/heuristic kernel selection. The OMKR problem is more challenging than regular kernel-based regression tasks since we have to on-the-fly determine both the optimal kernel-based regressor for each individual kernel and the best combination of the multiple kernel regressors. In this paper, we propose a family of OMKR algorithms for regression and discuss their application to time series prediction tasks. We also analyze the theoretical bounds of the proposed OMKR method and conduct extensive experiments to evaluate its empirical performance on both real-world regression and times series prediction tasks.",2014,Knowledge Discovery and Data Mining,Fields of study: kernel embedding of distributionstree kernelgraph kernelpolynomial kernelradial basis function kernelleast squares support vector machinekernel regressionkernel methodtime serieseconometricsdata miningpattern recognitionmachine learningstatisticscomputer sciencemathematics
ISIS: a networked-epidemiology based pervasive web app for infectious disease pandemic planning and response,Richard J. Beckman (Virginia Tech)Keith R. Bisset (Virginia Tech)Jiangzhuo Chen (Virginia Tech)Bryan L. Lewis (Virginia Tech)Madhav V. Marathe (Virginia Tech)Paula Elaine Stretz (Virginia Tech),"2165388458,2290866383,2140873248,2099398193,2242145496,701833623","We describe ISIS, a high-performance-computing-based application to support computational epidemiology of infectious diseases. ISIS has been developed over the last seven years in close coordination with public health and policy experts. It has been used in a number of important federal planning and response exercises. ISIS grew out of years of experience in developing and using HPC-oriented models of complex socially coupled systems. This identified the guiding principle that complex models will be used by domain experts only if they can do realistic analysis without becoming computing experts . Using ISIS, one can carry out detailed computational experiments as they pertain to planning and response in the event of a pandemic. ISIS is designed to support networked epidemiology -- study of epidemic processes over social contact networks. The current system can handle airborne infectious diseases such as influenza, pertussis, and smallpox. ISIS is comprised of the following basic components: ( i ) a web app that serves as the user-interface, ( ii ) a middleware that coordinates user interaction via the web app with backend models and databases, ( iii ) a backend computational modeling framework that is comprised of highly resolved epidemic simulations combined with highly realistic control strategies that include pharmaceutical as well as non-pharmaceutical interventions and ( iv ) a backend data management framework that manages complex unstructured and semi-structured data. ISIS has been used in over a dozen case studies defined by the DoD, DHHS, NIH, BARDA and NSC. We describe three recent studies illustrating the use of ISIS in real-world settings:( i ) uses of ISIS during the H1N1 pandemic, Cii ) supporting a US military planning exercise, and ( iii ) distribution of limited stockpile of pharmaceuticals using public and private outlets.",2014,Knowledge Discovery and Data Mining,Fields of study: web applicationpublic healthoperations researchdata miningsimulationcomputer science
Unifying learning to rank and domain adaptation: enabling cross-task document scoring,Mianwei Zhou (University of Illinois at Urbana–Champaign)Kevin Chen-Chuan Chang (University of Illinois at Urbana–Champaign),"2135817545,2096749051","For document scoring, although learning to rank and domain adaptation are treated as two different problems in previous works, we discover that they actually share the same challenge of adapting keyword contribution across different queries or domains. In this paper, we propose to study the cross-task document scoring problem, where a task refers to a query to rank or a domain to adapt to, as the first attempt to unify these two problems. Existing solutions for learning to rank and domain adaptation either leave the heavy burden of adapting keyword contribution to feature designers, or are difficult to be generalized. To resolve such limitations, we abstract the keyword scoring principle, pointing out that the contribution of a keyword essentially depends on, first, its importance to a task and, second, its importance to the document. For determining these two aspects of keyword importance, we further propose the concept of feature decoupling, suggesting using two types of easy-to-design features: meta-features and intra-features. Towards learning a scorer based on the decoupled features, we require that our framework fulfill inferred sparsity to eliminate the interference of noisy keywords, and employ distant supervision to tackle the lack of keyword labels. We propose the Tree-structured Boltzmann Machine (T-RBM), a novel two-stage Markov Network, as our solution. Experiments on three different applications confirm the effectiveness of T-RBM, which achieves significant improvement compared with four state-of-the-art baseline methods.",2014,Knowledge Discovery and Data Mining,Fields of study: learning to rankdata miningpattern recognitionmachine learningcomputer science
iPinYou Global RTB Bidding Algorithm Competition Dataset,Hairen LiaoLingxiao PengZhenchuan LiuXuehua Shen,"2227714011,2685645803,2629320924,2626085818","RTB (Real Time Bidding) is one of the most exciting developments in computational advertising in recent years. It drives transparency and efficiency in the display advertising ecosystem and facilitates the healthy growth of the display advertising industry. It enables advertisers to deliver the right message to the right person at the right time, publishers to better monetize their content by leveraging their website audience, and consumers to view relevant information through personalized ads. However, researchers in computational advertising area have been suffering from lack of publicly available datasets. iPinYou organizes a three-season global RTB algorithm competition in 2013. For each season, there is offline stage and online stage. On the offline stage, iPinYou releases a dataset for model training and reserves a dataset for testing. The dataset includes logs of ad biddings, impressions, clicks, and final conversions. After the whole competition ends, iPinYou organizes and releases all these three datasets for public use. These datasets can support experiments of some important research problems such as bid optimization and CTR estimation. To the best of our knowledge, this is the first publicly available dataset on RTB display advertising. In this paper, we give descriptions of these datasets to further boost the interests of computational advertising research community using this dataset.",2014,Knowledge Discovery and Data Mining,Fields of study: world wide webdata mining
Active-transductive learning with label-adapted kernels,Dan Kushnir (Alcatel-Lucent),2229137154,"This paper presents an efficient active-transductive approach for classification. A common approach of active learning algorithms is to focus on querying points near the class boundary in order to refine it. However, for certain data distributions, this approach has been shown to lead to uninformative samples. More recent approaches consider combining data exploration with traditional refinement techniques. These techniques typically require tuning sampling of unexplored regions with refinement of detected class boundaries. They also involve significant computational costs for the exploration of informative query candidates. We present a novel iterative active learning algorithm designed to overcome these shortcomings by using a linear running-time active-transductive learning approach that naturally switches from exploration to refinement. The passive classifier employed in our algorithm builds a random-walk on the data graph based on a modified graph geometry that combines the data distribution with current label hypothesis; while the query component uses the uncertainty of the evolving hypothesis. Our supporting theory draws the link between the spectral properties of our iteration matrix and a solution to the minimal-cut problem for a fused hypothesis-data graph. Experiments demonstrate computational complexity that is orders of magnitude lower than state-of-the-art, and competitive results on benchmark data and real churn prediction data.",2014,Knowledge Discovery and Data Mining,Fields of study: transductionactive learninggraphbiological classificationactive learningsemi supervised learningdata miningpattern recognitionmachine learningstatisticscomputer sciencemathematics
The battle for the future of data mining,Oren Etzioni (University of Washington),57747768,"Deep learning has catapulted to the front page of the New York Times, formed the core of the so-called 'Google brain', and achieved impressive results in vision, speech recognition, and elsewhere. Yet researchers have offered simple conundrums that deep learning doesn't address. For example, consider the sentence: 'The large ball crashed right through the table because it was made of Styrofoam.' What was made of Styrofoam? The large ball? Or the table? The answer is obviously 'the table', but if we change the word 'Styrofoam' to 'steel', the answer is clearly 'the large ball'. To automatically answer this type of question, our computers require an extensive body of knowledge. We believe that text mining can provide the requisite body of knowledge. My talk will describe work at the new Allen Institute for AI towards building the next-generation of text-mining systems.",2014,Knowledge Discovery and Data Mining,Fields of study: deep learningtext miningdata sciencedata miningartificial intelligencemachine learningcomputer science
Budget pacing for targeted online advertisements at LinkedIn,Deepak Agarwal (LinkedIn)Souvik Ghosh (LinkedIn)Kai Wei (LinkedIn)Siyu You (LinkedIn),"2116605949,2097018069,2140564781,2231625825",Targeted online advertising is a prime source of revenue for many Internet companies. It is a common industry practice to use a generalized second price auction mechanism to rank advertisements at every opportunity of an impression. This greedy algorithm is suboptimal for both advertisers and publishers when advertisers have a finite budget. In a greedy mechanism high performing advertisers tend to drop out of the auction marketplace fast and that adversely affects both the advertiser experience and the publisher revenue. We describe a method for improving such ad serving systems by including a budget pacing component that serves ads by being aware of global supply patterns. Such a system is beneficial for both advertisers and publishers. We demonstrate the benefits of this component using experiments we conducted on advertising at LinkedIn.,2014,Knowledge Discovery and Data Mining,Fields of study: share of voicegeneralized second price auction
Learning with dual heterogeneity: a nonparametric bayes model,Hongxia Yang (IBM)Jingrui He (Arizona State University),"2309208648,2693123770","Traditional data mining techniques are designed to model a single type of heterogeneity, such as multi-task learning for modeling task heterogeneity, multi-view learning for modeling view heterogeneity, etc. Recently, a variety of real applications emerged, which exhibit dual heterogeneity, namely both task heterogeneity and view heterogeneity. Examples include insider threat detection across multiple organizations, web image classification in different domains, etc. Existing methods for addressing such problems typically assume that multiple tasks are equally related and multiple views are equally consistent, which limits their application in complex settings with varying task relatedness and view consistency. In this paper, we advance state-of-the-art techniques by adaptively modeling task relatedness and view consistency via a nonparametric Bayes model: we model task relatedness using normal penalty with sparse covariances, and view consistency using matrix Dirichlet process. Based on this model, we propose the NOBLE algorithm using an efficient Gibbs sampler. Experimental results on multiple real data sets demonstrate the effectiveness of the proposed algorithm.",2014,Knowledge Discovery and Data Mining,Fields of study: gibbs samplingeconometricsdata miningmachine learningstatisticscomputer science
Proactive workflow modeling by stochastic processes with application to healthcare operation and management,Chuanren Liu (Rutgers–Newark)Yong Ge (University of North Carolina at Charlotte)Hui Xiong (Rutgers–Newark)Keli Xiao (Stony Brook University)Wei GengMatt Perkins,"2169554947,2218492437,2153710278,2241204989,2482595915,2678238573","Advances in real-time location system (RTLS) solutions have enabled us to collect massive amounts of fine-grained semantically rich location traces, which provide unparalleled opportunities for understanding human activities and discovering useful knowledge. This, in turn, delivers intelligence for real-time decision making in various fields, such as workflow management. Indeed, it is a new paradigm for workflow modeling by the knowledge discovery in location traces. To that end, in this paper, we provide a focused study of workflow modeling by the integrated analysis of indoor location traces in the hospital environment. In comparison with conventional workflow modeling based on passive workflow logs, one salient feature of our approach is that it can proactively unravel the workflow patterns hidden in the location traces, by automatically constructing the workflow states and estimating parameters describing the transition patterns of moving objects. Specifically, to determine a meaningful granularity for the model, the workflow states are first constructed as regions associated with specific healthcare activities. Then, we transform the original indoor location traces to the sequences of workflow states and model the workflow transition patterns by finite state machines. Furthermore, we leverage the correlations in the location traces between related types of medical devices to reinforce the modeling performance and enable more applications. The results show that the proposed framework can not only model the workflow patterns effectively, but also have managerial applications in workflow monitoring, auditing, and inspection of workflow compliance, which are critical in the healthcare industry.",2014,Knowledge Discovery and Data Mining,Fields of study: workflow technologyworkflow engineworkflow management systemworkflowdata scienceknowledge managementdata mining
Data science through the lens of social science,Drew Conway,2676238593,"In this talk, Drew will examine data science through the lens of the social scientist. He will discuss how the various skills and disciplines combine into data science. Drew will also present a motivating example directly from his work as a senior advisor to NYC's Mayor's Office of Analytics.",2014,Knowledge Discovery and Data Mining,Fields of study: outline of social sciencesocial science educationscience educationdata sciencedata miningcomputer science
Predicting long-term impact of CQA posts: a comprehensive viewpoint,Yuan Yao (Nanjing University)Hanghang Tong (Arizona State University)Feng Xu (Nanjing University)Jian Lu (Nanjing University),"2617797049,2667261544,2720220320,2097600081","Community Question Answering (CQA) sites have become valuable platforms to create, share, and seek a massive volume of human knowledge. How can we spot an insightful question that would inspire massive further discussions in CQA sites? How can we detect a valuable answer that benefits many users? The long-term impact (e.g., the size of the population a post benefits) of a question/answer post is the key quantity to answer these questions. In this paper, we aim to predict the long-term impact of questions/answers shortly after they are posted in the CQA sites. In particular, we propose a family of algorithms for the prediction problem by modeling three key aspects, i.e., non-linearity, question/answer coupling, and dynamics. We analyze our algorithms in terms of optimality, correctness, and complexity. We conduct extensive experimental evaluations on two real CQA data sets to demonstrate the effectiveness and efficiency of our algorithms.",2014,Knowledge Discovery and Data Mining,Fields of study: question answeringdata scienceworld wide webdata miningcomputer science
A Dynamic Pricing Model for Unifying Programmatic Guarantee and Real-Time Bidding in Display Advertising,Bowei Chen (University College London)Shuai Yuan (University College London)Jun Wang (University College London),"2224779522,2131459235,2557836567","There are two major ways of selling impressions in display advertising. They are either sold in spot through auction mechanisms or in advance via guaranteed contracts. The former has achieved a significant automation via real-time bidding (RTB); however, the latter is still mainly done over the counter through direct sales. This paper proposes a mathematical model that allocates and prices the future impressions between real-time auctions and guaranteed contracts. Under conventional economic assumptions, our model shows that the two ways can be seamless combined programmatically and the publisher's revenue can be maximized via price discrimination and optimal allocation. We consider advertisers are risk-averse, and they would be willing to purchase guaranteed impressions if the total costs are less than their private values. We also consider that an advertiser's purchase behavior can be affected by both the guaranteed price and the time interval between the purchase time and the impression delivery date. Our solution suggests an optimal percentage of future impressions to sell in advance and provides an explicit formula to calculate at what prices to sell. We find that the optimal guaranteed prices are dynamic and are non-decreasing over time. We evaluate our method with RTB datasets and find that the model adopts different strategies in allocation and pricing according to the level of competition. From the experiments we find that, in a less competitive market, lower prices of the guaranteed contracts will encourage the purchase in advance and the revenue gain is mainly contributed by the increased competition in future RTB. In a highly competitive market, advertisers are more willing to purchase the guaranteed contracts and thus higher prices are expected. The revenue gain is largely contributed by the guaranteed selling.",2014,Knowledge Discovery and Data Mining,Fields of study: mathematical economicsadvertisingoperations managementmicroeconomicsmarketingeconomics
Modeling impression discounting in large-scale recommender systems,Pei Lee (University of British Columbia)Laks V.S. Lakshmanan (University of British Columbia)Mitul Tiwari (LinkedIn)Sam Shah (LinkedIn),"2479271178,2289816208,2296043577,2132445812","Recommender systems have become very important for many online activities, such as watching movies, shopping for products, and connecting with friends on social networks. User behavioral analysis and user feedback (both explicit and implicit) modeling are crucial for the improvement of any online recommender system. Widely adopted recommender systems at LinkedIn such as ""People You May Know"" and ""Endorsements"" are evolving by analyzing user behaviors on impressed recommendation items. In this paper, we address modeling impression discounting of recommended items, that is, how to model user's no-action feedback on impressed recommended items. The main contributions of this paper include (1) large-scale analysis of impression data from LinkedIn and KDD Cup; (2) novel anti-noise regression techniques, and its application to learn four different impression discounting functions including linear decay, inverse decay, exponential decay, and quadratic decay; (3) applying these impression discounting functions to LinkedIn's ""People You May Know"" and ""Endorsements"" recommender systems.",2014,Knowledge Discovery and Data Mining,Fields of study: recommender systemmultimediamachine learningcomputer science
"Pleasing the advertising oracle: Probabilistic prediction from sampled, aggregated ground truth",Melinda Han WilliamsClaudia Perlich (IBM)Brian DalessandroFoster J. Provost (New York University Stern School of Business),"2507887416,164824025,1876520739,2158932634","Most video advertising campaigns today are still evaluated based on aggregate demographic audience metrics, rather than measures of individual impact or even individual demographic reach. To fit in with advertisers' evaluations, campaigns must be optimized toward validation by third-party measurement companies, which act as ""oracles"" in assessing ground truth. However, information is only available from such oracles in aggregate, leading to a setting with incomplete ground truth. We explore methods for building probabilistic classification models using these aggregate data. If they perform well, such models can be used to create new ""engineered"" segments that perform better than existing segments, in terms of lift and/or reach. We focus on the setting where companies already have machinery in place for high-performance predictive modeling from traditional, individual-level data. We show that model building, evaluation, and selection can be reliably carried out even with access only to aggregate ground truth data. We show various concrete results, highlighting confounding aspects of the problem, such as the tendency for pre-existing ""in-target"" segments actually to comprise biased subpopulations, which has implications both for campaign performance and modeling performance. The paper's main results show that these methods lead to engineered segments that can substantially improve lift and/or reach---as verified by a leading third-party oracle. For example, for lifts of 2-3X, segment reach can be increased to 57 times that of comparable, pre-existing segments.",2014,Knowledge Discovery and Data Mining,Fields of study: logistic regressiononline advertisingdata miningmachine learningsimulationstatisticscomputer science
Unveiling clusters of events for alert and incident management in large-scale enterprise it,Derek LinRashmi RaghuVivek RamamurthyJin YuRegunathan RadhakrishnanJoseph Fernandez,"2717748394,2704564976,2658720289,2312102778,2718219179,2559347692","Large enterprise IT (Information Technology) infrastructure components generate large volumes of alerts and incident tickets. These are manually screened, but it is otherwise difficult to extract information automatically from them to gain insights in order to improve operational efficiency. We propose a framework to cluster alerts and incident tickets based on the text in them, using unsupervised machine learning. This would be a step towards eliminating manual classification of the alerts and incidents, which is very labor intense and costly. Our framework can handle the semi-structured text in alerts generated by IT infrastructure components such as storage devices, network devices, servers etc., as well as the unstructured text in incident tickets created manually by operations support personnel. After text pre-processing and application of appropriate distance metrics, we apply different graph-theoretic approaches to cluster the alerts and incident tickets, based on their semi-structured and unstructured text respectively. For automated interpretation and read-ability on semi-structured text clusters, we propose a method to visualize clusters that preserves the structure and human-readability of the text data as compared to traditional word clouds where the text structure is not preserved; for unstructured text clusters, we find a simple way to define prototypes of clusters for easy interpretation. This framework for clustering and visualization will enable enterprises to prioritize the issues in their IT infrastructure and improve the reliability and availability of their services.",2014,Knowledge Discovery and Data Mining,Fields of study: complete linkageconnected componentcutnon negative matrix factorizationk d treehierarchical clusteringworld wide webdata miningdatabasemachine learningcomputer science
"Data, predictions, and decisions in support of people and society",Eric Horvitz (Microsoft),1970391018,"Deep societal benefits will spring from advances in data availability and in computational procedures for mining insights and inferences from large data sets. I will describe efforts to harness data for making predictions and guiding decisions, touching on work in transportation, healthcare, online services, and interactive systems. I will start with efforts to learn and field predictive models that forecast flows of traffic in greater city regions. Moving from the ground to the air, I will discuss fusing data from aircraft to make inferences about atmospheric conditions and using these results to enhance air transport. I will then focus on experiences with building and fielding predictive models in clinical medicine. I will show how inferences about outcomes and interventions can provide insights and guide decision making. Moving beyond data captured by hospitals, I will discuss the promise of transforming anonymized behavioral data drawn from web services into large-scale sensor networks for public health, including efforts to identify adverse effects of medications and to understand illness in populations. I will conclude by describing how we can use machine learning to leverage the complementarity of human and machine intellect to solve challenging problems in science and society.",2014,Knowledge Discovery and Data Mining,Fields of study: graphical modeltransportdecision analysismanagement sciencedata sciencedata miningmachine learningstatisticscomputer science
Inside the atoms: ranking on a network of networks,Jingchao Ni (Case Western Reserve University)Hanghang Tong (Arizona State University)Wei Fan (Huawei)Xiang Zhang (Case Western Reserve University),"2223717457,2667261544,2422054197,2553248206","Networks are prevalent and have posed many fascinating research questions. How can we spot similar users, e.g., virtual identical twins, in Cleveland for a New Yorker? Given a query disease, how can we prioritize its candidate genes by incorporating the tissue-specific protein interaction networks of those similar diseases? In most, if not all, of the existing network ranking methods, the nodes are the ranking objects with the finest granularity. In this paper, we propose a new network data model, a Network of Networks (NoN), where each node of the main network itself can be further represented as another (domain-specific) network. This new data model enables to compare the nodes in a broader context and rank them at a finer granularity. Moreover, such an NoN model enables much more efficient search when the ranking targets reside in a certain domain-specific network. We formulate ranking on NoN as a regularized optimization problem; propose efficient algorithms and provide theoretical analysis, such as optimality, convergence, complexity and equivalence. Extensive experimental evaluations demonstrate the effectiveness and the efficiency of our methods.",2014,Knowledge Discovery and Data Mining,Fields of study: rankingevolving networksdynamic network analysisnetwork formationrankingnetwork simulationtheoretical computer sciencedata miningmachine learningstatisticscomputer science
Empirical glitch explanations,Tamraparni Dasu (AT&T Labs)Ji Meng Loh (New Jersey Institute of Technology)Divesh Srivastava (AT&T Labs),"425249397,2124479687,2196778472","Data glitches are unusual observations that do not conform to data quality expectations, be they logical, semantic or statistical. By applying data integrity constraints, potentially large sections of data could be flagged as being noncompliant. Ignoring or repairing significant sections of the data could fundamentally bias the results and conclusions drawn from analyses. In the context of Big Data where large numbers and volumes of feeds from disparate sources are integrated, it is likely that significant portions of seemingly noncompliant data are actually legitimate usable data. In this paper, we introduce the notion of Empirical Glitch Explanations - concise, multi-dimensional descriptions of subsets of potentially dirty data - and propose a scalable method for empirically generating such explanatory characterizations. The explanations could serve two valuable functions: (1) Provide a way of identifying legitimate data and releasing it back into the pool of clean data. In doing so, we reduce cleaning-related statistical distortion of the data; (2) Used to refine existing data quality constraints and generate and formalize domain knowledge. We conduct experiments using real and simulated data to demonstrate the scalability of our method and the robustness of explanations. In addition, we use two real world examples to demonstrate the utility of the explanations where we reclaim over 99% of the suspicious data, keeping data repair related statistical distortion close to 0.",2014,Knowledge Discovery and Data Mining,Fields of study: data qualitydata miningdatabasestatisticsalgorithmcomputer science
Correlation clustering: from theory to practice,Francesco Bonchi (Yahoo!)David Garcia-Soriano (Yahoo!)Edo Liberty (Yahoo!),"2176652147,45643653,1215165747","Correlation clustering is arguably the most natural formulation of clustering. Given a set of objects and a pairwise similarity measure between them, the goal is to cluster the objects so that, to the best possible extent, similar objects are put in the same cluster and dissimilar objects are put in different clusters. As it just needs a definition of similarity, its broad generality makes it applicable to a wide range of problems in different contexts, and in particular makes it naturally suitable to clustering structured objects for which feature vectors can be difficult to obtain. Despite its simplicity, generality and wide applicability, correlation clustering has so far received much more attention from the algorithmic theory community than from the data mining community. The goal of this tutorial is to show how correlation clustering can be a powerful addition to the toolkit of the data mining researcher and practitioner, and to encourage discussions and further research in the area. In the tutorial we will survey the problem and its most common variants, with an emphasis on the algorithmic techniques and key ideas developed to derive efficient solutions. We will motivate the problems and discuss real-world applications, the scalability issues that may arise, and the existing approaches to handle them.",2014,Knowledge Discovery and Data Mining,Fields of study: brown clusteringconstrained clusteringfuzzy clusteringclustering high dimensional datacluster analysisconsensus clusteringdata sciencedata miningmachine learningstatisticscomputer sciencemathematics
An efficient algorithm for weak hierarchical lasso,Yashu Liu (Arizona State University)Jie Wang (Arizona State University)Jieping Ye (Arizona State University),"2658031297,2471893859,2305258894","Linear regression is a widely used tool in data mining and machine learning. In many applications, fitting a regression model with only linear effects may not be sufficient for predictive or explanatory purposes. One strategy which has recently received increasing attention in statistics is to include feature interactions to capture the nonlinearity in the regression model. Such model has been applied successfully in many biomedical applications. One major challenge in the use of such model is that the data dimensionality is significantly higher than the original data, resulting in the small sample size large dimension problem. Recently, weak hierarchical Lasso, a sparse interaction regression model, is proposed that produces sparse and hierarchical structured estimator by exploiting the Lasso penalty and a set of hierarchical constraints. However, the hierarchical constraints make it a non-convex problem and the existing method finds the solution of its convex relaxation, which needs additional conditions to guarantee the hierarchical structure. In this paper, we propose to directly solve the non-convex weak hierarchical Lasso by making use of the GIST (General Iterative Shrinkage and Thresholding) optimization framework which has been shown to be efficient for solving non-convex sparse formulations. The key step in GIST is to compute a sequence of proximal operators. One of our key technical contributions is to show that the proximal operator associated with the non-convex weak hierarchical Lasso admits a closed form solution. However, a naive approach for solving each subproblem of the proximal operator leads to a quadratic time complexity, which is not desirable for large size problems. To this end, we further develop an efficient algorithm for computing the subproblems with a linearithmic time complexity. We have conducted extensive experiments on both synthetic and real data sets. Results show that our proposed algorithm is much more efficient and effective than its convex relaxation.",2014,Knowledge Discovery and Data Mining,Fields of study: machine learningmathematical optimizationstatisticsmathematics
Provable deterministic leverage score sampling,Dimitris S. Papailiopoulos (University of Texas at Austin)Anastasios Kyrillidis (École Polytechnique Fédérale de Lausanne)Christos Boutsidis (Yahoo!),"2685211751,2288456018,180556512","We explain theoretically a curious empirical phenomenon: ""Approximating a matrix by deterministically selecting a subset of its columns with the corresponding largest leverage scores results in a good low-rank matrix surrogate"". In this work, we provide a novel theoretical analysis of deterministic leverage score sampling. We show that such sampling can be provably as accurate as its randomized counterparts, if the leverage scores follow a moderately steep power-law decay. We support this power-law assumption by providing empirical evidence that such decay laws are abundant in real-world data sets. We then demonstrate empirically the performance of deterministic leverage score sampling, which many times matches or outperforms the state-of-the-art techniques.",2014,Knowledge Discovery and Data Mining,Fields of study: leverageeconometricsmathematical optimizationstatisticsmathematics
Safe and efficient screening for sparse support vector machine,Zheng Zhao (SAS Institute)Jun Liu (SAS Institute)James Cox (SAS Institute),"2564693768,2717490000,2123483863","Sparse support vector machine (SVM) is a robust predictive model that can effectively remove noise and preserve signals. Like Lasso, it can efficiently learn a solution path based on a set of predefined parameters and therefore provides strong support for model selection. Sparse SVM has been successfully applied in a variety of data mining applications including text mining, bioinformatics, and image processing. The emergence of big-data analysis poses new challenges for model selection with large-scale data that consist of tens of millions samples and features. In this paper, a novel screening technique is proposed to accelerate model selection for l 1 -regularized l 2 -SVM and effectively improve its scalability. This technique can precisely identify inactive features in the optimal solution of a l 1 -regularized l 2 -SVM model and remove them before training. The technique makes use of the variational inequality and provides a closed-form solution for screening inactive features in different situations. Every feature that is removed by the screening technique is guaranteed to be inactive in the optimal solution. Therefore, when l 1 -regularized l 2 -SVM uses the features selected by the technique, it achieves exactly the same result as when it uses the full feature set. Because the technique can remove a large number of inactive features, it can greatly increase the efficiency of model selection for l 1 -regularized l 2 -SVM. Experimental results on five high-dimensional benchmark data sets demonstrate the power of the proposed technique.",2014,Knowledge Discovery and Data Mining,Fields of study: electric field screeningfeature selectiondata miningpattern recognitionmachine learningcomputer science
SMVC: semi-supervised multi-view clustering in subspace projections,Stephan Günnemann (Carnegie Mellon University)Ines Färber (RWTH Aachen University)Matthias Rüdiger (RWTH Aachen University)Thomas Seidl (RWTH Aachen University),"316694267,2015027442,2223599623,2140301036","Since data is often multi-faceted in its very nature, it might not adequately be summarized by just a single clustering. To better capture the data's complexity, methods aiming at the detection of multiple, alternative clusterings have been proposed. Independent of this research area, semi-supervised clustering techniques have shown to substantially improve clustering results for single-view clustering by integrating prior knowledge. In this paper, we join both research areas and present a solution for integrating prior knowledge in the process of detecting multiple clusterings. We propose a Bayesian framework modeling multiple clusterings of the data by multiple mixture distributions, each responsible for an individual set of relevant dimensions. In addition, our model is able to handle prior knowledge in the form of instance-level constraints indicating which objects should or should not be grouped together. Since a priori the assignment of constraints to specific views is not necessarily known, our technique automatically determines their membership. For efficient learning, we propose the algorithm SMVC using variational Bayesian methods. With experiments on various real-world data, we demonstrate SMVC's potential to detect multiple clustering views and its capability to improve the result by exploiting prior knowledge.",2014,Knowledge Discovery and Data Mining,Fields of study: canopy clustering algorithmcorrelation clusteringconstrained clusteringdata stream clusteringcure data clustering algorithmfuzzy clusteringclustering high dimensional datacluster analysissemi supervised learningdata miningpattern recognitionmachine learningcomputer sciencemathematics
Scaling out big data missing value imputations: pythia vs. godzilla,Christos Anagnostopoulos (University of Glasgow)Peter Triantafillou (University of Glasgow),"2070810372,318333653","Solving the missing-value (MV) problem with small estimation errors in big data environments is a notoriously resource-demanding task. As datasets and their user community continuously grow, the problem can only be exacerbated. Assume that it is possible to have a single machine (`Godzilla'), which can store the massive dataset and support an ever-growing community submitting MV imputation requests. Is it possible to replace Godzilla by employing a large number of cohort machines so that imputations can be performed much faster, engaging cohorts in parallel, each of which accesses much smaller partitions of the original dataset? If so, it would be preferable for obvious performance reasons to access only a subset of all cohorts per imputation. In this case, can we decide swiftly which is the desired subset of cohorts to engage per imputation? But efficiency and scalability is just one key concern! Is it possible to do the above while ensuring comparable or even better than Godzilla's imputation estimation errors? In this paper we derive answers to these fundamentals questions and develop principled methods and a framework which offer large performance speed-ups and better, or comparable, errors to that of Godzilla, independently of which missing-value imputation algorithm is used. Our contributions involve Pythia, a framework and algorithms for providing the answers to the above questions and for engaging the appropriate subset of cohorts per MV imputation request. Pythia functionality rests on two pillars: (i) dataset (partition) signatures, one per cohort, and (ii) similarity notions and algorithms, which can identify the appropriate subset of cohorts to engage. Comprehensive experimentation with real and synthetic datasets showcase our efficiency, scalability, and accuracy claims.",2014,Knowledge Discovery and Data Mining,Fields of study: imputationcluster analysisbig datadata sciencedata miningmachine learningstatisticscomputer science
"LWI-SVD: low-rank, windowed, incremental singular value decompositions on time-evolving data sets",Xilun Chen (Arizona State University)K. Selcuk Candan (Arizona State University),"2224976689,674992784","Singular Value Decomposition (SVD) is computationally costly and therefore a naive implementation does not scale to the needs of scenarios where data evolves continuously. While there are various on-line analysis and incremental decomposition techniques, these may not accurately represent the data or may be slow for the needs of many applications. To address these challenges, in this paper, we propose a Low-rank, Windowed, Incremental SVD (LWI-SVD) algorithm, which (a) leverages efficient and accurate low-rank approximations to speed up incremental SVD updates and (b) uses a window-based approach to aggregate multiple incoming updates (insertions or deletions of rows and columns) and, thus, reduces on- line processing costs. We also present an LWI-SVD with restarts (LWI2-SVD) algorithm which leverages a novel highly efficient partial reconstruction based change detection scheme to support timely refreshing of the decomposition with significant changes in the data and prevent accumulation of errors over time. Experiment results, including comparisons to other state of the art techniques on different data sets and under different parameter settings, confirm that LWI-SVD and LWI2-SVD are both efficient and accurate in maintaining decompositions.",2014,Knowledge Discovery and Data Mining,Fields of study: theoretical computer sciencedata miningmachine learningmathematical optimizationcomputer science
Improved testing of low rank matrices,Yi Li (Max Planck Society)Zhengyu Wang (Tsinghua University)David P. Woodruff (IBM),"2703121277,2113965138,2142501412","We study the problem of determining if an input matrix A e R m x n can be well-approximated by a low rank matrix. Specifically, we study the problem of quickly estimating the rank or stable rank of A , the latter often providing a more robust measure of the rank. Since we seek significantly sublinear time algorithms, we cast these problems in the property testing framework. In this framework, A either has low rank or stable rank, or is far from having this property. The algorithm should read only a small number of entries or rows of A and decide which case A is in with high probability. If neither case occurs, the output is allowed to be arbitrary. We consider two notions of being far: (1) A requires changing at least an e-fraction of its entries, or (2) A requires changing at least an e-fraction of its rows. We call the former the ""entry model"" and the latter the ""row model"". We show: For testing if a matrix has rank at most d in the entry model, we improve the previous number of entries of A that need to be read from O ( d 2 /e 2 ) (Krauthgamer and Sasson, SODA 2003) to O (d 2 /e). Our algorithm is the first to adaptively query the entries of A , which for constant d we show is necessary to achieve O (1/e) queries. For the important case of d = 1 we also give a new non-adaptive algorithm, improving the previous O (1/e 2 ) queries to O (log 2 (1/e) / e). For testing if a matrix has rank at most d in the row model, we prove an Ω( d /e) lower bound on the number of rows that need to be read, even for adaptive algorithms. Our lower bound matches a non-adaptive upper bound of Krauthgamer and Sasson. For testing if a matrix has stable rank at most d in the row model or requires changing an e/ d -fraction of its rows in order to have stable rank at most d , we prove that reading θ( d /e 2 ) rows is necessary and sufficient. We also give an empirical evaluation of our rank and stable rank algorithms on real and synthetic datasets.",2014,Knowledge Discovery and Data Mining,Fields of study: rankmean reciprocal rankproperty testingrankdimensionality reductionprincipal component analysisrobustnessdiscrete mathematicscombinatoricsmachine learningstatisticscomputer sciencemathematics
Management and analytic of biomedical big data with cloud-based in-memory database and dynamic querying: a hands-on experience with real-world data,Mengling Feng (Massachusetts Institute of Technology)Mohammad M. Ghassemi (Massachusetts Institute of Technology)Thomas Brennan (Massachusetts Institute of Technology)John EllenbergerIshrar HussainRoger G. Mark (Massachusetts Institute of Technology),"2115837136,2142459951,2090051592,2659158852,2689068858,2155871323","Analyzing Biomedical Big Data (BBD) is computationally expensive due to high dimensionality and large data volume. Performance and scalability issues of traditional database management systems (DBMS) often limit the usage of more sophisticated and complex data queries and analytic models. Moreover, in the conventional setting, data management and analysis use separate software platforms. Exporting and importing large amounts of data across platforms require a significant amount of computational and I/O resources, as well as potentially putting sensitive data at a security risk. In this tutorial, the participants will learn the difference between in-memory DBMS and traditional DBMS through hands-on exercises using SAP's cloud-based HANA in-memory DBMS in conjunction with the Multi-parameter Intelligent Monitoring in Intensive Care (MIMIC) dataset. MIMIC is an open-access critical care EHR archive (over 4TB in size) and consists of structured, unstructured and waveform data. Furthermore, this tutorial will seek to educate the participants on how a combination of dynamic querying, and in-memory DBMS may enhance the management and analysis of complex clinical data.",2014,Knowledge Discovery and Data Mining,Fields of study: component oriented databasedata independencebig datadata sciencedata miningdatabasecomputer science
Profit-maximizing cluster hires,Behzad Golshan (Boston University)Theodoros Lappas (Stevens Institute of Technology)Evimaria Terzi (Boston University),"2056540169,2028397797,2110675235","Team formation has been long recognized as a natural way to acquire a diverse pool of useful skills, by combining experts with complementary talents. This allows organizations to effectively complete beneficial projects from different domains, while also helping individual experts position themselves and succeed in highly competitive job markets. Here, we assume a collection of projects ensuremath{P}, where each project requires a certain set of skills, and yields a different benefit upon completion. We are further presented with a pool of experts ensuremath{X}, where each expert has his own skillset and compensation demands. Then, we study the problem of hiring a cluster of experts T ⊆ X, so that the overall compensation (cost) does not exceed a given budget B, and the total benefit of the projects that this team can collectively cover is maximized. We refer to this as the ClusterHire problem. Our work presents a detailed analysis of the computational complexity and hardness of approximation of the problem, as well as heuristic, yet effective, algorithms for solving it in practice. We demonstrate the efficacy of our approaches through experiments on real datasets of experts, and demonstrate their advantage over intuitive baselines. We also explore additional variants of the fundamental problem formulation, in order to account for constraints and considerations that emerge in realistic cluster-hiring scenarios. All variants considered in this paper have immediate applications in the cluster hiring process, as it emerges in the context of different organizational settings.",2014,Knowledge Discovery and Data Mining,Fields of study: management scienceknowledge managementdata miningmachine learningsimulationcomputer science
Large-scale adaptive semi-supervised learning via unified inductive and transductive model,De Wang (University of Texas at Arlington)Feiping Nie (University of Texas at Arlington)Heng Huang (University of Texas at Arlington),"2147695980,2245267964,2137533801","Most semi-supervised learning models propagate the labels over the Laplacian graph, where the graph should be built beforehand. However, the computational cost of constructing the Laplacian graph matrix is very high. On the other hand, when we do classification, data points lying around the decision boundary (boundary points) are noisy for learning the correct classifier and deteriorate the classification performance. To address these two challenges, in this paper, we propose an adaptive semi-supervised learning model. Different from previous semi-supervised learning approaches, our new model needn't construct the graph Laplacian matrix. Thus, our method avoids the huge computational cost required by previous methods, and achieves a computational complexity linear to the number of data points. Therefore, our method is scalable to large-scale data. Moreover, the proposed model adaptively suppresses the weights of boundary points, such that our new model is robust to the boundary points. An efficient algorithm is derived to alternatively optimize the model parameter and class probability distribution of the unlabeled data, such that the induction of classifier and the transduction of labels are adaptively unified into one framework. Extensive experimental results on six real-world data sets show that the proposed semi-supervised learning model outperforms other related methods in most cases.",2014,Knowledge Discovery and Data Mining,Fields of study: stabilitytransductionsemi supervised learningunsupervised learningdata miningpattern recognitionmachine learningcomputer sciencemathematics
Deep learning,Ruslan Salakhutdinov (University of Toronto),2031945151,"Building intelligent systems that are capable of extracting high-level representations from high-dimensional data lies at the core of solving many AI related tasks, including visual object or pattern recognition, speech perception, and language understanding. Theoretical and biological arguments strongly suggest that building such systems requires deep architectures that involve many layers of nonlinear processing. Many existing learning algorithms use shallow architectures, including neural networks with only one hidden layer, support vector machines, kernel logistic regression, and many others. The internal representations learned by such systems are necessarily simple and are incapable of extracting some types of complex structure from high-dimensional input. In the past few years, researchers across many different communities, from applied statistics to engineering, computer science, and neuroscience, have proposed several deep (hierarchical) models that are capable of extracting meaningful, high-level representations. An important property of these models is that they can extract complex statistical dependencies from data and efficiently learn high-level representations by re-using and combining intermediate concepts, allowing these models to generalize well across a wide variety of tasks. The learned high-level representations have been shown to give state-of-the-art results in many challenging learning problems and have been successfully applied in a wide variety of application domains, including visual object recognition, information retrieval, natural language processing, and speech perception. A few notable examples of such models include Deep Belief Networks, Deep Boltzmann Machines, Deep Autoencoders, and sparse coding-based methods. The goal of the tutorial is to introduce the recent developments of various deep learning methods to the KDD community. The core focus will be placed on algorithms that can learn multi-layer hierarchies of representations, emphasizing their applications in information retrieval, object recognition, and speech perception.",2014,Knowledge Discovery and Data Mining,Fields of study: neural codinggraphical modeldeep belief networkdeep learningtheoretical computer sciencedata miningartificial intelligencemachine learningcomputer sciencemathematics
Statistically sound pattern discovery,"Wilhelmiina Hämäläinen (University of Eastern Finland)Geoffrey I. Webb (Monash University, Clayton campus)","2275152698,2126304162","Pattern discovery is a core data mining activity. Initial approaches were dominated by the frequent pattern discovery paradigm -- only patterns that occur frequently in the data were explored. Having been thoroughly researched and its limitations now well understood, this paradigm is giving way to a new one, which can be called statistically sound pattern discovery. In this paradigm, the main impetus is to discover statistically significant patterns, which are unlikely to have occurred by chance and are likely to hold in future data. Thus, the new paradigm provides a strict control over false discoveries and overfitting. This tutorial covers both classic and cutting-edge research topics on pattern discovery combined to statistical significance testing. We start with an advanced introduction to the relevant forms of statistical significance testing, including different schools and alternative models, their underlying assumptions, practical issues, and limitations. We then discuss their application to data mining specific problems, including evaluation of nested patterns, the multiple testing problem, algorithmic strategies and real-world considerations. We present the current state-of-the art solutions and explore in detail how this approach to pattern discovery can deliver efficient and effective discovery of small sets of interesting patterns.",2014,Knowledge Discovery and Data Mining,Fields of study: k optimal pattern discoverydata sciencebioinformaticsdata miningstatisticscomputer science
Who are experts specializing in landscape photography?: analyzing topic-specific authority on content sharing services,"Bin Bi (University of California, Los Angeles)Ben Kao (University of Hong Kong)Chang Wan (University of Hong Kong)Junghoo Cho (University of California, Los Angeles)","2139260861,1911907851,2685739726,2167033418","With the rapid growth of Web 2.0, a variety of content sharing services, such as Flickr, YouTube, Blogger , and TripAdvisor etc, have become extremely popular over the last decade. On these websites, users have created and shared with each other various kinds of resources, such as photos, video, and travel blogs. The sheer amount of user-generated content varies greatly in quality, which calls for a principled method to identify a set of authorities, who created high-quality resources, from a massive number of contributors of content. Since most previous studies only infer global authoritativeness of a user, there is no way to differentiate the authoritativeness in different aspects of life (topics). In this paper, we propose a novel model of Topic-specific Authority Analysis (TAA), which addresses the limitations of the previous approaches, to identify authorities specific to given query topic(s) on a content sharing service. This model jointly leverages the usage data collected from the sharing log and the favorite log. The parameters in TAA are learned from a constructed training dataset, for which a novel logistic likelihood function is specifically designed. To perform Bayesian inference for TAA with the new logistic likelihood, we extend typical Gibbs sampling by introducing auxiliary variables. Thorough experiments with two real-world datasets demonstrate the effectiveness of TAA in topic-specific authority identification as well as the generalizability of the TAA generative model.",2014,Knowledge Discovery and Data Mining,Fields of study: bayesian inferenceinternet privacyworld wide webdata miningmachine learningstatisticscomputer science
Bugbears or legitimate threats?: (social) scientists' criticisms of machine learning?,Sendhil Mullainathan (Harvard University),109652375,"Social scientists increasingly criticize the use of machine learning techniques to understand human behavior. Criticisms include: (1) They are atheoretical and hence of limited scientific value; (2) They do not address causality and are hence of limited policy value; and (3) They are uninterpretable and hence of limited generalizability value (outside contexts very narrowly similar to the training dataset). These criticisms, I argue, miss the enormous opportunity offered by ML techniques to fundamentally improve the practice of empirical social science. Yet each criticism does contain a grain of truth and overcoming them will require innovations to existing methodologies. Some of these innovations are being developed today and some are yet to be tackled. I will in this talk sketch (1) what these innovations look like or should look like; (2) why they are needed; and (3) the technical challenges they raise. I will illustrate my points using a set of applications that range from financial markets to social policy problems to computational models of basic psychological processes. This talk describes joint work with Jon Kleinberg and individual projects with Himabindu Lakkaraju, Jure Leskovec, Jens Ludwig, Anuj Shah, Chenhao Tan, Mike Yeomans and Tom Zimmerman.",2014,Knowledge Discovery and Data Mining,Fields of study: data miningartificial intelligencecomputer science
Leveraging user libraries to bootstrap collaborative filtering,Laurent Charlin (Princeton University)Richard S. Zemel (University of Toronto)Hugo Larochelle (Université de Sherbrooke),"242798638,2149392531,1963576484","We introduce a novel graphical model, the collaborative score topic model (CSTM), for personal recommendations of textual documents. CSTM's chief novelty lies in its learned model of individual libraries, or sets of documents, associated with each user. Overall, CSTM is a joint directed probabilistic model of user-item scores (ratings), and the textual side information in the user libraries and the items. Creating a generative description of scores and the text allows CSTM to perform well in a wide variety of data regimes, smoothly combining the side information with observed ratings as the number of ratings available for a given user ranges from none to many. Experiments on real-world datasets demonstrate CSTM's performance. We further demonstrate its utility in an application for personal recommendations of posters which we deployed at the NIPS 2013 conference.",2014,Knowledge Discovery and Data Mining,Fields of study: cold starttopic modelcollaborative filteringworld wide webinformation retrievaldata miningmachine learningcomputer science
Shallow semantic parsing of product offering titles (for better automatic hyperlink insertion),Gabor Melli (Simon Fraser University),1997038375,"With billions of database-generated pages on the Web where consumers can readily add priced product offerings to their virtual shopping cart, several opportunities will become possible once we can automatically recognize what exactly is being offered for sale on each page. We present a case study of a deployed data-driven system that first chunks individual titles into semantically classified sub-segments, and then uses this information to improve a hyperlink insertion service. To accomplish this process, we propose an annotation structure that is general enough to apply to offering titles from most e-commerce industries while also being specific enough to identify useful semantics about each offer. To automate the parsing task we apply the best-practices approach of training a supervised conditional random fields model and discover that creating separate prediction models for some of the industries along with the use of model-ensembles achieves the best performance to date. We further report on a real-world application of the trained parser to the task of growing a lexical dictionary of product-related terms which critically provides background knowledge to an affiliate-marketing hyperlink insertion service. On a regular basis we apply the parser to offering titles to produce a large set of labeled terms. From these candidates we select the most confidently predicted novel terms for review by crowd-sourced annotators. The agreed on terms are then added into a dictionary which significantly improves the performance of the link-insertion service. Finally, to continually improve system performance, we retrain the model in an online fashion by performing additional annotations on titles with incorrect predictions on each batch.",2014,Knowledge Discovery and Data Mining,Fields of study: world wide webdata miningdatabasemachine learningcomputer science
Scalable noise mining in long-term electrocardiographic time-series to predict death following heart attacks,Chih Chun Chia (University of Michigan)Zeeshan Syed (University of Michigan),"2601991876,2148329557","Cardiac disease is the leading cause of death around the world; with ischemic heart disease alone claiming 7 million lives in 2011. This burden can be attributed, in part, to the absence of biomarkers that can reliably identify high risk patients and match them to treatments that are appropriate for them. In recent clinical studies, we have demonstrated the ability of computation to extract information with substantial prognostic utility that is typically disregarded in time-series data collected from cardiac patients. Of particular interest are subtle variations in long-term electrocardiographic (ECG) data that are usually overlooked as noise but provide a useful assessment of myocardial instability. In multiple clinical cohorts, we have developed the pathophysiological basis for studying probabilistic variations in long-term ECG and demonstrated the ability of this information to effectively risk stratify patients at risk of dying following heart attacks. In this paper, we extend this work and focus on the question of how to reduce its computational complexity for scalable use in large datasets or energy constrained embedded devices. Our basic approach to uncovering pathological structure within the ECG focuses on characterizing beat-to-beat time-warped shape deformations of the ECG using a modified dynamic time-warping (DTW) and Lomb-Scargle periodogram-based algorithm. As part of our efforts to scale this work up, we explore a novel approach to address the quadratic runtime of DTW. We achieve this by developing the idea of adaptive downsampling to reduce the size of the inputs presented to DTW, and describe changes to the dynamic programming problem underlying DTW to exploit adaptively downsampled ECG signals. When evaluated on data from 765 patients in the DISPERSE2-TIMI33 trial, our results show that high morphologic variability is associated with an 8- to 9-fold increased risk of death within 90 days of a heart attack. Moreover, the use of adaptive downsampling with a modified DTW formulation achieves a 7- to almost 20-fold reduction in runtime relative to DTW, without a significant change in biomarker discrimination.",2014,Knowledge Discovery and Data Mining,Fields of study: dynamic time warpingdata miningartificial intelligencemachine learningsimulationstatisticscomputer science
User effort minimization through adaptive diversification,"Mahbub Hasan (University of California, Riverside)Abhijith Kashyap (University of California, Riverside)Vagelis Hristidis (University of California, Riverside)Vassilis J. Tsotras (University of California, Riverside)","2229252499,2036986288,238786035,8132329","Ambiguous queries, which are typical on search engines and recommendation systems, often return a large number of results from multiple interpretations. Given that many users often perform their searches on limited size screens (e.g. mobile phones), an important problem is which results to display first. Recent work has suggested displaying a set of results (Top-k) based on their relevance score with respect to the query and their diversity with respect to each other. However, previous works balance relevance and diversity mostly by a predefined fixed way. In this paper, we show that for different search tasks there is a different ideal balance of relevance and diversity. We propose a principled method for adaptive diversification of query results that minimizes the user effort to find the desired results, by dynamically balancing the relevance and diversity at each query step (e.g. when refining the query or viewing the next page of results). We introduce a navigation cost model as a means to estimate the effort required to navigate the query-results, and show that the problem of estimating the ideal amount of diversification at each step is NP-Hard. We propose an efficient approximate algorithm to select a near-optimal subset of the query results that minimizes the expected user effort. Finally we demonstrate the efficacy and efficiency of our solution in minimizing user effort, compared to state-of-the-art ranking methods, by means of an extensive experimental evaluation and a comprehensive user study on Amazon Mechanical Turk.",2014,Knowledge Discovery and Data Mining,Fields of study: rankingworld wide webdata miningmachine learningsimulationcomputer science
Detecting moving object outliers in massive-scale trajectory streams,Yanwei Yu (Yantai University)Lei Cao (Worcester Polytechnic Institute)Elke A. Rundensteiner (Worcester Polytechnic Institute)Qin Wang (University of Science and Technology Beijing),"2309633275,2114558166,2289414603,2645615848","The detection of abnormal moving objects over high-volume trajectory streams is critical for real time applications ranging from military surveillance to transportation management. Yet this problem remains largely unexplored. In this work, we first propose classes of novel trajectory outlier definitions that model the anomalous behavior of moving objects for a large range of real time applications. Our theoretical analysis and empirical study on the Beijing Taxi and GMTI (Ground Moving Target Indicator) datasets demonstrate its effectiveness in capturing abnormal moving objects. Furthermore we propose a general strategy for efficiently detecting the new outlier classes. It features three fundamental optimization principles designed to minimize the detection costs. Our comprehensive experimental studies demonstrate that our proposed strategy drives the detection costs 100-fold down into practical realm for applications producing high volume trajectory streams to utilize.",2014,Knowledge Discovery and Data Mining,Fields of study: outliercomputer visiondata miningsimulationstatistics
The recommender problem revisited: morning tutorial,Xavier Amatriain (Netflix)Bamshad Mobasher (DePaul University),"2571014644,1892801027","In 2006, Netflix announced a $1M prize competition to advance recommendation algorithms. The recommendation problem was simplified as the accuracy in predicting a user rating measured by the Root Mean Squared Error. While that formulation helped get the attention of the research community in the area, it may have put an excessive focus on what is simply one of possible approaches to recommendations. In this tutorial we will describe different components of modern recommender systems such as: personalized ranking, similarity, explanations, context-awareness, or search as recommendation. In the first part, we will use the Netflix use case as a driving example of a prototypical industrial-scale recommender system. We will also review the usage of modern algorithmic approaches that include algorithms such as Factorization Machines, Restricted Boltzmann Machines, SimRank, Deep Neural Networks, or Listwise Learning-to-rank. In the second part, we will focus on the area of context-aware recommendations where the two dimensional user-item recommender problem is turned into an n-dimensional space.",2014,Knowledge Discovery and Data Mining,Fields of study: personalizationrecommender systemdata sciencedata miningartificial intelligencemachine learningcomputer science
Relevant overlapping subspace clusters on categorical data,Xiao He (Ludwig Maximilian University of Munich)Jing Feng (Ludwig Maximilian University of Munich)Bettina Konte (Ludwig Maximilian University of Munich)Son T. Mai (Ludwig Maximilian University of Munich)Claudia Plant (Technische Universität München),"2280202269,2302555519,1907870702,2167132615,2122910652","Clustering categorical data poses some unique challenges: Due to missing order and spacing among the categories, selecting a suitable similarity measure is a difficult task. Many existing techniques require the user to specify input parameters which are difficult to estimate. Moreover, many techniques are limited to detect clusters in the full-dimensional data space. Only few methods exist for subspace clustering and they produce highly redundant results. Therefore, we propose ROCAT (Relevant Overlapping Subspace Clusters on Categorical Data), a novel technique based on the idea of data compression. Following the Minimum Description Length principle, ROCAT automatically detects the most relevant subspace clusters without any input parameter. The relevance of each cluster is validated by its contribution to compress the data. Optimizing the trade-off between goodness-of-fit and model complexity, ROCAT automatically determines a meaningful number of clusters to represent the data. ROCAT is especially designed to detect subspace clusters on categorical data which may overlap in objects and/or attributes; i.e. objects can be assigned to different clusters in different subspaces and attributes may contribute to different subspaces containing clusters. ROCAT naturally avoids undesired redundancy in clusters and subspaces by allowing overlap only if it improves the compression rate. Extensive experiments demonstrate the effectiveness and efficiency of our approach.",2014,Knowledge Discovery and Data Mining,Fields of study: minimum description lengthcategorical variabledata miningpattern recognitionmachine learningstatisticscomputer sciencemathematics
Skim-reading thousands of documents in one minute: Data indexing and visualization for multifarious search,Alessandro Perina (Istituto Italiano di Tecnologia)Dongwoo Kim (Australian National University)Andrzej TurskiNebojsa Jojic (Microsoft),"1972696923,2170939500,2685145981,2092023269",-,2014,Knowledge Discovery and Data Mining,Fields of study: world wide webinformation retrievaldata miningcomputer science
LaSEWeb: automating search strategies over semi-structured web data,Oleksandr Polozov (University of Washington)Sumit Gulwani (Microsoft),"976660552,310804771","We show how to programmatically model processes that humans use when extracting answers to queries (e.g., ""Who invented typewriter?"", ""List of Washington national parks"") from semi-structured Web pages returned by a search engine. This modeling enables various applications including automating repetitive search tasks, and helping search engine developers design micro-segments of factoid questions. We describe the design and implementation of a domain-specific language that enables extracting data from a webpage based on its structure, visual layout, and linguistic patterns. We also describe an algorithm to rank multiple answers extracted from multiple webpages. On 100,000+ queries (across 7 micro-segments) obtained from Bing logs, our system LaSEWeb answered queries with an average recall of 71%. Also, the desired answer(s) were present in top-3 suggestions for 95%+ cases.",2014,Knowledge Discovery and Data Mining,Fields of study: semi structured datadomain specific languagequestion answeringinformation retrievaldata miningdatabasemachine learningcomputer science
Learning multifractal structure in large networks,Austin R. Benson (Stanford University)Carlos Riquelme (Stanford University)Sven Schmit (Stanford University),"2117961877,2227134385,2167299351","Using random graphs to model networks has a rich history. In this paper, we analyze and improve the multifractal network generators (MFNG) introduced by Palla et al . We provide a new result on the probability of subgraphs existing in graphs generated with MFNG. This allows us to quickly compute moments of an important set of graph properties, such as the expected number of edges, stars, and cliques for graphs generated using MFNG. Specifically, we show how to compute these moments in time complexity independent of the size of the graph and the number of recursive levels in the generative model. We leverage this theory to propose a new method of moments algorithm for fitting MFNG to large networks. Empirically, this new approach effectively simulates properties of several social and information networks. In terms of matching subgraph counts, our method outperforms similar algorithms used with the Stochastic Kronecker Graph model. Furthermore, we present a fast approximation algorithm to generate graph instances following the multifractal structure. The approximation scheme is an improvement over previous methods, which ran in time complexity quadratic in the number of vertices. Combined, our method of moments and fast sampling scheme provide the first scalable framework for effectively modeling large networks with MFNG.",2014,Knowledge Discovery and Data Mining,Fields of study: factor critical graphdistance hereditary graphstrength of a graphvoltage graphcomplement graph1 planar graphgraph poweruniversal graphnull graphgraph productclique widthsplit graphpathwidthline graphmultifractal systemmethod of momentsrandom graphdiscrete mathematicscombinatoricsmachine learningstatisticsmathematics
Network mining and analysis for social applications,"Feida Zhu (Singapore Management University)Huan Sun (University of California, Santa Barbara)Xifeng Yan (University of California, Santa Barbara)","2160602068,2618514357,2116657824","The recent blossom of social network and communication services in both public and corporate settings have generated a staggering amount of network data of all kinds. Unlike the bio-networks and the chemical compound graph data often used in traditional network mining and analysis, the new network data grown out of the social applications are characterized by their rich attributes, high heterogeneity, enormous sizes and complex patterns of various semantic meanings, all of which have posed significant research challenges to the graph/network mining community. In this tutorial, we aim to examine some recent advances in network mining and analysis for social applications, covering a diverse collection of methodologies and applications from the perspectives of event, relationship, collaboration, and network pattern . We would present the problem settings, the challenges, the recent research advances and some future directions for each perspective. Topics include but are not limited to correlation mining, iceberg finding, anomaly detection, relationship discovery, information flow, task routing, and pattern mining.",2014,Knowledge Discovery and Data Mining,Fields of study: organizational network analysisdynamic network analysisnetwork sciencenetwork analysisdata stream miningdata scienceoperations researchdata miningmachine learningcomputer science
From labor to trader: opinion elicitation via online crowds as a market,Caleb Chen Cao (Hong Kong University of Science and Technology)Lei Chen (Hong Kong University of Science and Technology)Hosagrahar Visvesvaraya Jagadish (University of Michigan),"2136041333,2318776917,360112113","We often care about people's degrees of belief about certain events: e.g. causality between an action and the outcomes, odds distribution among the outcome of a horse race and so on. It is well recognized that the best form to elicit opinion from human is probability distribution instead of simple voting, because the form of distribution retains the delicate information that an opinion expresses. In the past, opinion elicitation has relied on experts, who are expensive and not always available. More recently, crowdsourcing has gained prominence as an inexpensive way to get a great deal of human input. However, traditional crowdsourcing has primarily focused on issuing very simple (e.g. binary decision) tasks to the crowd. In this paper, we study how to use crowds for Opinion Elicitation. There are three major challenges to eliciting opinion information in the form of probability distributions: how to measure the quality of distribution; how to aggregate the distributions; and, how to strategically implement such a system. To address these challenges, we design and implement COPE Crowd-powered OPinion Elicitation market. COPE models crowdsourced work as a trading market, where the ""workers"" behave like ""traders"" to maximize their profit by presenting their opinion. Among the innovative features in this system, we design COPE updating to combine the multiple elicited distributions following a Bayesian scheme. Also to provide more flexibility while running COPE, we propose a series of efficient algorithms and a slope based strategy to manage the ending condition of COPE. We then demonstrate the implementation of COPE and report experimental results running on real commercial platform to demonstrate the practical value of this system.",2014,Knowledge Discovery and Data Mining,Fields of study: crowdsourcingsocial mediadata miningmachine learningsimulationcomputer science
Factorized sparse learning models with interpretable high order feature interactions,Sanjay Purushotham (University of Southern California)Martin Renqiang Min (NEC)C.-C. Jay Kuo (University of Southern California)Rachel Ostroff (New York University School of Medicine),"2115359007,2154299949,2670973419,2715717713","Identifying interpretable discriminative high-order feature interactions given limited training data in high dimensions is challenging in both machine learning and data mining. In this paper, we propose a factorization based sparse learning framework termed FHIM for identifying high-order feature interactions in linear and logistic regression models, and study several optimization methods for solving them. Unlike previous sparse learning methods, our model FHIM recovers both the main effects and the interaction terms accurately without imposing tree-structured hierarchical constraints. Furthermore, we show that FHIM has oracle properties when extended to generalized linear regression models with pairwise interactions. Experiments on simulated data show that FHIM outperforms the state-of-the-art sparse lear-ning techniques. Further experiments on our experimentally generated data from patient blood samples using a novel SOMAmer (Slow Off-rate Modified Aptamer) technology show that, FHIM performs blood-based cancer diagnosis and bio-marker discovery for Renal Cell Carcinoma much better than other competing methods, and it identifies interpretable block-wise high-order gene interactions predictive of cancer stages of samples. A literature survey shows that the interactions identified by FHIM play important roles in cancer development.",2014,Knowledge Discovery and Data Mining,Fields of study: sparse approximationfeature selectiondata miningpattern recognitionmachine learningstatisticscomputer science
Streamed approximate counting of distinct elements: beating optimal batch methods,Daniel Ting (Facebook),2230432365,"Counting the number of distinct elements in a large dataset is a common task in web applications and databases. This problem is difficult in limited memory settings where storing a large hash table table is intractable. This paper advances the state of the art in probabilistic methods for estimating the number of distinct elements in a streaming setting New streaming algorithms are given that provably beat the ""optimal"" errors for Min-count and HyperLogLog while using the same sketch. This paper also contributes to the understanding and theory of probabilistic cardinality estimation introducing the concept of an area cutting process and the martingale estimator. These ideas lead to theoretical analyses of both old and new sketches and estimators and show the new estimators are optimal for several streaming settings while also providing accurate error bounds that match those obtained via simulation. Furthermore, the area cutting process provides a geometric intuition behind all methods for counting distinct elements which are not affected by duplicates. This intuition leads to a new sketch, Discrete Max-count, and the analysis of a class of sketches, self-similar area cutting decompositions that have attractive properties and unbiased estimators for both streaming and non-streaming settings. Together, these contributions lead to multi-faceted advances in sketch construction, cardinality and error estimation, the theory, and intuition for the problem of approximate counting of distinct elements for both the streaming and non-streaming cases.",2014,Knowledge Discovery and Data Mining,Fields of study: martingalerandomized algorithmtheoretical computer sciencecombinatoricsdata miningmachine learningstatisticsalgorithmcomputer sciencemathematics
Computational epidemiology,Madhav V. Marathe (Virginia Tech)Anil Kumar S. Vullikanti (Virginia Tech),"2242145496,393896382","As recent pandemics such as SARS and the Swine Flu outbreak have shown, diseases spread very fast in today's interconnected world, making public health an important research area. Some of the basic questions are: How can an outbreak be contained before it becomes an epidemic, and what disease surveillance strategies should be implemented? These problems have been studied traditionally using differential equation methods, which are amenable to analysis and closed form solutions. However, these models are based on complete mixing assumptions, which do not hold for realistic populations, thereby limiting their utility. In this tutorial, we focus on an approach based on diffusion processes on complex networks. This captures more realistic populations, but leads to novel mathematical and computational challenges. The structure of the underlying networks has a significant impact on the dynamical properties, motivating the need for improved network models, and efficient algorithms for computing network and dynamical properties that scale to large networks. We provide an overview of the state of the art in computational epidemiology, which is a multi-disciplinary research area, that overlaps different areas in computer science, including data mining, machine learning, high performance computing and theoretical computer science, as well as mathematics, economics and statistics. Specifically, we will discuss mathematical and computational models, problems of inference, forecasting and state assessment, and epidemic containment.",2014,Knowledge Discovery and Data Mining,Fields of study: social networkdiffusionepidemiologymanagement scienceoperations researchdata miningmachine learningsimulationstatisticscomputer science
Medicine in the age of electronic health records,Nigam Shah (Stanford University),2150446796,"In the era of EHRs, it is possible to examine the outcomes of decisions made by doctors during clinical practice to identify patterns of care---generating evidence based on the collective practice of experts. We will discuss methods that use unstructured patient data to monitor for adverse drug events, profile specific drugs, identify off-label drug usage, uncover 'natural experiments' and generate practice-based evidence for difficult-to-test clinical hypotheses. We will describe how to detect associations among drugs and their adverse events several years before an alert is issued as well as compute the true rate of drug-drug interactions. We will present approaches to identify novel off-label uses of drugs using the patient feature matrix along with prior knowledge about drugs, diseases, and known usage. We will review a natural experiment--where a subset of congestive heart failure patients who were prescribed Cilostazol despite its black box warning--and profile its safety. We will discuss the testing of a clinical hypothesis about an association between allergic conditions and chronic uveitis in patients with juvenile idiopathic arthritis.",2014,Knowledge Discovery and Data Mining,Fields of study: data mining
Parallel gibbs sampling for hierarchical dirichlet processes via gamma processes equivalence,Dehua Cheng (University of Southern California)Yan Liu (University of Southern California),"2166044507,2240541904","The hierarchical Dirichlet process (HDP) is an intuitive and elegant technique to model data with latent groups. However, it has not been widely used for practical applications due to the high computational costs associated with inference. In this paper, we propose an effective parallel Gibbs sampling algorithm for HDP by exploring its connections with the gamma-gamma-Poisson process. Specifically, we develop a novel framework that combines bootstrap and Reversible Jump MCMC algorithm to enable parallel variable updates. We also provide theoretical convergence analysis based on Gibbs sampling with asynchronous variable updates. Experiment results on both synthetic datasets and two large-scale text collections show that our algorithm can achieve considerable speedup as well as better inference accuracy for HDP compared with existing parallel sampling algorithms.",2014,Knowledge Discovery and Data Mining,Fields of study: topic modelhierarchical dirichlet processlatent dirichlet allocationdata miningpattern recognitionmachine learningstatisticscomputer science
Semantic visualization for spherical representation,Tuan M. V. Le (Singapore Management University)Hady Wirawan Lauw (Singapore Management University),"2144042818,2024254804","Visualization of high-dimensional data such as text documents is widely applicable. The traditional means is to find an appropriate embedding of the high-dimensional representation in a low-dimensional visualizable space. As topic modeling is a useful form of dimensionality reduction that preserves the semantics in documents, recent approaches aim for a visualization that is consistent with both the original word space, as well as the semantic topic space. In this paper, we address the semantic visualization problem. Given a corpus of documents, the objective is to simultaneously learn the topic distributions as well as the visualization coordinates of documents. We propose to develop a semantic visualization model that approximates L2-normalized data directly. The key is to associate each document with three representations: a coordinate in the visualization space, a multinomial distribution in the topic space, and a directional vector in a high-dimensional unit hypersphere in the word space. We join these representations in a unified generative model, and describe its parameter estimation through variational inference. Comprehensive experiments on real-life text datasets show that the proposed method outperforms the existing baselines on objective evaluation metrics for visualization quality and topic interpretability.",2014,Knowledge Discovery and Data Mining,Fields of study: topic modelgenerative modeldimensionality reductioninformation visualizationdata miningpattern recognitionmachine learningcomputer sciencemathematics
A multi-class boosting method with direct optimization,Shaodan Zhai (Wright State University)Tian Xia (Wright State University)Shaojun Wang (Wright State University),"2148838363,2304900629,2167395975","We present a direct multi-class boosting (DMCBoost) method for classification with the following properties: (i) instead of reducing the multi-class classification task to a set of binary classification tasks, DMCBoost directly solves the multi-class classification problem, and only requires very weak base classifiers; (ii) DMCBoost builds an ensemble classifier by directly optimizing the non-convex performance measures, including the empirical classification error and margin functions, without resorting to any upper bounds or approximations. As a non-convex optimization method, DMCBoost shows competitive or better results than state-of-the-art convex relaxation boosting methods, and it performs especially well on the noisy cases.",2014,Knowledge Discovery and Data Mining,Fields of study: brownboostlpboostgradient boostingone class classificationlinear classifierboostingbiological classificationsupervised learningdata miningpattern recognitionmachine learningcomputer sciencemathematics
Distance queries from sampled data: accurate and efficient,Edith Cohen (Microsoft),2167811012,"Distance queries are a basic tool in data analysis. They are used for detection and localization of change for the purpose of anomaly detection, monitoring, or planning. Distance queries are particularly useful when data sets such as measurements, snapshots of a system, content, traffic matrices, and activity logs are collected repeatedly. Random sampling, which can be efficiently performed over streamed or distributed data, is an important tool for scalable data analysis. The sample constitutes an extremely flexible summary, which naturally supports domain queries and scalable estimation of statistics, which can be specified after the sample is generated. The effectiveness of a sample as a summary, however, hinges on the estimators we have. We derive novel estimators for estimating $L_p$ distance from sampled data. Our estimators apply with the most common weighted sampling schemes: Poisson Probability Proportional to Size (PPS) and its fixed sample size variants. They also apply when the samples of different data sets are independent or coordinated. Our estimators are admissible (Pareto optimal in terms of variance) and have compelling properties. We study the performance of our Manhattan and Euclidean distance (p=1,2) estimators on diverse datasets, demonstrating scalability and accuracy even when a small fraction of the data is sampled. Our work, for the first time, facilitates effective distance estimation over sampled data.",2014,Knowledge Discovery and Data Mining,Fields of study: euclidean distancedata miningmachine learningstatisticsmathematics
Product selection problem: improve market share by learning consumer behavior,Silei Xu (The Chinese University of Hong Kong)John Chi-Shing Lui (The Chinese University of Hong Kong),"2227703894,2045404162","It is often crucial for manufacturers to decide what products to produce so that they can increase their market share in an increasingly fierce market. To decide which products to produce, manufacturers need to analyze the consumers’ requirements and how consumers make their purchase decisions so that the new products will be competitive in the market. In this paper, we first present a general distance-based product adoption model to capture consumers’ purchase behavior. Using this model, various distance metrics can be used to describe different real life purchase behavior. We then provide a learning algorithm to decide which set of distance metrics one should use when we are given some accessible historical purchase data. Based on the product adoption model, we formalize the k most marketable products (or k- MMP ) selection problem and formally prove that the problem is NP-hard . To tackle this problem, we propose an efficient greedy-based approximation algorithm with a provable solution guarantee. Using submodularity analysis, we prove that our approximation algorithm can achieve at least 63p of the optimal solution. We apply our algorithm on both synthetic datasets and real-world datasets (TripAdvisor.com), and show that our algorithm can easily achieve five or more orders of speedup over the exhaustive search and achieve about 96p of the optimal solution on average. Our experiments also demonstrate the robustness of our distance metric learning method, and illustrate how one can adopt it to improve the accuracy of product selection.",2014,Knowledge Discovery and Data Mining,Fields of study: submodular set functionapproximation algorithmdata miningmachine learningmathematics
Multi-task copula by sparse graph regression,"Tianyi Zhou (University of Washington)Dacheng Tao (University of Technology, Sydney)","2153604853,2104129307","This paper proposes multi-task copula (MTC) that can handle a much wider class of tasks than mean regression with Gaussian noise in most former multi-task learning (MTL). While former MTL emphasizes shared structure among models, MTC aims at joint prediction to exploit inter-output correlation. Given input, the outputs of MTC are allowed to follow arbitrary joint continuous distribution. MTC captures the joint likelihood of multi-output by learning the marginal of each output firstly and then a sparse and smooth output dependency graph function. While the former can be achieved by classical MTL, learning graphs dynamically varying with input is quite a challenge. We address this issue by developing sparse graph regression (SpaGraphR), a non-parametric estimator incorporating kernel smoothing, maximum likelihood, and sparse graph structure to gain fast learning algorithm. It starts from a few seed graphs on a few input points, and then updates the graphs on other input points by a fast operator via coarse-to-fine propagation. Due to the power of copula in modeling semi-parametric distributions, SpaGraphR can model a rich class of dynamic non-Gaussian correlations. We show that MTC can address more flexible and difficult tasks that do not fit the assumptions of former MTL nicely, and can fully exploit their relatedness. Experiments on robotic control and stock price prediction justify its appealing performance in challenging MTL problems.",2014,Knowledge Discovery and Data Mining,Fields of study: multi task learningcopulaeconometricsdata miningmachine learningstatisticscomputer sciencemathematics
Does social good justify risking personal privacy,"Raghu Ramakrishnan (Microsoft)Geoffrey I. Webb (Monash University, Clayton campus)","2574082543,2126304162","When data-driven improvements involve personally identifiable data, or even data that can be used to infer sensitive information about individuals, we face the dilemma that we potentially risk compromising privacy. As we see increased emphasis on using data mining to effect improvements in a range of socially beneficial activities, from improving matching of talented students to opportunities for higher education, or improving allocation of funds across competing school programs, or reducing hospitalization time following surgery, the dilemma can often be especially acute. The data involved often is personally identifiable or revealing and sensitive, and many of the institutions that must be involved in gathering and maintaining custody of the data are not equipped to adequately secure the data, raising the risk of privacy breaches. How should we approach this trade-off? Can we assess the risks? Can we control or mitigate them? Can we develop guidelines for when the risk is or is not worthwhile, and for how best to handle data in different common scenarios? Chairs Raghu Ramakrishnan and Geoffrey I. Webb bring this panel of leading data miners and privacy experts together to address these critical issues.",2014,Knowledge Discovery and Data Mining,Fields of study: common goodpersonally identifiable informationprivacycomputer securitydata miningcomputer science
Supervised deep learning with auxiliary networks,Junbo Zhang (Southwest Jiaotong University)Guangjian Tian (Huawei)Yadong Mu (Huawei)Wei Fan (Baidu),"2627476963,2635072737,2151696581,2422054197","Deep learning well demonstrates its potential in learning latent feature representations. Recent years have witnessed an increasing enthusiasm for regularizing deep neural networks by incorporating various side information, such as user-provided labels or pairwise constraints. However, the effectiveness and parameter sensitivity of such algorithms have been major obstacles for putting them into practice. The major contribution of our work is the exposition of a novel supervised deep learning algorithm, which distinguishes from two unique traits. First, it regularizes the network construction by utilizing similarity or dissimilarity constraints between data pairs, rather than sample-specific annotations. Such kind of side information is more flexible and greatly mitigates the workload of annotators. Secondly, unlike prior works, our proposed algorithm decouples the supervision information and intrinsic data structure. We design two heterogeneous networks, each of which encodes either supervision or unsupervised data structure respectively. Specifically, we term the supervision-oriented network as ""auxiliary network"" since it is principally used for facilitating the parameter learning of the other one and will be removed when handling out-of-sample data. The two networks are complementary to each other and bridged by enforcing the correlation of their parameters. We name the proposed algorithm SUpervision-Guided AutoencodeR (SUGAR). Comparing prior works on unsupervised deep networks and supervised learning, SUGAR better balances numerical tractability and the flexible utilization of supervision information. The classification performance on MNIST digits and eight benchmark datasets demonstrates that SUGAR can effectively improve the performance by using the auxiliary networks, on both shallow and deep architectures. Particularly, when multiple SUGARs are stacked, the performance is significantly boosted. On the selected benchmarks, ours achieve up to 11.35% relative accuracy improvement compared to the state-of-the-art models.",2014,Knowledge Discovery and Data Mining,Fields of study: autoencoderdeep belief networkdeep learningunsupervised learningdata miningartificial intelligencemachine learningstatisticscomputer science
Reducing gang violence through network influence based targeting of social programs,Paulo Shakarian (Arizona State University)Joseph Salmento (Military Academy)William R. Pulleyblank (Military Academy)John Bertetto (Chicago Police Department),"102302551,2225408473,2712671821,2222983607","In this paper, we study a variant of the social network maximum influence problem and its application to intelligently approaching individual gang members with incentives to leave a gang. The goal is to identify individuals who when influenced to leave gangs will propagate this action. We study this emerging application by exploring specific facets of the problem that must be addressed when modeling this particular situation. We formulate a new influence maximization variant - the ""social incentive influence"" (SII) problem and study it both formally and in the context of the law-enforcement domain. Using new techniques from unconstrained submodular maximization, we develop an approximation algorithm for SII and present a suite of experimental results - including tests on real-world police data from Chicago.",2014,Knowledge Discovery and Data Mining,Fields of study: complex networkcomputer securitydata miningmachine learningsimulationcomputer science
The PlaceIQ Analytic Platform: Location Oriented Approaches to Mobile Audiences,Juan M. Huerta (IBM)Jonathan LenaghanSteve MiltonKevin BrackneyAmol KapilaRoman ShragaRachit SrivastavaCarmem Domingues Khoa PhamEliza ChangSusan ZhangEvan PicardAlexandra DangoRanqi Zhu,"2097738040,2572887686,2228901416,2224037833,2720283688,2424201648,2706830457,2228889316,2659141237,2709244080,2223916335,2222525625,2641102910","The PlaceIQ platform is a large-scale data analysis system created on the concept of location. This paper is a system overview where we provide an description of the nature of the data sources, frame of reference, abstractions, algorithmic approaches, as well as the main design tradeoffs of this system. We also provide a list of some of the lessons we learned after deploying and using this platform for numerous actual mobile advertisement campaigns. Additionally, we describe the Place Visit Rate, which is a location-based criterion for quantifying campaign response. We believe that the lift we see in Place Visit Rate in actual mobile campaigns using PIQ's audiences is a form of validation of our overall approach and provides us with guidance and feedback regarding the quality of the algorithms and audiences we create.",2014,Knowledge Discovery and Data Mining,Fields of study: logistic regressiononline advertisingmultimediadata miningmachine learningsimulationcomputer science
Sleep analytics and online selective anomaly detection,Tahereh Babaie (University of Sydney)Sanjay Chawla (University of Sydney)Romesh G. Abeysuriya (University of Sydney),"1988843042,2201421368,1998611010","We introduce a new problem, the Online Selective Anomaly Detection (OSAD), to model a specific scenario emerging from research in sleep science. Scientists have segmented sleep into several stages and stage two is characterized by two patterns (or anomalies) in the EEG time series recorded on sleep subjects. These two patterns are sleep spindle (SS) and K-complex. The OSAD problem was introduced to design a residual system, where all anomalies (known and unknown) are detected but the system only triggers an alarm when non-SS anomalies appear. The solution of the OSAD problem required us to combine techniques from both data mining and control theory. Experiments on data from real subjects attest to the effectiveness of our approach.",2014,Knowledge Discovery and Data Mining,Fields of study: data miningartificial intelligencemachine learning
Scalable histograms on large probabilistic data,Mingwang Tang (University of Utah)Feifei Li (University of Utah),"2170236580,2472728339","Histogram construction is a fundamental problem in data management, and a good histogram supports numerous mining operations. Recent work has extended histograms to probabilistic data. However, constructing histograms for probabilistic data can be extremely expensive, and existing studies suffer from limited scalability. This work designs novel approximation methods to construct scalable histograms on probabilistic data. We show that our methods provide constant approximations compared to the optimal histograms produced by the state-of-the-art in the worst case. We also extend our methods to parallel and distributed settings so that they can run gracefully in a cluster of commodity machines. We introduced novel synopses to reduce communication cost when running our methods in such settings. Extensive experiments on large real data sets have demonstrated the superb scalability and efficiency achieved by our methods, when compared to the state-of-the-art methods. They also achieved excellent approximation quality in practice.",2014,Knowledge Discovery and Data Mining,Fields of study: probabilistic databasehistogramtheoretical computer sciencedata miningdatabasestatisticscomputer science
Dual beta process priors for latent cluster discovery in chronic obstructive pulmonary disease,James C. Ross (Brigham and Women's Hospital)Peter J. Castaldi (Brigham and Women's Hospital)Michael H. Cho (Brigham and Women's Hospital)Jennifer G. Dy (Northeastern University),"2156384870,2080945126,2116921408,2239241780","Chronic obstructive pulmonary disease (COPD) is a lung disease characterized by airflow limitation usually associated with an inflammatory response to noxious particles, such as cigarette smoke. COPD is currently the third leading cause of death in the United States and is the only leading cause of death that is increasing in prevalence. It also represents an enormous financial burden to society, costing tens of billions of dollars annually in the U.S. It is widely accepted by the medical community that COPD is a heterogeneous disease, with substantial evidence indicating that genetic variation contributes to varying levels of disease susceptibility. This heterogeneity makes it difficult to predict health decline and develop targeted treatments for better patient care. Although researchers have made several attempts to discover disease subtypes, results have been inconclusive, in part because standard clustering methods have not properly dealt with disease manifestations that may worsen with increased exposure. In this paper we introduce a transformative way of looking at the COPD subtyping task. Specifically, we model the relationship between risk factors (such as age and smoke exposure) and manifestations of disease severity using Gaussian Processes, which allow us to represent so-called ""disease trajectories"". We also posit that individuals can be associated with multiple disease types (latent clusters), which we assume are influenced by genetics. Furthermore, we predict that only subsets of the numerous disease-related quantitative features are useful for describing each latent subtype. We model these associations using two separate beta process priors, and we describe a variational inference approach to discover the most probable latent cluster assignments. Results are validated with associations to genetic markers.",2014,Knowledge Discovery and Data Mining,Fields of study: gaussian processstatistics
Novel geospatial interpolation analytics for general meteorological measurements,Bingsheng Wang (Virginia Tech)Jinjun Xiong (IBM),"2120130302,2646943361","This paper addresses geospatial interpolation for meteorological measurements in which we estimate the values of climatic metrics at unsampled sites with existing observations. Providing climatological and meteorological conditions covering a large region is potentially useful in many applications, such as smart grid. However, existing research works on interpolation either cause a large number of complex calculations or are lack of high accuracy. We propose a Bayesian compressed sensing based non-parametric statistical model to efficiently perform the spatial interpolation task. Student-t priors are employed to model the sparsity of unknown signals' coefficients, and the Approximated Variational Inference (AVI) method is provided for effective and fast learning. The presented model has been deployed at IBM, targeting for aiding the intelligent management of smart grid. The evaluations on two real world datasets demonstrate that our algorithm achieves state-of-the-art performance in both effectiveness and efficiency.",2014,Knowledge Discovery and Data Mining,Fields of study: analyticssmart gridbayesian inferencedata sciencedata miningstatisticscomputer science
Can Television Advertising Impact Be Measured on the Web? Web Spike Response as a Possible Conversion Tracking System for Television,Brendan Kitts (Brandeis University)Michael BardaroDyng AuAl LeeSawin LeeJon BorchardtCraig SchwartzJohn SobieskiJohn Wadsworth-Drake,"2159085996,2643889050,2311596393,2716495132,2226087550,2227914747,2710019852,2700118881,2223368614","Consumers are increasingly using internet-connected devices while watching television. This paper will show that it is possible to measure web activity bursts that peak about 13 seconds after the end of traditional TV ad broadcasts. By measuring this effect, we propose that it may be possible to deploy a web-based TV conversion tracking system that will work on TV systems.",2014,Knowledge Discovery and Data Mining,Fields of study: internet televisioninteractive televisiondigital televisiontargetingtelevisionmultimediatelecommunicationscomputer science
A data driven approach to diagnosing and treating disease,Eric E. Schadt (Icahn School of Medicine at Mount Sinai),668066243,"Throughout the biomedical and life sciences research community, advanced integrative biology algorithms are employed to integrate large scale data across many different high-dimensional datatypes to construct predictive network models of disease. The causal inference approaches we employ for this purpose well complement the types of natural artificial intelligence/machine learning approaches that have become nearly standard in the life and biomedical sciences for building classifiers for a range of problems, from disease classification and subtype stratification, to the identification of responders and non-responders for a given treatment strategy. By building a causal network model that spans multiple scales (from the molecular to the cellular, to the tissue/organ, to the organism and community) we can understand the flow of information and how best to modulate that flow to improve human wellbeing, whether better diagnosing and treating disease or improving overall health( 1-4 ). More specifically, we have constructed predictive network models for Alzheimer's disease, along with other common human diseases such as obesity, diabetes, heart disease, and inflammatory bowel disease, and cancer, and demonstrated a causal network common across all of these diseases( 3, 5-10 ). Not only do we demonstrate that our predictive models uncover important mechanisms of disease and mechanistic connections among different diseases, but that they have led to a natural way to prioritize therapeutic points of intervention and provide optimal molecular phenotypes for high throughput screening. Our application of these models in a number of disease areas has led to the identification of novel genes that are causal for disease and that may serve as efficacious points of therapeutic intervention, as well as to personalized treatment strategies that provide a more quantitative and accurate approach to tailoring treatments to specific forms of disease.",2014,Knowledge Discovery and Data Mining,Fields of study: drug discoverysystems biologybioinformaticsdata miningcomputer science
Improving the modified nyström method using spectral shifting,Shusen Wang (Zhejiang University)Chao Zhang (Zhejiang University)Hui Qian (Zhejiang University)Zhihua Zhang (Shanghai Jiao Tong University),"2108094886,2618902948,2100359847,2111291218","The Nystrom method is an efficient approach to enabling large-scale kernel methods. The Nystrom method generates a fast approximation to any large-scale symmetric positive semidefinete (SPSD) matrix using only a few columns of the SPSD matrix. However, since the Nystrom approximation is low-rank, when the spectrum of the SPSD matrix decays slowly, the Nystrom approximation is of low accuracy. In this paper, we propose a variant of the Nystrom method called the modified Nystrom by spectral shifting (SS-Nystrom). The SS-Nystrom method works well no matter whether the spectrum of SPSD matrix decays fast or slow. We prove that our SS-Nystrom has a much stronger error bound than the standard and modified Nystrom methods, and that SS-Nystrom can be even more accurate than the truncated SVD of the same scale in some cases. We also devise an algorithm such that the SS-Nystrom approximation can be computed nearly as efficient as the modified Nystrom approximation. Finally, our SS-Nystrom method demonstrates significant improvements over the standard and modified Nystrom methods on several real-world datasets.",2014,Knowledge Discovery and Data Mining,Fields of study: calculusmathematical analysismathematical optimizationmathematics
Large scale predictive modeling for micro-simulation of 3G air interface load,Dejan Radosavljevik (Leiden University)Peter van der Putten (Leiden University),"163798874,2046754366","This paper outlines the approach developed together with the Radio Network Strategy & Design Department of a large European telecom operator in order to forecast the Air-Interface load in their 3G network, which is used for planning network upgrades and budgeting purposes. It is based on large scale intelligent data analysis and modeling at the level of thousands of individual radio cells resulting in 30,000 models per day. It has been embedded into a scenario simulation framework that is used by end users not experienced in data mining for studying and simulating the behavior of this complex networked system, as an example of a systematic approach to the deployment step in the KDD process. This system is already in use for two years in the country where it was developed and it is a part of a standard business process. In the last six months this national operator became a competence center for predictive modeling for micro-simulation of 3G air interface load for four other operators of the same parent company.",2014,Knowledge Discovery and Data Mining,Fields of study: cellular networklinear regressiondata miningreal time computingmachine learningsimulationcomputer science
Batch discovery of recurring rare classes toward identifying anomalous samples,Murat Dundar (Indiana University – Purdue University Indianapolis)Halid Ziya Yerebakan (Indiana University – Purdue University Indianapolis)Bartek Rajwa (Purdue University),"2104557900,2094180671,2011395205","We present a clustering algorithm for discovering rare yet significant recurring classes across a batch of samples in the presence of random effects. We model each sample data by an infinite mixture of Dirichlet-process Gaussian-mixture models (DPMs) with each DPM representing the noisy realization of its corresponding class distribution in a given sample. We introduce dependencies across multiple samples by placing a global Dirichlet process prior over individual DPMs. This hierarchical prior introduces a sharing mechanism across samples and allows for identifying local realizations of classes across samples. We use collapsed Gibbs sampler for inference to recover local DPMs and identify their class associations. We demonstrate the utility of the proposed algorithm, processing a flow cytometry data set containing two extremely rare cell populations, and report results that significantly outperform competing techniques. The source code of the proposed algorithm is available on the web via the link:http://cs.iupui.edu/~dundar/aspire.htm.",2014,Knowledge Discovery and Data Mining,Fields of study: hierarchical dirichlet processrandom effects modelanomaly detectiondata miningmachine learningstatisticscomputer sciencemathematics
Filling context-ad vocabulary gaps with click logs,Yukihiro Tagami (Yahoo!)Toru Hotta (Yahoo!)Yusuke Tanaka (Yahoo!)Shingo Ono (Yahoo!)Koji Tsukamoto (Yahoo!)Akira Tajima (Yahoo!),"2112635383,2143215282,2146066583,2120416050,2051352073,2159701148","Contextual advertising is a form of textual advertising usually displayed on third party Web pages. One of the main problems with contextual advertising is determining how to select ads that are relevant to the page content and/or the user information in order to achieve both effective advertising and a positive user experience. Typically, the relevance of an ad to page content is indicated by a tf-idf score that measures the word overlap between the page and the ad content, so this problem is transformed into a similarity search in a vector space. However, such an approach is not useful if the vocabulary used on the page is expected to be different from that in the ad. There have been studies proposing the use of semantic categories or hidden classes to overcome this problem. With these approaches it is necessary to expand the ad retrieval system or build new index to handle the categories or classes, and it is not always easy to maintain the number of categories and classes required for business needs. In this work, we propose a translation method that learns the mapping of the contextual information to the textual features of ads by using past click data. The contextual information includes the user's demographic information and behavioral information as well as page content information. The proposed method is able to retrieve more preferable ads while maintaining the sparsity of the inverted index and the performance of the ad retrieval system. In addition, it is easy to implement and there is no need to modify an existing ad retrieval system. We evaluated this approach offline on a data set based on logs from an ad network. Our method achieved better results than existing methods. We also applied our approach with a real ad serving system and compared the online performance using A/B testing. Our approach achieved an improvement over the existing production system.",2014,Knowledge Discovery and Data Mining,Fields of study: systems modelingcontextual advertisinglearning to rankmultimediaworld wide webdata miningmachine learningcomputer science
Predictive modeling in practice: a case study from sprint,Tracy De Poalo (Sprint Corporation)Jeremy Howard,"2227234919,2721103548","It's not often we get to peer into the details of how big corporates use predictive modeling in practice. In this talk, Sprint's Head of Predictive Modeling, Tracey De Poalo, will talk about the process she developed using SAS and logistic regression to build a wide range of models. Jeremy Howard will discuss his experience as a consultant at Sprint comparing R and random forests to the existing process, and will show the pros and cons of each approach. The talk will also cover the 'real world' issues that Tracey has dealt with in creating a reusable process, including data mart development, sampling, testing, reporting, and implementation with internal customers. Sprint's existing process is the best that Jeremy has seen in industry, and shows a lot of best practices in data structure, documentation, testing, automation, and customer interaction, and modeling.",2014,Knowledge Discovery and Data Mining,Fields of study: random forestlogistic regressiondata miningmachine learningsimulationstatisticscomputer science
Fast DTT: a near linear algorithm for decomposing a tensor into factor tensors,Xiaomin Fang (Sun Yat-sen University)Rong Pan (Sun Yat-sen University),"2648688744,2194641296","As tensors provide a natural representation for the higher-order relations, tensor factorization techniques such as Tucker decomposition and CANDECOMP/PARAFAC decomposition have been applied to many fields. Tucker decomposition has strong capacity of expression, but the time complexity is unpractical for the large-scale real problems. On the other hand, CANDECOMP/PARAFAC decomposition is linear in the feature dimensionality, but the assumption is so strong that it abandons some important information. Besides, both of TD and CP decompose a tensor into several factor matrices. However, the factor matrices are not natural for the representation of the higher-order relations. To overcome these problems, we propose a near linear tensor factorization approach, which decompose a tensor into factor tensors in order to model the higher-order relations, without loss of important information. In addition, to reduce the time complexity and the number of the parameters, we decompose each slice of the factor tensors into two smaller matrices. We conduct experiments on both synthetic datasets and real datasets. The experimental results on the synthetic datasets validate that our model has strong capacity of expression. The results on the real datasets show that our approach outperforms the state-of-the-art tensor factorization methods.",2014,Knowledge Discovery and Data Mining,Fields of study: tensordiscrete mathematicscombinatoricsmathematical optimizationcomputer sciencemathematics
Information environment security,Rand Waltzman (DARPA),2108181368,"The US Department of Defense Dictionary of Military terms defines the Information Environment as 'the aggregate of individuals, organizations, and systems that collect, process, disseminate, or act on information.' The decisions and actions we take both as individuals and collectively simultaneously shape and are shaped by the information environment in which we live. The nature of our interaction with the information environment is rapidly evolving and old models are becoming irrelevant faster than we can develop new ones. This results in uncertainties that leave us exposed to dangerous influences without proper defenses. The purpose of this talk is to help frame a new science of Information Environment Security (IES) whose goal is to create and apply the tools needed to discover and maintain fundamental models of our ever-changing information environment and to defend us in that environment, both as individuals and collectively, against intentional as well as unintentional attempts to deceive, misinform and otherwise manipulate us. IES is an interdisciplinary science that will require bringing together experts working in areas such as cognitive science, computer science, social science, security, marketing, political campaigning, public policy, and psychology to develop a theoretical as well as an applied engineering methodology for managing the full spectrum of information environment security issues.",2014,Knowledge Discovery and Data Mining,Fields of study: public policyknowledge managementdata miningcomputer science
Frontiers in E-commerce personalization,Sri Subramaniam,2716684431,"E-commerce has largely been a 'Pull' model to date. Offline retailers have nailed discovery, delight, serendipity, and impulse purchases in person with greater success than online commerce sites. However, in an always-on, mobile-first world, companies like Groupon have the opportunity to push the frontier even further than offline retailers or comprehensive sites due to the fact that our smartphones are always with us. The challenge is to provide the right deals to the right user at the right time. That involves learning about the users and their locations, their personal preferences, and predicting which deals are likely to delight them, presenting diversity, discovery and engaging UX to gather user preferences and semantic graph approaches for user-deal matching. This presentation will give insight into how Groupon manages to grapple with these challenges via a data-driven system in order to delight and surprise customers.",2014,Knowledge Discovery and Data Mining,Fields of study: mobile technologye commerceworld wide webdata miningcomputer science
Bringing data science to the speakers of every language,Robert Munro,2603355351,"Speakers of more than 5,000 languages have access to internet and communication technologies. The majority of phones, tablets and computers now ship with language-enabled capabilities like speech-recognition and intelligent auto-correction, and people increasingly interact with data-intensive cloud-based language technologies like search-engines and spam-filters. For both personal and large-scale technologies, the service quality drops or disappears entirely outside of a handful of languages. Speakers of low-resource languages correlate with lower access to healthcare, education and higher vulnerability to disasters. Serving the broadest possible range of languages is crucial to ensuring equitable participation in the global information economy. I will present examples of how natural language processing and distributed human computing are improving the lives of speakers of all the world's languages, in areas including education, disaster-response, health and access to employment. When applying natural language processing to the full diversity of the world's communications, we need to go beyond simple keyword analysis and implement complex technologies that require human-in-the-loop processing to ensure usable accuracy. In recent work where more than a million human judgments were collected on unstructured text and imagery data around natural disasters, I will present observations that debunk recent over-optimistic claims about the utility of social media following disasters. On the positive side, I will share results that show how for-profit technologies are improving people's lives by providing sustainable economic growth opportunities when they support more languages, aligning business objectives with global diversity.",2014,Knowledge Discovery and Data Mining,Fields of study: crowdsourcingsocial changemultimediadata miningsimulationcomputer science
Big data for social good,Nathan Eagle,2710034020,"Petabytes of data about human movements, transactions and communication patterns are being generated by everyday technologies such as mobile phones & credit cards. This unprecedented volume of information facilitates a novel set of research questions applicable to a wide range of development issues. In collaboration involving 237 mobile phone operators across 102 countries, Jana's mobile technology platform can instantly poll and compensate 3.48 billion active mobile subscriptions. This talk will discuss how insights gained from living in Kenya became the genesis of a technology company currently working with global clients in over 50 countries, including P&G, Google, Unilever, Danone, General Mills, Nestle, Johnson & Johnson, Microsoft, the World Bank, and the United Nations. After providing an overview of the mobile and social media landscapes in emerging markets, we discuss a system that implements polls & mobile subscription compensation. The presentation will conclude by emphasizing the value of consumer data in underserved and understudied regions of the world.",2014,Knowledge Discovery and Data Mining,Fields of study: mobile paymentsocial mediaemerging marketsmobile technologydata miningsimulationcomputer science
Proceedings of the 8th Workshop on Social Network Mining and Analysis,Feida Zhu,2658149462,-,2014,Knowledge Discovery and Data Mining,-
Classification of sports types from tracklets,Rikke Gade (MediaTech Institute)Thomas B. Moeslund (Aalborg University),"2227356519,1992979146",-,2014,Knowledge Discovery and Data Mining,-
Interactive Data Mining Considered Harmful (If Done Wrong),Pauli Miettinen (Max Planck Society),2015634213,-,2014,Knowledge Discovery and Data Mining,Fields of study: internet privacyworld wide webdata mining
OpenML: networked science in machine learning,Joaquin Vanschoren (Eindhoven University of Technology)Jan N. van Rijn (Leiden University)Bernd Bischl (Technical University of Dortmund)Luis Torgo (University of Porto),"1983999465,1997815817,161409135,1973271071","Many sciences have made significant breakthroughs by adopting online tools that help organize, structure and mine information that is too detailed to be printed in journals. In this paper, we introduce OpenML, a place for machine learning researchers to share and organize data in fine detail, so that they can work more effectively, be more visible, and collaborate with others to tackle harder problems. We discuss how OpenML relates to other examples of networked science and what benefits it brings for machine learning research, individual scientists, as well as students and practitioners.",2014,Knowledge Discovery and Data Mining,Fields of study: estimationdata sciencemultimediadata miningmachine learningstatisticscomputer science
Mining social media with social theories: a survey,Jiliang Tang (Arizona State University)Yi Chang (Yahoo!)Huan Liu (Arizona State University),"2147392410,2168000538,2122391114","The increasing popularity of social media encourages more and more users to participate in various online activities and produces data in an unprecedented rate. Social media data is big, linked, noisy, highly unstructured and in- complete, and differs from data in traditional data mining, which cultivates a new research field - social media mining. Social theories from social sciences are helpful to explain social phenomena. The scale and properties of social media data are very different from these of data social sciences use to develop social theories. As a new type of social data, social media data has a fundamental question - can we apply social theories to social media data? Recent advances in computer science provide necessary computational tools and techniques for us to verify social theories on large-scale social media data. Social theories have been applied to mining social media. In this article, we review some key social theories in mining social media, their verification approaches, interesting findings, and state-of-the-art algorithms. We also discuss some future directions in this active area of mining social media with social theories.",2014,Knowledge Discovery and Data Mining,Fields of study: social heuristicsclustering high dimensional datasocial networksocial computingdocument clusteringdata sciencedata miningmachine learningcomputer science
Brain network analysis: a data mining perspective,Xiangnan Kong (University of Illinois at Chicago)Philip S. Yu (University of Illinois at Chicago),"2204127537,2125104194","Following the recent advances in neuroimaging technology, the research on brain network analysis becomes an emerging area in data mining community. Brain network data pose many unique challenges for data mining research. For example, in brain networks, the nodes (i.e., the brain regions) and edges (i.e., relationships between brain regions) are usually not given, but should be derived from the neuroimaging data. The network structure can be very noisy and uncertain. Therefore, innovative methods are required for brain network analysis. Many research efforts have been devoted to this area. They have achieved great success in various applications, such as brain network extraction, graph mining, neuroimaging data analysis. In this paper, we review some recent data mining methods which are used in the literature for mining brain network data.",2014,Knowledge Discovery and Data Mining,Fields of study: data sciencebioinformaticsdata miningcomputer science
Mining text and social streams: a review,Charu C. Aggarwal (IBM),2146335907,"The large amount of text data which are continuously produced over time in a variety of large scale applications such as social networks results in massive streams of data. Typically massive text streams are created by very large scale interactions of individuals, or by structured creations of particular kinds of content by dedicated organizations. An example in the latter category would be the massive text streams created by news-wire services. Such text streams provide unprecedented challenges to data mining algorithms from an efficiency perspective. In this paper, we review text stream mining algorithms for a wide variety of problems in data mining such as clustering, classification and topic modeling. A recent challenge arises in the context of social streams , which are generated by large social networks such as Twitter . We also discuss a number of future challenges in this area of research.",2014,Knowledge Discovery and Data Mining,Fields of study: co occurrence networksclustering high dimensional datadocument clusteringtext miningdata scienceworld wide webdata miningmachine learningcomputer science
Clustering high dimensional data: examining differences and commonalities between subspace clustering and text clustering - a position paper,Hans-Peter Kriegel (Ludwig Maximilian University of Munich)Eirini Ntoutsi (Ludwig Maximilian University of Munich),"1919135125,147176459","The goal of this position paper is to contribute to a clear understanding of the commonalities and differences between subspace clustering and text clustering. Often text data is foisted as an ideal fit for subspace clustering due to its high dimensional nature and sparsity of the data. Indeed, the areas of subspace clustering and text clustering share similar challenges and the same goal, the simultaneous extraction of both clusters and the dimensions where these clusters are defined. However, there are fundamental differences between the two areas w.r.t object feature representation, dimension weighting and incorporation of these weights in the dissimilarity computation. We make an attempt to bridge these two domains in order to facilitate the exchange of ideas and best practices between them.",2014,Knowledge Discovery and Data Mining,Fields of study: k medians clusteringflame clusteringsubclubrown clusteringcanopy clustering algorithmdbscancorrelation clusteringconstrained clusteringcure data clustering algorithmsingle linkage clusteringaffinity propagationfuzzy clusteringclustering high dimensional datahierarchical clusteringcluster analysisconsensus clusteringbiclusteringconceptual clusteringdocument clusteringdata miningpattern recognitionmachine learningcomputer science
Predictive analysis of engine health for decision support,Shubhabrata Mukherjee (University of Washington)Aparna S. Varde (Montclair State University)Giti Javidi (Virginia State University)Ehsan Sheyban (Virginia State University),"2157659606,74820909,2034864828,2346886388","Data mining, the discovery of knowledge from data, bridges several disciplines such as database management, artificial intelligence, statistics, visualization and the domain of the data, e.g., biology or engineering. Knowledge discovered by mining the data can be used for various purposes such as developing decision support systems and intelligent tutors. In this paper we present such a data mining problem in the mechanical engineering domain where knowledge discovery from the data is performed using statistical approaches, to conduct predictive analysis for decision support. More specifically, we focus on the engine health problem which consists of using existing data on the behavior of an engine in order to predict whether the engine is capable of functioning well (i.e., it is healthy) and to offer suggestions on preventive maintenance. The data we use for this predictive analysis consists of graphs that plot process parameters such as the vibration and temperature of the engine with respect to time. In this paper we define the problem in detail, propose a solution based on statistical inference techniques, summarize our experimental evaluation and discuss the applications of this work in various fields from a decision support angle.",2014,Knowledge Discovery and Data Mining,Fields of study: estimationintelligent decision support systemdata sciencedata miningmachine learningsimulationstatisticscomputer science
Comprehensible classification models: a position paper,Alex Alves Freitas (University of Kent),2131502281,"The vast majority of the literature evaluates the performance of classification models using only the criterion of predictive accuracy. This paper reviews the case for considering also the comprehensibility (interpretability) of classification models, and discusses the interpretability of five types of classification models, namely decision trees, classification rules, decision tables, nearest neighbors and Bayesian network classifiers. We discuss both interpretability issues which are specific to each of those model types and more generic interpretability issues, namely the drawbacks of using model size as the only criterion to evaluate the comprehensibility of a model, and the use of monotonicity constraints to improve the comprehensibility and acceptance of classification models by users.",2014,Knowledge Discovery and Data Mining,Fields of study: decision tabledecision treedata miningpattern recognitionmachine learningcomputer science
Ensembles for unsupervised outlier detection: challenges and research questions a position paper,Arthur Zimek (Ludwig Maximilian University of Munich)Ricardo J.G.B. Campello (University of São Paulo)Jörg Sander (University of Alberta),"242745652,2043417111,2118842476","Ensembles for unsupervised outlier detection is an emerging topic that has been neglected for a surprisingly long time (although there are reasons why this is more difficult than supervised ensembles or even clustering ensembles). Aggarwal recently discussed algorithmic patterns of outlier detection ensembles, identified traces of the idea in the literature, and remarked on potential as well as unlikely avenues for future transfer of concepts from supervised ensembles. Complementary to his points, here we focus on the core ingredients for building an outlier ensemble, discuss the first steps taken in the literature, and identify challenges for future research.",2014,Knowledge Discovery and Data Mining,Fields of study: decision tabledecision treedata miningpattern recognitionmachine learningcomputer science
Research issues in outlier detection for data streams,Shiblee Sadik (University of Oklahoma)Le Gruenwald (University of Oklahoma),"2165192556,2188709943","In applications, such as sensor networks and power usage monitoring, data are in the form of streams, each of which is an infinite sequence of data points with explicit or implicit timestamps and has special characteristics, such as transiency, uncertainty, dynamic data distribution, multidimensionality, and dynamic relationship. These characteristics introduce new research issues that make outlier detection for stream data more challenging than that for regular (non-stream) data. This paper discusses those research issues for applications where data come from a single stream as well as multiple streams.",2014,Knowledge Discovery and Data Mining,Fields of study: semantic computingworld wide webdata miningdatabasecomputer science
On the Management and Analysis of Our LifeSteps,Nikos Pelekis (University of Piraeus)Yannis Theodoridis (University of Piraeus)Davy Janssens (University of Hasselt),"2020175353,31261168,2132963385","Huge volumes of location information are available nowadays due to the rapid growth of positioning devices (GPS-enabled smartphones and tablets, on-board navigation systems in vehicles, vessels and planes, smart chips for animals, etc.). In the near future, it is unavoidable that this explosion will contribute in what is called the Big Data era, raising high challenges for the data management research community. Instead of trying to manage bigger and bigger volumes of raw data, future Moving Object Database (MOD) systems need to extract and manage (the minimum necessary) semantics of movement. Such semantics can foster next-generation location-based services (LBS) and locationbased social networking (LBSN) applications, building more efficient and effective applications, while in parallel opening new research directions in the field of transportation, urban planning etc. In this article, we first present a novel model that enables the unified management of (raw GPS) trajectories and their semantic counterpart, and then we discuss challenges and solutions on the multidimensional analysis of such real-world semantic-aware mobility databases and data warehouses. Our recent experience from an interdisciplinary EU project we've been participating makes us confident that the envisioned approach will inspire the next wave of research in mobility data management and exploration field.",2014,Knowledge Discovery and Data Mining,Fields of study: mobility modelworld wide webdata miningmachine learningsimulationcomputer science
20 years of pattern mining: a bibliometric survey,Arnaud Giacometti (François Rabelais University)Dominique Haoyuan Li (François Rabelais University)Patrick Marcel (François Rabelais University)Arnaud Soulet (François Rabelais University),"2129444726,2422722325,2137225863,2470748226","In 1993, Rakesh Agrawal, Tomasz Imielinski and Arun N. Swami published one of the founding papers of Pattern Mining: ""Mining Association Rules between Sets of Items in Large Databases"". Beyond the introduction to a new problem, it introduced a new methodology in terms of resolution and evaluation. For two decades, Pattern Mining has been one of the most active fields in Knowledge Discovery in Databases. This paper provides a bibliometric survey of the literature relying on 1,087 publications from five major international conferences: KDD, PKDD, PAKDD, ICDM and SDM. We first measured a slowdown of research dedicated to Pattern Mining while the KDD field continues to grow. Then, we quantified the main contributions with respect to languages, constraints and condensed representations to outline the current directions. We observe a sophistication of languages over the last 20 years, although association rules and itemsets are so far the most studied ones. As expected, the minimal support constraint predominates the extraction of patterns with approximately 50% of the publications. Finally, condensed representations used in 10% of the papers had relative success particularly between 2005 and 2008.",2014,Knowledge Discovery and Data Mining,Fields of study: data scienceworld wide webdata miningmachine learningcomputer science
Adaptive Representations for Tracking Breaking News on Twitter,Igor Brigadir (University College Dublin)Derek Greene (University College Dublin)Pádraig Cunningham (University College Dublin),"258442675,2131371111,2120527611","Twitter is often the most up-to-date source for find- ing and tracking breaking news stories. Therefore, there is considerable interest in developing filters for tweet streams in order to track and summarize stories. This is a non-trivial text analytics task as tweets are short, and standard text similarity metrics often fail as stories evolve over time. In this paper we examine the effective- ness of adaptive text similarity mechanisms for track- ing and summarizing breaking news stories. We evalu- ate the effectiveness of these mechanisms on a number of recent news events for which manually curated time- lines are available. Assessments based on the ROUGE metric indicate that an adaptive similarity mechanism is best suited for tracking evolving stories on Twitter. This retraining strategy allows us to track a news event as it evolves, since the vocabulary used to describe it will naturally change as it develops over time. Given a seed query, our approach can automatically generate chronologi- cal timelines of events from a stream of tweets, while con- tinuously learning new representations of relevant words, phrases, and entities as the story changes. Evaluations per- formed in relation to a set of real-world news events indicate that this approach allows us to track events more accurately, when compared to nonadaptive models and traditional ""bag- of-words"" representations.",2014,Knowledge Discovery and Data Mining,Fields of study: multimediaworld wide webinformation retrievaldata miningmachine learningstatisticscomputer science
"Knowledge Discovery and Data Mining in Biomedical Informatics: The Future Is in Integrative, Interactive Machine Learning Solutions",Andreas Holzinger (Sofia Medical University)Igor Jurisica (University Health Network),"2175384922,164646474","Biomedical research is drowning in data, yet starving for knowledge. Current challenges in biomedical research and clinical practice include information overload - the need to combine vast amounts of structured, semi- structured, weakly structured data and vast amounts of unstructured information - and the need to optimize workflows, processes and guidelines, to increase capacity while reducing costs and improving efficiencies. In this paper we provide a very short overview on interactive and integrative solutions for knowledge discovery and data mining. In particular, we emphasize the benefits of including the end user into the ""interactive"" knowledge discovery process. We describe some of the most important challenges, including the need to develop and apply novel methods, algorithms and tools for the integration, fusion, pre-processing, mapping, analysis and interpretation of complex biomedical data with the aim to identify testable hypotheses, and build realistic models. The HCI-KDD approach, which is a synergistic combination of methodologies and approaches of two areas, Human-Computer Interaction (HCI) and Knowledge Discovery & Data Mining (KDD), offer ideal conditions towards solving these challenges: with the goal of supporting human intelligence with machine intelligence. There is an urgent need for integrative and interactive machine learning solutions, because no medical doctor or biomedical researcher can keep pace today with the increasingly large and complex data sets - often called ""Big Data"".",2014,Knowledge Discovery and Data Mining,Fields of study: knowledge extractiondata scienceknowledge managementdata miningmachine learningcomputer science
"Biomedical Text Mining: State-of-the-Art, Open Problems and Future Challenges",Andreas Holzinger (Sofia Medical University)Johannes Schantl (Sofia Medical University)Miriam Schroettner (Sofia Medical University)Christin Seifert (University of Passau)Karin Verspoor (University of Melbourne),"2175384922,2143053657,2413031073,2114505653,717012417",-,2014,Knowledge Discovery and Data Mining,Fields of study: data scienceinformation retrievaldata mining
"Intelligent Integrative Knowledge Bases: Bridging Genomics, Integrative Biology and Translational Medicine",Hoan Nguyen (Centre national de la recherche scientifique)Julie D. Thompson (Centre national de la recherche scientifique)Patrick SchutzOlivier Poch (University of Strasbourg),"2423587024,2153440608,2100866327,2042388667",-,2014,Knowledge Discovery and Data Mining,Fields of study: computational biologybioinformatics
On Computationally-Enhanced Visual Analysis of Heterogeneous Data and Its Application in Biomedical Informatics,Cagatay Turkay (City University of Seattle)Fleur Jeanquartier (Sofia Medical University)Andreas Holzinger (Sofia Medical University)Helwig Hauser (University of Bergen),"2033870557,307740072,2175384922,2138458960","With the advance of new data acquisition and generation technologies, the biomedical domain is becoming increasingly data-driven. Thus, understanding the information in large and complex data sets has been in the focus of several research fields such as statistics, data mining, machine learning, and visualization. While the first three fields predomi- nantly rely on computational power, visualization relies mainly on human perceptual and cognitive capabilities for extracting information. Data vi- sualization, similar to Human-Computer Interaction, attempts an appro- priate interaction between human and data to interactively exploit data sets. Specifically within the analysis of complex data sets, visualization researchers have integrated computational methods to enhance the in- teractive processes. In this state-of-the-art report, we investigate how such an integration is carried out. We study the related literature with respect to the underlying analytical tasks and methods of integration. In addition, we focus on how such methods are applied to the biomedical domain and present a concise overview within our taxonomy. Finally, we discuss some open problems and future challenges.",2014,Knowledge Discovery and Data Mining,Fields of study: information visualizationdata sciencedata miningmachine learningcomputer science
Interactive Data Exploration Using Pattern Mining,Matthijs van Leeuwen (Katholieke Universiteit Leuven),2143928993,-,2014,Knowledge Discovery and Data Mining,Fields of study: web miningtext miningdata miningcomputer science
On Entropy-Based Data Mining,Andreas Holzinger (Sofia Medical University)Matthias Hörtenhuber (Austrian Institute of Technology)Christopher C. Mayer (Austrian Institute of Technology)Martin Bachler (Austrian Institute of Technology)Siegfried Wassertheurer (Austrian Institute of Technology)Armando J. Pinho (University of Aveiro)David Koslicki (Oregon State University),"2175384922,1956771834,2107120584,1974055686,2233095327,2067434268,233787725","In the real world, we are confronted not only with complex and high-dimensional data sets, but usually with noisy, incomplete and uncertain data, where the application of traditional methods of knowl- edge discovery and data mining always entail the danger of modeling artifacts. Originally, information entropy was introduced by Shannon (1949), as a measure of uncertainty in the data. But up to the present, there have emerged many different types of entropy methods with a large number of different purposes and possible application areas. In this pa- per, we briefly discuss the applicability of entropy methods for the use in knowledge discovery and data mining, with particular emphasis on biomedical data. We present a very short overview of the state-of-the- art, with focus on four methods: Approximate Entropy (ApEn), Sample Entropy (SampEn), Fuzzy Entropy (FuzzyEn), and Topological Entropy (FiniteTopEn). Finally, we discuss some open problems and future re- search challenges.",2014,Knowledge Discovery and Data Mining,Fields of study: information diagramdata sciencedata miningartificial intelligencemachine learningstatisticsmathematics
Darwin or Lamarck? Future Challenges in Evolutionary Algorithms for Knowledge Discovery and Data Mining,Katharina Holzinger (Sofia Medical University)Vasile Palade (Coventry University)Raul Rabadan (Columbia University)Andreas Holzinger (Sofia Medical University),"786595236,206588628,2035355927,2175384922","Evolutionary Algorithms (EAs) are a fascinating branch of computational intelligence with much potential for use in many application areas. The fundamental principle of EAs is to use ideas inspired by the biological mechanisms observed in nature, such as selection and genetic changes, to find the best solution for a given optimization problem. Generally, EAs use iterative processes, by growing a population of solutions selected in a guided random search and using parallel processing, in order to achieve a desired result. Such population based approaches, for example particle swarm and ant colony optimization (inspired from biology), are among the most popular metaheuristic methods being used in machine learning, along with others such as the simulated annealing (inspired from thermodynamics). In this paper, we provide a short survey on the state-of-the-art of EAs, beginning with some background on the theory of evolution and contrasting the original ideas of Darwin and Lamarck; we then continue with a discussion on the analogy between biological and computational sciences, and briefly describe some fundamentals of EAs, including the Genetic Algorithms, Genetic Programming, Evolution Strategies, Swarm Intelligence Algorithms (i.e., Particle Swarm Optimization, Ant Colony Optimization, Bacteria Foraging Algorithms, Bees Algorithm, Invasive Weed Optimization), Memetic Search, Differential Evolution Search, Artificial Immune Systems, Gravitational Search Algorithm, Intelligent Water Drops Algorithm. We conclude with a short description of the usefulness of EAs for Knowledge Discovery and Data Mining tasks and present some open problems and challenges to further stimulate research.",2014,Knowledge Discovery and Data Mining,Fields of study: meta optimizationmulti swarm optimizationparallel metaheuristicmetaheuristicevolutionary computationevolutionary algorithmbioinformaticsartificial intelligencemachine learning
Multi-touch Graph-Based Interaction for Knowledge Discovery on Mobile Devices: State-of-the-Art and Future Challenges,Andreas Holzinger (Sofia Medical University)Bernhard Ofner (Sofia Medical University)Matthias Dehmer (Vienna University of Technology),"2175384922,2034196914,2046685608","Graph-based knowledge representation is a hot topic for some years and still has a lot of research potential, particularly in the ad- vancement in the application of graph-theory for creating benefits in the biomedical domain. Graphs are most powerful tools to map struc- tures within a given data set and to recognize relationships between specific data objects. Many advantages of graph-based data structures can be found in the applicability of methods from network analysis, topo- logy and data mining (e.g. small-world phenomenon, cluster analysis). In this paper we present the state-of-the-art in graph-based approaches for multi-touch interaction on mobile devices and we highlight some open problems to stimulate further research and future developments. This is particularly important in the medical domain, as a conceptual graph analysis may provide novel insights on hidden patterns in data, hence support interactive knowledge discovery.",2014,Knowledge Discovery and Data Mining,Fields of study: data sciencebioinformaticsdata miningmachine learningcomputer science
Protecting Anonymity in Data-Driven Biomedical Science,Peter Kieseberg (Sofia Medical University)Heidelinde HobelSebastian Schrittwieser (St. Pölten University of Applied Sciences)Edgar R. Weippl (Vienna University of Technology)Andreas Holzinger (Sofia Medical University),"180899930,1979468461,2015770213,989270,2175384922",-,2014,Knowledge Discovery and Data Mining,Fields of study: internet privacy
Visual Data Mining: Effective Exploration of the Biological Universe,David Otasek (University Health Network)Chiara Pastrello (University Health Network)Andreas Holzinger (Sofia Medical University)Igor Jurisica (University Health Network),"7810615,1980032963,2175384922,164646474","Visual Data Mining (VDM) is supported by interactive and scalable network visualization and analysis, which in turn enables effective exploration and communication of ideas within multiple biological and biomedical fields. Large networks, such as the protein interactome or transcriptional regulatory networks, contain hundreds of thousands of objects and millions of relationships. These networks are continuously evolving as new knowledge becomes available, and their content is richly annotated and can be presented in many different ways. Attempting to discover knowledge and new theories within this complex data sets can involve many workflows, such as accurately representing many formats of source data, merging heterogeneous and distributed data sources, complex database searching, integrating results from multiple computational and mathematical analyses, and effectively visualizing properties and results. Our experience with biology researchers has required us to address their needs and requirements in the design and development of a scalable and interactive network visualization and analysis platform, NAViGaTOR, now in its third major release.",2014,Knowledge Discovery and Data Mining,Fields of study: data sciencebioinformaticsdata miningmachine learningcomputer science
"Understanding Passenger Patterns in Public Transit Through Smart Card and Socioeconomic Data: A case study in Rennes, France",Mohamed Khalil El Mahrsi (IFSTTAR)Etienne Come (Institut national de recherche sur les transports et leur sécurité)Johanna BaroLatifa Oukhellou (IFSTTAR),"2541806543,1941834561,2175208763,2003561032","Data collected by Automated Fare Collection (AFC) systems are a valuable resource for studying the travel habits of large city inhabitants. In this paper, we present an approach to mining the temporal behavior of the passengers in a public transportation system in order to extract relevant and easily interpretable clusters. Such classification can be useful for several applications. It may help transport operators better know the demand of their customers and propose targeted incentives, services, and tools accordingly. From a city perspective, this may also help redesign and improve existing transportation policies. To achieve this objective, an additional step of analysis is required and the clustering results need to be contextualized. The spatial location of the different types of passengers is then of great interest. We propose a first step in this direction through a rough estimation of the regular passengers' 'residence' location and the analysis of socioeconomic information available at a fine-grained spatial level. The approach is applied on a real dataset from the metropolitan area of Rennes (France) with four weeks of smart card data containing trips made by both bus and subway.",2014,Knowledge Discovery and Data Mining,Fields of study: public transport
Biobanks - A Source of Large Biological Data Sets: Open Problems and Future Challenges,Berthold Huppertz (Medical University of Graz)Andreas Holzinger (Medical University of Graz),"2427096653,2175384922","Biobanks are collections of biological samples (e.g. tissues, blood and derivatives, other body fluids, cells, DNA, etc.) and their associated data. Consequently, human biobanks represent collections of human samples and data and are of fundamental importance for scientific research as they are an excellent resource to access and measure biological constituents that can be used to monitor the status and trends of both health and disease. Most -omics data trust on a secure access to these collections of stored human samples to provide the basis for establishing the ranges and frequencies of expression. However, there are many open questions and future challenges associated with the large amounts of heterogeneous data, ranging from pre-processing, data integration and data fusion to knowledge discovery and data mining along with a strong focus on privacy, data protection, safety and security.",2014,Knowledge Discovery and Data Mining,Fields of study: data sciencebioinformaticsdata miningcomputer science
On the Generation of Point Cloud Data Sets: Step One in the Knowledge Discovery Process,Andreas Holzinger (Sofia Medical University)Bernd Malle (Sofia Medical University)Marcus Bloice (Sofia Medical University)Marco Wiltgen (Sofia Medical University)Massimo Ferri (University of Bologna)Ignazio Stanganelli (Czech Radio)Rainer Hofmann-Wellenhof (University of Graz),"2175384922,2103161152,2020313699,755903096,2123126515,2084705057,1952126307",-,2014,Knowledge Discovery and Data Mining,Fields of study: data pre processingwatershedcutmathematical morphologygraphimage analysistheoretical computer sciencecomputer visiondata miningcomputer science
A Policy-Based Cleansing and Integration Framework for Labour and Healthcare Data,Roberto Boselli (University of Milano-Bicocca)Mirko Cesarini (University of Milano-Bicocca)Fabio Mercorio (University of Milano-Bicocca)Mario Mezzanzanica (University of Milano-Bicocca),"2689059873,2691200976,2677117856,2618707453",-,2014,Knowledge Discovery and Data Mining,Fields of study: operating systemdatabasecomputer science
On Topological Data Mining,Andreas Holzinger (Sofia Medical University),2175384922,-,2014,Knowledge Discovery and Data Mining,-
A Kernel-Based Framework for Medical Big-Data Analytics,David Windridge (University of Surrey)Miroslaw Bober (University of Surrey),"1920375269,2571009248","The recent trend towards standardization of Electronic Health Records (EHRs) represents a significant opportunity and challenge for medical big-data analytics. The challenge typically arises from the nature of the data which may be heterogeneous, sparse, very high-dimensional, incomplete and inaccurate. Of these, standard pattern recognition methods can typically address issues of high-dimensionality, sparsity and inaccuracy. The remaining issues of incompleteness and heterogeneity however are problematic; data can be as diverse as handwritten notes, blood-pressure readings and MR scans, and typically very little of this data will be co-present for each patient at any given time interval. We therefore advocate a kernel-based framework as being most appropriate for handling these issues, using the neutral point substitution method to accommodate missing inter-modal data. For pre-processing of image-based MR data we advocate a Deep Learning solution for contextual areal segmentation, with edit-distance based kernel measurement then used to characterize relevant morphology.",2014,Knowledge Discovery and Data Mining,Fields of study: data sciencedata miningartificial intelligencemachine learningstatisticscomputer science
Resources for Studying Statistical Analysis of Biomedical Data and R,Mei Kobayashi (IBM),2255525341,-,2014,Knowledge Discovery and Data Mining,-
Formalising the subjective interestingness of a linear projection of a data set : two examples,De BieTijl,"2671491033,2646176241",-,2014,Knowledge Discovery and Data Mining,Fields of study: theoretical computer sciencediscrete mathematicsdatabase
Addressing Big Data Time Series: Mining Trillions of Time Series Subsequences Under Dynamic Time Warping,"Thanawin Rakthanmanon (University of California, Riverside)Bilson J. L. Campana (University of California, Riverside)Abdullah Mueen (University of California, Riverside)Gustavo E. A. P. A. Batista (University of São Paulo)M. Brandon Westover (Brigham and Women's Hospital)Qiang Zhu 0002 (University of California, Riverside)Jesin Zakaria (University of California, Riverside)Eamonn J. Keogh (University of California, Riverside)","141314290,2125777994,2083987245,2165222361,2037513643,2312006013,2129110089,2170070822","Most time series data mining algorithms use similarity search as a core subroutine, and thus the time taken for similarity search is the bottleneck for virtually all time series data mining algorithms, including classification, clustering, motif discovery, anomaly detection, and so on. The difficulty of scaling a search to large datasets explains to a great extent why most academic work on time series data mining has plateaued at considering a few millions of time series objects, while much of industry and science sits on billions of time series objects waiting to be explored. In this work we show that by using a combination of four novel ideas we can search and mine massive time series for the first time. We demonstrate the following unintuitive fact: in large datasets we can exactly search under Dynamic Time Warping (DTW) much more quickly than the current state-of-the-art Euclidean distance search algorithms. We demonstrate our work on the largest set of time series experiments ever attempted. In particular, the largest dataset we consider is larger than the combined size of all of the time series datasets considered in all data mining papers ever published. We explain how our ideas allow us to solve higher-level time series data mining problems such as motif discovery and clustering at scales that would otherwise be untenable. Moreover, we show how our ideas allow us to efficiently support the uniform scaling distance measure, a measure whose utility seems to be underappreciated, but which we demonstrate here. In addition to mining massive datasets with up to one trillion datapoints, we will show that our ideas also have implications for real-time monitoring of data streams, allowing us to handle much faster arrival rates and/or use cheaper and lower powered devices than are currently possible.",2013,Knowledge Discovery and Data Mining,Fields of study: time seriesdata sciencedata miningmachine learningstatisticscomputer science
PathSelClus: Integrating Meta-Path Selection with User-Guided Object Clustering in Heterogeneous Information Networks,"Yizhou Sun (University of Illinois at Urbana–Champaign)Brandon Norick (University of Illinois at Urbana–Champaign)Jiawei Han (University of Illinois at Urbana–Champaign)Xifeng Yan (University of California, Santa Barbara)Philip S. Yu (University of Illinois at Chicago)Xiao Yu (University of Illinois at Urbana–Champaign)","2131539564,2222819066,2121939561,2116657824,2125104194,2160715520","Real-world, multiple-typed objects are often interconnected, forming heterogeneous information networks. A major challenge for link-based clustering in such networks is their potential to generate many different results, carrying rather diverse semantic meanings. In order to generate desired clustering, we propose to use meta-path , a path that connects object types via a sequence of relations, to control clustering with distinct semantics. Nevertheless, it is easier for a user to provide a few examples (seeds) than a weighted combination of sophisticated meta-paths to specify her clustering preference. Thus, we propose to integrate meta-path selection with user-guided clustering to cluster objects in networks, where a user first provides a small set of object seeds for each cluster as guidance. Then the system learns the weight for each meta-path that is consistent with the clustering result implied by the guidance, and generates clusters under the learned weights of meta-paths. A probabilistic approach is proposed to solve the problem, and an effective and efficient iterative algorithm, PathSelClus , is proposed to learn the model, where the clustering quality and the meta-path weights mutually enhance each other. Our experiments with several clustering tasks in two real networks and one synthetic network demonstrate the power of the algorithm in comparison with the baselines.",2013,Knowledge Discovery and Data Mining,Fields of study: flame clusteringhierarchical clustering of networksbrown clusteringcanopy clustering algorithmdbscancorrelation clusteringhierarchical network modelconstrained clusteringdata stream clusteringcure data clustering algorithmaffinity propagationfuzzy clusteringclustering high dimensional datacluster analysisconsensus clusteringbiclusteringconceptual clusteringtheoretical computer sciencedata miningmachine learningcomputer science
Instance Annotation for Multi-Instance Multi-Label Learning,Forrest Briggs (Oregon State University)Xiaoli Z. Fern (Oregon State University)Raviv Raich (Oregon State University)Qi Lou (Oregon State University),"2165737930,2018589324,2150540218,2112239549","Multi-instance multi-label learning (MIML) is a framework for supervised classification where the objects to be classified are bags of instances associated with multiple labels. For example, an image can be represented as a bag of segments and associated with a list of objects it contains. Prior work on MIML has focused on predicting label sets for previously unseen bags. We instead consider the problem of predicting instance labels while learning from data labeled only at the bag level. We propose a regularized rank-loss objective designed for instance annotation, which can be instantiated with different aggregation models connecting instance-level labels with bag-level label sets. The aggregation models that we consider can be factored as a linear function of a “support instance” for each class, which is a single feature vector representing a whole bag. Hence we name our proposed methods rank-loss Support Instance Machines (SIM). We propose two optimization methods for the rank-loss objective, which is nonconvex. One is a heuristic method that alternates between updating support instances, and solving a convex problem in which the support instances are treated as constant. The other is to apply the constrained concave-convex procedure (CCCP), which can also be interpreted as iteratively updating support instances and solving a convex problem. To solve the convex problem, we employ the Pegasos framework of primal subgradient descent, and prove that it finds an e-suboptimal solution in runtime that is linear in the number of bags, instances, and 1/e. Additionally, we suggest a method of extending the linear learning algorithm to nonlinear classification, without increasing the runtime asymptotically. Experiments on artificial and real-world datasets including images and audio show that the proposed methods achieve higher accuracy than other loss functions used in prior work, e.g., Hamming loss, and recent work in ambiguous label classification.",2013,Knowledge Discovery and Data Mining,Fields of study: bioacousticssubgradient methodsupport vector machineautomatic image annotationinstance based learningdata miningpattern recognitionmachine learningcomputer sciencemathematics
Introduction to the Special Issue ACM SIGKDD 2012,Deepak Agarwal (Yahoo!)Rich Caruana (Cornell University)Jian Pei (Simon Fraser University)Ke Wang,"2591515730,2106722673,2126330539,2677045115",-,2013,Knowledge Discovery and Data Mining,-
Active Sampling for Entity Matching with Guarantees,Kedar Bellare (Yahoo!)Suresh Iyengar (Yahoo!)Aditya G. Parameswaran (Yahoo!)Vibhor Rastogi (Yahoo!),"1829235141,2230643152,2077695977,2184633580","In entity matching, a fundamental issue while training a classifier to label pairs of entities as either duplicates or nonduplicates is the one of selecting informative training examples. Although active learning presents an attractive solution to this problem, previous approaches minimize the misclassification rate (0--1 loss) of the classifier, which is an unsuitable metric for entity matching due to class imbalance (i.e., many more nonduplicate pairs than duplicate pairs). To address this, a recent paper [Arasu et al. 2010] proposes to maximize recall of the classifier under the constraint that its precision should be greater than a specified threshold. However, the proposed technique requires the labels of all n input pairs in the worst case. Our main result is an active learning algorithm that approximately maximizes recall of the classifier while respecting a precision constraint with provably sublinear label complexity (under certain distributional assumptions). Our algorithm uses as a black box any active learning module that minimizes 0--1 loss. We show that label complexity of our algorithm is at most log n times the label complexity of the black box, and also bound the difference in the recall of classifier learnt by our algorithm and the recall of the optimal classifier satisfying the precision constraint. We provide an empirical evaluation of our algorithm on several real-world matching data sets that demonstrates the effectiveness of our approach.",2013,Knowledge Discovery and Data Mining,Fields of study: data deduplicationactive learningdata miningpattern recognitionmachine learningstatisticscomputer sciencemathematics
U-Air: when urban air quality inference meets big data,Yu Zheng (Microsoft)Furui Liu (Zhejiang University)Hsun-Ping Hsieh (National Taiwan University),"2145115012,2149758747,2109618838","Information about urban air quality, e.g., the concentration of PM2.5, is of great importance to protect human health and control air pollution. While there are limited air-quality-monitor-stations in a city, air quality varies in urban spaces non-linearly and depends on multiple factors, such as meteorology, traffic volume, and land uses. In this paper, we infer the real-time and fine-grained air quality information throughout a city, based on the (historical and real-time) air quality data reported by existing monitor stations and a variety of data sources we observed in the city, such as meteorology, traffic flow, human mobility, structure of road networks, and point of interests (POIs). We propose a semi-supervised learning approach based on a co-training framework that consists of two separated classifiers. One is a spatial classifier based on an artificial neural network (ANN), which takes spatially-related features (e.g., the density of POIs and length of highways) as input to model the spatial correlation between air qualities of different locations. The other is a temporal classifier based on a linear-chain conditional random field (CRF), involving temporally-related features (e.g., traffic and meteorology) to model the temporal dependency of air quality in a location. We evaluated our approach with extensive experiments based on five real data sources obtained in Beijing and Shanghai. The results show the advantages of our method over four categories of baselines, including linear/Gaussian interpolations, classical dispersion models, well-known classification models like decision tree and CRF, and ANN.",2013,Knowledge Discovery and Data Mining,Fields of study: air quality indexdata miningsimulation
Ad click prediction: a view from the trenches,H. Brendan McMahan (Google)Gary Holt (Google)David Sculley (Google)Michael Young (Google)Dietmar Ebner (Google)Julian Grady (Google)Lan Nie (Google)Todd Phillips (Google)Eugene Davydov (Google)Daniel Golovin (Google)Sharat Chikkerur (Google)Dan Liu (Google)Martin Wattenberg (Google)Arnar Mar Hrafnkelsson (Google)Tom Boulos (Google)Jeremy Kubica (Google),"2388919843,2107544790,2134436889,2344430977,2232032090,2162465107,2224745174,2693567963,2131657938,2323802842,989069167,2617453220,2139539936,2224402626,2439775324,2612075600","Predicting ad click-through rates (CTR) is a massive-scale learning problem that is central to the multi-billion dollar online advertising industry. We present a selection of case studies and topics drawn from recent experiments in the setting of a deployed CTR prediction system. These include improvements in the context of traditional supervised learning based on an FTRL-Proximal online learning algorithm (which has excellent sparsity and convergence properties) and the use of per-coordinate learning rates. We also explore some of the challenges that arise in a real-world system that may appear at first to be outside the domain of traditional machine learning research. These include useful tricks for memory savings, methods for assessing and visualizing performance, practical methods for providing confidence estimates for predicted probabilities, calibration methods, and methods for automated management of features. Finally, we also detail several directions that did not turn out to be beneficial for us, despite promising results elsewhere in the literature. The goal of this paper is to highlight the close relationship between theoretical advances and practical engineering in this industrial setting, and to show the depth of challenges that appear when applying traditional machine learning methods in a complex dynamic system.",2013,Knowledge Discovery and Data Mining,Fields of study: online machine learningstabilityactive learningonline advertisingdata sciencedata miningmachine learningsimulationstatisticscomputer science
Connecting users across social media sites: a behavioral-modeling approach,Reza Zafarani (Arizona State University)Huan Liu (Arizona State University),"2055981231,2122391114","People use various social media for different purposes. The information on an individual site is often incomplete. When sources of complementary information are integrated, a better profile of a user can be built to improve online services such as verifying online information. To integrate these sources of information, it is necessary to identify individuals across social media sites. This paper aims to address the cross-media user identification problem. We introduce a methodology (MOBIUS) for finding a mapping among identities of individuals across social media sites. It consists of three key components: the first component identifies users' unique behavioral patterns that lead to information redundancies across sites; the second component constructs features that exploit information redundancies due to these behavioral patterns; and the third component employs machine learning for effective user identification. We formally define the cross-media user identification problem and show that MOBIUS is effective in identifying users across social media sites. This study paves the way for analysis and mining across social media sites, and facilitates the creation of novel online services across sites.",2013,Knowledge Discovery and Data Mining,Fields of study: mobius stripmultimediaworld wide webdata mining
Auto-WEKA: combined selection and hyperparameter optimization of classification algorithms,Chris Thornton (University of British Columbia)Frank Hutter (University of British Columbia)Holger H. Hoos (University of British Columbia)Kevin Leyton-Brown (University of British Columbia),"2237870136,2149806870,87749410,191979884","Many different machine learning algorithms exist; taking into account each algorithm's hyperparameters, there is a staggeringly large number of possible alternatives overall. We consider the problem of simultaneously selecting a learning algorithm and setting its hyperparameters, going beyond previous work that attacks these issues separately. We show that this problem can be addressed by a fully automated approach, leveraging recent innovations in Bayesian optimization. Specifically, we consider a wide range of feature selection techniques (combining 3 search and 8 evaluator methods) and all classification approaches implemented in WEKA's standard distribution, spanning 2 ensemble methods, 10 meta-methods, 27 base classifiers, and hyperparameter settings for each classifier. On each of 21 popular datasets from the UCI repository, the KDD Cup 09, variants of the MNIST dataset and CIFAR-10, we show classification performance often much better than using standard selection and hyperparameter optimization methods. We hope that our approach will help non-expert users to more effectively identify machine learning algorithms and hyperparameter settings appropriate to their applications, and hence to achieve improved performance.",2013,Knowledge Discovery and Data Mining,Fields of study: hyperparameter optimizationmodel selectiondata miningpattern recognitionmachine learningstatisticscomputer science
LCARS: a location-content-aware recommender system,"Hongzhi Yin (Peking University)Yizhou Sun (Northeastern University)Bin Cui (Peking University)Zhiting Hu (Peking University)Ling Chen (University of Technology, Sydney)","2145818752,2131539564,2070862325,2156033562,2633547329","Newly emerging location-based and event-based social network services provide us with a new platform to understand users' preferences based on their activity history. A user can only visit a limited number of venues/events and most of them are within a limited distance range, so the user-item matrix is very sparse, which creates a big challenge for traditional collaborative filtering-based recommender systems. The problem becomes more challenging when people travel to a new city where they have no activity history. In this paper, we propose LCARS, a location-content-aware recommender system that offers a particular user a set of venues (e.g., restaurants) or events (e.g., concerts and exhibitions) by giving consideration to both personal interest and local preference. This recommender system can facilitate people's travel not only near the area in which they live, but also in a city that is new to them. Specifically, LCARS consists of two components: offline modeling and online recommendation. The offline modeling part, called LCA-LDA, is designed to learn the interest of each individual user and the local preference of each individual city by capturing item co-occurrence patterns and exploiting item contents. The online recommendation part automatically combines the learnt interest of the querying user and the local preference of the querying city to produce the top- k recommendations. To speed up this online process, a scalable query processing technique is developed by extending the classic Threshold Algorithm (TA). We evaluate the performance of our recommender system on two large-scale real data sets, DoubanEvent and Foursquare. The results show the superiority of LCARS in recommending spatial items for users, especially when traveling to new cities, in terms of both effectiveness and efficiency.",2013,Knowledge Discovery and Data Mining,Fields of study: cold startlocation based servicerecommender systemmultimediaworld wide webdata miningmachine learningcomputer science
Real-time air quality monitoring through mobile sensing in metropolitan areas,Srinivas Devarakonda (Rutgers University)Parveen Sevusu (Rutgers University)Hongzhang Liu (Rutgers University)Ruilin Liu (Rutgers University)Liviu Iftode (Rutgers University)Badri Nath (Rutgers University),"1992616387,2227050094,2142624386,2129014003,1982523417,2124176769","Traditionally, pollution measurements are performed using expensive equipment at fixed locations or dedicated mobile equipment laboratories. This is a coarse-grained and expensive approach where the pollution measurements are few and far in-between. In this paper, we present a vehicular-based mobile approach for measuring fine-grained air quality in real-time. We propose two cost effective data farming models -- one that can be deployed on public transportation and the second a personal sensing device. We present preliminary prototypes and discuss implementation challenges and early experiments.",2013,Knowledge Discovery and Data Mining,Fields of study: air quality indexpollutionsocial network
Why people hate your app: making sense of user feedback in a mobile app store,"Bin Fu (Carnegie Mellon University)Jialiu Lin (Carnegie Mellon University)Lei Li (University of California, Berkeley)Christos Faloutsos (Carnegie Mellon University)Jason I. Hong (Carnegie Mellon University)Norman M. Sadeh (Carnegie Mellon University)","2658876590,2113828048,2608982952,2198983026,2141131632,100648250","User review is a crucial component of open mobile app markets such as the Google Play Store. How do we automatically summarize millions of user reviews and make sense out of them? Unfortunately, beyond simple summaries such as histograms of user ratings, there are few analytic tools that can provide insights into user reviews. In this paper, we propose Wiscom, a system that can analyze tens of millions user ratings and comments in mobile app markets at three different levels of detail. Our system is able to (a) discover inconsistencies in reviews; (b) identify reasons why users like or dislike a given app, and provide an interactive, zoomable view of how users' reviews evolve over time; and (c) provide valuable insights into the entire app market, identifying users' major concerns and preferences of different types of apps. Results using our techniques are reported on a 32GB dataset consisting of over 13 million user reviews of 171,493 Android apps in the Google Play Store. We discuss how the techniques presented herein can be deployed to help a mobile app market operator such as Google as well as individual app developers and end-users.",2013,Knowledge Discovery and Data Mining,Fields of study: topic modelsentiment analysistext mininginternet privacymultimediaworld wide webdata miningmachine learningcomputer science
Learning geographical preferences for point-of-interest recommendation,Bin Liu (Rutgers–Newark)Yanjie Fu (Rutgers–Newark)Zijun Yao (Rutgers–Newark)Hui Xiong (Rutgers–Newark),"2428181972,2168873515,2229271911,2153710278","The problem of point of interest (POI) recommendation is to provide personalized recommendations of places of interests, such as restaurants, for mobile users. Due to its complexity and its connection to location based social networks (LBSNs), the decision process of a user choose a POI is complex and can be influenced by various factors, such as user preferences, geographical influences, and user mobility behaviors. While there are some studies on POI recommendations, it lacks of integrated analysis of the joint effect of multiple factors. To this end, in this paper, we propose a novel geographical probabilistic factor analysis framework which strategically takes various factors into consideration. Specifically, this framework allows to capture the geographical influences on a user's check-in behavior. Also, the user mobility behaviors can be effectively exploited in the recommendation model. Moreover, the recommendation model can effectively make use of user check-in count data as implicity user feedback for modeling user preferences. Finally, experimental results on real-world LBSNs data show that the proposed recommendation method outperforms state-of-the-art latent factor models with a significant margin.",2013,Knowledge Discovery and Data Mining,Fields of study: point of interestuser modelingrecommender systeminternet privacyworld wide webdata mining
TurboGraph: a fast parallel graph engine handling billion-scale graphs in a single PC,Wook-Shin Han (Pohang University of Science and Technology)Sangyeon Lee (Pohang University of Science and Technology)Kyungyeol Park (Pohang University of Science and Technology)Jeong-Hoon Lee (Pohang University of Science and Technology)Min-Soo Kim (Daegu Gyeongbuk Institute of Science and Technology)Jinha Kim (Pohang University of Science and Technology)Hwanjo Yu (Pohang University of Science and Technology),"2307556295,2226489450,2229720092,2143564406,2261297707,2603150764,2257366035","Graphs are used to model many real objects such as social networks and web graphs. Many real applications in various fields require efficient and effective management of large-scale graph structured data. Although distributed graph engines such as GBase and Pregel handle billion-scale graphs, the user needs to be skilled at managing and tuning a distributed system in a cluster, which is a nontrivial job for the ordinary user. Furthermore, these distributed systems need many machines in a cluster in order to provide reasonable performance. In order to address this problem, a disk-based parallel graph engine called Graph-Chi, has been recently proposed. Although Graph-Chi significantly outperforms all representative (disk-based) distributed graph engines, we observe that Graph-Chi still has serious performance problems for many important types of graph queries due to 1) limited parallelism and 2) separate steps for I/O processing and CPU processing. In this paper, we propose a general, disk-based graph engine called TurboGraph to process billion-scale graphs very efficiently by using modern hardware on a single PC. TurboGraph is the first truly parallel graph engine that exploits 1) full parallelism including multi-core parallelism and FlashSSD IO parallelism and 2) full overlap of CPU processing and I/O processing as much as possible. Specifically, we propose a novel parallel execution model, called pin-and-slide . TurboGraph also provides engine-level operators such as BFS which are implemented under the pin-and-slide model. Extensive experimental results with large real datasets show that TurboGraph consistently and significantly outperforms Graph-Chi by up to four orders of magnitude ! Our implementation of TurboGraph is available at ``http://wshan.net/turbograph}"" as executable files.",2013,Knowledge Discovery and Data Mining,Fields of study: graph productbig datatheoretical computer scienceparallel computingdistributed computingdata miningmachine learningcomputer science
"A review of urban computing for mobile phone traces: current methods, challenges and opportunities",Shan Jiang (Massachusetts Institute of Technology)Gaston A. Fiore (Massachusetts Institute of Technology)Yingxiang Yang (Massachusetts Institute of Technology)Joseph Ferreira (Massachusetts Institute of Technology)Emilio Frazzoli (Massachusetts Institute of Technology)Marta C. González (Massachusetts Institute of Technology),"2674653191,2133609701,2146850136,2147702459,2281670987,2277322729","In this work, we present three classes of methods to extract information from triangulated mobile phone signals, and describe applications with different goals in spatiotemporal analysis and urban modeling. Our first challenge is to relate extracted information from phone records (i.e., a set of time-stamped coordinates estimated from signal strengths) with destinations by each of the million anonymous users. By demonstrating a method that converts phone signals into small grid cell destinations, we present a framework that bridges triangulated mobile phone data with previously established findings obtained from data at more coarse-grained resolutions (such as at the cell tower or census tract levels). In particular, this method allows us to relate daily mobility networks, called motifs here, with trip chains extracted from travel diary surveys. Compared with existing travel demand models mainly relying on expensive and less-frequent travel survey data, this method represents an advantage for applying ubiquitous mobile phone data to urban and transportation modeling applications. Second, we present a method that takes advantage of the high spatial resolution of the triangulated phone data to infer trip purposes by examining semantic-enriched land uses surrounding destinations in individual's motifs. In the final section, we discuss a portable computational architecture that allows us to manage and analyze mobile phone data in geospatial databases, and to map mobile phone trips onto spatial networks such that further analysis about flows and network performances can be done. The combination of these three methods demonstrate the state-of-the-art algorithms that can be adapted to triangulated mobile phone data for the context of urban computing and modeling applications.",2013,Knowledge Discovery and Data Mining,Fields of study: mobile phone trackingglobal positioning systemland usetelecommunicationsdata miningsimulation
Spotting opinion spammers using behavioral footprints,Arjun Mukherjee (University of Illinois at Chicago)Abhinav Kumar (University of Illinois at Chicago)Bing Liu (University of Illinois at Chicago)Junhui Wang (University of Illinois at Chicago)Meichun Hsu (HP Labs)Malu Castellanos (HP Labs)Riddhiman Ghosh (HP Labs),"2154911241,2190848417,2244698799,2665317319,2164671343,2148997595,2171300665","Opinionated social media such as product reviews are now widely used by individuals and organizations for their decision making. However, due to the reason of profit or fame, people try to game the system by opinion spamming (e.g., writing fake reviews) to promote or to demote some target products. In recent years, fake review detection has attracted significant attention from both the business and research communities. However, due to the difficulty of human labeling needed for supervised learning and evaluation, the problem remains to be highly challenging. This work proposes a novel angle to the problem by modeling spamicity as latent. An unsupervised model, called Author Spamicity Model (ASM), is proposed. It works in the Bayesian setting, which facilitates modeling spamicity of authors as latent and allows us to exploit various observed behavioral footprints of reviewers. The intuition is that opinion spammers have different behavioral distributions than non-spammers. This creates a distributional divergence between the latent population distributions of two clusters: spammers and non-spammers. Model inference results in learning the population distributions of the two clusters. Several extensions of ASM are also considered leveraging from different priors. Experiments on a real-life Amazon review dataset demonstrate the effectiveness of the proposed models which significantly outperform the state-of-the-art competitors.",2013,Knowledge Discovery and Data Mining,Fields of study: data miningmachine learning
"Who, where, when and what: discover spatio-temporal topics for twitter users",Quan Yuan (Nanyang Technological University)Gao Cong (Nanyang Technological University)Zongyang Ma (Nanyang Technological University)Aixin Sun (Nanyang Technological University)Nadia Magnenat- Thalmann (Nanyang Technological University),"2163879794,2295915604,2111067505,2124989948,2631572519","Micro-blogging services, such as Twitter, and location-based social network applications have generated short text messages associated with geographic information, posting time, and user ids. The availability of such data received from users offers a good opportunity to study the user's spatial-temporal behavior and preference. In this paper, we propose a probabilistic model W 4 (short for W ho+ W here+ W hen+ W hat) to exploit such data to discover individual users' mobility behaviors from spatial, temporal and activity aspects. To the best of our knowledge, our work offers the first solution to jointly model individual user's mobility behavior from the three aspects. Our model has a variety of applications, such as user profiling and location prediction; it can be employed to answer questions such as ``Can we infer the location of a user given a tweet posted by the user and the posting time?"" Experimental results on two real-world datasets show that the proposed model is effective in discovering users' spatial-temporal topics, and outperforms state-of-the-art baselines significantly for the task of location prediction for tweets.",2013,Knowledge Discovery and Data Mining,Fields of study: user modelinggraphical modelinternet privacyworld wide webdata miningmachine learningcomputer science
Online controlled experiments at large scale,Ron Kohavi (Microsoft)Alex Deng (Microsoft)Brian Frasca (Microsoft)Toby Walker (Microsoft)Ya Xu (Microsoft)Nils Pohlmann (Microsoft),"73615348,2172042952,1598186712,2623983410,2310280520,2222844576","Web-facing companies, including Amazon, eBay, Etsy, Facebook, Google, Groupon, Intuit, LinkedIn, Microsoft, Netflix, Shop Direct, StumbleUpon, Yahoo, and Zynga use online controlled experiments to guide product development and accelerate innovation. At Microsoft's Bing, the use of controlled experiments has grown exponentially over time, with over 200 concurrent experiments now running on any given day. Running experiments at large scale requires addressing multiple challenges in three areas: cultural/organizational, engineering, and trustworthiness. On the cultural and organizational front, the larger organization needs to learn the reasons for running controlled experiments and the tradeoffs between controlled experiments and other methods of evaluating ideas. We discuss why negative experiments, which degrade the user experience short term, should be run, given the learning value and long-term benefits. On the engineering side, we architected a highly scalable system, able to handle data at massive scale: hundreds of concurrent experiments, each containing millions of users. Classical testing and debugging techniques no longer apply when there are billions of live variants of the site, so alerts are used to identify issues rather than relying on heavy up-front testing. On the trustworthiness front, we have a high occurrence of false positives that we address, and we alert experimenters to statistical interactions between experiments. The Bing Experimentation System is credited with having accelerated innovation and increased annual revenues by hundreds of millions of dollars, by allowing us to find and focus on key ideas evaluated through thousands of controlled experiments. A 1% improvement to revenue equals more than $10M annually in the US, yet many ideas impact key metrics by 1% and are not well estimated a-priori. The system has also identified many negative features that we avoided deploying, despite key stakeholders' early excitement, saving us similar large amounts.",2013,Knowledge Discovery and Data Mining,Fields of study: randomized experimentworld wide webdata miningmachine learningsimulationstatisticscomputer science
Denser than the densest subgraph: extracting optimal quasi-cliques with quality guarantees,Charalampos E. Tsourakakis (Carnegie Mellon University)Francesco Bonchi (Yahoo!)Aristides Gionis (Aalto University)Francesco Gullo (Yahoo!)Maria A. Tsiarli (University of Pittsburgh),"750472553,2176652147,737311942,1979201319,1525090750","Finding dense subgraphs is an important graph-mining task with many applications. Given that the direct optimization of edge density is not meaningful, as even a single edge achieves maximum density, research has focused on optimizing alternative density functions. A very popular among such functions is the average degree, whose maximization leads to the well-known densest-subgraph notion. Surprisingly enough, however, densest subgraphs are typically large graphs, with small edge density and large diameter. In this paper, we define a novel density function, which gives subgraphs of much higher quality than densest subgraphs: the graphs found by our method are compact, dense, and with smaller diameter. We show that the proposed function can be derived from a general framework, which includes other important density functions as subcases and for which we show interesting general theoretical properties. To optimize the proposed function we provide an additive approximation algorithm and a local-search heuristic. Both algorithms are very efficient and scale well to large graphs. We evaluate our algorithms on real and synthetic datasets, and we also devise several application studies as variants of our original problem. When compared with the method that finds the subgraph of the largest average degree, our algorithms return denser subgraphs with smaller diameter. Finally, we discuss new interesting research directions that our problem leaves open.",2013,Knowledge Discovery and Data Mining,Fields of study: discrete mathematicscombinatoricsmathematical optimizationmathematics
Simple and deterministic matrix sketching,Edo Liberty (Yahoo!),1215165747,"A sketch of a matrix A is another matrix B which is significantly smaller than A but still approximates it well. Finding such sketches efficiently is an important building block in modern algorithms for approximating, for example, the PCA of massive matrices. This task is made more challenging in the streaming model, where each row of the input matrix can only be processed once and storage is severely limited. In this paper we adapt a well known streaming algorithm for approximating item frequencies to the matrix sketching setting. The algorithm receives n rows of a large matrix A e ℜ n x m one after the other in a streaming fashion. It maintains a sketch B ℜ l x m containing only l n rows but still guarantees that A T A B T B . More accurately, ∀ x || x ,||=1 0≤|| Ax || 2 - || Bx || 2 ≤ 2|| A ||_ f 2 l Or B T B prec A T A and || A T A - B T B || ≤ 2 || A || f 2 l . This gives a streaming algorithm whose error decays proportional to 1/ l using O ( ml ) space. For comparison, random-projection, hashing or sampling based algorithms produce convergence bounds proportional to 1/√ l . Sketch updates per row in A require amortized O ( ml ) operations and the algorithm is perfectly parallelizable. Our experiments corroborate the algorithm's scalability and improved convergence rate. The presented algorithm also stands out in that it is deterministic, simple to implement and elementary to prove.",2013,Knowledge Discovery and Data Mining,Fields of study: theoretical computer sciencecombinatoricsmathematical optimizationstatisticsalgorithmcomputer sciencemathematics
Geo-spotting: mining online location-based services for optimal retail store placement,Dmytro Karamshuk (IMT Institute for Advanced Studies Lucca)Anastasios Noulas (University of Cambridge)Salvatore Scellato (University of Cambridge)Vincenzo Nicosia (Queen Mary University of London)Cecilia Mascolo (University of Cambridge),"2569942295,1995754924,2311840536,1988998094,388119112","The problem of identifying the optimal location for a new retail store has been the focus of past research, especially in the field of land economy, due to its importance in the success of a business. Traditional approaches to the problem have factored in demographics, revenue and aggregated human flow statistics from nearby or remote areas. However, the acquisition of relevant data is usually expensive. With the growth of location-based social networks, fine grained data describing user mobility and popularity of places has recently become attainable. In this paper we study the predictive power of various machine learning features on the popularity of retail stores in the city through the use of a dataset collected from Foursquare in New York. The features we mine are based on two general signals: geographic, where features are formulated according to the types and density of nearby places, and user mobility, which includes transitions between venues or the incoming flow of mobile users from distant areas. Our evaluation suggests that the best performing features are common across the three different commercial chains considered in the analysis, although variations may exist too, as explained by heterogeneities in the way retail facilities attract users. We also show that performance improves significantly when combining multiple features in supervised learning algorithms, suggesting that the retail success of a business may depend on multiple factors.",2013,Knowledge Discovery and Data Mining,Fields of study: location based serviceworld wide webdata miningmachine learningcomputer science
FISM: factored item similarity models for top-N recommender systems,Santosh Kabbur (University of Minnesota)Xia Ning (NEC)George Karypis (University of Minnesota),"2060234691,2151954795,219814910","The effectiveness of existing top- N recommendation methods decreases as the sparsity of the datasets increases. To alleviate this problem, we present an item-based method for generating top- N recommendations that learns the item-item similarity matrix as the product of two low dimensional latent factor matrices. These matrices are learned using a structural equation modeling approach, wherein the value being estimated is not used for its own estimation. A comprehensive set of experiments on multiple datasets at three different sparsity levels indicate that the proposed methods can handle sparse datasets effectively and outperforms other state-of-the-art top- N recommendation methods. The experimental results also show that the relative performance gains compared to competing methods increase as the data gets sparser.",2013,Knowledge Discovery and Data Mining,Fields of study: sparse matrixrecommender systeminformation retrievaldata miningmachine learningcomputer science
Real-time disease surveillance using Twitter data: demonstration on flu and cancer,Kathy Lee (Northwestern University)Ankit Agrawal (Northwestern University)Alok N. Choudhary (Northwestern University),"2145321431,2160807299,2147783234","Social media is producing massive amounts of data on an unprecedented scale. Here people share their experiences and opinions on various topics, including personal health issues, symptoms, treatments, side-effects, and so on. This makes publicly available social media data an invaluable resource for mining interesting and actionable healthcare insights. In this paper, we describe a novel real-time flu and cancer surveillance system that uses spatial, temporal, and text mining on Twitter data. The real-time analysis results are reported visually in terms of US disease surveillance maps, distribution and timelines of disease types, symptoms, and treatments, in addition to overall disease activity timelines on our project website. Our surveillance system can be very useful not only for early prediction of seasonal disease outbreaks such as flu, but also for monitoring distribution of cancer patients with different cancer types and symptoms in each state and the popularity of treatments used. The resulting insights are expected to help facilitate faster response to and preparation for epidemics and also be very useful for both patients and doctors to make more informed decisions.",2013,Knowledge Discovery and Data Mining,Fields of study: social mediacancerpublic healthdata miningcomputer science
Big data analytics for healthcare,Jimeng Sun (IBM)Chandan K. Reddy (Wayne State University),"2110385854,2100435683","Large amounts of heterogeneous medical data have become available in various healthcare organizations (payers, providers, pharmaceuticals). Those data could be an enabling resource for deriving insights for improving care delivery and reducing waste. The enormity and complexity of these datasets present great challenges in analyses and subsequent applications to a practical clinical environment. In this tutorial, we introduce the characteristics and related mining challenges on dealing with big medical data. Many of those insights come from medical informatics community, which is highly related to data mining but focuses on biomedical specifics. We survey various related papers from data mining venues as well as medical informatics venues to share with the audiences key problems and trends in healthcare analytics research, with different applications ranging from clinical text mining, predictive modeling, survival analysis, patient similarity, genetic data analysis, and public health. The tutorial will include several case studies dealing with some of the important healthcare applications.",2013,Knowledge Discovery and Data Mining,Fields of study: association rule learningcluster analysisdata scienceworld wide webdata miningmachine learningcomputer science
Evaluating the crowd with confidence,Manas Joglekar (Stanford University)Hector Garcia-Molina (Stanford University)Aditya G. Parameswaran (Stanford University),"2246810848,237419955,2077695977","Worker quality control is a crucial aspect of crowdsourcing systems; typically occupying a large fraction of the time and money invested on crowdsourcing. In this work, we devise techniques to generate confidence intervals for worker error rate estimates, thereby enabling a better evaluation of worker quality. We show that our techniques generate correct confidence intervals on a range of real-world datasets, and demonstrate wide applicability by using them to evict poorly performing workers, and provide confidence intervals on the accuracy of the answers.",2013,Knowledge Discovery and Data Mining,Fields of study: crowdsourcingconfidencedata sciencedata miningstatisticscomputer science
Combining latent factor model with location features for event-based group recommendation,Wei Zhang (Tsinghua University)Jianyong Wang (Tsinghua University)Wei Feng (Tsinghua University),"2723820743,2105625159,2647481813","Groups play an essential role in many social websites which promote users' interactions and accelerate the diffusion of information. Recommending groups that users are really interested to join is significant for both users and social media. While traditional group recommendation problem has been extensively studied, we focus on a new type of the problem, i.e., event-based group recommendation. Unlike the other forms of groups, users join this type of groups mainly for participating offline events organized by group members or inviting other users to attend events sponsored by them. These characteristics determine that previously proposed approaches for group recommendation cannot be adapted to the new problem easily as they ignore the geographical influence and other explicit features of groups and users. In this paper, we propose a method called Pairwise Tag enhAnced and featuRe-based Matrix factorIzation for Group recommendAtioN (PTARMIGAN), which considers location features, social features, and implicit patterns simultaneously in a unified model. More specifically, we exploit matrix factorization to model interactions between users and groups. Meanwhile, we incorporate their profile information into pairwise enhanced latent factors respectively. We also utilize the linear model to capture explicit features. Due to the reinforcement between explicit features and implicit patterns, our approach can provide better group recommendations. We conducted a comprehensive performance evaluation on real word data sets and the experimental results demonstrate the effectiveness of our method.",2013,Knowledge Discovery and Data Mining,Fields of study: world wide webdata mining
Stochastic collapsed variational Bayesian inference for latent Dirichlet allocation,"James R. Foulds (University of California, Irvine)Levi Boyles (University of California, Irvine)Christopher DuBois (University of California, Irvine)Padhraic Smyth (University of California, Irvine)Max Welling (University of Amsterdam)","2170533113,2171757934,2153139629,2137074633,2014508897","There has been an explosion in the amount of digital text information available in recent years, leading to challenges of scale for traditional inference algorithms for topic models. Recent advances in stochastic variational inference algorithms for latent Dirichlet allocation (LDA) have made it feasible to learn topic models on very large-scale corpora, but these methods do not currently take full advantage of the collapsed representation of the model. We propose a stochastic algorithm for collapsed variational Bayesian inference for LDA, which is simpler and more efficient than the state of the art method. In experiments on large-scale text corpora, the algorithm was found to converge faster and often to a better solution than previous methods. Human-subject experiments also demonstrated that the method can learn coherent topics in seconds on small corpora, facilitating the use of topic models in interactive document analysis software.",2013,Knowledge Discovery and Data Mining,Fields of study: dynamic topic modelvariational message passingtopic modellatent dirichlet allocationdata miningpattern recognitionmachine learningstatisticscomputer science
The Microsoft academic search dataset and KDD Cup 2013,Senjuti Basu Roy (University of Washington)Martine De Cock (Ghent University)Vani Mandava (Microsoft)Swapna Savanna (University of Washington)Brian DalessandroClaudia Perlich (IBM)William CukierskiBen Hamner (Université de Montréal),"2097289406,2017159822,2225359940,2223747691,1876520739,164824025,2722265494,2101754164","KDD Cup 2013 challenged participants to tackle the problem of author name ambiguity in a digital library of scientific publications. The competition consisted of two tracks, which were based on large-scale datasets from a snapshot of Microsoft Academic Search, taken in January 2013 and including 250K authors and 2.5M papers. Participants were asked to determine which papers in an author profile are truly written by a given author (track 1), as well as to identify duplicate author profiles (track 2). Track 1 and track 2 were launched respectively on April 18 and April 20, 2013, with a common final submission deadline on June 12, 2013. For track 1 a training dataset with correct labels was diclosed at the start of the competition. This track was the most popular one, attracting submissions of 561 different teams. Track 2, which was formulated as an unsupervised learning task, received submissions from 241 participants. This paper presents details about the problem definitions, the datasets, the evaluation metrics and the results.",2013,Knowledge Discovery and Data Mining,Fields of study: data scienceworld wide webdata miningcomputer science
Cascading outbreak prediction in networks: a data-driven approach,Peng Cui (Tsinghua University)Shifei Jin (Tsinghua University)Linyun Yu (Tsinghua University)Fei Wang (IBM)Wenwu Zhu (Tsinghua University)Shiqiang Yang (Tsinghua University),"2113115369,2222108273,2223066044,2465953593,2111511002,2127183023","Cascades are ubiquitous in various network environments such as epidemic networks, traffic networks, water distribution networks and social networks. The outbreaks of cascades will often bring bad or even devastating effects. How to accurately predict the cascading outbreaks in early stage is of paramount importance for people to avoid these bad effects. Although there have been some pioneering works on cascading outbreaks detection, how to predict, rather than detect, the cascading outbreaks is still an open problem. In this paper, we attempt harnessing historical cascade data, propose a novel data driven approach to select important nodes as sensors, and predict the outbreaks based on the cascading behaviors of these sensors. In particular, we propose Orthogonal Sparse LOgistic Regression (OSLOR) method to jointly optimize node selection and outbreak prediction, where the prediction loss are combined with an orthogonal regularizer and L1 regularizer to guarantee good prediction accuracy, as well as the sparsity and low-redundancy of selected sensors. We evaluate the proposed method on a real online social network dataset including 182.7 million information cascades. The experimental results show that the proposed OSLOR significantly and consistently outperform topological measure based method and other data driven methods in prediction performances.",2013,Knowledge Discovery and Data Mining,Fields of study: information cascadesocial networkdata miningmachine learningsimulation
Linking named entities in Tweets with knowledge base via user interest modeling,Wei Shen (Tsinghua University)Jianyong Wang (Tsinghua University)Ping Luo (HP Labs)Min Wang (Google),"2667078396,2105625159,2291210646,2467205710","Twitter has become an increasingly important source of information, with more than 400 million tweets posted per day. The task to link the named entity mentions detected from tweets with the corresponding real world entities in the knowledge base is called tweet entity linking. This task is of practical importance and can facilitate many different tasks, such as personalized recommendation and user interest discovery. The tweet entity linking task is challenging due to the noisy, short, and informal nature of tweets. Previous methods focus on linking entities in Web documents, and largely rely on the context around the entity mention and the topical coherence between entities in the document. However, these methods cannot be effectively applied to the tweet entity linking task due to the insufficient context information contained in a tweet. In this paper, we propose KAURI, a graph-based framework to collectively link all the named entity mentions in all tweets posted by a user via modeling the user's topics of interest. Our assumption is that each user has an underlying topic interest distribution over various named entities. KAURI integrates the intra-tweet local information with the inter-tweet user interest information into a unified graph-based framework. We extensively evaluated the performance of KAURI over manually annotated tweet corpus, and the experimental results show that KAURI significantly outperforms the baseline methods in terms of accuracy, and KAURI is efficient and scales well to tweet stream.",2013,Knowledge Discovery and Data Mining,Fields of study: entity linkingknowledge baseworld wide webinformation retrievaldata miningcomputer science
Detecting insider threats in a real corporate database of computer usage activity,Ted E. Senator (Science Applications International Corporation)Henry G. Goldberg (Science Applications International Corporation)Alex Memory (Science Applications International Corporation)William T. Young (Science Applications International Corporation)Brad Rees (Science Applications International Corporation)Robert Pierce (Science Applications International Corporation)Daniel Huang (Science Applications International Corporation)Matthew Reardon (Science Applications International Corporation)David A. Bader (Georgia Institute of Technology)Edmond Chow (Georgia Institute of Technology)Irfan A. Essa (Georgia Institute of Technology)Joshua Jones (Georgia Institute of Technology)Vinay Bettadapura (Georgia Institute of Technology)Duen Horng Chau (Georgia Institute of Technology)Oded Green (Georgia Institute of Technology)Oguz Kaya (Georgia Institute of Technology)Anita Zakrzewska (Georgia Institute of Technology)Erica Briscoe (Georgia Institute of Technology)Rudolph L. Mappus (Georgia Institute of Technology)Robert McColl (Georgia Institute of Technology)Lora Weiss (Georgia Institute of Technology)Thomas G. Dietterich (Oregon State University)Alan Fern (Oregon State University)Weng-Keen Wong (Oregon State University)Shubhomoy Das (Oregon State University)Andrew Emmott (Oregon State University)Jed Irvine (Oregon State University)Jay Yoon Lee (Carnegie Mellon University)Danai Koutra (Carnegie Mellon University)Christos Faloutsos (Carnegie Mellon University)Daniel D. Corkill (University of Massachusetts Amherst)Lisa Friedland (University of Massachusetts Amherst)Amanda Gentzel (University of Massachusetts Amherst)David D. Jensen (University of Massachusetts Amherst),"1935278117,2137686017,1964480931,2230024146,2223195539,2640413832,2308770314,2304218308,2148365076,2015985982,574770096,2167709312,1665580359,2024561599,2156572721,2224610080,2034493427,2142287801,2082700008,2309337722,2476790693,160031478,2139785505,2102128069,2121364888,2338058579,2132031971,2169155435,1524801041,2198983026,104489385,2117937420,2326203580,2124028641","This paper reports on methods and results of an applied research project by a team consisting of SAIC and four universities to develop, integrate, and evaluate new approaches to detect the weak signals characteristic of insider threats on organizations' information systems. Our system combines structural and semantic information from a real corporate database of monitored activity on their users' computers to detect independently developed red team inserts of malicious insider activities. We have developed and applied multiple algorithms for anomaly detection based on suspected scenarios of malicious insider behavior, indicators of unusual activities, high-dimensional statistical patterns, temporal sequences, and normal graph evolution. Algorithms and representations for dynamic graph processing provide the ability to scale as needed for enterprise-level deployments on real-time data streams. We have also developed a visual language for specifying combinations of features, baselines, peer groups, time periods, and algorithms to detect anomalies suggestive of instances of insider threat behavior. We defined over 100 data features in seven categories based on approximately 5.5 million actions per day from approximately 5,500 users. We have achieved area under the ROC curve values of up to 0.979 and lift values of 65 on the top 50 user-days identified on two months of real data.",2013,Knowledge Discovery and Data Mining,Fields of study: anomaly detectionworld wide webcomputer securitydata miningmachine learningcomputer science
Mining evidences for named entity disambiguation,"Yang Li (University of California, Santa Barbara)Chi Wang (University of Illinois at Urbana–Champaign)Fangqiu Han (University of California, Santa Barbara)Jiawei Han (University of Illinois at Urbana–Champaign)Dan Roth (University of Illinois at Urbana–Champaign)Xifeng Yan (University of California, Santa Barbara)","2294602198,2461963590,2168772039,2121939561,2122007671,2116657824","Named entity disambiguation is the task of disambiguating named entity mentions in natural language text and link them to their corresponding entries in a knowledge base such as Wikipedia. Such disambiguation can help enhance readability and add semantics to plain text. It is also a central step in constructing high-quality information network or knowledge graph from unstructured text. Previous research has tackled this problem by making use of various textual and structural features from a knowledge base. Most of the proposed algorithms assume that a knowledge base can provide enough explicit and useful information to help disambiguate a mention to the right entity. However, the existing knowledge bases are rarely complete (likely will never be), thus leading to poor performance on short queries with not well-known contexts. In such cases, we need to collect additional evidences scattered in internal and external corpus to augment the knowledge bases and enhance their disambiguation power. In this work, we propose a generative model and an incremental algorithm to automatically mine useful evidences across documents. With a specific modeling of ""background topic"" and ""unknown entities"", our model is able to harvest useful evidences out of noisy information. Experimental results show that our proposed method outperforms the state-of-the-art approaches significantly: boosting the disambiguation accuracy from 43% (baseline) to 86% on short queries derived from tweets.",2013,Knowledge Discovery and Data Mining,Fields of study: generative modelentity linkingsemi supervised learningnatural language processingdata miningpattern recognitionmachine learningcomputer science
Assessing team strategy using spatiotemporal data,Patrick Lucey (Disney Research)Dean OliverPeter Carr 0001 (Disney Research)Joe Roth (Disney Research)Iain A. Matthews (Disney Research),"1986214492,2661496451,2049576458,2232502054,2227920884","The ""Moneyball"" revolution coincided with a shift in the way professional sporting organizations handle and utilize data in terms of decision making processes. Due to the demand for better sports analytics and the improvement in sensor technology, there has been a plethora of ball and player tracking information generated within professional sports for analytical purposes. However, due to the continuous nature of the data and the lack of associated high-level labels to describe it - this rich set of information has had very limited use especially in the analysis of a team's tactics and strategy. In this paper, we give an overview of the types of analysis currently performed mostly with hand-labeled event data and highlight the problems associated with the influx of spatiotemporal data. By way of example, we present an approach which uses an entire season of ball tracking data from the English Premier League (2010-2011 season) to reinforce the common held belief that teams should aim to ""win home games and draw away ones"". We do this by: i) forming a representation of team behavior by chunking the incoming spatiotemporal signal into a series of quantized bins, and ii) generate an expectation model of team behavior based on a code-book of past performances. We show that home advantage in soccer is partly due to the conservative strategy of the away team. We also show that our approach can flag anomalous team behavior which has many potential applications.",2013,Knowledge Discovery and Data Mining,Fields of study: representationoperations researchdata miningmachine learningsimulationcomputer science
Big data analytics with small footprint: squaring the cloud,"John F. Canny (University of California, Berkeley)Huasha Zhao (University of California, Berkeley)","2101610026,2161004278","This paper describes the BID Data Suite, a collection of hardware, software and design patterns that enable fast, large-scale data mining at very low cost. By co-designing all of these elements we achieve single-machine performance levels that equal or exceed reported cluster implementations for common benchmark problems. A key design criterion is rapid exploration of models, hence the system is interactive and primarily single-user. The elements of the suite are: (i) the data engine, a hardware design pattern that balances storage, CPU and GPU acceleration for typical data mining workloads, (ii) BIDMat, an interactive matrix library that integrates CPU and GPU acceleration and novel computational kernels (iii), BIDMach, a machine learning system that includes very efficient model optimizers, (iv) Butterfly mixing, a communication strategy that hides the latency of frequent model updates needed by fast optimizers and (v) Design patterns to improve performance of iterative update algorithms. We present several benchmark problems to show how the above elements combine to yield multiple orders-of-magnitude improvements for each problem.",2013,Knowledge Discovery and Data Mining,Fields of study: clusterdata miningreal time computingmachine learningsimulationcomputer science
Confluence: conformity influence in large social networks,Jie Tang (Tsinghua University)Sen Wu (Tsinghua University)Jimeng Sun (IBM),"2158012360,2443423181,2110385854","Conformity is a type of social influence involving a change in opinion or behavior in order to fit in with a group. Employing several social networks as the source for our experimental data, we study how the effect of conformity plays a role in changing users' online behavior. We formally define several major types of conformity in individual, peer, and group levels. We propose Confluence model to formalize the effects of social conformity into a probabilistic model. Confluence can distinguish and quantify the effects of the different types of conformities. To scale up to large social networks, we propose a distributed learning method that can construct the Confluence model efficiently with near-linear speedup. Our experimental results on four different types of large social networks, i.e., Flickr, Gowalla, Weibo and Co-Author, verify the existence of the conformity phenomena. Leveraging the conformity information, Confluence can accurately predict actions of users. Our experiments show that Confluence significantly improves the prediction accuracy by up to 5-10% compared with several alternative methods.",2013,Knowledge Discovery and Data Mining,Fields of study: conformitysocial influencesocial networkknowledge managementartificial intelligence
Understanding urban human activity and mobility patterns using large-scale location-based data from online social media,Samiul Hasan (Purdue University)Xianyuan Zhan (Purdue University)Satish V. Ukkusuri (Purdue University),"2218520530,2127915341,2294323226",Location-based check-in services enable individuals to share their activity-related choices providing a new source of human activity data for researchers. In this paper urban human mobility and activity patterns are analyzed using location-based data collected from social media applications (e.g. Foursquare and Twitter). We first characterize aggregate activity patterns by finding the distributions of different activity categories over a city geography and thus determine the purpose-specific activity distribution maps. We then characterize individual activity patterns by finding the timing distribution of visiting different places depending on activity category. We also explore the frequency of visiting a place with respect to the rank of the place in individual's visitation records and show interesting match with the results from other studies based on mobile phone data.,2013,Knowledge Discovery and Data Mining,Fields of study: social mediainternet privacyworld wide webdata miningcomputer science
Knowledge discovery from massive healthcare claims data,Varun Chandola (Oak Ridge National Laboratory)Sreenivas R. Sukumar (Oak Ridge National Laboratory)Jack C. Schryver (Oak Ridge National Laboratory),"2096898130,2178049252,2068369565","he role of big data in addressing the needs of the present healthcare system in US and rest of the world has been echoed by government, private, and academic sectors. There has been a growing emphasis to explore the promise of big data analytics in tapping the potential of the massive healthcare data emanating from private and government health insurance providers. While the domain implications of such collaboration are well known, this type of data has been explored to a limited extent in the data mining community. The objective of this paper is two fold: first , we introduce the emerging domain of ""big"" healthcare claims data to the KDD community, and second , we describe the success and challenges that we encountered in analyzing this data using state of art analytics for massive data. Specifically, we translate the problem of analyzing healthcare data into some of the most well-known analysis problems in the data mining community, social network analysis , text mining , and temporal analysis and higher order feature construction , and describe how advances within each of these areas can be leveraged to understand the domain of healthcare. Each case study illustrates a unique intersection of data mining and healthcare with a common objective of improving the cost-care ratio by mining for opportunities to improve healthcare operations and reducing what seems to fall under fraud, waste, and abuse.",2013,Knowledge Discovery and Data Mining,Fields of study: data scienceknowledge managementdata mining
The role of information diffusion in the evolution of social networks,Lilian Weng (Indiana University Bloomington)Jacob Ratkiewicz (Google)Nicola Perra (Northeastern University)Bruno Gonçalves (Aix-Marseille University)Carlos Castillo (Qatar Computing Research Institute)Francesco Bonchi (Yahoo!)Rossano Schifanella (University of Turin)Filippo Menczer (Indiana University Bloomington)Alessandro Flammini (Indiana University Bloomington),"2120179359,2005128872,2083100286,2110466001,2479708560,2176652147,2200389328,2248192384,2012879311","Every day millions of users are connected through online social networks, generating a rich trove of data that allows us to study the mechanisms behind human interactions. Triadic closure has been treated as the major mechanism for creating social links: if Alice follows Bob and Bob follows Charlie, Alice will follow Charlie. Here we present an analysis of longitudinal micro-blogging data, revealing a more nuanced view of the strategies employed by users when expanding their social circles. While the network structure affects the spread of information among users, the network is in turn shaped by this communication activity. This suggests a link creation mechanism whereby Alice is more likely to follow Charlie after seeing many messages by Charlie. We characterize users with a set of parameters associated with different link creation strategies, estimated by a Maximum-Likelihood approach. Triadic closure does have a strong effect on link formation, but shortcuts based on traffic are another key factor in interpreting network evolution. However, individual strategies for following other users are highly heterogeneous. Link creation behaviors can be summarized by classifying users in different categories with distinct structural and behavioral characteristics. Users who are popular, active, and influential tend to create traffic-based shortcuts, making the information diffusion process more efficient in the network.",2013,Knowledge Discovery and Data Mining,Fields of study: social mediaworld wide webartificial intelligencemachine learningsimulationcomputer science
Mining high utility episodes in complex event sequences,Cheng-Wei Wu (National Cheng Kung University)Yu-Feng Lin (National Cheng Kung University)Philip S. Yu (University of Illinois at Chicago)Vincent S. Tseng (National Cheng Kung University),"2111809384,2652518183,2125104194,2023431847","Frequent episode mining (FEM) is an interesting research topic in data mining with wide range of applications. However, the traditional framework of FEM treats all events as having the same importance/utility and assumes that a same type of event appears at most once at any time point. These simplifying assumptions do not reflect the characteristics of scenarios in real applications and thus the useful information of episodes in terms of utilities such as profits is lost. Furthermore, most studies on FEM focused on mining episodes in simple event sequences and few considered the scenario of complex event sequences, where different events can occur simultaneously. To address these issues, in this paper, we incorporate the concept of utility into episode mining and address a new problem of mining high utility episodes from complex event sequences, which has not been explored so far. In the proposed framework, the importance/utility of different events is considered and multiple events can appear simultaneously. Several novel features are incorporated into the proposed framework to resolve the challenges raised by this new problem, such as the absence of anti-monotone property and the huge set of candidate episodes. Moreover, an efficient algorithm named UP-Span (Utility ePisodes mining by Spanning prefixes) is proposed for mining high utility episodes with several strategies incorporated for pruning the search space to achieve high efficiency. Experimental results on real and synthetic datasets show that UP-Span has excellent performance and serves as an effective solution to the new problem of mining high utility episodes from complex event sequences.",2013,Knowledge Discovery and Data Mining,Fields of study: data sciencedata miningsimulation
Restreaming graph partitioning: simple versatile algorithms for advanced balancing,Joel Nishimura (Cornell University)Johan Ugander (Cornell University),"2342754627,1992714166","Partitioning large graphs is difficult, especially when performed in the limited models of computation afforded to modern large scale computing systems. In this work we introduce restreaming graph partitioning and develop algorithms that scale similarly to streaming partitioning algorithms yet empirically perform as well as fully offline algorithms. In streaming partitioning, graphs are partitioned serially in a single pass. Restreaming partitioning is motivated by scenarios where approximately the same dataset is routinely streamed, making it possible to transform streaming partitioning algorithms into an iterative procedure. This combination of simplicity and powerful performance allows restreaming algorithms to be easily adapted to efficiently tackle more challenging partitioning objectives. In particular, we consider the problem of stratified graph partitioning, where each of many node attribute strata are balanced simultaneously. As such, stratified partitioning is well suited for the study of network effects on social networks, where it is desirable to isolate disjoint dense subgraphs with representative user demographics. To demonstrate, we partition a large social network such that each partition exhibits the same degree distribution in the original graph --- a novel achievement for non-regular graphs. As part of our results, we also observe a fundamental difference in the ease with which social graphs are partitioned when compared to web graphs. Namely, the modular structure of web graphs appears to motivate full offline optimization, whereas the locally dense structure of social graphs precludes significant gains from global manipulations.",2013,Knowledge Discovery and Data Mining,Fields of study: modular decompositionspace partitioningclustering coefficientgraph partitionsocial networktheoretical computer sciencecombinatoricsmachine learningmathematics
Discovering latent influence in online social activities via shared cascade poisson processes,Tomoharu Iwata (University of Cambridge)Amar Shah (University of Cambridge)Zoubin Ghahramani (University of Cambridge),"2110635863,2278341994,617670330","Many people share their activities with others through online communities. These shared activities have an impact on other users' activities. For example, users are likely to become interested in items that are adopted (e.g. liked, bought and shared) by their friends. In this paper, we propose a probabilistic model for discovering latent influence from sequences of item adoption events. An inhomogeneous Poisson process is used for modeling a sequence, in which adoption by a user triggers the subsequent adoption of the same item by other users. For modeling adoption of multiple items, we employ multiple inhomogeneous Poisson processes, which share parameters, such as influence for each user and relations between users. The proposed model can be used for finding influential users, discovering relations between users and predicting item popularity in the future. We present an efficient Bayesian inference procedure of the proposed model based on the stochastic EM algorithm. The effectiveness of the proposed model is demonstrated by using real data sets in a social bookmark sharing service.",2013,Knowledge Discovery and Data Mining,Fields of study: bayesian inferencedata scienceworld wide webdata miningmachine learningstatisticscomputer science
A phrase mining framework for recursive construction of a topical hierarchy,Chi Wang (University of Illinois at Urbana–Champaign)Marina Danilevsky (University of Illinois at Urbana–Champaign)Nihit Desai (University of Illinois at Urbana–Champaign)Yinan Zhang (University of Illinois at Urbana–Champaign)Phuong Nguyen (University of Illinois at Urbana–Champaign)Thrivikrama Taula (University of Illinois at Urbana–Champaign)Jiawei Han (University of Illinois at Urbana–Champaign),"2461963590,2046577858,2146108112,2674366442,2438808258,2223039520,2121939561","A high quality hierarchical organization of the concepts in a dataset at different levels of granularity has many valuable applications such as search, summarization, and content browsing. In this paper we propose an algorithm for recursively constructing a hierarchy of topics from a collection of content-representative documents. We characterize each topic in the hierarchy by an integrated ranked list of mixed-length phrases. Our mining framework is based on a phrase-centric view for clustering, extracting, and ranking topical phrases. Experiments with datasets from three different domains illustrate our ability to generate hierarchies of high quality topics represented by meaningful phrases.",2013,Knowledge Discovery and Data Mining,Fields of study: topic modelnetwork analysisnatural language processinginformation retrievaldata miningartificial intelligencemachine learningcomputer science
Location-aware publish/subscribe,Guoliang Li (Tsinghua University)Yang WangTing Wang (Tsinghua University)Jianhua Feng (Tsinghua University),"2171804313,2649859012,2697901808,1995232797","Location-based services have become widely available on mobile devices. Existing methods employ a pull model or user-initiated model, where a user issues a query to a server which replies with location-aware answers. To provide users with instant replies, a push model or server-initiated model is becoming an inevitable computing model in the next-generation location-based services. In the push model, subscribers register spatio-textual subscriptions to capture their interests, and publishers post spatio-textual messages. This calls for a high-performance location-aware publish/subscribe system to deliver publishers' messages to relevant subscribers.In this paper, we address the research challenges that arise in designing a location-aware publish/subscribe system. We propose an rtree based index structure by integrating textual descriptions into rtree nodes. We devise efficient filtering algorithms and develop effective pruning techniques to improve filtering efficiency. Experimental results show that our method achieves high performance. For example, our method can filter 500 tweets in a second for 10 million registered subscriptions on a commodity computer.",2013,Knowledge Discovery and Data Mining,Fields of study: internet privacyworld wide webdata miningcomputer science
Privacy-preserving data exploration in genome-wide association studies,Aaron Johnson (United States Naval Research Laboratory)Vitaly Shmatikov (University of Texas at Austin),"2570324162,8790322","Genome-wide association studies (GWAS) have become a popular method for analyzing sets of DNA sequences in order to discover the genetic basis of disease. Unfortunately, statistics published as the result of GWAS can be used to identify individuals participating in the study. To prevent privacy breaches, even previously published results have been removed from public databases, impeding researchers' access to the data and hindering collaborative research. Existing techniques for privacy-preserving GWAS focus on answering specific questions, such as correlations between a given pair of SNPs (DNA sequence variations). This does not fit the typical GWAS process, where the analyst may not know in advance which SNPs to consider and which statistical tests to use, how many SNPs are significant for a given dataset, etc. We present a set of practical, privacy-preserving data mining algorithms for GWAS datasets. Our framework supports exploratory data analysis, where the analyst does not know a priori how many and which SNPs to consider. We develop privacy-preserving algorithms for computing the number and location of SNPs that are significantly associated with the disease, the significance of any statistical test between a given SNP and the disease, any measure of correlation between SNPs, and the block structure of correlations. We evaluate our algorithms on real-world datasets and demonstrate that they produce significantly more accurate results than prior techniques while guaranteeing differential privacy.",2013,Knowledge Discovery and Data Mining,Fields of study: differential privacygenome wide association studydata sciencebioinformaticsdata miningmachine learningcomputer science
Inferring social roles and statuses in social networks,Yuchen Zhao (University of Illinois at Chicago)Guan Wang (University of Illinois at Chicago)Philip S. Yu (University of Illinois at Chicago)Shaobo Liu (LinkedIn)Simon Zhang (LinkedIn),"2147878578,2152311907,2125104194,2103730481,2715396429","Users in online social networks play a variety of social roles and statuses. For example, users in Twitter can be represented as advertiser, content contributor, information receiver, etc; users in Linkedin can be in different professional roles, such as engineer, salesperson and recruiter. Previous research work mainly focuses on using categorical and textual information to predict the attributes of users. However, it cannot be applied to a large number of users in real social networks, since much of such information is missing, outdated and non-standard. In this paper, we investigate the social roles and statuses that people act in online social networks in the perspective of network structures, since the uniqueness of social networks is connecting people. We quantitatively analyze a number of key social principles and theories that correlate with social roles and statuses. We systematically study how the network characteristics reflect the social situations of users in an online society. We discover patterns of homophily, the tendency of users to connect with users with similar social roles and statuses. In addition, we observe that different factors in social theories influence the social role/status of an individual user to various extent, since these social principles represent different aspects of the network. We then introduce an optimization framework based on Factor Conditioning Symmetry , and we propose a probabilistic model to integrate the optimization framework on local structural information as well as network influence to infer the unknown social roles and statuses of online users. We will present experiment results to show the effectiveness of the inference.",2013,Knowledge Discovery and Data Mining,Fields of study: social heuristicsdynamic network analysissocial learninguser modelingsocial statusrolesocial networksocial computingknowledge managementdata mining
Exploring human movements in Singapore: a comparative analysis based on mobile phone and taxicab usages,Chaogui Kang (Massachusetts Institute of Technology)Stanislav Sobolevsky (Massachusetts Institute of Technology)Yu Liu (Peking University)Carlo Ratti (Massachusetts Institute of Technology),"2135734817,2024722252,2658457813,2113839928","Existing studies extensively utilized taxicab trips and individuals' movements captured by mobile phone usages (referred as ""mobile phone movements"" hereafter) to understand human mobility patterns in an area. However, all these studies analyze taxicab trips and mobile phone movements separately. In this paper, we: (1) integrate mobile phone and taxicab usages together to explore human movements in Singapore and reveal that mobile phone movements as a general proxy to all kinds of human mobility has substantially different characteristics compared to taxicab trips, which are one of the frequently used means of transportation; (2) investigate the ratio of taxicab trips and mobile phone movements between two arbitrary locations, which not only characterizes taxicab demands between these locations but also sheds light on underlying land use patterns. In details, we quantify the distinct characteristics of mobile phone movements and taxicab trips, and particularly confirm that the number of taxicab trips decays with distance more slowly compared to mobile phone movements. From a spatial network perspective, taxicab trips largely reflect interactions between further-separating locations than mobile phone movements, resulting in emergence of larger spatial communities (delineated based on people mobility) in Singapore. The contribution of this research is two-fold: (1) we clarified the divergences between observed human mobility patterns based on taxicab and mobile phone data; (2) we implemented an integrated approach of taxicab and mobile phone usages for gaining more informative insights in population dynamics, transportation and urban configuration.",2013,Knowledge Discovery and Data Mining,Fields of study: trajectorysimulation
Graph cluster randomization: network exposure to multiple universes,Johan Ugander (Cornell University)Brian Karrer (Facebook)Lars Backstrom (Facebook)Jon M. Kleinberg (Cornell University),"1992714166,2633680482,2096207090,2261367123","A/B testing is a standard approach for evaluating the effect of online experiments; the goal is to estimate the `average treatment effect' of a new feature or condition by exposing a sample of the overall population to it. A drawback with A/B testing is that it is poorly suited for experiments involving social interference, when the treatment of individuals spills over to neighboring individuals along an underlying social network. In this work, we propose a novel methodology using graph clustering to analyze average treatment effects under social interference. To begin, we characterize graph-theoretic conditions under which individuals can be considered to be `network exposed' to an experiment. We then show how graph cluster randomization admits an efficient exact algorithm to compute the probabilities for each vertex being network exposed under several of these exposure conditions. Using these probabilities as inverse weights, a Horvitz-Thompson estimator can then provide an effect estimate that is unbiased, provided that the exposure model has been properly specified. Given an estimator that is unbiased, we focus on minimizing the variance. First, we develop simple sufficient conditions for the variance of the estimator to be asymptotically small in n, the size of the graph. However, for general randomization schemes, this variance can be lower bounded by an exponential function of the degrees of a graph. In contrast, we show that if a graph satisfies a restricted-growth condition on the growth rate of neighborhoods, then there exists a natural clustering algorithm, based on vertex neighborhoods, for which the variance of the estimator can be upper bounded by a linear function of the degrees. Thus we show that proper cluster randomization can lead to exponentially lower estimator variance when experimentally measuring average treatment effects under interference.",2013,Knowledge Discovery and Data Mining,Fields of study: clustering coefficientcausal inferenceinterferencesocial networknetwork effectcombinatoricssocial sciencemachine learningmathematical optimizationstatisticsmathematics
A space efficient streaming algorithm for triangle counting using the birthday paradox,Madhav Jha (Pennsylvania State University)Comandur Seshadhri (Sandia National Laboratories)Ali Pinar (Sandia National Laboratories),"2311563867,1214261146,2104623905","We design a space efficient algorithm that approximates the transitivity (global clustering coefficient) and total triangle count with only a single pass through a graph given as a stream of edges. Our procedure is based on the classic probabilistic result, the birthday paradox . When the transitivity is constant and there are more edges than wedges (common properties for social networks), we can prove that our algorithm requires O (√ n ) space ( n is the number of vertices) to provide accurate estimates. We run a detailed set of experiments on a variety of real graphs and demonstrate that the memory requirement of the algorithm is a tiny fraction of the graph. For example, even for a graph with 200 million edges, our algorithm stores just 60,000 edges to give accurate results. Being a single pass streaming algorithm, our procedure also maintains a real-time estimate of the transitivity/number of triangles of a graph, by storing a miniscule fraction of edges.",2013,Knowledge Discovery and Data Mining,Fields of study: strength of a graphhopcroft karp algorithmcomplement graphmultiple edgeshypercube graphmixed graphpseudoforestpathtopological graphcycle graphstreaming algorithmmultigraphmatchingdiscrete mathematicscombinatoricsmathematical optimizationalgorithmcomputer sciencemathematics
Discriminant malware distance learning on structural information for automated malware classification,Deguang Kong (University of Texas at Arlington)Guanhua Yan (Los Alamos National Laboratory),"2687063374,2169199873","The voluminous malware variants that appear in the Internet have posed severe threats to its security. In this work, we explore techniques that can automatically classify malware variants into their corresponding families. We present a generic framework that extracts structural information from malware programs as attributed function call graphs, in which rich malware features are encoded as attributes at the function level. Our framework further learns discriminant malware distance metrics that evaluate the similarity between the attributed function call graphs of two malware programs. To combine various types of malware attributes, our method adaptively learns the confidence level associated with the classification capability of each attribute type and then adopts an ensemble of classifiers for automated malware classification. We evaluate our approach with a number of Windows-based malware instances belonging to 11 families, and experimental results show that our automated malware classification method is able to achieve high classification accuracy.",2013,Knowledge Discovery and Data Mining,Fields of study: malwaredistance educationattributionmatchingstructuredata miningpattern recognitionmachine learningcomputer science
Modeling and probabilistic reasoning of population evacuation during large-scale disaster,Xuan Song (University of Tokyo)Quanshi Zhang (University of Tokyo)Yoshihide Sekimoto (University of Tokyo)Teerayut Horanont (University of Tokyo)Satoshi Ueyama (University of Tokyo)Ryosuke Shibasaki (University of Tokyo),"2127257590,2162175003,2086893366,2058931874,2252096319,291396532","The Great East Japan Earthquake and the Fukushima nuclear accident cause large human population movements and evacuations. Understanding and predicting these movements is critical for planning effective humanitarian relief, disaster management, and long-term societal reconstruction. In this paper, we construct a large human mobility database that stores and manages GPS records from mobile devices used by approximately 1.6 million people throughout Japan from 1 August 2010 to 31 July 2011. By mining this enormous set of Auto-GPS mobile sensor data, the short-term and long-term evacuation behaviors for individuals throughout Japan during this disaster are able to be automatically discovered. To better understand and simulate human mobility during the disasters, we develop a probabilistic model that is able to be effectively trained by the discovered evacuations via machine learning technique. Based on our training model, population mobility in various cities impacted by the disasters throughout the country is able to be automatically simulated or predicted. On the basis of the whole database, developed model, and experimental results, it is easy for us to find some new features or population mobility patterns after the recent severe earthquake, tsunami and release of radioactivity in Japan, which are likely to play a vital role in future disaster relief and management worldwide.",2013,Knowledge Discovery and Data Mining,Fields of study: data miningsimulationcomputer science
From data to knowledge: city-wide traffic flows analysis and prediction using bing maps,Anna Izabel J. Tostes (Universidade Federal de Minas Gerais)Fátima de L. P. Duarte-Figueiredo (Pontifícia Universidade Católica de Minas Gerais)Renato Assunção (Universidade Federal de Minas Gerais)Juliana Salles (Microsoft)Antonio A. F. Loureiro (Universidade Federal de Minas Gerais),"2465171595,366313435,2001700347,2143416671,2046347786","Traffic jam is a common contemporary society issue in urban areas. City-wide traffic modeling, visualization, analysis, and prediction are still challenges in this context. Based on Bing Maps information, this work aims to acquire, aggregate, analyze, visualize, and predict traffic jam. Chicago area was evaluated as case study. The flow intensity (free or congested) was analyzed to allow the identification of phase transitions (shocks in the system). Also, a prediction model was developed based on logistic regression to correct discovery future flow intensities for a target street.",2013,Knowledge Discovery and Data Mining,Fields of study: visualizationanalysisoperations researchdata miningcomputer science
A comparison of Foursquare and Instagram to the study of city dynamics and urban social behavior,Thiago H. Silva (Universidade Federal de Minas Gerais)Pedro O. S. Vaz de Melo (Universidade Federal de Minas Gerais)Jussara M. Almeida (Universidade Federal de Minas Gerais)Juliana F. S. Salles (Microsoft)Antonio A. F. Loureiro (Universidade Federal de Minas Gerais),"2159064255,2162838672,2130973600,2143416671,2046347786","Social media systems allow a user connected to the Internet to provide useful data about the context in which they are at any given moment, such as Instagram and Foursquare, which are called participatory sensing systems. Location sharing services are examples of participatory sensing systems. The sensed data is a check-in of a particular place that indicates, for instance, a restaurant in a specific location, and also a signal from a user expressing his/her preference. From a participatory sensing system we can derive a participatory sensor network. In this work we compare two different participatory sensor networks, one derived from Instagram, and another one derived from Foursquare. In Instagram, the sensed data is a picture of a specific place. On the other hand, in Foursquare the sensed data is the actual location associated with a specific category of place (e.g., restaurant). Using those social networks we can extract information in many ways. In this work we are interested in comparing two datasets of Foursquare and two datasets of Instagram. We analyze those datasets to investigate whether we can observe the same users' movement pattern, the popularity of regions in cities, the activities of users who use those social networks, and how users share their content along the time. In answering those questions, we want to better understand location-related information, which is an important aspect of the urban phenomena.",2013,Knowledge Discovery and Data Mining,Fields of study: social mediawireless sensor networksocial behaviordata miningsimulationcomputer science
Enhancing one-class support vector machines for unsupervised anomaly detection,Mennatallah Amer (German University in Cairo)Markus Goldstein (German Research Centre for Artificial Intelligence)Slim Abdennadher (German University in Cairo),"2345302489,2500882517,2305324094","Support Vector Machines (SVMs) have been one of the most successful machine learning techniques for the past decade. For anomaly detection, also a semi-supervised variant, the one-class SVM, exists. Here, only normal data is required for training before anomalies can be detected. In theory, the one-class SVM could also be used in an unsupervised anomaly detection setup, where no prior training is conducted. Unfortunately, it turns out that a one-class SVM is sensitive to outliers in the data. In this work, we apply two modifications in order to make one-class SVMs more suitable for unsupervised anomaly detection: Robust one-class SVMs and eta one-class SVMs. The key idea of both modifications is, that outliers should contribute less to the decision boundary as normal instances. Experiments performed on datasets from UCI machine learning repository show that our modifications are very promising: Comparing with other standard unsupervised anomaly detection algorithms, the enhanced one-class SVMs are superior on two out of four datasets. In particular, the proposed eta one-class SVM has shown the most promising results.",2013,Knowledge Discovery and Data Mining,Fields of study: support vector machineanomaly detectiondata miningpattern recognitionmachine learningcomputer science
Fast and scalable polynomial kernels via explicit feature maps,Ninh Pham (IT University of Copenhagen)Rasmus Pagh (IT University of Copenhagen),"2162985535,1864519460","Approximation of non-linear kernels using random feature mapping has been successfully employed in large-scale data analysis applications, accelerating the training of kernel machines. While previous random feature mappings run in O(ndD) time for $n$ training samples in d-dimensional space and D random feature maps, we propose a novel randomized tensor product technique, called Tensor Sketching, for approximating any polynomial kernel in O(n(d+D \log{D})) time. Also, we introduce both absolute and relative error bounds for our approximation to guarantee the reliability of our estimation algorithm. Empirically, Tensor Sketching achieves higher accuracy and often runs orders of magnitude faster than the state-of-the-art approach for large-scale real-world datasets.",2013,Knowledge Discovery and Data Mining,Fields of study: polynomial kerneltensor productsupport vector machinefast fourier transformdiscrete mathematicsmachine learningmathematical optimizationstatisticscomputer sciencemathematics
The bang for the buck: fair competitive viral marketing from the host perspective,Wei Lu (University of British Columbia)Francesco Bonchi (Yahoo!)Amit Goyal (University of British Columbia)Laks V.S. Lakshmanan (University of British Columbia),"2596596230,2176652147,2675938765,2659892814","The key algorithmic problem in viral marketing is to identify a set of influential users (called seeds ) in a social network, who, when convinced to adopt a product, shall influence other users in the network, leading to a large number of adoptions. When two or more players compete with similar products on the same network we talk about competitive viral marketing , which so far has been studied exclusively from the perspective of one of the competing players. In this paper we propose and study the novel problem of competitive viral marketing from the perspective of the host , i.e., the owner of the social network platform. The host sells viral marketing campaigns as a service to its customers, keeping control of the selection of seeds. Each company specifies its budget and the host allocates the seeds accordingly. From the host's perspective, it is important not only to choose the seeds to maximize the collective expected spread, but also to assign seeds to companies so that it guarantees the ""bang for the buck"" for all companies is nearly identical, which we formalize as the fair seed allocation problem. We propose a new propagation model capturing the competitive nature of viral marketing. Our model is intuitive and retains the desired properties of monotonicity and submodularity. We show that the fair seed allocation problem is NP-hard, and develop an efficient algorithm called Needy Greedy . We run experiments on three real-world social networks, showing that our algorithm is effective and scalable.",2013,Knowledge Discovery and Data Mining,Fields of study: viral marketingsocial network
Social influence based clustering of heterogeneous information networks,Yang Zhou (Georgia Institute of Technology)Ling Liu (Georgia Institute of Technology),"2625264668,2125988131","Social networks continue to grow in size and the type of information hosted. We witness a growing interest in clustering a social network of people based on both their social relationships and their participations in activity based information networks. In this paper, we present a social influence based clustering framework for analyzing heterogeneous information networks with three unique features. First, we introduce a novel social influence based vertex similarity metric in terms of both self-influence similarity and co-influence similarity. We compute self-influence and co-influence based similarity based on social graph and its associated activity graphs and influence graphs respectively. Second, we compute the combined social influence based similarity between each pair of vertices by unifying the self-similarity and multiple co-influence similarity scores through a weight function with an iterative update method. Third, we design an iterative learning algorithm, SI-Cluster, to dynamically refine the K clusters by continuously quantifying and adjusting the weights on self-influence similarity and on multiple co-influence similarity scores towards the clustering convergence. To make SI-Cluster converge fast, we transformed a sophisticated nonlinear fractional programming problem of multiple weights into a straightforward nonlinear parametric programming problem of single variable. Our experiment results show that SI-Cluster not only achieves a better balance between self-influence and co-influence similarities but also scales extremely well for large graph clustering.",2013,Knowledge Discovery and Data Mining,Fields of study: correlation clusteringclustering coefficientfuzzy clusteringheterogeneous networksocial influencetheoretical computer sciencedata miningmachine learningstatisticscomputer sciencemathematics
Inferring human activities from GPS tracks,Barbara Furletti (Istituto di Scienza e Tecnologie dell'Informazione)Paolo Cintia (Istituto di Scienza e Tecnologie dell'Informazione)Chiara Renso (Istituto di Scienza e Tecnologie dell'Informazione)Laura Spinsanti (Judge Rotenberg Educational Center),"2716014144,2086384284,55173896,2574986070","The collection of huge amount of tracking data made possible by the widespread use of GPS devices, enabled the analysis of such data for several applications domains, ranging from traffic management to advertisement and social studies. However, the raw positioning data, as it is detected by GPS devices, lacks of semantic information since this data does not natively provide any additional contextual information like the places that people visited or the activities performed. Traditionally, this information is collected by hand filled questionnaire where a limited number of users are asked to annotate their tracks with the activities they have done. With the purpose of getting large amount of semantically rich trajectories, we propose an algorithm for automatically annotating raw trajectories with the activities performed by the users. To do this, we analyse the stops points trying to infer the Point Of Interest (POI) the user has visited. Based on the category of the POI and a probability measure based on the gravity law, we infer the activity performed. We experimented and evaluated the method in a real case study of car trajectories, manually annotated by users with their activities. Experimental results are encouraging and will drive our future works.",2013,Knowledge Discovery and Data Mining,Fields of study: world wide webdata miningsimulation
"Information cartography: creating zoomable, large-scale maps of information",Dafna Shahaf (Stanford University)Jaewon Yang (Stanford University)Caroline Suen (Stanford University)Jeff Jacobs (Stanford University)Heidi Wang (Stanford University)Jure Leskovec (Stanford University),"1275334909,2131921352,2141378315,2190570706,2230555426,1878631932","In an era of information overload, many people struggle to make sense of complex stories, such as presidential elections or economic reforms. We propose a methodology for creating structured summaries of information, which we call zoomable metro maps. Just as cartographic maps have been relied upon for centuries to help us understand our surroundings, metro maps can help us understand the information landscape. Given large collection of news documents our proposed algorithm generates a map of connections that explicitly captures story development. As different users might be interested in different levels of granularity, the maps are zoomable, with each level of zoom showing finer details and interactions. In this paper, we formalize characteristics of good zoomable maps and formulate their construction as an optimization problem. We provide efficient, scalable methods with theoretical guarantees for generating maps. Pilot user studies over real-world datasets demonstrate that our method helps users comprehend complex stories better than prior work.",2013,Knowledge Discovery and Data Mining,Fields of study: informationautomatic summarizationmultimediaworld wide webdata miningstatisticscomputer science
Network discovery via constrained tensor analysis of fMRI data,"Ian N. Davidson (University of California, Davis)Sean Gilpin (University of California, Davis)Owen T. Carmichael (University of California, Davis)Peter B. Walker (Naval Medical Research Center)","2560595684,2021798691,311514008,2136497257","We pose the problem of network discovery which involves simplifying spatio-temporal data into cohesive regions (nodes) and relationships between those regions (edges). Such problems naturally exist in fMRI scans of human subjects. These scans consist of activations of thousands of voxels over time with the aim to simplify them into the underlying cognitive network being used. We propose supervised and semi-supervised variations of this problem and postulate a constrained tensor decomposition formulation and a corresponding alternating least squares solver that is easy to implement. We show this formulation works well in controlled experiments where supervision is incomplete, superfluous and noisy and is able to recover the underlying ground truth network. We then show that for real fMRI data our approach can reproduce well known results in neurology regarding the default mode network in resting-state healthy and Alzheimer affected individuals. Finally, we show that the reconstruction error of the decomposition provides a useful measure of the network strength and is useful at predicting key cognitive scores both by itself and with clinical information.",2013,Knowledge Discovery and Data Mining,Fields of study: data miningartificial intelligencemachine learningcomputer science
Trace complexity of network inference,Bruno D. Abrahao (Cornell University)Flavio Chierichetti (Sapienza University of Rome)Robert Kleinberg (Cornell University)Alessandro Panconesi (Sapienza University of Rome),"2094811419,2082432826,2006104002,2193688032","The network inference problem consists of reconstructing the edge set of a network given traces representing the chronology of infection times as epidemics spread through the network. This problem is a paradigmatic representative of prediction tasks in machine learning that require deducing a latent structure from observed patterns of activity in a network, which often require an unrealistically large number of resources (e.g., amount of available data, or computational time). A fundamental question is to understand which properties we can predict with a reasonable degree of accuracy with the available resources, and which we cannot. We define the trace complexity as the number of distinct traces required to achieve high fidelity in reconstructing the topology of the unobserved network or, more generally, some of its properties. We give algorithms that are competitive with, while being simpler and more efficient than, existing network inference approaches. Moreover, we prove that our algorithms are nearly optimal, by proving an information-theoretic lower bound on the number of traces that an optimal inference algorithm requires for performing this task in the general case. Given these strong lower bounds, we turn our attention to special cases, such as trees and bounded-degree graphs, and to property recovery tasks, such as reconstructing the degree distribution without inferring the network. We show that these problems require a much smaller (and more realistic) number of traces, making them potentially solvable in practice.",2013,Knowledge Discovery and Data Mining,Fields of study: theoretical computer sciencecombinatoricsmachine learningmathematical optimizationstatisticsalgorithmmathematics
One theme in all views: modeling consensus topics in multiple contexts,Jian Tang (Peking University)Ming Zhang (Peking University)Qiaozhu Mei (University of Michigan),"2625476853,2714461950,2166036605","New challenges have been presented to classical topic models when applied to social media, as user-generated content suffers from significant problems of data sparseness. A variety of heuristic adjustments to these models have been proposed, many of which are based on the use of context information to improve the performance of topic modeling. Existing contextualized topic models rely on arbitrary manipulation of the model structure, by incorporating various context variables into the generative process of classical topic models in an ad hoc manner. Such manipulations usually result in much more complicated model structures, sophisticated inference procedures, and low generalizability to accommodate arbitrary types or combinations of contexts. In this paper we explore a different direction. We propose a general solution that is able to exploit multiple types of contexts without arbitrary manipulation of the structure of classical topic models. We formulate different types of contexts as multiple views of the partition of the corpus. A co-regularization framework is proposed to let these views collaborate with each other, vote for the consensus topics, and distinguish them from view-specific topics. Experiments with real-world datasets prove that the proposed method is both effective and flexible to handle arbitrary types of contexts.",2013,Knowledge Discovery and Data Mining,Fields of study: topic modeluser generated contenttheoretical computer sciencedata miningmachine learningcomputer science
WiseMarket: a new paradigm for managing wisdom of online social users,Caleb Chen Cao (Hong Kong University of Science and Technology)Yongxin Tong (Hong Kong University of Science and Technology)Lei Chen (Hong Kong University of Science and Technology)H. V. Jagadish (University of Michigan),"2136041333,2114386387,2318776917,360112113","The benefits of crowdsourcing are well-recognized today for an increasingly broad range of problems. Meanwhile, the rapid development of social media makes it possible to seek the wisdom of a crowd of targeted users. However, it is not trivial to implement the crowdsourcing platform on social media, specifically to make social media users as workers, we need to address the following two challenges: 1) how to motivate users to participate in tasks, and 2) how to choose users for a task. In this paper, we present Wise Market as an effective framework for crowdsourcing on social media that motivates users to participate in a task with care and correctly aggregates their opinions on pairwise choice problems. The Wise Market consists of a set of investors each with an associated individual confidence in his/her prediction, and after the investment, only the ones whose choices are the same as the whole market are granted rewards. Therefore, a social media user has to give his/her ``best'' answer in order to get rewards, as a consequence, careless answers from sloppy users are discouraged. Under the Wise Market framework, we define an optimization problem to minimize expected cost of paying out rewards while guaranteeing a minimum confidence level, called the Effective Market Problem (EMP) . We propose exact algorithms for calculating the market confidence and the expected cost with O ( n log 2 n ) time cost in a Wise Market with n investors. To deal with the enormous number of users on social media, we design a Central Limit Theorem-based approximation algorithm to compute the market confidence with O ( n ) time cost, as well as a bounded approximation algorithm to calculate the expected cost with O ( n ) time cost. Finally, we have conducted extensive experiments to validate effectiveness of the proposed algorithms on real and synthetic data.",2013,Knowledge Discovery and Data Mining,Fields of study: crowdsourcingsocial mediamanagement sciencedata miningmachine learningcomputer science
SIGMa: simple greedy matching for aligning large knowledge bases,Simon Lacoste-Julien (French Institute for Research in Computer Science and Automation)Konstantina Palla (University of Cambridge)Alex Davies (University of Cambridge)Gjergji Kasneci (Microsoft)Thore Graepel (Microsoft)Zoubin Ghahramani (University of Cambridge),"2302858353,1981745320,2496733916,42507994,2032008572,617670330","The Internet has enabled the creation of a growing number of large-scale knowledge bases in a variety of domains containing complementary information. Tools for automatically aligning these knowledge bases would make it possible to unify many sources of structured knowledge and answer complex queries. However, the efficient alignment of large-scale knowledge bases still poses a considerable challenge. Here, we present Simple Greedy Matching (SiGMa), a simple algorithm for aligning knowledge bases with millions of entities and facts. SiGMa is an iterative propagation algorithm that leverages both the structural information from the relationship graph and flexible similarity measures between entity properties in a greedy local search, which makes it scalable. Despite its greedy nature, our experiments indicate that SiGMa can efficiently match some of the world's largest knowledge bases with high accuracy. We provide additional experiments on benchmark datasets which demonstrate that SiGMa can outperform state-of-the-art approaches both in accuracy and efficiency.",2013,Knowledge Discovery and Data Mining,Fields of study: entitygreedy algorithmknowledge basetheoretical computer sciencedata miningartificial intelligencemachine learningcomputer sciencemathematics
The online revolution: education for everyone,Andrew Y. Ng (Stanford University)Daphne Koller (Stanford University),"2104401652,2167404190","In 2011, Stanford University offered three online courses, which anyone in the world could enroll in and take for free. Together, these three courses had enrollments of around 350,000 students, making this one of the largest experiments in online education ever performed. Since the beginning of 2012, we have transitioned this effort into a new venture, Coursera, a social entrepreneurship company whose mission is to make high-quality education accessible to everyone by allowing the best universities to offer courses to everyone around the world, for free. Coursera classes provide a real course experience to students, including video content, interactive exercises with meaningful feedback, using both auto-grading and peer-grading, and a rich peer-to-peer interaction around the course materials. Currently, Coursera has 62 university partners, and over 3 million students enrolled in its over 300 courses. These courses span a range of topics including computer science, business, medicine, science, humanities, social sciences, and more. In this talk, I'll report on this far-reaching experiment in education, and why we believe this model can provide both an improved classroom experience for our on-campus students, via a flipped classroom model, as well as a meaningful learning experience for the millions of students around the world who would otherwise never have access to education of this quality.",2013,Knowledge Discovery and Data Mining,Fields of study: mathematics educationmultimediacomputer science
Recursive regularization for large-scale classification with hierarchical and graphical dependencies,Siddharth Gopal (Carnegie Mellon University)Yiming Yang (Carnegie Mellon University),"2128212913,2159253281","The two key challenges in hierarchical classification are to leverage the hierarchical dependencies between the class-labels for improving performance, and, at the same time maintaining scalability across large hierarchies. In this paper we propose a regularization framework for large-scale hierarchical classification that addresses both the problems. Specifically, we incorporate the hierarchical dependencies between the class-labels into the regularization structure of the parameters thereby encouraging classes nearby in the hierarchy to share similar model parameters. Furthermore, we extend our approach to scenarios where the dependencies between the class-labels are encoded in the form of a graph rather than a hierarchy. To enable large-scale training, we develop a parallel-iterative optimization scheme that can handle datasets with hundreds of thousands of classes and millions of instances and learning terabytes of parameters. Our experiments showed a consistent improvement over other competing approaches and achieved state-of-the-art results on benchmark datasets.",2013,Knowledge Discovery and Data Mining,Fields of study: data miningpattern recognitionmachine learningmathematics
Entity resolution for big data,"Lise Getoor (University of Maryland, College Park)Ashwin Machanavajjhala (Duke University)","1984940772,2073648588","Entity resolution (ER), the problem of extracting, matching and resolving entity mentions in structured and unstructured data, is a long-standing challenge in database management, information retrieval, machine learning, natural language processing and statistics. Accurate and fast entity resolution has huge practical implications in a wide variety of commercial, scientific and security domains. Despite the long history of work on entity resolution, there is still a surprising diversity of approaches, and lack of guiding theory. Meanwhile, in the age of big data, the need for high quality entity resolution is growing, as we are inundated with more and more data, all of which needs to be integrated, aligned and matched, before further utility can be extracted. In this tutorial, we bring together perspectives on entity resolution from a variety of fields, including databases, information retrieval, natural language processing and machine learning, to provide, in one setting, a survey of a large body of work. We discuss both the practical aspects and theoretical underpinnings of ER. We describe existing solutions, current challenges and open research problems. In addition to giving attendees a thorough understanding of existing ER models, algorithms and evaluation methods, the tutorial will cover important research topics such as scalable ER, active and lightly supervised ER, and query-driven ER.",2013,Knowledge Discovery and Data Mining,Fields of study: association rule learningcluster analysisdata sciencedata miningartificial intelligencemachine learningstatisticscomputer science
Subsampling for efficient and effective unsupervised outlier detection ensembles,Arthur Zimek (University of Alberta)Matthew Gaudet (University of Alberta)Ricardo J.G.B. Campello (University of Alberta)Jörg Sander (University of Alberta),"242745652,2169637448,2043417111,2118842476","Outlier detection and ensemble learning are well established research directions in data mining yet the application of ensemble techniques to outlier detection has been rarely studied. Here, we propose and study subsampling as a technique to induce diversity among individual outlier detectors. We show analytically and experimentally that an outlier detector based on a subsample per se, besides inducing diversity, can, under certain conditions, already improve upon the results of the same outlier detector on the complete dataset. Building an ensemble on top of several subsamples is further improving the results. While in the literature so far the intuition that ensembles improve over single outlier detectors has just been transferred from the classification literature, here we also justify analytically why ensembles are also expected to work in the unsupervised area of outlier detection. As a side effect, running an ensemble of several outlier detectors on subsamples of the dataset is more efficient than ensembles based on other means of introducing diversity and, depending on the sample rate and the size of the ensemble, can be even more efficient than just the single outlier detector on the complete data.",2013,Knowledge Discovery and Data Mining,Fields of study: ensemblanomaly detectiondata miningpattern recognitionmachine learningstatisticscomputer sciencemathematics
Scalable all-pairs similarity search in metric spaces,Ye Wang (Ohio State University)Ahmed Metwally (Google)Srinivasan Parthasarathy (Ohio State University),"2306889374,2109236845,2106796124","Given a set of entities, the all-pairs similarity search aims at identifying all pairs of entities that have similarity greater than (or distance smaller than) some user-defined threshold. In this article, we propose a parallel framework for solving this problem in metric spaces. Novel elements of our solution include: i) flexible support for multiple metrics of interest; ii) an autonomic approach to partition the input dataset with minimal redundancy to achieve good load-balance in the presence of limited computing resources; iii) an on-the- fly lossless compression strategy to reduce both the running time and the final output size. We validate the utility, scalability and the effectiveness of the approach on hundreds of machines using real and synthetic datasets.",2013,Knowledge Discovery and Data Mining,Fields of study: theoretical computer sciencedata miningmachine learningcomputer sciencemathematics
One click mining: interactive local pattern discovery through implicit preference and performance learning,Mario Boley (Fraunhofer Society)Michael Mampaey (Fraunhofer Society)Bo Kang (Fraunhofer Society)Pavel Tokmakov (Fraunhofer Society)Stefan Wrobel (Fraunhofer Society),"2074964178,2645533625,2228844688,2581670731,2113978936","It is known that productive pattern discovery from data has to interactively involve the user as directly as possible. State-of-the-art toolboxes require the specification of sophisticated workflows with an explicit selection of a data mining method, all its required parameters, and a corresponding algorithm. This hinders the desired rapid interaction---especially with users that are experts of the data domain rather than data mining experts. In this paper, we present a fundamentally new approach towards user involvement that relies exclusively on the implicit feedback available from the natural analysis behavior of the user, and at the same time allows the user to work with a multitude of pattern classes and discovery algorithms simultaneously without even knowing the details of each algorithm. To achieve this goal, we are relying on a recently proposed co-active learning model and a special feature representation of patterns to arrive at an adaptively tuned user interestingness model. At the same time, we propose an adaptive time-allocation strategy to distribute computation time among a set of underlying mining algorithms. We describe the technical details of our approach, present the user interface for gathering implicit feedback, and provide preliminary evaluation results.",2013,Knowledge Discovery and Data Mining,Fields of study: data miningmachine learningsimulationcomputer science
Understanding Twitter data with TweetXplorer,Fred Morstatter (Arizona State University)Shamanth Kumar (Arizona State University)Huan Liu (Arizona State University)Ross Maciejewski (Arizona State University),"107712532,2166799513,2122391114,2142123371","In the era of big data it is increasingly difficult for an analyst to extract meaningful knowledge from a sea of information. We present TweetXplorer, a system for analysts with little information about an event to gain knowledge through the use of effective visualization techniques. Using tweets collected during Hurricane Sandy as an example, we will lead the reader through a workflow that exhibits the functionality of the system.",2013,Knowledge Discovery and Data Mining,Fields of study: geospatial analysisbig datadata scienceworld wide webdata miningcomputer science
DTW-D: time series semi-supervised learning from a single example,"Yanping Chen 0005 (University of California, Riverside)Bing Hu 0001 (University of California, Riverside)Eamonn J. Keogh (University of California, Riverside)Gustavo E. A. P. A. Batista (University of São Paulo)","2472401419,2105942667,2170070822,2165222361","Classification of time series data is an important problem with applications in virtually every scientific endeavor. The large research community working on time series classification has typically used the UCR Archive to test their algorithms. In this work we argue that the availability of this resource has isolated much of the research community from the following reality, labeled time series data is often very difficult to obtain. The obvious solution to this problem is the application of semi-supervised learning; however, as we shall show, direct applications of off-the-shelf semi-supervised learning algorithms do not typically work well for time series. In this work we explain why semi-supervised learning algorithms typically fail for time series problems, and we introduce a simple but very effective fix. We demonstrate our ideas on diverse real word problems.",2013,Knowledge Discovery and Data Mining,Fields of study: time seriesbiological classificationsemi supervised learningdata miningartificial intelligencemachine learningstatisticscomputer science
Multi-source learning with block-wise missing data for Alzheimer's disease prediction,"Shuo Xiang (Arizona State University)Lei Yuan (Arizona State University)Wei Fan (Huawei)Yalin Wang (Arizona State University)Paul M. Thompson (University of California, Los Angeles)Jieping Ye (Arizona State University)","2121591154,2243895322,2422054197,2607855732,2114138198,2305258894","With the advances and increasing sophistication in data collection techniques, we are facing with large amounts of data collected from multiple heterogeneous sources in many applications. For example, in the study of Alzheimer's Disease (AD), different types of measurements such as neuroimages, gene/protein expression data, genetic data etc. are often collected and analyzed together for improved predictive power. It is believed that a joint learning of multiple data sources is beneficial as different data sources may contain complementary information, and feature-pruning and data source selection are critical for learning interpretable models from high-dimensional data. Very often the collected data comes with block-wise missing entries; for example, a patient without the MRI scan will have no information in the MRI data block, making his/her overall record incomplete. There has been a growing interest in the data mining community on expanding traditional techniques for single-source complete data analysis to the study of multi-source incomplete data. The key challenge is how to effectively integrate information from multiple heterogeneous sources in the presence of block-wise missing data. In this paper we first investigate the situation of complete data and present a unified ``bi-level"" learning model for multi-source data. Then we give a natural extension of this model to the more challenging case with incomplete data. Our major contributions are threefold: (1) the proposed models handle both feature-level and source-level analysis in a unified formulation and include several existing feature learning approaches as special cases; (2) the model for incomplete data avoids direct imputation of the missing elements and thus provides superior performances. Moreover, it can be easily generalized to other applications with block-wise missing data sources; (3) efficient optimization algorithms are presented for both the complete and incomplete models. We have performed comprehensive evaluations of the proposed models on the application of AD diagnosis. Our proposed models compare favorably against existing approaches.",2013,Knowledge Discovery and Data Mining,Fields of study: data pre processingimputationdata sciencedata miningmachine learningstatisticscomputer science
Accurate intelligible models with pairwise interactions,Yin Lou (Cornell University)Rich Caruana (Microsoft)Johannes Gehrke (Cornell University)Giles Hooker (Cornell University),"2111965133,2680453709,2083845045,1528203563","Standard generalized additive models (GAMs) usually model the dependent variable as a sum of univariate models. Although previous studies have shown that standard GAMs can be interpreted by users, their accuracy is significantly less than more complex models that permit interactions. In this paper, we suggest adding selected terms of interacting pairs of features to standard GAMs. The resulting models, which we call GA 2 {M}$-models, for Generalized Additive Models plus Interactions , consist of univariate terms and a small number of pairwise interaction terms. Since these models only include one- and two-dimensional components, the components of GA 2 M-models can be visualized and interpreted by users. To explore the huge (quadratic) number of pairs of features, we develop a novel, computationally efficient method called FAST for ranking all possible pairs of features as candidates for inclusion into the model. In a large-scale empirical study, we show the effectiveness of FAST in ranking candidate pairs of features. In addition, we show the surprising result that GA 2 M-models have almost the same performance as the best full-complexity models on a number of real datasets. Thus this paper postulates that for many problems, GA 2 M-models can yield models that are both intelligible and accurate.",2013,Knowledge Discovery and Data Mining,Fields of study: regressionbiological classificationeconometricsdata miningmachine learningstatisticsmathematics
Automatic selection of social media responses to news,Tadej ŠtajnerBart Thomee (Yahoo!)Ana-Maria Popescu (Yahoo!)Marco Pennacchiotti (eBay)Alejandro Jaimes (Yahoo!),"2637264824,114219197,2616900583,2618302306,2193615068","Social media responses to news have increasingly gained in importance as they can enhance a consumer's news reading experience, promote information sharing and aid journalists in assessing their readership's response to a story. Given that the number of responses to an online news article may be huge, a common challenge is that of selecting only the most interesting responses for display. This paper addresses this challenge by casting message selection as an optimization problem. We define an objective function which jointly models the messages' utility scores and their entropy. We propose a near-optimal solution to the underlying optimization problem, which leverages the submodularity property of the objective function. Our solution first learns the utility of individual messages in isolation and then produces a diverse selection of interesting messages by maximizing the defined objective function. The intuitions behind our work are that an interesting selection of messages contains diverse, informative, opinionated and popular messages referring to the news article, written mostly by users that have authority on the topic. Our intuitions are embodied by a rich set of content, social and user features capturing the aforementioned aspects. We evaluate our approach through both human and automatic experiments, and demonstrate it outperforms the state of the art. Additionally, we perform an in-depth analysis of the annotated ``interesting'' responses, shedding light on the subjectivity around the selection process and the perception of interestingness.",2013,Knowledge Discovery and Data Mining,Fields of study: microbloggingsocial mediasamplingautomatic summarizationmultimediaworld wide webdata miningmachine learningcomputer science
Comparing apples to oranges: a scalable solution with heterogeneous hashing,Mingdong Ou (Tsinghua University)Peng Cui (Tsinghua University)Fei Wang (IBM)Jun Wang (IBM)Wenwu Zhu (Tsinghua University)Shiqiang Yang (Tsinghua University),"2157830877,2113115369,2465953593,2567112495,2111511002,2127183023","Although hashing techniques have been popular for the large scale similarity search problem, most of the existing methods for designing optimal hash functions focus on homogeneous similarity assessment, i.e., the data entities to be indexed are of the same type. Realizing that heterogeneous entities and relationships are also ubiquitous in the real world applications, there is an emerging need to retrieve and search similar or relevant data entities from multiple heterogeneous domains, e.g., recommending relevant posts and images to a certain Facebook user. In this paper, we address the problem of ``comparing apples to oranges'' under the large scale setting. Specifically, we propose a novel Relation-aware Heterogeneous Hashing (RaHH), which provides a general framework for generating hash codes of data entities sitting in multiple heterogeneous domains. Unlike some existing hashing methods that map heterogeneous data in a common Hamming space, the RaHH approach constructs a Hamming space for each type of data entities, and learns optimal mappings between them simultaneously. This makes the learned hash codes flexibly cope with the characteristics of different data domains. Moreover, the RaHH framework encodes both homogeneous and heterogeneous relationships between the data entities to design hash functions with improved accuracy. To validate the proposed RaHH method, we conduct extensive evaluations on two large datasets; one is crawled from a popular social media sites, Tencent Weibo , and the other is an open dataset of Flickr (NUS-WIDE). The experimental results clearly demonstrate that the RaHH outperforms several state-of-the-art hashing methods with significant performance gains.",2013,Knowledge Discovery and Data Mining,Fields of study: feature hashingdynamic perfect hashinguniversal hashinglocality sensitive hashingheterogeneous networkhash functionscalabilitybig dataworld wide webdata miningdatabasecomputer science
Text-based measures of document diversity,"Kevin Bache (University of California, Irvine)David Newman (University of California, Irvine)Padhraic Smyth (University of California, Irvine)","1969118307,1994620265,2137074633","Quantitative notions of diversity have been explored across a variety of disciplines ranging from conservation biology to economics. However, there has been relatively little work on measuring the diversity of text documents via their content. In this paper we present a text-based framework for quantifying how diverse a document is in terms of its content. The proposed approach learns a topic model over a corpus of documents, and computes a distance matrix between pairs of topics using measures such as topic co-occurrence. These pairwise distance measures are then combined with the distribution of topics within a document to estimate each document's diversity relative to the rest of the corpus. The method provides several advantages over existing methods. It is fully data-driven, requiring only the text from a corpus of documents as input, it produces human-readable explanations, and it can be generalized to score diversity of other entities such as authors, academic departments, or journals. We describe experimental results on several large data sets which suggest that the approach is effective and accurate in quantifying how diverse a document is relative to other documents in a corpus.",2013,Knowledge Discovery and Data Mining,Fields of study: data scienceinformation retrievaldata miningmachine learningcomputer science
Online spatial data analysis and visualization system,Yun Lu (Florida International University)Mingjin Zhang (Florida International University)Tao Li (Florida International University)Yudong Guang (Florida International University)Naphtali Rishe (Florida International University),"2292763978,2228178917,2472069284,2222106588,2019309144","With the exponential growth of the usage of web map services, the geo data analysis has become more and more popular. This paper develops an online spatial data analysis and visualization system, TerraFly GeoCloud, which facilitates end users to visualize and analyze spatial data, and to share the analysis results. Built on the TerraFly Geo spatial database, TerraFly GeoCloud is an extra layer running upon the TerraFly map and can efficiently support many different visualization functions and spatial data analysis models. Furthermore, users can create unique URLs to visualize and share the analysis results. TerraFly GeoCloud also enables the MapQL technology to customize map visualization using SQL-like statements. The system is available at http://terrafly.fiu.edu/GeoCloud/.",2013,Knowledge Discovery and Data Mining,Fields of study: dimensionality reductioncluster analysisvisual analyticsinteractive visualizationdata scienceworld wide webdata miningmachine learningcomputer science
Extracting social events for learning better information diffusion models,Shuyang Lin (University of Illinois at Chicago)Fengjiao Wang (University of Illinois at Chicago)Qingbo Hu (University of Illinois at Chicago)Philip S. Yu (University of Illinois at Chicago),"2096615129,2153251044,2105415358,2125104194","Learning of the information diffusion model is a fundamental problem in the study of information diffusion in social networks. Existing approaches learn the diffusion models from events in social networks. However, events in social networks may have different underlying reasons. Some of them may be caused by the social influence inside the network, while others may reflect external trends in the ``real world''. Most existing work on the learning of diffusion models does not distinguish the events caused by the social influence from those caused by external trends. In this paper, we extract social events from data streams in social networks, and then use the extracted social events to improve the learning of information diffusion models. We propose a LADP (Latent Action Diffusion Path) model to incorporate the information diffusion model with the model of external trends, and then design an EM-based algorithm to infer the diffusion probabilities, the external trends and the sources of events efficiently.",2013,Knowledge Discovery and Data Mining,Fields of study: social influencedata miningartificial intelligence
Synthetic review spamming and defense,"Huan Sun (University of California, Santa Barbara)Alex Morales (University of California, Santa Barbara)Xifeng Yan (University of California, Santa Barbara)","2618514357,2106875415,2116657824","Online reviews have been popularly adopted in many applications. Since they can either promote or harm the reputation of a product or a service, buying and selling fake reviews becomes a profitable business and a big threat. In this paper, we introduce a very simple, but powerful review spamming technique that could fail the existing feature-based detection algorithms easily. It uses one truthful review as a template, and replaces its sentences with those from other reviews in a repository. Fake reviews generated by this mechanism are extremely hard to detect: Both the state-of-the-art computational approaches and human readers acquire an error rate of 35%-48%, just slightly better than a random guess. While it is challenging to detect such fake reviews, we have made solid progress in suppressing them. A novel defense method that leverages the difference of semantic flows between synthetic and truthful reviews is developed, which is able to reduce the detection error rate to approximately 22%, a significant improvement over the performance of existing approaches. Nevertheless, it is still a challenging research task to further decrease the error rate. Synthetic Review Spamming Demo: www.cs.ucsb.edu/~alex_morales/reviewspam/",2013,Knowledge Discovery and Data Mining,Fields of study: biological classificationworld wide webcomputer securitydata mining
A general bootstrap performance diagnostic,"Ariel Kleiner (University of California, Berkeley)Ameet Talwalkar (University of California, Berkeley)Sameer Agarwal (University of California, Berkeley)Ion Stoica (University of California, Berkeley)Michael I. Jordan (University of California, Berkeley)","2098834125,2042309534,2175835430,2161479384,2435751034","As datasets become larger, more complex, and more available to diverse groups of analysts, it would be quite useful to be able to automatically and generically assess the quality of estimates, much as we are able to automatically train and evaluate predictive models such as classifiers. However, despite the fundamental importance of estimator quality assessment in data analysis, this task has eluded highly automatic solutions. While the bootstrap provides perhaps the most promising step in this direction, its level of automation is limited by the difficulty of evaluating its finite sample performance and even its asymptotic consistency. Thus, we present here a general diagnostic procedure which directly and automatically evaluates the accuracy of the bootstrap's outputs, determining whether or not the bootstrap is performing satisfactorily when applied to a given dataset and estimator. We show that our proposed diagnostic is effective via an extensive empirical evaluation on a variety of estimators and simulated and real datasets, including a real-world query workload from Conviva, Inc. involving 1.7TB of data (i.e., approximately 0.5 billion data points).",2013,Knowledge Discovery and Data Mining,Fields of study: performanceeconometricsdata miningstatisticscomputer science
Multi-space probabilistic sequence modeling,Shuo Chen (Cornell University)Jiexun Xu (Cornell University)Thorsten Joachims (Cornell University),"2295814016,2225184691,245171893","Learning algorithms that embed objects into Euclidean space have become the methods of choice for a wide range of problems, ranging from recommendation and image search to playlist prediction and language modeling. Probabilistic embedding methods provide elegant approaches to these problems, but can be expensive to train and store as a large monolithic model. In this paper, we propose a method that trains not one monolithic model, but multiple local embeddings for a class of pairwise conditional models especially suited for sequence and co-occurrence modeling. We show that computation and memory for training these multi-space models can be efficiently parallelized over many nodes of a cluster. Focusing on sequence modeling for music playlists, we show that the method substantially speeds up training while maintaining high model quality.",2013,Knowledge Discovery and Data Mining,Fields of study: embeddingsequencetheoretical computer sciencedata miningmachine learningsimulationcomputer sciencemathematics
Multi-label classification by mining label and instance correlations from heterogeneous information networks,Xiangnan Kong (University of Illinois at Chicago)Bokai Cao (Renmin University of China)Philip S. Yu (University of Illinois at Chicago),"2204127537,2131202988,2125104194","Multi-label classification is prevalent in many real-world applications, where each example can be associated with a set of multiple labels simultaneously. The key challenge of multi-label classification comes from the large space of all possible label sets, which is exponential to the number of candidate labels. Most previous work focuses on exploiting correlations among different labels to facilitate the learning process. It is usually assumed that the label correlations are given beforehand or can be derived directly from data samples by counting their label co-occurrences. However, in many real-world multi-label classification tasks, the label correlations are not given and can be hard to learn directly from data samples within a moderate-sized training set. Heterogeneous information networks can provide abundant knowledge about relationships among different types of entities including data samples and class labels. In this paper, we propose to use heterogeneous information networks to facilitate the multi-label classification process. By mining the linkage structure of heterogeneous information networks, multiple types of relationships among different class labels and data samples can be extracted. Then we can use these relationships to effectively infer the correlations among different class labels in general, as well as the dependencies among the label sets of data examples inter-connected in the network. Empirical studies on real-world tasks demonstrate that the performance of multi-label classification can be effectively boosted using heterogeneous information net- works.",2013,Knowledge Discovery and Data Mining,Fields of study: data miningpattern recognitionmachine learningcomputer science
Information cascade at group scale,Milad Eftekhar (University of Toronto)Yashar Ganjali (University of Toronto)Nick Koudas (University of Toronto),"2123746623,1982628752,335443309","Identifying the k most influential individuals in a social network is a well-studied problem. The objective is to detect k individuals in a (social) network who will influence the maximum number of people, if they are independently convinced of adopting a new strategy (product, idea, etc). There are cases in real life, however, where we aim to instigate groups instead of individuals to trigger network diffusion. Such cases abound, e.g., billboards, TV commercials and newspaper ads are utilized extensively to boost the popularity and raise awareness. In this paper, we generalize the ""influential nodes"" problem. Namely we are interested to locate the most ""influential groups"" in a network. As the first paper to address this problem: we (1) propose a fine-grained model of information diffusion for the group-based problem, (2) show that the process is submodular and present an algorithm to determine the influential groups under this model (with a precise approximation bound), (3) propose a coarse-grained model that inspects the network at group level (not individuals) significantly speeding up calculations for large networks, (4) show that the diffusion function we design here is submodular in general case, and propose an approximation algorithm for this coarse-grained model, and finally by conducting experiments on real datasets, (5) demonstrate that seeding members of selected groups to be the first adopters can broaden diffusion (when compared to the influential individuals case). Moreover, we can identify these influential groups much faster (up to 12 million times speedup), delivering a practical solution to this problem.",2013,Knowledge Discovery and Data Mining,Fields of study: information cascadesocial networkdata miningartificial intelligencemachine learningsimulationstatisticsmathematics
Building blocks for exploratory data analysis tools,"Sara Alspaugh (University of California, Berkeley)Marti A. Hearst (University of California, Berkeley)Archana Ganapathi (University of California, Berkeley)Randy H. Katz (University of California, Berkeley)","2048793609,2169348332,2101558141,2246122413","Data exploration is largely manual and labor intensive. Although there are various tools and statistical techniques that can be applied to data sets, there is little help to identify what questions to ask of a data set, let alone what domain knowledge is useful in answering the questions. In this paper, we study user queries against production data sets in Splunk. Specifically, we characterize the interplay between data sets and the operations used to analyze them using latent semantic analysis, and discuss how this characterization serves as a building block for a data analysis recommendation system. This is a work-in-progress paper.",2013,Knowledge Discovery and Data Mining,Fields of study: data sciencedata miningdatabasecomputer science
Making recommendations from multiple domains,Wei Chen (National University of Singapore)Wynne Hsu (National University of Singapore)Mong Li Lee (National University of Singapore),"2634686055,2123778117,2159408573","Given the vast amount of information on the World Wide Web, recommender systems are increasingly being used to help filter irrelevant data and suggest information that would interest users. Traditional systems make recommendations based on a single domain e.g., movie or book domain. Recent work has examined the correlations in different domains and designed models that exploit user preferences on a source domain to predict user preferences on a target domain. However, these methods are based on matrix factorization and can only be applied to two-dimensional data. Transferring high dimensional data from one domain to another requires decomposing the high dimensional data to binary relations which results in information loss. Furthermore, this decomposition creates a large number of matrices that need to be transferred and combining them in the target domain is non-trivial. Separately, researchers have looked into using social network information to improve recommendation. However, this social network information has not been explored in cross domain collaborative filtering. In this work, we propose a generalized cross domain collaborative filtering framework that integrates social network information seamlessly with cross domain data. This is achieved by utilizing tensor factorization with topic based social regularization. This framework is able to transfer high dimensional data without the need for decomposition by finding shared implicit cluster-level tensor from multiple domains. Extensive experiments conducted on real world datasets indicate that the proposed framework outperforms state-of-art algorithms for item recommendation, user recommendation and tag recommendation.",2013,Knowledge Discovery and Data Mining,Fields of study: domain engineeringpersonalizationdomain analysiscollaborative filteringworld wide webinformation retrievaldata miningmachine learningcomputer science
Systematic construction of anomaly detection benchmarks from real data,Andrew F. Emmott (Oregon State University)Shubhomoy Das (Oregon State University)Thomas Dietterich (Oregon State University)Alan Fern (Oregon State University)Weng-Keen Wong (Oregon State University),"2338058579,2121364888,160031478,2139785505,2102128069","Research in anomaly detection suffers from a lack of realistic and publicly-available problem sets. This paper discusses what properties such problem sets should possess. It then introduces a methodology for transforming existing classification data sets into ground-truthed benchmark data sets for anomaly detection. The methodology produces data sets that vary along three important dimensions: (a) point difficulty, (b) relative frequency of anomalies, and (c) clusteredness. We apply our generated datasets to benchmark several popular anomaly detection algorithms under a range of different conditions.",2013,Knowledge Discovery and Data Mining,Fields of study: one class classificationsupport vector machineanomaly detectiondata miningpattern recognitionmachine learningcomputer sciencemathematics
Cost-sensitive online active learning with application to malicious URL detection,Peilin Zhao (Nanyang Technological University)Steven C.H. Hoi (Nanyang Technological University),"2096910461,108406206","Malicious Uniform Resource Locator (URL) detection is an important problem in web search and mining, which plays a critical role in internet security. In literature, many existing studies have attempted to formulate the problem as a regular supervised binary classification task, which typically aims to optimize the prediction accuracy. However, in a real-world malicious URL detection task, the ratio between the number of malicious URLs and legitimate URLs is highly imbalanced, making it very inappropriate for simply optimizing the prediction accuracy. Besides, another key limitation of the existing work is to assume a large amount of training data is available, which is impractical as the human labeling cost could be potentially quite expensive. To solve these issues, in this paper, we present a novel framework of Cost-Sensitive Online Active Learning (CSOAL), which only queries a small fraction of training data for labeling and directly optimizes two cost-sensitive measures to address the class-imbalance issue. In particular, we propose two CSOAL algorithms and analyze their theoretical performance in terms of cost-sensitive bounds. We conduct an extensive set of experiments to examine the empirical performance of the proposed algorithms for a large-scale challenging malicious URL detection task, in which the encouraging results showed that the proposed technique by querying an extremely small-sized labeled data (about 0.5% out of 1-million instances) can achieve better or highly comparable classification performance in comparison to the state-of-the-art cost-insensitive and cost-sensitive online classification algorithms using a huge amount of labeled data.",2013,Knowledge Discovery and Data Mining,Fields of study: active learninginternet privacyworld wide webdata miningmachine learningcomputer science
Model-based kernel for efficient time series analysis,Huanhuan Chen (University of Science and Technology of China)Fengzhen Tang (University of Birmingham)Peter Tino (University of Birmingham)Xin Yao (University of Birmingham),"2162722686,2122821842,54544322,2106268727","We present novel, efficient, model based kernels for time series data rooted in the reservoir computation framework. The kernels are implemented by fitting reservoir models sharing the same fixed deterministically constructed state transition part to individual time series. The proposed kernels can naturally handle time series of different length without the need to specify a parametric model class for the time series. Compared with most time series kernels, our kernels are computationally efficient. We show how the model distances used in the kernel can be calculated analytically or efficiently estimated. The experimental results on synthetic and benchmark time series classification tasks confirm the efficiency of the proposed kernel in terms of both generalization accuracy and computational speed. This paper also investigates on-line reservoir kernel construction for extremely long time series.",2013,Knowledge Discovery and Data Mining,Fields of study: kernel embedding of distributionstree kernelvariable kernel density estimationreservoir computingkernel smootherkernel methodtime serieseconometricsmachine learningmathematical optimizationcomputer sciencemathematics
Flexible and robust co-regularized multi-domain graph clustering,"Wei Cheng (University of North Carolina at Chapel Hill)Xiang Zhang (Case Western Reserve University)Zhishan Guo (University of North Carolina at Chapel Hill)Yubao Wu (Case Western Reserve University)Patrick F. Sullivan (University of North Carolina at Chapel Hill)Wei Wang (University of California, Los Angeles)","2620045292,2553248206,2165841276,2097850430,2153068280,2315689540","Multi-view graph clustering aims to enhance clustering performance by integrating heterogeneous information collected in different domains. Each domain provides a different view of the data instances. Leveraging cross-domain information has been demonstrated an effective way to achieve better clustering results. Despite the previous success, existing multi-view graph clustering methods usually assume that different views are available for the same set of instances. Thus instances in different domains can be treated as having strict one-to-one relationship. In many real-life applications, however, data instances in one domain may correspond to multiple instances in another domain. Moreover, relationships between instances in different domains may be associated with weights based on prior (partial) knowledge. In this paper, we propose a flexible and robust framework, CGC (Co-regularized Graph Clustering), based on non-negative matrix factorization (NMF), to tackle these challenges. CGC has several advantages over the existing methods. First, it supports many-to-many cross-domain instance relationship. Second, it incorporates weight on cross-domain relationship. Third, it allows partial cross-domain mapping so that graphs in different domains may have different sizes. Finally, it provides users with the extent to which the cross-domain instance relationship violates the in-domain clustering structure, and thus enables users to re-evaluate the consistency of the relationship. Extensive experimental results on UCI benchmark data sets, newsgroup data sets and biological interaction networks demonstrate the effectiveness of our approach.",2013,Knowledge Discovery and Data Mining,Fields of study: canopy clustering algorithmcorrelation clusteringconstrained clusteringdata stream clusteringcure data clustering algorithmclustering coefficientfuzzy clusteringnon negative matrix factorizationcluster analysisdata miningpattern recognitionmachine learningcomputer sciencemathematics
A new collaborative filtering approach for increasing the aggregate diversity of recommender systems,Katja Niemann (Fraunhofer Society)Martin Wolpers (Fraunhofer Society),"2096504505,408098239","In order to satisfy and positively surprise the users, a recommender system needs to recommend items the users will like and most probably would not have found on their own. This requires the recommender system to recommend a broader range of items including niche items as well. Such an approach also support online-stores that often offer more items than traditional stores and need recommender systems to enable users to find the not so popular items as well. However, popular items that hold a lot of usage data are more easy to recommend and, thus, niche items are often excluded from the recommendations. In this paper, we propose a new collaborative filtering approach that is based on the items' usage contexts. The approach increases the rating predictions for niche items with fewer usage data available and improves the aggragate diversity of the recommendations.",2013,Knowledge Discovery and Data Mining,Fields of study: long tailrecommender systemworld wide webdata miningcomputer science
A “semi-lazy” approach to probabilistic path prediction in dynamic environments,"Jingbo Zhou (National University of Singapore)Anthony K.H. Tung (National University of Singapore)Wei Wu (Agency for Science, Technology and Research)Wee Siong Ng (Agency for Science, Technology and Research)","2140736868,121171588,2311660342,2136598690","Path prediction is useful in a wide range of applications. Most of the existing solutions, however, are based on eager learning methods where models and patterns are extracted from historical trajectories and then used for future prediction. Since such approaches are committed to a set of statistically significant models or patterns, problems can arise in dynamic environments where the underlying models change quickly or where the regions are not covered with statistically significant models or patterns. We propose a ""semi-lazy"" approach to path prediction that builds prediction models on the fly using dynamically selected reference trajectories. Such an approach has several advantages. First, the target trajectories to be predicted are known before the models are built, which allows us to construct models that are deemed relevant to the target trajectories. Second, unlike the lazy learning approaches, we use sophisticated learning algorithms to derive accurate prediction models with acceptable delay based on a small number of selected reference trajectories. Finally, our approach can be continuously self-correcting since we can dynamically re-construct new models if the predicted movements do not match the actual ones. Our prediction model can construct a probabilistic path whose probability of occurrence is larger than a threshold and which is furthest ahead in term of time. Users can control the confidence of the path prediction by setting a probability threshold. We conducted a comprehensive experimental study on real-world and synthetic datasets to show the effectiveness and efficiency of our approach.",2013,Knowledge Discovery and Data Mining,Fields of study: data miningartificial intelligencemachine learningcomputer science
Analyzing the composition of cities using spatial clustering,Zechun Cao (University of Houston)Sujing Wang (University of Houston)Germain Forestier (University of Strasbourg)Anne Puissant (University of Strasbourg)Christoph F. Eick (University of Houston),"2231790940,2482609005,2008768452,2112917604,715851671","Cities all around the world are in constant evolution due to numerous factors, such as fast urbanization and new ways of communication and transportation. Since understanding the composition of cities is the key to intelligent urbanization, there is a growing need to develop urban computing and analysis tools to guide the orderly development of cities, as well as to enhance their smooth and beneficiary evolution. This paper presents a spatial clustering approach to discover interesting regions and regions which serve different functions in cities. Spatial clustering groups the objects in a spatial dataset and identifies contiguous regions in the space of the spatial attributes. We formally define the task of finding uniform regions in spatial data as a maximization problem of a plug-in measure of uniformity and introduce a prototype-based clustering algorithm named CLEVER to find such regions. Moreover, polygon models which capture the scope of a spatial cluster and histogram-style distribution signatures are used to annotate the content of a spatial cluster in the proposed methodology; they play a key role in summarizing the composition of a spatial dataset. Furthermore, algorithms for identifying popular distribution signatures and approaches for identifying regions which express a particular distribution signature will be presented. The proposed methodology is demonstrated and evaluated in a challenging real-world case study centering on analyzing the composition of the city of Strasbourg in France.",2013,Knowledge Discovery and Data Mining,Fields of study: spatial databasedata sciencedata mining
Multi-source deep learning for information trustworthiness estimation,Liang Ge (State University of New York System)Jing Gao (State University of New York System)Xiaoyi Li (State University of New York System)Aidong Zhang (State University of New York System),"2120511578,2096731881,2230170377,2228514421","In recent years, information trustworthiness has become a serious issue when user-generated contents prevail in our information world. In this paper, we investigate the important problem of estimating information trustworthiness from the perspective of correlating and comparing multiple data sources. To a certain extent, the consistency degree is an indicator of information reliability--Information unanimously agreed by all the sources is more likely to be reliable. Based on this principle, we develop an effective computational approach to identify consistent information from multiple data sources. Particularly, we analyze vast amounts of information collected from multiple review platforms (multiple sources) in which people can rate and review the items they have purchased. The major challenge is that different platforms attract diverse sets of users, and thus information cannot be compared directly at the surface. However, latent reasons hidden in user ratings are mostly shared by multiple sources, and thus inconsistency about an item only appears when some source provides ratings deviating from the common latent reasons. Therefore, we propose a novel two-step procedure to calculate information consistency degrees for a set of items which are rated by multiple sets of users on different platforms. We first build a Multi-Source Deep Belief Network (MSDBN) to identify the common reasons hidden in multi-source rating data, and then calculate a consistency score for each item by comparing individual sources with the reconstructed data derived from the latent reasons. We conduct experiments on real user ratings collected from Orbitz, Priceline and TripAdvisor on all the hotels in Las Vegas and New York City. Experimental results demonstrate that the proposed approach successfully finds the hotels that receive inconsistent, and possibly unreliable, ratings.",2013,Knowledge Discovery and Data Mining,Fields of study: deep learningdata scienceworld wide webdata miningmachine learningstatisticscomputer science
On-line relevant anomaly detection in the Twitter stream: an efficient bursty keyword detection model,Jheser Guzman (University of Chile)Barbara Poblete (University of Chile),"2222910563,2039372632","On-line social networks have become a massive communication and information channel for users world-wide. In particular, the microblogging platform Twitter, is characterized by short-text message exchanges at extremely high rates. In this type of scenario, the detection of emerging topics in text streams becomes an important research area, essential for identifying relevant new conversation topics, such as breaking news and trends. Although emerging topic detection in text is a well established research area, its application to large volumes of streaming text data is quite novel. Making scalability, efficiency and rapidness , the key aspects for any emerging topic detection algorithm in this type of environment. Our research addresses the aforementioned problem by focusing on detecting significant and unusual bursts in keyword arrival rates or bursty keywords . We propose a scalable and fast on-line method that uses normalized individual frequency signals per term and a windowing variation technique. This method reports keyword bursts which can be composed of single or multiple terms, ranked according to their importance. The average complexity of our method is O ( n log n ), where n is the number of messages in the time window. This complexity allows our approach to be scalable for large streaming datasets. If bursts are only detected and not ranked, the algorithm remains with lineal complexity O ( n ), making it the fastest in comparison to the current state-of-the-art. We validate our approach by comparing our performance to similar systems using the TREC Tweet 2011 Challenge tweets, obtaining 91% of matches with LDA, an off-line gold standard used in similar evaluations. In addition, we study Twitter messages related to the SuperBowl football events in 2011 and 2013.",2013,Knowledge Discovery and Data Mining,Fields of study: text mininginternet privacyworld wide webdata miningcomputer science
"FIU-Miner: a fast, integrated, and user-friendly system for data mining in distributed environment",Chunqiu Zeng (Florida International University)Yexi Jiang (Florida International University)Li Zheng (Florida International University)Jingxuan Li (Florida International University)Lei Li (Florida International University)Hongtai Li (Florida International University)Chao Shen (Florida International University)Wubai Zhou (Florida International University)Tao Li (Florida International University)Bing Duan (College of the Canyons)Ming Lei (College of the Canyons)Pengnian Wang (College of the Canyons),"2155150411,2402026574,2112178922,2164615415,2432045905,2154980435,2171136085,2227176090,2472069284,2230928869,2308153979,2223530132","The advent of Big Data era drives data analysts from different domains to use data mining techniques for data analysis. However, performing data analysis in a specific domain is not trivial; it often requires complex task configuration, onerous integration of algorithms, and efficient execution in distributed environments.Few efforts have been paid on developing effective tools to facilitate data analysts in conducting complex data analysis tasks. In this paper, we design and implement FIU-Miner, a Fast, Integrated, and User-friendly system to ease data analysis. FIU-Miner allows users to rapidly configure a complex data analysis task without writing a single line of code. It also helps users conveniently import and integrate different analysis programs . Further, it significantly balances resource utilization and task execution in heterogeneous environments . A case study of a real-world application demonstrates the efficacy and effectiveness of our proposed system.",2013,Knowledge Discovery and Data Mining,Fields of study: workflowdistributed computing environmentdata miningdatabasereal time computingcomputer science
Density-based logistic regression,Wenlin Chen (Washington University in St. Louis)Yixin Chen (Washington University in St. Louis)Yi Mao (Xidian University)Baolong Guo (Xidian University),"2231133452,2295009377,2466982642,2649426309","This paper introduces a nonlinear logistic regression model for classification. The main idea is to map the data to a feature space based on kernel density estimation. A discriminative model is then learned to optimize the feature weights as well as the bandwidth of a Nadaraya-Watson kernel density estimator. We then propose a hierarchical optimization algorithm for learning the coefficients and kernel bandwidths in an integrated way. Compared to other nonlinear models such as kernel logistic regression (KLR) and SVM, our approach is far more efficient since it solves an optimization problem with a much smaller size. Two other major advantages are that it can cope with categorical attributes in a unified fashion and naturally handle multi-class problems. Moveover, our approach inherits from logistic regression good interpretability of the model, which is important for clinical applications but not offered by KLR and SVM. Extensive results on real datasets, including a clinical prediction application currently under deployment in a major hospital, show that our approach not only achieves superior classification accuracy, but also drastically reduces the computing time as compared to other leading methods.",2013,Knowledge Discovery and Data Mining,Fields of study: logistic model treevariable kernel density estimationmultinomial logistic regressionprincipal component regressionkerneldensity estimationlogistic regressioneconometricsdata miningpattern recognitionmachine learningstatisticscomputer sciencemathematics
Maximizing acceptance probability for active friending in online social networks,De-Nian Yang (Academia Sinica)Hui-Ju Hung (Academia Sinica)Wang-Chien Lee (Pennsylvania State University)Wei Chen (Microsoft),"2096343151,2145006012,2143778659,2527738285","Friending recommendation has successfully contributed to the explosive growth of online social networks. Most friending recommendation services today aim to support passive friending, where a user passively selects friending targets from the recommended candidates. In this paper, we advocate a recommendation support for active friending, where a user actively specifies a friending target. To the best of our knowledge, a recommendation designed to provide guidance for a user to systematically approach his friending target has not been explored for existing online social networking services. To maximize the probability that the friending target would accept an invitation from the user, we formulate a new optimization problem, namely, Acceptance Probability Maximization (APM) , and develop a polynomial time algorithm, called Selective Invitation with Tree and In-Node Aggregation (SITINA) , to find the optimal solution. We implement an active friending service with SITINA on Facebook to validate our idea. Our user study and experimental results reveal that SITINA outperforms manual selection and the baseline approach in solution quality efficiently.",2013,Knowledge Discovery and Data Mining,Fields of study: social influencesocial networkworld wide websocial sciencedata miningmachine learningsimulation
KeySee: supporting keyword search on evolving events in social streams,Pei Lee (University of British Columbia)Laks V. S. Lakshmanan (University of British Columbia)Evangelos E. Milios (Dalhousie University),"2479271178,2289816208,2231563531","Online social streams such as Twitter/Facebook timelines and forum discussions have emerged as prevalent channels for information dissemination. As these social streams surge quickly, information overload has become a huge problem. Existing keyword search engines on social streams like Twitter Search are not successful in overcoming the problem, because they merely return an overwhelming list of posts, with little aggregation or semantics. In this demo, we provide a new solution called \keysee by grouping posts into events, and track the evolution patterns of events as new posts stream in and old posts fade out. Noise and redundancy problems are effectively addressed in our system. Our demo supports refined keyword query on evolving events by allowing users to specify the time span and designated evolution pattern. For each event result, we provide various analytic views such as frequency curves, word clouds and GPS distributions. We deploy \keysee on real Twitter streams and the results show that our demo outperforms existing keyword search engines on both quality and usability.",2013,Knowledge Discovery and Data Mining,Fields of study: internet privacyworld wide webdata miningcomputer science
Risk-O-Meter: an intelligent clinical risk calculator,Kiyana Zolfaghar (University of Washington)Jayshree Agarwal (University of Washington)Deepthi Sistla (University of Washington)Si-Chi Chin (University of Washington)Senjuti Basu Roy (University of Washington)Nele Verbiest (Ghent University),"1147347457,2132120020,2232306470,2590850971,2097289406,1206748541","We present a system called Risk-O-Meter to predict and an- alyze clinical risk via data imputation, visualization, predic- tive modeling, and association rule exploration. Clinical risk calculators provide information about a person's chance of having a disease or encountering a clinical event. Such tools could be highly useful to educate patients to understand and monitor their health conditions. Unlike existing risk calcu- lators that are primarily designed for domain experts, Risk- O-Meter is useful to patients who are unfamiliar with medi- cal terminologies, or providers who have limited information about a patient. Risk-O-Meter is designed in a way such that it is flexible enough to accept limited or incomplete data in- puts, and still manages to predict the clinical risk efficiently and effectively. Current version of Risk-O-Meter evaluates 30-day risk of hospital readmission. However, the proposed system framework is applicable to general clinical risk pre- dictions. In this demonstration paper, we describe different components of Risk-O-Meter and the intelligent algorithms associated with each of these components to evaluate risk of readmission using incomplete patient data inputs.",2013,Knowledge Discovery and Data Mining,Fields of study: association rule learningcluster analysisdata sciencedata miningmachine learningsimulationcomputer science
Debiasing social wisdom,Abhimanyu Das (Microsoft)Sreenivas Gollapudi (Microsoft)Rina Panigrahy (Microsoft)Mahyar Salek (Microsoft),"2480652307,2023254819,1923488504,2231322432","With the explosive growth of social networks, many applications are increasingly harnessing the pulse of online crowds for a variety of tasks such as marketing, advertising, and opinion mining. An important example is the wisdom of crowd effect that has been well studied for such tasks when the crowd is non-interacting. However, these studies don't explicitly address the network effects in social networks. A key difference in this setting is the presence of social influences that arise from these interactions and can undermine the wisdom of the crowd [17]. Using a natural model of opinion formation, we analyze the effect of these interactions on an individual's opinion and estimate her propensity to conform. We then propose efficient sampling algorithms incorporating these conformity values to arrive at a debiased estimate of the wisdom of a crowd. We analyze the trade-off between the sample size and estimation error and validate our algorithms using both real data obtained from online user experiments and synthetic data.",2013,Knowledge Discovery and Data Mining,Fields of study: social networkdata miningmachine learning
Cross-task crowdsourcing,Kaixiang Mo (Hong Kong University of Science and Technology)Erheng Zhong (Hong Kong University of Science and Technology)Qiang Yang (Hong Kong University of Science and Technology),"2026218191,2108094379,2109031554","Crowdsourcing is an effective method for collecting labeled data for various data mining tasks. It is critical to ensure the veracity of the produced data because responses collected from different users may be noisy and unreliable. Previous works solve this veracity problem by estimating both the user ability and question difficulty based on the knowledge in each task individually. In this case, each single task needs large amounts of data to provide accurate estimations. However, in practice, budgets provided by customers for a given target task may be limited, and hence each question can be presented to only a few users where each user can answer only a few questions. This data sparsity problem can cause previous approaches to perform poorly due to the overfitting problem on rare data and eventually damage the data veracity. Fortunately, in real-world applications, users can answer questions from multiple historical tasks. For example, one can annotate images as well as label the sentiment of a given title. In this paper, we employ transfer learning, which borrows knowledge from auxiliary historical tasks to improve the data veracity in a given target task. The motivation is that users have stable characteristics across different crowdsourcing tasks and thus data from different tasks can be exploited collectively to estimate users' abilities in the target task. We propose a hierarchical Bayesian model, TLC (Transfer Learning for Crowdsourcing), to implement this idea by considering the overlapping users as a bridge. In addition, to avoid possible negative impact, TLC introduces task-specific factors to model task differences. The experimental results show that TLC significantly improves the accuracy over several state-of-the-art non-transfer-learning approaches under very limited budget in various labeling tasks.",2013,Knowledge Discovery and Data Mining,Fields of study: crowdsourcingtransfer of learningdata scienceworld wide webdata miningmachine learningcomputer science
Adaptive collective routing using gaussian process dynamic congestion models,Siyuan Liu (Carnegie Mellon University)Yisong Yue (Carnegie Mellon University)Ramayya Krishnan (Carnegie Mellon University),"2151890734,2659898960,2163624134","We consider the problem of adaptively routing a fleet of cooperative vehicles within a road network in the presence of uncertain and dynamic congestion conditions. To tackle this problem, we first propose a Gaussian Process Dynamic Congestion Model that can effectively characterize both the dynamics and the uncertainty of congestion conditions. Our model is efficient and thus facilitates real-time adaptive routing in the face of uncertainty. Using this congestion model, we develop an efficient algorithm for non-myopic adaptive routing to minimize the collective travel time of all vehicles in the system. A key property of our approach is the ability to efficiently reason about the long-term value of exploration, which enables collectively balancing the exploration/exploitation trade-off for entire fleets of vehicles. We validate our approach based on traffic data from two large Asian cities. We show that our congestion model is effective in modeling dynamic congestion conditions. We also show that our routing algorithm generates significantly faster routes compared to standard baselines, and achieves near-optimal performance compared to an omniscient routing algorithm. We also present the results from a preliminary field study, which showcases the efficacy of our approach.",2013,Knowledge Discovery and Data Mining,Fields of study: static routingslow startdestination sequenced distance vector routingmultipath routinggaussian processroutingsimulationmathematical optimizationstatistics
Diversity maximization under matroid constraints,Zeinab Abbassi (Columbia University)Vahab S. Mirrokni (Google)Mayur Thakur (Google),"209695283,2331823467,2235422528","Aggregator websites typically present documents in the form of representative clusters. In order for users to get a broader perspective, it is important to deliver a diversified set of representative documents in those clusters. One approach to diversification is to maximize the average dissimilarity among documents. Another way to capture diversity is to avoid showing several documents from the same category (e.g. from the same news channel). We combine the above two diversification concepts by modeling the latter approach as a (partition) matroid constraint, and study diversity maximization problems under matroid constraints. We present the first constant-factor approximation algorithm for this problem, using a new technique. Our local search 0.5-approximation algorithm is also the first constant-factor approximation for the max-dispersion problem under matroid constraints. Our combinatorial proof technique for maximizing diversity under matroid constraints uses the existence of a family of Latin squares which may also be of independent interest. In order to apply these diversity maximization algorithms in the context of aggregator websites and as a preprocessing step for our diversity maximization tool, we develop greedy clustering algorithms that maximize weighted coverage of a predefined set of topics. Our algorithms are based on computing a set of cluster centers, where clusters are formed around them. We show the better performance of our algorithms for diversity and coverage maximization by running experiments on real (Twitter) and synthetic data in the context of real-time search over micro-posts. Finally we perform a user study validating our algorithms and diversity metrics.",2013,Knowledge Discovery and Data Mining,Fields of study: cluster analysisapproximation algorithmcombinatoricsdata miningmachine learningmathematical optimizationcomputer sciencemathematics
Effective string processing and matching for author disambiguation,Wei-Sheng Chin (National Taiwan University)Yu-Chin Juan (National Taiwan University)Yong Zhuang (National Taiwan University)Felix Wu (National Taiwan University)Hsiao-Yu Tung (National Taiwan University)Tong Yu (National Taiwan University)Jui-Pin Wang (National Taiwan University)Cheng-Xia Chang (National Taiwan University)Chun-Pai Yang (National Taiwan University)Wei-Cheng Chang (National Taiwan University)Kuan-Hao Huang (National Taiwan University)Tzu-Ming Kuo (National Taiwan University)Shan-Wei Lin (National Taiwan University)Young-San Lin (National Taiwan University)Yu-Chen Lu (National Taiwan University)Yu-Chuan Su (National Taiwan University)Cheng-Kuang Wei (National Taiwan University)Tu-Chun Yin (National Taiwan University)Chun-Liang Li (National Taiwan University)Ting-Wei Lin (National Taiwan University)Cheng-Hao Tsai (National Taiwan University)Shou-De Lin (National Taiwan University)Hsuan-Tien Lin (National Taiwan University)Chih-Jen Lin (National Taiwan University),"2116836739,2120348443,2682293323,2112385292,2130712876,2104216230,2119739827,2162752662,2108673390,2591117145,2113276713,2135302315,2300711308,2138722980,2155265242,2130181892,2118530536,2165888709,2157689689,2140714400,2141326772,2114357324,2127632057,2168176072","Track 2 in KDD Cup 2013 aims at determining duplicated authors in a data set from Microsoft Academic Search. This type of problems appears in many large-scale applications that compile information from different sources. This paper describes our solution developed at National Taiwan University to win the first prize of the competition. We propose an effective name matching framework and realize two implementations. An important strategy in our approach is to consider Chinese and non-Chinese names separately because of their different naming conventions. Post-processing including merging results of two predictions further boosts the performance. Our approach achieves F1-score 0.99202 on the private leader board, while 0.99195 on the public leader board.",2013,Knowledge Discovery and Data Mining,Fields of study: featureworld wide webdata miningartificial intelligencemachine learningcomputer sciencemathematics
Selective sampling on graphs for classification,Quanquan Gu (University of Illinois at Urbana–Champaign)Charu C. Aggarwal (IBM)Jialu Liu (University of Illinois at Urbana–Champaign)Jiawei Han (University of Illinois at Urbana–Champaign),"2167348148,2146335907,2095732305,2121939561","Selective sampling is an active variant of online learning in which the learner is allowed to adaptively query the label of an observed example. The goal of selective sampling is to achieve a good trade-off between prediction performance and the number of queried labels. Existing selective sampling algorithms are designed for vector-based data. In this paper, motivated by the ubiquity of graph representations in real-world applications, we propose to study selective sampling on graphs. We first present an online version of the well-known Learning with Local and Global Consistency method (OLLGC). It is essentially a second-order online learning algorithm, and can be seen as an online ridge regression in the Hilbert space of functions defined on graphs. We prove its regret bound in terms of the structural property (cut size) of a graph. Based on OLLGC, we present a selective sampling algorithm, namely Selective Sampling with Local and Global Consistency (SSLGC), which queries the label of each node based on the confidence of the linear function on graphs. Its bound on the label complexity is also derived. We analyze the low-rank approximation of graph kernels, which enables the online algorithms scale to large graphs. Experiments on benchmark graph datasets show that OLLGC outperforms the state-of-the-art first-order algorithm significantly, and SSLGC achieves comparable or even better results than OLLGC while querying substantially fewer nodes. Moreover, SSLGC is overwhelmingly better than random sampling.",2013,Knowledge Discovery and Data Mining,Fields of study: combinatoricsdata miningpattern recognitionmachine learningstatisticsmathematics
EventCube: multi-dimensional search and mining of structured and text data,Fangbo Tao (University of Illinois at Urbana–Champaign)Kin Hou Lei (University of Illinois at Urbana–Champaign)Jiawei Han (University of Illinois at Urbana–Champaign)Chengxiang Zhai (University of Illinois at Urbana–Champaign)Xiao Cheng (University of Illinois at Urbana–Champaign)Marina Danilevsky (University of Illinois at Urbana–Champaign)Nihit Desai (University of Illinois at Urbana–Champaign)Bolin Ding (University of Illinois at Urbana–Champaign)Jing Ge (University of Illinois at Urbana–Champaign)Heng Ji (City University of New York)Rucha Kanade (University of Illinois at Urbana–Champaign)Anne KaoQi Li (City University of New York)Yanen Li (University of Illinois at Urbana–Champaign)Cindy Xide Lin (University of Illinois at Urbana–Champaign)Jialu Liu (University of Illinois at Urbana–Champaign)Nikunj C. Oza (Ames Research Center)Ashok N. Srivastava (Ames Research Center)Rodney TjoelkerChi Wang 0001 (University of Illinois at Urbana–Champaign)Duo Zhang (University of Illinois at Urbana–Champaign)Bo Zhao (University of Illinois at Urbana–Champaign),"2130643217,2104651410,2121939561,2152766206,2704936845,2046577858,2146108112,2642048689,2228729169,2619850707,2224466355,2683715212,2575709086,2158342695,2130957644,2095732305,2047114418,2143728639,2638713317,2461963590,2100860776,2674375462","A large portion of real world data is either text or structured (e.g., relational) data. Moreover, such data objects are often linked together (e.g., structured specification of products linking with the corresponding product descriptions and customer comments). Even for text data such as news data, typed entities can be extracted with entity extraction tools. The EventCube project constructs TextCube and TopicCube from interconnected structured and text data (or from text data via entity extraction and dimension building), and performs multidimensional search and analysis on such datasets, in an informative, powerful, and user-friendly manner. This proposed EventCube demo will show the power of the system not only on the originally designed ASRS (Aviation Safety Report System) data sets, but also on news datasets collected from multiple news agencies, and academic datasets constructed from the DBLP and web data. The system has high potential to be extended in many powerful ways and serve as a general platform for search, OLAP (online analytical processing) and data mining on integrated text and structured data. After the system demo in the conference, the system will be put on the web for public access and evaluation.",2013,Knowledge Discovery and Data Mining,Fields of study: text mininginformation retrievaldata miningdatabasecomputer science
Understanding evolution of research themes: a probabilistic generative model for citations,Xiaolong Wang (University of Illinois at Urbana–Champaign)Chengxiang Zhai (University of Illinois at Urbana–Champaign)Dan Roth (University of Illinois at Urbana–Champaign),"2591453536,2152766206,2122007671","Understanding how research themes evolve over time in a research community is useful in many ways (e.g., revealing important milestones and discovering emerging major research trends). In this paper, we propose a novel way of analyzing literature citation to explore the research topics and the theme evolution by modeling article citation relations with a probabilistic generative model. The key idea is to represent a research paper by a ``bag of citations'' and model such a ``citation document'' with a probabilistic topic model. We explore the extension of a particular topic model, i.e., Latent Dirichlet Allocation~(LDA), for citation analysis, and show that such a Citation-LDA can facilitate discovering of individual research topics as well as the theme evolution from multiple related topics, both of which in turn lead to the construction of evolution graphs for characterizing research themes. We test the proposed citation-LDA on two datasets: the ACL Anthology Network(AAN) of natural language research literatures and PubMed Central(PMC) archive of biomedical and life sciences literatures, and demonstrate that Citation-LDA can effectively discover the evolution of research themes, with better formed topics than (conventional) Content-LDA.",2013,Knowledge Discovery and Data Mining,Fields of study: data sciencedata miningartificial intelligencecomputer science
Querying discriminative and representative samples for batch mode active learning,Zheng Wang (Arizona State University)Jieping Ye (Arizona State University),"2701179173,2305258894","Empirical risk minimization (ERM) provides a principled guideline for many machine learning and data mining algorithms. Under the ERM principle, one minimizes an upper bound of the true risk, which is approximated by the summation of empirical risk and the complexity of the candidate classifier class. To guarantee a satisfactory learning performance, ERM requires that the training data are i.i.d. sampled from the unknown source distribution. However, this may not be the case in active learning, where one selects the most informative samples to label, and these data may not follow the source distribution. In this article, we generalize the ERM principle to the active learning setting. We derive a novel form of upper bound for the true risk in the active learning setting; by minimizing this upper bound, we develop a practical batch mode active learning method. The proposed formulation involves a nonconvex integer programming optimization problem. We solve it efficiently by an alternating optimization method. Our method is shown to query the most informative samples while preserving the source distribution as much as possible, thus identifying the most uncertain and representative queries. We further extend our method to multiclass active learning by introducing novel pseudolabels in the multiclass case and developing an efficient algorithm. Experiments on benchmark datasets and real-world applications demonstrate the superior performance of our proposed method compared to state-of-the-art methods.",2013,Knowledge Discovery and Data Mining,Fields of study: stabilityempirical risk minimizationactive learningactive learningsemi supervised learningdata miningpattern recognitionmachine learningstatisticscomputer science
Scalable text and link analysis with mixed-topic link models,"Yaojia Zhu (University of New Mexico)Xiaoran Yan (University of New Mexico)Lise Getoor (University of Maryland, College Park)Cristopher Moore (Santa Fe Institute)","2128265280,2114830456,1984940772,2132327253","Many data sets contain rich information about objects, as well as pairwise relations between them. For instance, in networks of websites, scientific papers, and other documents, each node has content consisting of a collection of words, as well as hyperlinks or citations to other nodes. In order to perform inference on such data sets, and make predictions and recommendations, it is useful to have models that are able to capture the processes which generate the text at each node and the links between them. In this paper, we combine classic ideas in topic modeling with a variant of the mixed-membership block model recently developed in the statistical physics community. The resulting model has the advantage that its parameters, including the mixture of topics of each document and the resulting overlapping communities, can be inferred with a simple and scalable expectation-maximization algorithm. We test our model on three data sets, performing unsupervised topic classification and link prediction. For both tasks, our model outperforms several existing state-of-the-art methods, achieving higher accuracy with significantly less computation, analyzing a data set with 1.3 million words and 44 thousand links in a few minutes.",2013,Knowledge Discovery and Data Mining,Fields of study: topic modeldata sciencedata miningmachine learningstatisticscomputer science
"Guided learning for role discovery (GLRD): framework, algorithms, and applications","Sean Gilpin (University of California, Davis)Tina Eliassi-Rad (Rutgers University)Ian N. Davidson (University of California, Davis)","2021798691,218538652,2560595684","Role discovery in graphs is an emerging area that allows analysis of complex graphs in an intuitive way. In contrast to community discovery, which finds groups of highly connected nodes, role discovery finds groups of nodes that share similar topological structure in the graph, and hence a common role (or function) such as being a broker or a periphery node. However, existing work so far is completely unsupervised, which is undesirable for a number of reasons. We provide an alternating least squares framework that allows convex constraints to be placed on the role discovery problem, which can provide useful supervision. In particular we explore supervision to enforce i) sparsity, ii) diversity, and iii) alternativeness in the roles. We illustrate the usefulness of this supervision on various data sets and applications.",2013,Knowledge Discovery and Data Mining,Fields of study: constrained clusteringcomputingdata sciencedata miningmachine learningcomputer science
Spatiotemporal periodical pattern mining in traffic data,Tanvi Jindal (University of Illinois at Urbana–Champaign)Prasanna Giridhar (University of Illinois at Urbana–Champaign)Lu An Tang (University of Illinois at Urbana–Champaign)Jun Li 0021 (University of Illinois at Urbana–Champaign)Jiawei Han (University of Illinois at Urbana–Champaign),"2564591858,2059487427,2143081449,2654643481,2121939561","The widespread use of road sensors has generated huge amount of traffic data, which can be mined and put to various different uses. Finding frequent trajectories from the road network of a big city helps in summarizing the way the traffic behaves in the city. It can be very useful in city planning and traffic routing mechanisms, and may be used to suggest the best routes given the region, road, time of day, day of week, season, weather, and events etc. Other than the frequent patterns, even the events that are not so frequent, such as those observed when there is heavy snowfall, other extreme weather conditions, long traffic jams, accidents, etc. might actually follow a periodic occurrence, and hence might be useful to mine. This problem of mining the frequent patterns from road traffic data has been addressed in previous works using the context knowledge of the road network of the city. In this paper, we have developed a method to mine spatiotemporal periodic patterns in the traffic data and use these periodic behaviors to summarize the huge road network. The first step is to find periodic patterns from the speed data of individual road sensor stations, and use their periods to represent the station's periodic behavior using probability distribution matrices. Then, we use density-based clustering to cluster the sensors on the road network based on the similarities between their periodic behavior as well as their geographical distance, thus combining similar nodes to form a road network with larger but fewer nodes.",2013,Knowledge Discovery and Data Mining,Fields of study: traffic congestion reconstruction with kerner s three phase theorykullback leibler divergencedata miningsimulationstatistics
A tool for collecting provenance data in social media,Pritam Gundecha (Arizona State University)Suhas Ranganath (Arizona State University)Zhuo Feng (Arizona State University)Huan Liu (Arizona State University),"1214756773,2154873622,2629557231,2122391114","In recent years, social media sites have provided a large amount of information. Recipients of such information need mechanisms to know more about the received information, including the provenance. Previous research has shown that some attributes related to the received information provide additional context, so that a recipient can assess the amount of value, trust, and validity to be placed in the received information. Personal attributes of a user, including name, location, education, ethnicity, gender, and political and religious affiliations, can be found in social media sites. In this paper, we present a novel web-based tool for collecting the attributes of interest associated with a particular social media user related to the received information. This tool provides a way to combine different attributes available at different social media sites into a single user profile. Using different types of Twitter users, we also evaluate the performance of the tool in terms of number of attribute values collected, validity of these values, and total amount of retrieval time.",2013,Knowledge Discovery and Data Mining,Fields of study: social mediainternet privacymultimediaworld wide webdata miningcomputer science
Ranking-based name matching for author disambiguation in bibliographic data,Jialu Liu (University of Illinois at Urbana–Champaign)Kin Hou Lei (University of Illinois at Urbana–Champaign)Jeffery Yufei Liu (University of Illinois at Urbana–Champaign)Chi Wang (University of Illinois at Urbana–Champaign)Jiawei Han (University of Illinois at Urbana–Champaign),"2095732305,2104651410,2232655059,2461963590,2121939561","Author name ambiguity is a frequently encountered problem in digital publication libraries such as Microsoft Academic Search. The cause of this problem mostly is that different authors may publish under the same name, while the same author could publish under various names due to abbreviations, nicknames, etc. Author disambiguation is exactly the goal of the Track II of KDD Cup Data Mining Contest 2013. In this paper we introduce our ranking-based name matching algorithm and system called RankMatch. One important feature of our solution is using heterogeneous meta-paths to evaluate the similarity between two potential duplicate authors whose names are compatible. We participated under team name ""SmallData"" and our final solution achieved a Mean F 1 score of 99.157%, ranking in the second place in the contest.",2013,Knowledge Discovery and Data Mining,Fields of study: fully qualified nameworld wide webinformation retrievaldata miningcomputer science
Silence is also evidence: interpreting dwell time for recommendation from psychological perspective,Peifeng Yin (Pennsylvania State University)Ping Luo (Hewlett-Packard)Wang-Chien Lee (Pennsylvania State University)Min Wang (Google),"2113084173,2291210646,2143778659,2467205710","Social media is a platform for people to share and vote content. From the analysis of the social media data we found that users are quite inactive in rating/voting. For example, a user on average only votes 2 out of 100 accessed items. Traditional recommendation methods are mostly based on users' votes and thus can not cope with this situation. Based on the observation that the dwell time on an item may reflect the opinion of a user, we aim to enrich the user-vote matrix by converting the dwell time on items into users' ``pseudo votes'' and then help improve recommendation performance. However, it is challenging to correctly interpret the dwell time since many subjective human factors, e.g. user expectation, sensitivity to various item qualities, reading speed, are involved into the casual behavior of online reading. In psychology, it is assumed that people have choice threshold in decision making. The time spent on making decision reflects the decision maker's threshold. This idea inspires us to develop a View-Voting model, which can estimate how much the user likes the viewed item according to her dwell time, and thus make recommendations even if there is no voting data available. Finally, our experimental evaluation shows that the traditional rate-based recommendation's performance is greatly improved with the support of VV model.",2013,Knowledge Discovery and Data Mining,Fields of study: dwell timedata miningmachine learningsimulation
An integrated framework for optimizing automatic monitoring systems in large IT infrastructures,Liang Tang (Florida International University)Tao Li (Florida International University)Larisa Shwartz (IBM)Florian Pinel (IBM)Genady Ya Grabarnik (St. John's University),"2128176332,2472069284,2038295021,2153765069,2223237609","The competitive business climate and the complexity of IT environments dictate efficient and cost-effective service delivery and support of IT services. These are largely achieved by automating routine maintenance procedures, including problem detection, determination and resolution. System monitoring provides an effective and reliable means for problem detection. Coupled with automated ticket creation, it ensures that a degradation of the vital signs, defined by acceptable thresholds or monitoring conditions, is flagged as a problem candidate and sent to supporting personnel as an incident ticket. This paper describes an integrated framework for minimizing false positive tickets and maximizing the monitoring coverage for system faults. In particular, the integrated framework defines monitoring conditions and the optimal corresponding delay times based on an off-line analysis of historical alerts and incident tickets. Potential monitoring conditions are built on a set of predictive rules which are automatically generated by a rule-based learning algorithm with coverage, confidence and rule complexity criteria. These conditions and delay times are propagated as configurations into run-time monitoring systems. Moreover, a part of misconfigured monitoring conditions can be corrected according to false negative tickets that are discovered by another text classification algorithm in this framework. This paper also provides implementation details of a program product that uses this framework and shows some illustrative examples of successful results.",2013,Knowledge Discovery and Data Mining,Fields of study: system monitoringcomputer securitydata miningsimulationcomputer science
Collaborative matrix factorization with multiple similarities for predicting drug-target interactions,Xiaodong Zheng (Fudan University)Hao Ding (Fudan University)Hiroshi Mamitsuka (Kyoto University)Shanfeng Zhu (Fudan University),"2200365102,2670785492,278261381,2099759720","We address the problem of predicting new drug-target interactions from three inputs: known interactions, similarities over drugs and those over targets. This setting has been considered by many methods, which however have a common problem of allowing to have only one similarity matrix over drugs and that over targets. The key idea of our approach is to use more than one similarity matrices over drugs as well as those over targets, where weights over the multiple similarity matrices are estimated from data to automatically select similarities, which are effective for improving the performance of predicting drug-target interactions. We propose a factor model, named Multiple Similarities Collaborative Matrix Factorization(MSCMF), which projects drugs and targets into a common low-rank feature space, which is further consistent with weighted similarity matrices over drugs and those over targets. These two low-rank matrices and weights over similarity matrices are estimated by an alternating least squares algorithm. Our approach allows to predict drug-target interactions by the two low-rank matrices collaboratively and to detect similarities which are important for predicting drug-target interactions. This approach is general and applicable to any binary relations with similarities over elements, being found in many applications, such as recommender systems. In fact, MSCMF is an extension of weighted low-rank approximation for one-class collaborative filtering. We extensively evaluated the performance of MSCMF by using both synthetic and real datasets. Experimental results showed nice properties of MSCMF on selecting similarities useful in improving the predictive performance and the performance advantage of MSCMF over six state-of-the-art methods for predicting drug-target interactions.",2013,Knowledge Discovery and Data Mining,Fields of study: cheminformaticsdata miningpattern recognitionmachine learningmathematics
STRIP: stream learning of influence probabilities,Konstantin Kutzkov (IT University)Albert Bifet (Yahoo!)Francesco Bonchi (Yahoo!)Aristides Gionis (Aalto University),"2567877090,307521372,2176652147,737311942","Influence-driven diffusion of information is a fundamental process in social networks. Learning the latent variables of such process, i.e., the influence strength along each link, is a central question towards understanding the structure and function of complex networks, modeling information cascades, and developing applications such as viral marketing. Motivated by modern microblogging platforms, such as twitter , in this paper we study the problem of learning influence probabilities in a data-stream scenario, in which the network topology is relatively stable and the challenge of a learning algorithm is to keep up with a continuous stream of tweets using a small amount of time and memory. Our contribution is a number of randomized approximation algorithms, categorized according to the available space (superlinear, linear, and sublinear in the number of nodes n ) and according to different models (landmark and sliding window). Among several results, we show that we can learn influence probabilities with one pass over the data, using O ( n log n ) space, in both the landmark model and the sliding-window model, and we further show that our algorithm is within a logarithmic factor of optimal. For truly large graphs, when one needs to operate with sublinear space, we show that we can still learn influence probabilities in one pass, assuming that we restrict our attention to the most active users. Our thorough experimental evaluation on large social graph demonstrates that the empirical performance of our algorithms agrees with that predicted by the theory.",2013,Knowledge Discovery and Data Mining,Fields of study: social influencesocial network analysistheoretical computer sciencedata miningartificial intelligencemachine learningstatisticscomputer sciencemathematics
Statistical quality estimation for general crowdsourcing tasks,Yukino Baba (University of Tokyo)Hisashi Kashima (University of Tokyo),"2554033023,2126337623","One of the biggest challenges for requesters and platform providers of crowdsourcing is quality control, which is to expect high-quality results from crowd workers who are neither necessarily very capable nor motivated. A common approach to tackle this problem is to introduce redundancy, that is, to request multiple workers to work on the same tasks. For simple multiple-choice tasks, several statistical methods to aggregate the multiple answers have been proposed. However, these methods cannot always be applied to more general tasks with unstructured response formats such as article writing, program coding, and logo designing, which occupy the majority on most crowdsourcing marketplaces. In this paper, we propose an unsupervised statistical quality estimation method for such general crowdsourcing tasks. Our method is based on the two-stage procedure; multiple workers are first requested to work on the same tasks in the creation stage, and then another set of workers review and grade each artifact in the review stage. We model the ability of each author and the bias of each reviewer, and propose a two-stage probabilistic generative model using the graded response model in the item response theory. Experiments using several general crowdsourcing tasks show that our method outperforms popular vote aggregation methods, which implies that our method can deliver high quality results with lower costs.",2013,Knowledge Discovery and Data Mining,Fields of study: crowdsourcing software developmentcrowdsourcingquality controldata sciencedata miningmachine learningsimulationstatisticscomputer science
Multi-label relational neighbor classification using social context features,Xi Wang (University of Central Florida)Gita Sukthankar (University of Central Florida),"2642658022,2041536743","Networked data, extracted from social media, web pages, and bibliographic databases, can contain entities of multiple classes, interconnected through different types of links. In this paper, we focus on the problem of performing multi-label classification on networked data, where the instances in the network can be assigned multiple labels. In contrast to traditional content-only classification methods, relational learning succeeds in improving classification performance by leveraging the correlation of the labels between linked instances. However, instances in a network can be linked for various causal reasons, hence treating all links in a homogeneous way can limit the performance of relational classifiers. In this paper, we propose a multi-label iterative relational neighbor classifier that employs social context features (SCRN). Our classifier incorporates a class propagation probability distribution obtained from instances' social features, which are in turn extracted from the network topology. This class-propagation probability captures the node's intrinsic likelihood of belonging to each class, and serves as a prior weight for each class when aggregating the neighbors' class labels in the collective inference procedure. Experiments on several real-world datasets demonstrate that our proposed classifier boosts classification performance over common benchmarks on networked multi-label data.",2013,Knowledge Discovery and Data Mining,Fields of study: statistical relational learningdata miningpattern recognitionmachine learningcomputer science
Mining lines in the sand: on trajectory discovery from untrustworthy data in cyber-physical system,Lu An Tang (University of Illinois at Urbana–Champaign)Xiao Yu (University of Illinois at Urbana–Champaign)Quanquan Gu (University of Illinois at Urbana–Champaign)Jiawei Han (University of Illinois at Urbana–Champaign)Alice Leung (BBN Technologies)Thomas F. La Porta (Pennsylvania State University),"2143081449,2160715520,2167348148,2121939561,2131880941,697457595","A Cyber-Physical System (CPS) integrates physical (i.e., sensor) devices with cyber (i.e., informational) components to form a context sensitive system that responds intelligently to dynamic changes in real-world situations. The CPS has wide applications in scenarios such as environment monitoring, battlefield surveillance and traffic control. One key research problem of CPS is called "" mining lines in the sand "". With a large number of sensors (sand) deployed in a designated area, the CPS is required to discover all the trajectories (lines) of passing intruders in real time. There are two crucial challenges that need to be addressed: (1) the collected sensor data are not trustworthy; (2) the intruders do not send out any identification information. The system needs to distinguish multiple intruders and track their movements. In this study, we propose a method called LiSM (Line-in-the-Sand Miner) to discover trajectories from untrustworthy sensor data. LiSM constructs a watching network from sensor data and computes the locations of intruder appearances based on the link information of the network. The system retrieves a cone-mode l from the historical trajectories and tracks multiple intruders based on this model. Finally the system validates the mining results and updates the sensor's reliability in a feedback process. Extensive experiments on big datasets demonstrate the feasibility and applicability of the proposed methods.",2013,Knowledge Discovery and Data Mining,Fields of study: cyber physical systemtrajectorywireless sensor networkcomputer securitydata miningsimulationcomputer science
On community detection in real-world networks and the importance of degree assortativity,Marek Ciglan (Slovak Academy of Sciences)Michal Laclavík (Slovak Academy of Sciences)Kjetil Nørvåg (Norwegian University of Science and Technology),"2720779581,3627031,202808005","Graph clustering, often addressed as community detection, is a prominent task in the domain of graph data mining with dozens of algorithms proposed in recent years. In this paper, we focus on several popular community detection algorithms with low computational complexity and with decent performance on the artificial benchmarks, and we study their behaviour on real-world networks. Motivated by the observation that there is a class of networks for which the community detection methods fail to deliver good community structure, we examine the assortativity coefficient of ground-truth communities and show that assortativity of a community structure can be very different from the assortativity of the original network. We then examine the possibility of exploiting the latter by weighting edges of a network with the aim to improve the community detection outputs for networks with assortative community structure. The evaluation shows that the proposed weighting can significantly improve the results of community detection methods on networks with assortative community structure.",2013,Knowledge Discovery and Data Mining,Fields of study: assortativitymixing patternsdata miningartificial intelligencemachine learningmathematics
Methods for exploring and mining tables on Wikipedia,Chandra Sekhar Bhagavatula (Northwestern University)Thanapon Noraset (Northwestern University)Doug Downey (Northwestern University),"2071644166,213474391,2098223845","Knowledge bases extracted automatically from the Web present new opportunities for data mining and exploration. Given a large, heterogeneous set of extracted relations, new tools are needed for searching the knowledge and uncovering relationships of interest. We present WikiTables , a Web application that enables users to interactively explore tabular knowledge extracted from Wikipedia. In experiments, we show that WikiTables substantially outperforms baselines on the novel task of automatically joining together disparate tables to uncover ""interesting"" relationships between table columns. We find that a ""Semantic Relatedness"" measure that leverages the Wikipedia link structure accounts for a majority of this improvement. Further, on the task of keyword search for tables, we show that WikiTables performs comparably to Google Fusion Tables despite using an order of magnitude fewer tables. Our work also includes the release of a number of public resources, including over 15 million tuples of extracted tabular data, manually annotated evaluation sets, and public APIs.",2013,Knowledge Discovery and Data Mining,Fields of study: data scienceinformation retrievaldata miningmachine learningcomputer science
Unsupervised link prediction using aggregative statistics on heterogeneous social networks,Tsung-Ting Kuo (National Taiwan University)Rui Yan (Peking University)Yu-Yang Huang (National Taiwan University)Perng-Hwa Kung (National Taiwan University)Shou-De Lin (National Taiwan University),"2112477248,2109109241,2142434089,2171590872,2114357324","The concern of privacy has become an important issue for online social networks. In services such as Foursquare.com, whether a person likes an article is considered private and therefore not disclosed; only the aggregative statistics of articles (i.e., how many people like this article) is revealed. This paper tries to answer a question: can we predict the opinion holder in a heterogeneous social network without any labeled data? This question can be generalized to a link prediction with aggregative statistics problem. This paper devises a novel unsupervised framework to solve this problem, including two main components: (1) a three-layer factor graph model and three types of potential functions; (2) a ranked-margin learning and inference algorithm. Finally, we evaluate our method on four diverse prediction scenarios using four datasets: preference (Foursquare), repost (Twitter), response (Plurk), and citation (DBLP). We further exploit nine unsupervised models to solve this problem as baselines. Our approach not only wins out in all scenarios, but on the average achieves 9.90% AUC and 12.59% NDCG improvement over the best competitors. The resources are available at http://www.csie.ntu.edu.tw/~d97944007/aggregative/",2013,Knowledge Discovery and Data Mining,Fields of study: data sciencedata miningmachine learningstatisticscomputer science
Inferring distant-time location in low-sampling-rate trajectories,Meng-Fen Chiang (National Chiao Tung University)Yung-Hsiang Lin (National Chiao Tung University)Wen-Chih Peng (National Chiao Tung University)Philip S. Yu (University of Illinois at Chicago),"2699076777,2110939445,2100961564,2125104194","With the growth of location-based services and social services, low- sampling-rate trajectories from check-in data or photos with geo- tag information becomes ubiquitous. In general, most detailed mov- ing information in low-sampling-rate trajectories are lost. Prior works have elaborated on distant-time location prediction in high- sampling-rate trajectories. However, existing prediction models are pattern-based and thus not applicable due to the sparsity of data points in low-sampling-rate trajectories. To address the sparsity in low-sampling-rate trajectories, we develop a Reachability-based prediction model on Time-constrained Mobility Graph (RTMG) to predict locations for distant-time queries. Specifically, we de- sign an adaptive temporal exploration approach to extract effective supporting trajectories that are temporally close to the query time. Based on the supporting trajectories, a Time-constrained mobility Graph (TG) is constructed to capture mobility information at the given query time. In light of TG, we further derive the reacha- bility probabilities among locations in TG. Thus, a location with maximum reachability from the current location among all possi- ble locations in supporting trajectories is considered as the predic- tion result. To efficiently process queries, we proposed the index structure Sorted Interval-Tree (SOIT) to organize location records. Extensive experiments with real data demonstrated the effective- ness and efficiency of RTMG. First, RTMG with adaptive tempo- ral exploration significantly outperforms the existing pattern-based prediction model HPM [2] over varying data sparsity in terms of higher accuracy and higher coverage. Also, the proposed index structure SOIT can efficiently speedup RTMG in large-scale trajec- tory dataset. In the future, we could extend RTMG by considering more factors (e.g., staying durations in locations, application us- ages in smart phones) to further improve the prediction accuracy.",2013,Knowledge Discovery and Data Mining,Fields of study: reachabilitysparsity of effects principledata miningmachine learningsimulationcomputer science
Gaussian multiple instance learning approach for mapping the slums of the world using very high resolution imagery,Ranga Raju Vatsavai (Oak Ridge National Laboratory),2121870003,"In this paper, we present a computationally efficient algorithm based on multiple instance learning for mapping informal settlements (slums) using very high-resolution remote sensing imagery. From remote sensing perspective, informal settlements share unique spatial characteristics that distinguish them from other urban structures like industrial, commercial, and formal residential settlements. However, regular pattern recognition and machine learning methods, which are predominantly single-instance or per-pixel classifiers, often fail to accurately map the informal settlements as they do not capture the complex spatial patterns. To overcome these limitations we employed a multiple instance based machine learning approach, where groups of contiguous pixels (image patches) are modeled as generated by a Gaussian distribution. We have conducted several experiments on very high-resolution satellite imagery, representing four unique geographic regions across the world. Our method showed consistent improvement in accurately identifying informal settlements.",2013,Knowledge Discovery and Data Mining,Fields of study: angular mildata miningmachine learning
AMETHYST: a system for mining and exploring topical hierarchies of heterogeneous data,Marina Danilevsky (University of Illinois at Urbana–Champaign)Chi Wang (University of Illinois at Urbana–Champaign)Fangbo Tao (University of Illinois at Urbana–Champaign)Son Nguyen (University of Illinois at Urbana–Champaign)Gong Chen (University of Illinois at Urbana–Champaign)Nihit Desai (University of Illinois at Urbana–Champaign)Lidan Wang (University of Illinois at Urbana–Champaign)Jiawei Han (University of Illinois at Urbana–Champaign),"2046577858,2461963590,2130643217,2234150714,2430335071,2146108112,2710695387,2121939561","In this demo we present AMETHYST, a system for exploring and analyzing a topical hierarchy constructed from a heterogeneous information network (HIN). HINs, composed of multiple types of entities and links are very common in the real world. Many have a text component, and thus can benefit from a high quality hierarchical organization of the topics in the network dataset. By organizing the topics into a hierarchy, AMETHYST helps understand search results in the context of an ontology, and explain entity relatedness at different granularities. The automatically constructed topical hierarchy reflects a domain-specific ontology, interacts with multiple types of linked entities, and can be tailored for both free text and OLAP queries.",2013,Knowledge Discovery and Data Mining,Fields of study: topic modelheterogeneous networknetwork analysisdata scienceworld wide webdata miningmachine learningcomputer science
STED: semi-supervised targeted-interest event detectionin in twitter,Ting Hua (Virginia Tech)Feng Chen (Carnegie Mellon University)Liang Zhao (Virginia Tech)Chang-Tien Lu (Virginia Tech)Naren Ramakrishnan (Virginia Tech),"2106233920,2601749234,2619584304,2112878203,2199255697","Social microblogs such as Twitter and Weibo are experiencing an explosive growth with billions of global users sharing their daily observations and thoughts. Beyond public interests (e.g., sports, music), microblogs can provide highly detailed information for those interested in public health, homeland security, and financial analysis. However, the language used in Twitter is heavily informal, ungrammatical, and dynamic. Existing data mining algorithms require extensive manually labeling to build and maintain a supervised system. This paper presents STED, a semi-supervised system that helps users to automatically detect and interactively visualize events of a targeted type from twitter, such as crimes, civil unrests, and disease outbreaks. Our model first applies transfer learning and label propagation to automatically generate labeled data, then learns a customized text classifier based on mini-clustering, and finally applies fast spatial scan statistics to estimate the locations of events. We demonstrate STED's usage and benefits using twitter data collected from Latin America countries, and show how our system helps to detect and track example events such as civil unrests and crimes.",2013,Knowledge Discovery and Data Mining,Fields of study: text mininginternet privacyworld wide webdata miningmachine learningcomputer science
Predictive model performance: offline and online evaluations,Jeonghee Yi (Microsoft)Ye Chen (Microsoft)Jie Li (Microsoft)Swaraj Sett (Microsoft)Tak W. Yan (Microsoft),"2226691631,2628943388,2481886166,2074870220,2139394280","We study the accuracy of evaluation metrics used to estimate the efficacy of predictive models. Offline evaluation metrics are indicators of the expected model performance on real data. However, in practice we often experience substantial discrepancy between the offline and online performance of the models. We investigate the characteristics and behaviors of the evaluation metrics on offline and online testing both analytically and empirically by experimenting them on online advertising data from the Bing search engine. One of our findings is that some offline metrics like AUC (the Area Under the Receiver Operating Characteristic Curve) and RIG (Relative Information Gain) that summarize the model performance on the entire spectrum of operating points could be quite misleading sometimes and result in significant discrepancy in offline and online metrics. For example, for click prediction models for search advertising, errors in predictions in the very low range of predicted click scores impact the online performance much more negatively than errors in other regions. Most of the offline metrics we studied including AUC and RIG, however, are insensitive to such model behavior. We designed a new model evaluation paradigm that simulates the online behavior of predictive models. For a set of ads selected by a new prediction model, the online user behavior is estimated from the historic user behavior in the search logs. The experimental results on click prediction model for search advertising are highly promising.",2013,Knowledge Discovery and Data Mining,Fields of study: likelihood ratio testmean squared prediction erroronline advertisingdata miningmachine learningsimulationstatisticscomputer science
MI2LS: multi-instance learning from multiple informationsources,Dan Zhang 0007 (Facebook)Jingrui He (Stevens Institute of Technology)Richard D. Lawrence (IBM),"2585585268,2716944071,1965188977","In Multiple Instance Learning (MIL), each entity is normally expressed as a set of instances. Most of the current MIL methods only deal with the case when each instance is represented by one type of features. However, in many real world applications, entities are often described from several different information sources/views. For example, when applying MIL to image categorization, the characteristics of each image can be derived from both its RGB features and SIFT features. Previous research work has shown that, in traditional learning methods, leveraging the consistencies between different information sources could improve the classification performance drastically. Out of a similar motivation, to incorporate the consistencies between different information sources into MIL, we propose a novel research framework -- Multi-Instance Learning from Multiple Information Sources (MI 2 LS). Based on this framework, an algorithm -- Fast MI 2 LS (FMI 2 LS) is designed, which combines Concave-Convex Constraint Programming (CCCP) method and an adapte- d Stoachastic Gradient Descent (SGD) method. Some theoretical analysis on the optimality of the adapted SGD method and the generalized error bound of the formulation are given based on the proposed method. Experimental results on document classification and a novel application -- Insider Threat Detection (ITD), clearly demonstrate the superior performance of the proposed method over state-of-the-art MIL methods.",2013,Knowledge Discovery and Data Mining,Fields of study: data miningpattern recognitionmachine learningcomputer science
Combination of feature engineering and ranking models for paper-author identification in KDD Cup 2013,Chun-Liang Li (National Taiwan University)Yu-Chuan Su (National Taiwan University)Ting-Wei Lin (National Taiwan University)Cheng-Hao Tsai (National Taiwan University)Wei-Cheng Chang (National Taiwan University)Kuan-Hao Huang (National Taiwan University)Tzu-Ming Kuo (National Taiwan University)Shan-Wei Lin (National Taiwan University)Young-San Lin (National Taiwan University)Yu-Chen Lu (National Taiwan University)Chun-Pai Yang (National Taiwan University)Cheng-Xia Chang (National Taiwan University)Wei-Sheng Chin (National Taiwan University)Yu-Chin Juan (National Taiwan University)Hsiao-Yu Tung (National Taiwan University)Jui-Pin Wang (National Taiwan University)Cheng-Kuang Wei (National Taiwan University)Felix Wu (National Taiwan University)Tu-Chun Yin (National Taiwan University)Tong Yu (National Taiwan University)Yong Zhuang (National Taiwan University)Shou-de Lin (National Taiwan University)Hsuan-Tien Lin (National Taiwan University)Chih-Jen Lin (National Taiwan University),"2157689689,2130181892,2140714400,2141326772,2591117145,2113276713,2135302315,2300711308,2138722980,2155265242,2108673390,2162752662,2116836739,2120348443,2130712876,2119739827,2118530536,2112385292,2165888709,2104216230,2110230645,2114357324,2127632057,2168176072","The track 1 problem in KDD Cup 2013 is to discriminate between papers confirmed by the given authors from the other deleted papers. This paper describes the winning solution of team National Taiwan University for track 1 of KDD Cup 2013. First, we conduct the feature engineering to transform the various provided text information into 97 features. Second, we train classification and ranking models using these features. Last, we combine our individual models to boost the performance by using results on the internal validation set and the official Valid set. Some effective post-processing techniques have also been proposed. Our solution achieves 0.98259 MAP score and ranks the first place on the private leaderboard of Test set.",2013,Knowledge Discovery and Data Mining,Fields of study: data sciencedata miningmachine learning
Fast rank-2 nonnegative matrix factorization for hierarchical document clustering,Da Kuang (Georgia Institute of Technology)Haesun Park (Georgia Institute of Technology),"2029526626,2123241397","Nonnegative matrix factorization (NMF) has been successfully used as a clustering method especially for flat partitioning of documents. In this paper, we propose an efficient hierarchical document clustering method based on a new algorithm for rank-2 NMF. When the two block coordinate descent framework of nonnegative least squares is applied to computing rank-2 NMF, each subproblem requires a solution for nonnegative least squares with only two columns in the matrix. We design the algorithm for rank-2 NMF by exploiting the fact that an exhaustive search for the optimal active set can be performed extremely fast when solving these NNLS problems. In addition, we design a measure based on the results of rank-2 NMF for determining which leaf node should be further split. On a number of text data sets, our proposed method produces high-quality tree structures in significantly less time compared to other methods such as hierarchical K-means, standard NMF, and latent Dirichlet allocation.",2013,Knowledge Discovery and Data Mining,Fields of study: active set methodnon negative matrix factorizationpattern recognitionmachine learningmathematical optimizationcomputer sciencemathematics
Approximate graph mining with label costs,Pranay Anchuri (Rensselaer Polytechnic Institute)Mohammed J. Zaki (Rensselaer Polytechnic Institute)Omer Barkol (HP Labs)Shahar Golan (HP Labs)Moshe Shamy (HP Software Division),"73046339,2165917828,160540224,1972300174,2230955865","Many real-world graphs have complex labels on the nodes and edges. Mining only exact patterns yields limited insights, since it may be hard to find exact matches. However, in many domains it is relatively easy to define a cost (or distance) between different labels. Using this information, it becomes possible to mine a much richer set of approximate subgraph patterns, which preserve the topology but allow bounded label mismatches. We present novel and scalable methods to efficiently solve the approximate isomorphism problem. We show that approximate mining yields interesting patterns in several real-world graphs ranging from IT and protein interaction networks to protein structures.",2013,Knowledge Discovery and Data Mining,Fields of study: molecule mininginduced subgraph isomorphism problemconfiguration management databasepower graph analysissubgraph isomorphism problemcombinatoricsdata miningmachine learningcomputer sciencemathematics
Mining frequent graph patterns with differential privacy,Entong Shen (North Carolina State University)Ting Yu (North Carolina State University),"2164186182,2558569803","Discovering frequent graph patterns in a graph database offers valuable information in a variety of applications. However, if the graph dataset contains sensitive data of individuals such as mobile phone-call graphs and web-click graphs, releasing discovered frequent patterns may present a threat to the privacy of individuals. Differential privacy has recently emerged as the de facto standard for private data analysis due to its provable privacy guarantee. In this paper we propose the first differentially private algorithm for mining frequent graph patterns. We first show that previous techniques on differentially private discovery of frequent itemsets cannot apply in mining frequent graph patterns due to the inherent complexity of handling structural information in graphs. We then address this challenge by proposing a Markov Chain Monte Carlo (MCMC) sampling based algorithm. Unlike previous work on frequent itemset mining, our techniques do not rely on the output of a non-private mining algorithm. Instead, we observe that both frequent graph pattern mining and the guarantee of differential privacy can be unified into an MCMC sampling framework. In addition, we establish the privacy and utility guarantee of our algorithm and propose an efficient neighboring pattern counting technique as well. Experimental results show that the proposed algorithm is able to output frequent patterns with good precision.",2013,Knowledge Discovery and Data Mining,Fields of study: differential privacydata sciencedata miningdatabasemachine learningcomputer science
On the equivalent of low-rank linear regressions and linear discriminant analysis based regressions,Xiao Cai (University of Texas at Arlington)Chris H. Q. Ding (University of Texas at Arlington)Feiping Nie (University of Texas at Arlington)Heng Huang (University of Texas at Arlington),"2671977003,2119616764,2245267964,2137533801","The low-rank regression model has been studied and applied to capture the underlying classes/tasks correlation patterns, such that the regression/classification results can be enhanced. In this paper, we will prove that the low-rank regression model is equivalent to doing linear regression in the linear discriminant analysis (LDA) subspace. Our new theory reveals the learning mechanism of low-rank regression, and shows that the low-rank structures exacted from classes/tasks are connected to the LDA projection results. Thus, the low-rank regression efficiently works for the high-dimensional data. Moreover, we will propose new discriminant low-rank ridge regression and sparse low-rank regression methods. Both of them are equivalent to doing regularized regression in the regularized LDA subspace. These new regularized objectives provide better data mining results than existing low-rank regression in both theoretical and empirical validations. We evaluate our discriminant low-rank regression methods by six benchmark datasets. In all empirical results, our discriminant low-rank models consistently show better results than the corresponding full-rank methods.",2013,Knowledge Discovery and Data Mining,Fields of study: proper linear modeloptimal discriminant analysisfactor regression modelcross sectional regressionsegmented regressionregression diagnosticbayesian multivariate linear regressionlocal regressionelastic net regularizationmultivariate adaptive regression splinesprincipal component regressionpolynomial regressionpartial least squares regressionnonparametric regressionlinear modellogistic regressionlinear regressionregression analysislinear discriminant analysiseconometricspattern recognitionmachine learningstatisticscomputer sciencemathematics
Uncertainty in online experiments with dependent data: an evaluation of bootstrap methods,Eytan Bakshy (Facebook)Dean Eckles (Facebook),"233957955,2624893860","Many online experiments exhibit dependence between users and items. For example, in online advertising, observations that have a user or an ad in common are likely to be associated. Because of this, even in experiments involving millions of subjects, the difference in mean outcomes between control and treatment conditions can have substantial variance. Previous theoretical and simulation results demonstrate that not accounting for this kind of dependence structure can result in confidence intervals that are too narrow, leading to inaccurate hypothesis tests. We develop a framework for understanding how dependence affects uncertainty in user-item experiments and evaluate how bootstrap methods that account for differing levels of dependence perform in practice. We use three real datasets describing user behaviors on Facebook - user responses to ads, search results, and News Feed stories - to generate data for synthetic experiments in which there is no effect of the treatment on average by design. We then estimate empirical Type I error rates for each bootstrap method. Accounting for dependence within a single type of unit (i.e., within-user dependence) is often sufficient to get reasonable error rates. But when experiments have effects, as one might expect in the field, accounting for multiple units with a multiway bootstrap can be necessary to get close to the advertised Type I error rates. This work provides guidance to practitioners evaluating large-scale experiments, and highlights the importance of analysis of inferential methods for complex dependence structures common to online experiments.",2013,Knowledge Discovery and Data Mining,Fields of study: bootstrappingfield experimentrandom effects modelstatistical inferenceeconometricsdata miningstatisticscomputer sciencemathematics
Modeling urban traffic dynamics in coexistence with urban data streams,Vahid Moosavi (ETH Zurich)Ludger Hovestadt (ETH Zurich),"1955498120,2466962599","Classic paradigm of scientific modeling is mainly based on a set of previously, accepted or assumed theories about the target phenomena and a validation procedure by limited observations. Therefore, normally data has a supporting role in the modeling process. On the other hand, recent advances in computing technology have brought us a data deluge that may change the classic paradigm of scientific modeling. Information flows and data streams have reached a level of maturity that they can play the main role in modeling of the real systems, without relying on lots of assumptions and rules in the first step. This turn may cause an inversion in the concept of modeling as a rational process. The proposed theoretical idea in this work is that traditional theory-driven models have a theoretical limit in modeling complex systems, known as curse of dimensionality and further, to highlight the fact that massive urban data streams can open up a new data-driven modeling approach, which goes beyond simple data driven analytics or eye catching info-graphics toward operational models of complex phenomena. In this work we describe a conceptual framework for modeling city wide traffic dynamics that proposes a way to encapsulate the complexity based on abstraction power of Markov chains in a coexistence with continuous data streams. Therefore, finally as an experimental set up, we applied the proposed model to a real data set, consisting of GPS traces of taxi cabs in Beijing and the results have been explained.",2013,Knowledge Discovery and Data Mining,Fields of study: markov chainsystems modelingdata miningartificial intelligencemachine learningsimulationstatisticscomputer science
Redundancy-aware maximal cliques,Jia Wang (The Chinese University of Hong Kong)James Cheng (The Chinese University of Hong Kong)Ada Wai-Chee Fu (The Chinese University of Hong Kong),"2427856141,2304873892,2165551961","Recent research efforts have made notable progress in improving the performance of (exhaustive) maximal clique enumeration (MCE). However, existing algorithms still suffer from exploring the huge search space of MCE. Furthermore, their results are often undesirable as many of the returned maximal cliques have large overlapping parts. This redundancy leads to problems in both computational efficiency and usefulness of MCE. In this paper, we aim at providing a concise and complete summary of the set of maximal cliques, which is useful to many applications. We propose the notion of τ- visible MCE to achieve this goal and design algorithms to realize the notion. Based on the refined output space, we further consider applications including an efficient computation of the top- k results with diversity and an interactive clique exploration process. Our experimental results demonstrate that our approach is capable of producing output of high usability and our algorithms achieve superior efficiency over classic MCE algorithms.",2013,Knowledge Discovery and Data Mining,Fields of study: discrete mathematicscombinatoricsmachine learningmathematical optimizationmathematics
An efficient ADMM algorithm for multidimensional anisotropic total variation regularization problems,Sen Yang (Arizona State University)Jie Wang (Arizona State University)Wei Fan (Huawei)Xiatian Zhang (Huawei)Peter Wonka (Arizona State University)Jieping Ye (Arizona State University),"2617528270,2471893859,2422054197,2125633937,729125449,2305258894","Total variation (TV) regularization has important applications in signal processing including image denoising, image deblurring, and image reconstruction. A significant challenge in the practical use of TV regularization lies in the nondifferentiable convex optimization, which is difficult to solve especially for large-scale problems. In this paper, we propose an efficient alternating augmented Lagrangian method (ADMM) to solve total variation regularization problems. The proposed algorithm is applicable for tensors, thus it can solve multidimensional total variation regularization problems. One appealing feature of the proposed algorithm is that it does not need to solve a linear system of equations, which is often the most expensive part in previous ADMM-based methods. In addition, each step of the proposed algorithm involves a set of independent and smaller problems, which can be solved in parallel. Thus, the proposed algorithm scales to large size problems. Furthermore, the global convergence of the proposed algorithm is guaranteed, and the time complexity of the proposed algorithm is O ( dN /e) on a d -mode tensor with N entries for achieving an e-optimal solution. Extensive experimental results demonstrate the superior performance of the proposed algorithm in comparison with current state-of-the-art methods.",2013,Knowledge Discovery and Data Mining,Fields of study: total variation denoisingcombinatoricsmathematical analysismathematical optimizationcomputer sciencemathematics
Palette power: enabling visual search through colors,Anurag Bhardwaj (eBay)Atish Das Sarma (eBay)Wei Di (eBay)Raffay Hamid (eBay)Robinson Piramuthu (eBay)Neel Sundaresan (eBay),"2152063300,2266878914,2311154374,2233189515,866809645,1981173961","With the explosion of mobile devices with cameras, online search has moved beyond text to other modalities like images, voice, and writing. For many applications like Fashion, image-based search offers a compelling interface as compared to text forms by better capturing the visual attributes. In this paper we present a simple and fast search algorithm that uses color as the main feature for building visual search. We show that low level cues such as color can be used to quantify image similarity and also to discriminate among products with different visual appearances. We demonstrate the effectiveness of our approach through a mobile shopping application\footnote{eBay Fashion App available at https://itunes.apple.com/us/app/ebay-fashion/id378358380?mt=8 and eBay image swatch is the feature indexing millions of real world fashion images}. Our approach outperforms several other state-of-the-art image retrieval algorithms for large scale image data.",2013,Knowledge Discovery and Data Mining,Fields of study: visual searchsearch enginee commercemultimediaworld wide webcomputer visioncomputer science
Amplifying the voice of youth in Africa via text analytics,Prem Melville (IBM)Vijil Chenthamarakshan (IBM)Richard D. Lawrence (IBM)James Powell (UNICEF)Moses Mugisha (UNICEF)Sharad Sapra (UNICEF)Rajesh Anandan (UNICEF)Solomon Assefa (IBM),"2073846700,2221080166,1965188977,2421277191,2227239797,2224788586,2232503495,2118626727","U-report is an open-source SMS platform operated by UNICEF Uganda, designed to give community members a voice on issues that impact them. Data received by the system are either SMS responses to a poll conducted by UNICEF, or unsolicited reports of a problem occurring within the community. There are currently 200,000 U-report participants, and they send up to 10,000 unsolicited text messages a week. The objective of the program in Uganda is to understand the data in real-time, and have issues addressed by the appropriate department in UNICEF in a timely manner. Given the high volume and velocity of the data streams, manual inspection of all messages is no longer sustainable. This paper describes an automated message-understanding and routing system deployed by IBM at UNICEF. We employ recent advances in data mining to get the most out of labeled training data, while incorporating domain knowledge from experts. We discuss the trade-offs, design choices and challenges in applying such techniques in a real-world deployment.",2013,Knowledge Discovery and Data Mining,Fields of study: data sciencedata miningmachine learningcomputer science
Augmenting MATLAB with semantic objects for an interactive visual environment,Changhyun Lee (Georgia Institute of Technology)Jaegul Choo (Georgia Institute of Technology)Duen Horng (Polo) Chau (Georgia Institute of Technology)Haesun Park (Georgia Institute of Technology),"2597514406,2148380128,2024561599,2123241397","Analysis tools such as Matlab, R, and SAS support a myriad of built-in computational functions and various standard visualization techniques. However, most of them provide little interaction from visualizations mainly due to the fact that the tools treat the data as just numerical vectors or matrices while ignoring any semantic meaning associated with them. To solve this limitation, we augment Matlab, one of the widely used data analysis tools, with the capability of directly handling the underlying semantic objects and their meanings. Such capabilities allow users to flexibly assign essential interaction capabilities, such as brushing-and-linking and details-on-demand interactions, to visualizations. To demonstrate the capabilities, two usage scenarios in document and graph analysis domains are presented.",2013,Knowledge Discovery and Data Mining,Fields of study: dimensionality reductioncluster analysisvisual analyticsinteractive visualizationdata sciencetheoretical computer sciencedata miningmachine learningcomputer science
JobMiner: a real-time system for mining job-related patterns from social media,Yu Cheng (Northwestern University)Yusheng Xie (Northwestern University)Zhengzhang Chen (Northwestern University)Ankit Agrawal (Northwestern University)Alok N. Choudhary (Northwestern University)Songtao Guo (LinkedIn),"2544119651,2123854923,2132666618,2160807299,2147783234,2700533644","The various kinds of booming social media not only provide a platform where people can communicate with each other, but also spread useful domain information, such as career and job market information. For example, LinkedIn publishes a large amount of messages either about people who want to seek jobs or companies who want to recruit new members. By collecting information, we can have a better understanding of the job market and provide insights to job-seekers, companies and even decision makers. In this paper, we analyze the job information from the social network point of view. We first collect the job-related information from various social media sources. Then we construct an inter-company job-hopping network, with the vertices denoting companies and the edges denoting flow of personnel between companies. We subsequently employ graphmining techniques to mine influential companies and related company groups based on the job-hopping network model. Demonstration on LinkedIn data shows that our system JobMiner can provide a better understanding of the dynamic processes and a more accurate identification of important entities in the job market.",2013,Knowledge Discovery and Data Mining,Fields of study: job analysissocial mediadata miningcomputer science
Active learning and search on low-rank matrices,Dougal J. Sutherland (Carnegie Mellon University)Barnabás Póczos (Carnegie Mellon University)Jeff G. Schneider (Carnegie Mellon University),"2122024741,297547182,2165347984","Collaborative prediction is a powerful technique, useful in domains from recommender systems to guiding the scientific discovery process. Low-rank matrix factorization is one of the most powerful tools for collaborative prediction. This work presents a general approach for active collaborative prediction with the Probabilistic Matrix Factorization model. Using variational approximations or Markov chain Monte Carlo sampling to estimate the posterior distribution over models, we can choose query points to maximize our understanding of the model, to best predict unknown elements of the data matrix, or to find as many ""positive"" data points as possible. We evaluate our methods on simulated data, and also show their applicability to movie ratings prediction and the discovery of drug-target interactions.",2013,Knowledge Discovery and Data Mining,Fields of study: cold startactive learningcollaborative filteringdrug discoverymatrix decompositionrecommender systemdata sciencedata miningmachine learningcomputer science
An integrated framework for suicide risk prediction,Truyen Tran 0001 (Deakin University)Dinh Q. Phung (Deakin University)Wei Luo (Deakin University)Richard Harvey 0002 (Barwon Health)Michael Berk (Deakin University)Svetha Venkatesh (Deakin University),"2157392948,2314522249,2601343041,2320159844,2075407771,2146461601","Suicide is a major concern in society. Despite of great attention paid by the community with very substantive medico-legal implications, there has been no satisfying method that can reliably predict the future attempted or completed suicide. We present an integrated machine learning framework to tackle this challenge. Our proposed framework consists of a novel feature extraction scheme, an embedded feature selection process, a set of risk classifiers and finally, a risk calibration procedure. For temporal feature extraction, we cast the patient's clinical history into a temporal image to which a bank of one-side filters are applied. The responses are then partly transformed into mid-level features and then selected in l 1 -norm framework under the extreme value theory. A set of probabilistic ordinal risk classifiers are then applied to compute the risk probabilities and further re-rank the features. Finally, the predicted risks are calibrated. Together with our Australian partner, we perform comprehensive study on data collected for the mental health cohort, and the experiments validate that our proposed framework outperforms risk assessment instruments by medical practitioners.",2013,Knowledge Discovery and Data Mining,Fields of study: filter bankdata sciencedata miningmachine learningstatisticscomputer science
Learning mixed kronecker product graph models with simulated method of moments,Sebastian I. Moreno (Purdue University)Jennifer Neville (Purdue University)Sergey Kirshner (Purdue University),"2104332529,2124572662,2119709874","There has recently been a great deal of work focused on developing statistical models of graph structure---with the goal of modeling probability distributions over graphs from which new, similar graphs can be generated by sampling from the estimated distributions. Although current graph models can capture several important characteristics of social network graphs (e.g., degree, path lengths), many of them do not generate graphs with sufficient variation to reflect the natural variability in real world graph domains. One exception is the mixed Kronecker Product Graph Model (mKPGM), a generalization of the Kronecker Product Graph Model, which uses parameter tying to capture variance in the underlying distribution [10]. The enhanced representation of mKPGMs enables them to match both the mean graph statistics and their spread as observed in real network populations, but unfortunately to date, the only method to estimate mKPGMs involves an exhaustive search over the parameters. In this work, we present the first learning algorithm for mKPGMs. The O (| E |) algorithm searches over the continuous parameter space using constrained line search and is based on simulated method of moments , where the objective function minimizes the distance between the observed moments in the training graph and the empirically estimated moments of the model. We evaluate the mKPGM learning algorithm by comparing it to several different graph models, including KPGMs. We use multi-dimensional KS distance to compare the generated graphs to the observed graphs and the results show mKPGMs are able to produce a closer match to real-world graphs (10-90% reduction in KS distance), while still providing natural variation in the generated graphs.",2013,Knowledge Discovery and Data Mining,Fields of study: voltage graphindifference graph1 planar graphcomparability graphlattice graphgraph productclique widthsplit graphmodular decompositionpathwidthnull modellink analysisdiscrete mathematicscombinatoricsmachine learningstatisticscomputer sciencemathematics
Mining evolutionary multi-branch trees from text streams,Xiting Wang (Tsinghua University)Shixia Liu (Microsoft)Yangqiu Song (Hong Kong University of Science and Technology)Baining Guo (Microsoft),"2096164459,2111177146,2099747503,2675612186","Understanding topic hierarchies in text streams and their evolution patterns over time is very important in many applications. In this paper, we propose an evolutionary multi-branch tree clustering method for streaming text data. We build evolutionary trees in a Bayesian online filtering framework. The tree construction is formulated as an online posterior estimation problem, which considers both the likelihood of the current tree and conditional prior given the previous tree. We also introduce a constraint model to compute the conditional prior of a tree in the multi-branch setting. Experiments on real world news data demonstrate that our algorithm can better incorporate historical tree information and is more efficient and effective than the traditional evolutionary hierarchical clustering algorithm.",2013,Knowledge Discovery and Data Mining,Fields of study: tree rearrangementorder statistic treeincremental decision treemetric treetree traversalk d treevisualizationtree structurecluster analysistime seriesdata miningpattern recognitionmachine learningstatisticscomputer science
Empirical bayes model to combine signals of adverse drug reactions,Rave Harpaz (Stanford University)William DuMouchel (Oracle Corporation)Paea LePendu (Stanford University)Nigam H. Shah (Stanford University),"2260547153,2272274628,336158105,2150446796","Data mining is a crucial tool for identifying risk signals of potential adverse drug reactions (ADRs). However, mining of ADR signals is currently limited to leveraging a single data source at a time. It is widely believed that combining ADR evidence from multiple data sources will result in a more accurate risk identification system. We present a methodology based on empirical Bayes modeling to combine ADR signals mined from ~5 million adverse event reports collected by the FDA, and healthcare data corresponding to 46 million patients' the main two types of information sources currently employed for signal detection. Based on four sets of test cases (gold standard), we demonstrate that our method leads to a statistically significant and substantial improvement in signal detection accuracy, averaging 40% over the use of each source independently, and an area under the ROC curve of 0.87. We also compare the method with alternative supervised learning approaches, and argue that our approach is preferable as it does not require labeled (training) samples whose availability is currently limited. To our knowledge, this is the first effort to combine signals from these two complementary data sources, and to demonstrate the benefits of a computationally integrative strategy for drug safety surveillance.",2013,Knowledge Discovery and Data Mining,Fields of study: pharmacovigilancedetection theorydata sciencedata miningstatisticscomputer science
Robust principal component analysis via capped norms,Qian Sun (Arizona State University)Shuo Xiang (Arizona State University)Jieping Ye (Arizona State University),"2620185474,2121591154,2305258894","In many applications such as image and video processing, the data matrix often possesses simultaneously a low-rank structure capturing the global information and a sparse component capturing the local information. How to accurately extract the low-rank and sparse components is a major challenge. Robust Principal Component Analysis (RPCA) is a general framework to extract such structures. It is well studied that under certain assumptions, convex optimization using the trace norm and l 1 -norm can be an effective computation surrogate of the difficult RPCA problem. However, such convex formulation is based on a strong assumption which may not hold in real-world applications, and the approximation error in these convex relaxations often cannot be neglected. In this paper, we present a novel non-convex formulation for the RPCA problem using the capped trace norm and the capped l 1 -norm. In addition, we present two algorithms to solve the non-convex optimization: one is based on the Difference of Convex functions (DC) framework and the other attempts to solve the sub-problems via a greedy approach. Our empirical evaluations on synthetic and real-world data show that both of the proposed algorithms achieve higher accuracy than existing convex formulations. Furthermore, between the two proposed algorithms, the greedy algorithm is more efficient than the DC programming, while they achieve comparable accuracy.",2013,Knowledge Discovery and Data Mining,Fields of study: proper convex functionlinear matrix inequalitysparsity of effects principleconvex optimizationimage processingdiscrete mathematicscombinatoricsmathematical optimizationstatisticscomputer sciencemathematics
Beyond myopic inference in big data pipelines,Karthik Raman (Cornell University)Adith Swaminathan (Cornell University)Johannes Gehrke (Cornell University)Thorsten Joachims (Cornell University),"2289464261,2144842984,2083845045,245171893","Big Data Pipelines decompose complex analyses of large data sets into a series of simpler tasks, with independently tuned components for each task. This modular setup allows re-use of components across several different pipelines. However, the interaction of independently tuned pipeline components yields poor end-to-end performance as errors introduced by one component cascade through the whole pipeline, affecting overall accuracy. We propose a novel model for reasoning across components of Big Data Pipelines in a probabilistically well-founded manner. Our key idea is to view the interaction of components as dependencies on an underlying graphical model. Different message passing schemes on this graphical model provide various inference algorithms to trade-off end-to-end performance and computational cost. We instantiate our framework with an efficient beam search algorithm, and demonstrate its efficiency on two Big Data Pipelines: parsing and relation extraction.",2013,Knowledge Discovery and Data Mining,Fields of study: modular designtheoretical computer sciencedata miningmachine learningcomputer science
Efficient single-source shortest path and distance queries on large graphs,Andy Diwen Zhu (Nanyang Technological University)Xiaokui Xiao (Nanyang Technological University)Sibo Wang (Nanyang Technological University)Wenqing Lin (Nanyang Technological University),"2059788142,2157867657,2231646109,2227061030","This paper investigates two types of graph queries: single source distance (SSD) queries and single source shortest path (SSSP) queries. Given a node v in a graph G , an SSD query from v asks for the distance from $v$ to any other node in G , while an SSSP query retrieves the shortest path from v to any other node. These two types of queries find important applications in graph analysis, especially in the computation of graph measures. Most of the existing solutions for SSD and SSSP queries, however, require that the input graph fits in the main memory, which renders them inapplicable for the massive disk-resident graphs commonly used in web and social applications. There are several techniques that are designed to be I/O efficient, but they all focus on undirected and/or unweighted graphs, and they only offer sub-optimal query efficiency. To address the deficiency of existing work, this paper presents Highways-on-Disk (HoD), a disk-based index that supports both SSD and SSSP queries on directed and weighted graphs. The key idea of HoD is to augment the input graph with a set of auxiliary edges, and exploit them during query processing to reduce I/O and computation costs. We experimentally evaluate HoD on both directed and undirected real-world graphs with up to billions of nodes and edges, and we demonstrate that HoD significantly outperforms alternative solutions in terms of query efficiency.",2013,Knowledge Discovery and Data Mining,Fields of study: complement graphcomparability graphblock graphdistancegraph databasepathpathwidthlongest path problemgraphgraphtheoretical computer sciencedatabase
Psychological advertising: exploring user psychology for click prediction in sponsored search,Taifeng Wang (Microsoft)Jiang Bian (Microsoft)Shusen Liu (South China University of Technology)Yuyu Zhang (Chinese Academy of Sciences)Tie-Yan Liu (Microsoft),"2157154139,2724056146,2665000343,2115324008,2108341226","Precise click prediction is one of the key components in the sponsored search system. Previous studies usually took advantage of two major kinds of information for click prediction, i.e., relevance information representing the similarity between ads and queries and historical click-through information representing users' previous preferences on the ads. These existing works mainly focused on interpreting ad clicks in terms of what users seek (i.e., relevance information) and how users choose to click (historically clicked-through information). However, few of them attempted to understand why users click the ads. In this paper, we aim at answering this ``why'' question. In our opinion, users click those ads that can convince them to take further actions, and the critical factor is if those ads can trigger users' desires in their hearts. Our data analysis on a commercial search engine reveals that specific text patterns, e.g., ``official site'', ``$x\%$ off'', and ``guaranteed return in $x$ days'', are very effective in triggering users' desires, and therefore lead to significant differences in terms of click-through rate (CTR). These observations motivate us to systematically model user psychological desire in order for a precise prediction on ad clicks. To this end, we propose modeling user psychological desire in sponsored search according to Maslow's desire theory, which categorizes psychological desire into five levels and each one is represented by a set of textual patterns automatically mined from ad texts. We then construct novel features for both ads and users based on our definition on psychological desire and incorporate them into the learning framework of click prediction. Large scale evaluations on the click-through logs from a commercial search engine demonstrate that this approach can result in significant improvement in terms of click prediction accuracy, for both the ads with rich historical data and those with rare one. Further analysis reveals that specific pattern combinations are especially effective in driving click-through rates, which provides a good guideline for advertisers to improve their ad textual descriptions.",2013,Knowledge Discovery and Data Mining,Fields of study: click pathonline advertisingmultimediaworld wide webdata miningmachine learningcomputer science
Link prediction with social vector clocks,Conrad Lee (University College Dublin)Bobo Nick (University of Konstanz)Ulrik Brandes (University of Konstanz)Pádraig Cunningham (University College Dublin),"2139875645,1968036321,1226911827,2120527611","State-of-the-art link prediction utilizes combinations of complex features derived from network panel data. We here show that computationally less expensive features can achieve the same performance in the common scenario in which the data is available as a sequence of interactions. Our features are based on social vector clocks, an adaptation of the vector-clock concept introduced in distributed computing to social interaction networks. In fact, our experiments suggest that by taking into account the order and spacing of interactions, social vector clocks exploit different aspects of link formation so that their combination with previous approaches yields the most accurate predictor to date.",2013,Knowledge Discovery and Data Mining,Fields of study: vector clockpreprintonline algorithmsocial networkrecommender systemworld wide websocial sciencedata miningartificial intelligencemachine learningcomputer science
A data-driven method for in-game decision making in MLB: when to pull a starting pitcher,Ganeshapillai Gartheeban (Massachusetts Institute of Technology)John Guttag (Massachusetts Institute of Technology),"2227752149,120651226","Professional sports is a roughly $500 billion dollar industry that is increasingly data-driven. In this paper we show how machine learning can be applied to generate a model that could lead to better on-field decisions by managers of professional baseball teams. Specifically we show how to use regularized linear regression to learn pitcher-specific predictive models that can be used to help decide when a starting pitcher should be replaced. A key step in the process is our method of converting categorical variables (e.g., the venue in which a game is played) into continuous variables suitable for the regression. Another key step is dealing with situations in which there is an insufficient amount of data to compute measures such as the effectiveness of a pitcher against specific batters. For each season we trained on the first 80% of the games, and tested on the rest. The results suggest that using our model could have led to better decisions than those made by major league managers. Applying our model would have led to a different decision 48% of the time. For those games in which a manager left a pitcher in that our model would have removed, the pitcher ended up performing poorly 60% of the time.",2013,Knowledge Discovery and Data Mining,Fields of study: predictive analyticsoperations researchdata miningmachine learningsimulationcomputer science
Using co-visitation networks for detecting large scale online display advertising exchange fraud,Ori StitelmanClaudia Perlich (IBM)Brian DalessandroRod HookTroy Raeder (University of Notre Dame)Foster J. Provost (New York University Stern School of Business),"2234062319,164824025,1876520739,2230241747,1999864269,2158932634","Data generated by observing the actions of web browsers across the internet is being used at an ever increasing rate for both building models and making decisions. In fact, a quarter of the industry-track papers for KDD in 2012 were based on data generated by online actions. The models, analytics and decisions they inform all stem from the assumption that observed data captures the intent of users. However, a large portion of these observed actions are not intentional, and are effectively polluting the models. Much of this observed activity is either generated by robots traversing the internet or the result of unintended actions of real users. These non-intentional actions observed in the web logs severely bias both analytics and the models created from the data. In this paper, we will show examples of how non-intentional traffic that is produced by fraudulent activities adversely affects both general analytics and predictive models, and propose an approach using co-visitation networks to identify sites that have large amounts of this fraudulent traffic. We will then show how this approach, along with a second stage classifier that identifies non-intentional traffic at the browser level, is deployed in production at Media6Degrees (m6d), a targeting technology company for display advertising. This deployed product acts both to filter out the fraudulent traffic from the input data and to insure that we don't serve ads during unintended website visits.",2013,Knowledge Discovery and Data Mining,Fields of study: web analyticsinternet privacyworld wide webdata miningcomputer science
Modeling the dynamics of composite social networks,Erheng Zhong (Hong Kong University of Science and Technology)Wei Fan (Huawei)Yin Zhu (Hong Kong University of Science and Technology)Qiang Yang (Hong Kong University of Science and Technology),"2108094379,2422054197,2104993999,2109031554","Modeling the dynamics of online social networks over time not only helps us understand the evolution of network structures and user behaviors, but also improves the performance of other analysis tasks, such as link prediction and community detection. Nowadays, users engage in multiple networks and form a ""composite social network"" by considering common users as the bridge. State-of-the-art network-dynamics analysis is performed in isolation for individual networks, but users' interactions in one network can influence their behaviors in other networks, and in an individual network, different types of user interactions also affect each other. Without considering the influences across networks, one may not be able to model the dynamics in a given network correctly due to the lack of information. In this paper, we study the problem of modeling the dynamics of composite networks, where the evolution processes of different networks are jointly considered. However, due to the difference in network properties, simply merging multiple networks into a single one is not ideal because individual evolution patterns may be ignored and network differences may bring negative impacts. The proposed solution is a nonparametric Bayesian model, which models each user's common latent features to extract the cross-network influences, and use network-specific factors to describe different networks' evolution patterns. Empirical studies on large-scale dynamic composite social networks demonstrate that the proposed approach improves the performance of link prediction over several state-of-the-art baselines and unfolds the network evolution accurately.",2013,Knowledge Discovery and Data Mining,Fields of study: organizational network analysisinterdependent networksevolving networkshierarchical network modeldynamic network analysistransfer of learningsocial network analysisnetwork simulationartificial intelligencemachine learningsimulationcomputer science
Active search on graphs,Xuezhi Wang (Carnegie Mellon University)Roman Garnett (Carnegie Mellon University)Jeff G. Schneider (Carnegie Mellon University),"2100349271,2266629704,2165347984","Active search is an increasingly important learning problem in which we use a limited budget of label queries to discover as many members of a certain class as possible. Numerous real-world applications may be approached in this manner, including fraud detection, product recommendation, and drug discovery. Active search has model learning and exploration/exploitation features similar to those encountered in active learning and bandit problems, but algorithms for those problems do not fit active search. Previous work on the active search problem [5] showed that the optimal algorithm requires a lookahead evaluation of expected utility that is exponential in the number of selections to be made and proposed a truncated lookahead heuristic. Inspired by the success of myopic methods for active learning and bandit problems, we propose a myopic method for active search on graphs. We suggest selecting points by maximizing a score considering the potential impact of selecting a node, meant to emulate lookahead while avoiding exponential search. We test the proposed algorithm empirically on real-world graphs and show that it outperforms popular approaches for active learning and bandit problems as well as truncated lookahead of a few steps.",2013,Knowledge Discovery and Data Mining,Fields of study: active learningactive learningdata miningmachine learningmathematical optimizationcomputer science
Forex-foreteller: currency trend modeling using news articles,Fang Jin (Virginia Tech)Nathan Self (Virginia Tech)Parang Saraf (Virginia Tech)Patrick Butler (Virginia Tech)Wei Wang (Virginia Tech)Naren Ramakrishnan (Virginia Tech),"2187881468,2093704914,2160730744,2293049679,2529750488,2199255697","Financial markets are quite sensitive to unanticipated news and events. Identifying the effect of news on the market is a challenging task. In this demo, we present Forex-foreteller (FF) which mines news articles and makes forecasts about the movement of foreign currency markets. The system uses a combination of language models, topic clustering, and sentiment analysis to identify relevant news articles. These articles along with the historical stock index and currency exchange values are used in a linear regression model to make forecasts. The system has an interactive visualizer designed specifically for touch-sensitive devices which depicts forecasts along with the chronological news events and financial data used for making the forecasts.",2013,Knowledge Discovery and Data Mining,Fields of study: sentiment analysisdata miningcomputer science
Efficiently rewriting large multimedia application execution traces with few event sequences,Christiane Kamdem Kengne (University of Ghana)Leon Constantin Fopa (University of Ghana)Alexandre Termier (University of Ghana)Noha Ibrahim (University of Ghana)Marie-Christine Rousset (University of Ghana)Takashi Washio (International Society for Intelligence Research)Miguel Santana (STMicroelectronics),"2231264273,2222533459,99113242,2584257595,2048382749,1794038515,2298691976","The analysis of multimedia application traces can reveal important information to enhance program execution comprehension. However typical size of traces can be in gigabytes, which hinders their effective exploitation by application developers. In this paper, we study the problem of finding a set of sequences of events that allows a reduced-size rewriting of the original trace. These sequences of events, that we call blocks , can simplify the exploration of large execution traces by allowing application developers to see an abstraction instead of low-level events. The problem of computing such set of blocks is NP-hard and naive approaches lead to prohibitive running times that prevent analysing real world traces. We propose a novel algorithm that directly mines the set of blocks. Our experiments show that our algorithm can analyse real traces of up to two hours of video. We also show experimentally the quality of the set of blocks proposed, and the interest of the rewriting to understand actual trace data.",2013,Knowledge Discovery and Data Mining,Fields of study: combinatorial optimizationtheoretical computer sciencedata miningreal time computingalgorithmcomputer science
Towards long-lead forecasting of extreme flood events: a data mining framework for precipitation cluster precursors identification,Dawei Wang (University of Massachusetts Boston)Wei Ding (University of Massachusetts Boston)Kui Yu (Hefei University of Technology)Xindong Wu (University of Vermont)Ping Chen (University of Houston–Downtown)David L. Small (Tufts University)Shafiqul Islam (Tufts University),"2308533282,2116574971,2567944696,2123651450,2667957185,2422163516,2160149565","The development of disastrous flood forecasting techniques able to provide warnings at a long lead-time (5-15 days) is of great importance to society. Extreme Flood is usually a consequence of a sequence of precipitation events occurring over from several days to several weeks. Though precise short-term forecasting the magnitude and extent of individual precipitation event is still beyond our reach, long-term forecasting of precipitation clusters can be attempted by identifying persistent atmospheric regimes that are conducive for the precipitation clusters. However, such forecasting will suffer from overwhelming number of relevant features and high imbalance of sample sets. In this paper, we propose an integrated data mining framework for identifying the precursors to precipitation event clusters and use this information to predict extended periods of extreme precipitation and subsequent floods. We synthesize a representative feature set that describes the atmosphere motion, and apply a streaming feature selection algorithm to online identify the precipitation precursors from the enormous feature space. A hierarchical re-sampling approach is embedded in the framework to deal with the imbalance problem. An extensive empirical study is conducted on historical precipitation and associated flood data collected in the State of Iowa. Utilizing our framework a few physically meaningful precipitation cluster precursor sets are identified from millions of features. More than 90% of extreme precipitation events are captured by the proposed prediction model using precipitation cluster precursors with a lead time of more than 5 days.",2013,Knowledge Discovery and Data Mining,Fields of study: flood forecastingdata mining
FeaFiner: biomarker identification from medical data through feature generalization and selection,Jiayu Zhou (Arizona State University)Zhaosong Lu (Simon Fraser University)Jimeng Sun (IBM)Lei Yuan (Arizona State University)Fei Wang (IBM)Jieping Ye (Arizona State University),"2097879502,2177275542,2110385854,2243895322,2465953593,2305258894","Traditionally, feature construction and feature selection are two important but separate processes in data mining. However, many real world applications require an integrated approach for creating, refining and selecting features. To address this problem, we propose FeaFiner (short for Feature Refiner), an efficient formulation that simultaneously generalizes low-level features into higher level concepts and then selects relevant concepts based on the target variable. Specifically, we formulate a double sparsity optimization problem that identifies groups in the low-level features, generalizes higher level features using the groups and performs feature selection. Since in many clinical researches non- overlapping groups are preferred for better interpretability, we further improve the formulation to generalize features using mutually exclusive feature groups. The proposed formulation is challenging to solve due to the orthogonality constraints, non-convexity objective and non-smoothness penal- ties. We apply a recently developed augmented Lagrangian method to solve this formulation in which each subproblem is solved by a non-monotone spectral projected gradient method. Our numerical experiments show that this approach is computationally efficient and also capable of producing solutions of high quality. We also present a generalization bound showing the consistency and the asymptotic behavior of the learning process of our proposed formulation. Finally, the proposed FeaFiner method is validated on Alzheimer's Disease Neuroimaging Initiative dataset, where low-level biomarkers are automatically generalized into robust higher level concepts which are then selected for predicting the disease status measured by Mini Mental State Examination and Alzheimer's Disease Assessment Scale cognitive subscore. Compared to existing predictive modeling methods, FeaFiner provides intuitive and robust feature concepts and competitive predictive accuracy.",2013,Knowledge Discovery and Data Mining,Fields of study: augmented lagrangian methodbiomarkerfeaturefeature selectiondata miningpattern recognitionmachine learningmathematical optimizationstatisticscomputer sciencemathematics
Outlier ensembles,Charu Agarwal (IBM),2293205010,"Ensemble analysis is a widely used meta-algorithm for many data mining problems such as classification and clustering. Numerous ensemble-based algorithms have been proposed in the literature for these problems. Compared to the clustering and classification problems, ensemble analysis has been studied in a limited way in the outlier detection literature. In some cases, ensemble analysis techniques have been implicitly used by many outlier analysis algorithms, but the approach is often buried deep into the algorithm and not formally recognized as a general-purpose meta-algorithm. This is in spite of the fact that this problem is rather important in the context of outlier analysis. This talk discusses the various methods which are used in the literature for outlier ensembles and the general principles by which such analysis can be made more effective. A discussion is also provided on how outlier ensembles relate to the ensemble-techniques used commonly for other data mining problems.",2013,Knowledge Discovery and Data Mining,Fields of study: data miningpattern recognitionmachine learningcomputer science
Scalable inference in max-margin topic models,Jun Zhu (Tsinghua University)Xun Zheng (Beihang University)Li Zhou (Tsinghua University)Bo Zhang (Tsinghua University),"2305755055,2132416899,2722239345,2438699337","Topic models have played a pivotal role in analyzing large collections of complex data. Besides discovering latent semantics, supervised topic models (STMs) can make predictions on unseen test data. By marrying with advanced learning techniques, the predictive strengths of STMs have been dramatically enhanced, such as max-margin supervised topic models, state-of-the-art methods that integrate max-margin learning with topic models. Though powerful, max-margin STMs have a hard non-smooth learning problem. Existing algorithms rely on solving multiple latent SVM subproblems in an EM-type procedure, which can be too slow to be applicable to large-scale categorization tasks. In this paper, we present a highly scalable approach to building max-margin supervised topic models. Our approach builds on three key innovations: 1) a new formulation of Gibbs max-margin supervised topic models for both multi-class and multi-label classification; 2) a simple ``augment-and-collapse"" Gibbs sampling algorithm without making restricting assumptions on the posterior distributions; 3) an efficient parallel implementation that can easily tackle data sets with hundreds of categories and millions of documents. Furthermore, our algorithm does not need to solve SVM subproblems. Though performing the two tasks of topic discovery and learning predictive models jointly, which significantly improves the classification performance, our methods have comparable scalability as the state-of-the-art parallel algorithms for the standard LDA topic models which perform the single task of topic discovery only. Finally, an open-source implementation is also provided at: http://www.ml-thu.net/~jun/medlda.",2013,Knowledge Discovery and Data Mining,Fields of study: dynamic topic modeltopic modeldata miningpattern recognitionmachine learningstatisticscomputer science
On the importance of temporal dynamics in modeling urban activity,Ke Zhang (University of Pittsburgh)Qiuye Jin (University of Pittsburgh)Konstantinos Pelechrinis (University of Pittsburgh)Theodoros Lappas (Stevens Institute of Technology),"2288398844,2222993258,155875377,2028397797","The vast amount of available spatio-temporal data of human activities and mobility has given raise to the rapidly emerging field of urban computing/informatics. Central to the latter is understanding the dynamics of the activities that take place in an urban area (e.g., a city). This can significantly enhance functionalities such as resource and service allocation within a city. Existing literature has paid a lot of attention on spatial dynamics, with the temporal ones often being neglected and left out. However, this can lead to non-negligible implications. For instance, while two areas can appear to exhibit similar activity when the latter is aggregated in time, they can be significantly different when introducing the temporal dimension. Furthermore, even when considering a specific area X alone, the transitions of the activity that takes place within X are important themselves. Using data from the most prevalent location-based social network (LBSN for short), Foursquare, we analyze the temporal dynamics of activities in New York City and San Francisco. Our results clearly show that considering the temporal dimension provides us with a different and more detailed description of urban dynamics. We envision this study to lead to more careful and detailed consideration of the temporal dynamics when analyzing urban activities.",2013,Knowledge Discovery and Data Mining,Fields of study: simulation
Learning to question: leveraging user preferences for shopping advice,Mahashweta Das (University of Texas at Arlington)Gianmarco De Francisci Morales (Yahoo!)Aristides Gionis (Aalto University)Ingmar Weber (Qatar Computing Research Institute),"2158266726,2153118160,737311942,2074066684","We present ShoppingAdvisor, a novel recommender system that helps users in shopping for technical products. ShoppingAdvisor leverages both user preferences and technical product attributes in order to generate its suggestions. The system elicits user preferences via a tree-shaped flowchart, where each node is a question to the user. At each node, ShoppingAdvisor suggests a ranking of products matching the preferences of the user, and that gets progressively refined along the path from the tree's root to one of its leafs. In this paper we show ( i ) how to learn the structure of the tree, i.e., which questions to ask at each node, and ( ii ) how to produce a suitable ranking at each node. First, we adapt the classical top-down strategy for building decision trees in order to find the best user attribute to ask at each node. Differently from decision trees, ShoppingAdvisor partitions the user space rather than the product space. Second, we show how to employ a learning-to-rank approach in order to learn, for each node of the tree, a ranking of products appropriate to the users who reach that node. We experiment with two real-world datasets for cars and cameras, and a synthetic one. We use mean reciprocal rank to evaluate ShoppingAdvisor, and show how the performance increases by more than 50% along the path from root to leaf. We also show how collaborative recommendation algorithms such as k -nearest neighbor benefits from feature selection done by the ShoppingAdvisor tree. Our experiments show that ShoppingAdvisor produces good quality interpretable recommendations, while requiring less input from users and being able to handle the cold-start problem.",2013,Knowledge Discovery and Data Mining,Fields of study: rankingworld wide webdata miningmachine learningstatisticscomputer science
Heat pump detection from coarse grained smart meter data with positive and unlabeled learning,Hongliang Fei (IBM)Younghun Kim (IBM)Sambit Sahu (IBM)Milind R. Naphade (IBM)Sanjay K. Mamidipalli (IBM)John Hutchinson (IBM),"2697307542,2120534724,2169578810,50079105,2051200954,2236135118","Recent advances in smart metering technology enable utility companies to have access to tremendous amount of smart meter data, from which the utility companies are eager to gain more insight about their customers. In this paper, we aim to detect electric heat pumps from coarse grained smart meter data for a heat pump marketing campaign. However, appliance detection is a challenging task, especially given a very low granularity and partial labeled even unlabeled data. Traditional methods install either a high granularity smart meter or sensors at every appliance, which is either too expensive or requires technical expertise. We propose a novel approach to detect heat pumps that utilizes low granularity smart meter data, prior sales data and weather data. In particular, motivated by the characteristics of heat pump consumption pattern, we extract novel features that are highly relevant to heat pump usage from smart meter data and weather data. Under the constraint that only a subset of heat pump users are available, we formalize the problem into a positive and unlabeled data classification and apply biased Support Vector Machine (BSVM) to our extracted features. Our empirical study on a real-world data set demonstrates the effectiveness of our method. Furthermore, our method has been deployed in a real-life setting where the partner electric company runs a targeted campaign for 292,496 customers. Based on the initial feedback, our detection algorithm can successfully detect substantial number of non-heat pump users who were identified heat pump users with the prior algorithm the company had used.",2013,Knowledge Discovery and Data Mining,Fields of study: feature extractionembedded systemdata miningmachine learningsimulationcomputer science
Prediction of user location using the radiation model and social check-ins,"Alexey Tarasov (Dublin Institute of Technology)Felix Kling (Maynooth University)Alexei Pozdnoukhov (University of California, Berkeley)","2131892612,2661334520,2487145470","Location-based social networks serve as a source of data for a wide range of applications, from recommendation of places to visit to modelling of city traffic, and urban planning. One of the basic problems in all these areas is the formulation of a predictive model for the location of a certain user at a certain time. In this paper, we propose a new approach for predicting user location, which uses two components to make the prediction, based on (i) coordinates and times of user check-ins and (ii) social interaction between different users. We improve the performance of a state-of-the art model using the radiation model of spatial choice and a social component based on the frequency of matching check-ins of user's friends. Friendship is defined by the presence of reciprocal following on Twitter. Our empirical results highlight an improvement over the state-of-the-art in terms of accuracy, and suggest practical solutions for spatio-temporal and socially-inspired prediction of user location.",2013,Knowledge Discovery and Data Mining,Fields of study: social influenceworld wide webdata miningsimulation
SVM pAUC tight : a new support vector method for optimizing partial AUC based on a tight convex upper bound,Harikrishna Narasimhan (Indian Institute of Science)Shivani Agarwal (Indian Institute of Science),"2020253040,2099769723","The area under the ROC curve (AUC) is a well known performance measure in machine learning and data mining. In an increasing number of applications, however, ranging from ranking applications to a variety of important bioinformatics applications, performance is measured in terms of the partial area under the ROC curve between two specified false positive rates. In recent work, we proposed a structural SVM based approach for optimizing this performance measure (Narasimhan and Agarwal, 2013). In this paper, we develop a new support vector method, SVM pAUC tight , that optimizes a tighter convex upper bound on the partial AUC loss, which leads to both improved accuracy and reduced computational complexity. In particular, by rewriting the empirical partial AUC risk as a maximum over subsets of negative instances, we derive a new formulation, where a modified form of the earlier optimization objective is evaluated on each of these subsets, leading to a tighter hinge relaxation on the partial AUC loss. As with our previous method, the resulting optimization problem can be solved using a cutting-plane algorithm, but the new method has better run time guarantees. We also discuss a projected subgradient method for solving this problem, which offers additional computational savings in certain settings. We demonstrate on a wide variety of bioinformatics tasks, ranging from protein-protein interaction prediction to drug discovery tasks, that the proposed method does, in many cases, perform significantly better on the partial AUC measure than the previous structural SVM approach. In addition, we also develop extensions of our method to learn sparse and group sparse models, often of interest in biological applications.",2013,Knowledge Discovery and Data Mining,Fields of study: cutting plane methodreceiver operating characteristicsupport vector machinepattern recognitionmachine learningmathematical optimizationcomputer sciencemathematics
Representing documents through their readers,Khalid El-Arini (Facebook)Min Xu (Carnegie Mellon University)Emily B. Fox (University of Washington)Carlos Guestrin (University of Washington),"2310128673,2647098380,2130148372,1988556028","From Twitter to Facebook to Reddit, users have become accustomed to sharing the articles they read with friends or followers on their social networks. While previous work has modeled what these shared stories say about the user who shares them, the converse question remains unexplored: what can we learn about an article from the identities of its likely readers? To address this question, we model the content of news articles and blog posts by attributes of the people who are likely to share them. For example, many Twitter users describe themselves in a short profile, labeling themselves with phrases such as ""vegetarian"" or ""liberal."" By assuming that a user's labels correspond to topics in the articles he shares, we can learn a labeled dictionary from a training corpus of articles shared on Twitter. Thereafter, we can code any new document as a sparse non-negative linear combination of user labels, where we encourage correlated labels to appear together in the output via a structured sparsity penalty. Finally, we show that our approach yields a novel document representation that can be effectively used in many problem settings, from recommendation to modeling news dynamics. For example, while the top politics stories will change drastically from one month to the next, the ""politics"" label will still be there to describe them. We evaluate our model on millions of tweeted news articles and blog posts collected between September 2010 and September 2012, demonstrating that our approach is effective.",2013,Knowledge Discovery and Data Mining,Fields of study: multimediaworld wide webdata miningmachine learningcomputer science
Measuring spontaneous devaluations in user preferences,Komal Kapoor (University of Minnesota)Nisheeth Srivastava (University of Minnesota)Jaideep Srivastava (University of Minnesota)Paul R. Schrater (University of Minnesota),"2280298947,2162828272,2192802387,220185405","Spontaneous devaluation in preferences is ubiquitous, where yesterday's hit is today's affliction. Despite technological advances facilitating access to a wide range of media commodities, finding engaging content is a major enterprise with few principled solutions. Systems tracking spontaneous devaluation in user preferences can allow prediction of the onset of boredom in users potentially catering to their changed needs. In this work, we study the music listening histories of Last.fm users focusing on the changes in their preferences based on their choices for different artists at different points in time. A hazard function, commonly used in statistics for survival analysis, is used to capture the rate at which a user returns to an artist as a function of exposure to the artist. The analysis provides the first evidence of spontaneous devaluation in preferences of music listeners. Better understanding of the temporal dynamics of this phenomenon can inform solutions to the similarity-diversity dilemma of recommender systems.",2013,Knowledge Discovery and Data Mining,Fields of study: recommender systemmultimediadata miningmachine learningsimulationcomputer science
Daily travel behavior: lessons from a week-long survey for the extraction of human mobility motifs related information,Christian M. Schneider (Massachusetts Institute of Technology)Christian Rudloff (Austrian Institute of Technology)Dietmar Bauer (Austrian Institute of Technology)Marta C. González (Massachusetts Institute of Technology),"2471130815,2290097440,2149149383,2277322729","Multi-agent models for simulating the mobility behavior of the urban population are gaining momentum due to increasing computing power. Such models pose high demands in terms of input data in order to be reliably able to match real world behavior. To run the models a synthetic population mirroring typical mobility demand needs to be generated based on real world observations. Traditionally this is done using travel diary surveys, which are costly (and hence have relatively low sample size) and focus mainly on trip choice rather than on activities for an entire day. Thus in this setting the generation of synthetic populations either relies on resampling identical activity chains or on imposing independence of various trips occurring during the day. Both assumptions are not realistic. Using Call Detail Records (CDRs) it has been found that individual daily movement uses only a small number of movement patterns. These patterns, termed motifs, appear stably in many different cities, as has been shown for both CDR data as well as travel diaries. In this paper the relation between these motifs and other mobility related quantities like the distribution of travel distances and times as well as mode choice is investigated. Additionally transition probabilities both for motifs (relevant for multi-day simulations) and mode transitions are discussed. The main finding is that while some of the characteristics seem to be unrelated to motifs, others such as mode choice exhibit strong correlations which could improve the provision of synthetic populations for multi-agent models. Thus the results in this paper are seen as one step further towards the creation of realistic (with respect to mobility behavior) synthetic populations for multi-agent models in order to analyze the performance of multi-modal transportation systems or disease spreading in urban areas.",2013,Knowledge Discovery and Data Mining,Fields of study: mobility modeldata miningsimulation
A transfer learning based framework of crowd-selection on twitter,Zhou Zhao (Hong Kong University of Science and Technology)Da Yan (Hong Kong University of Science and Technology)Wilfred Siu Hung Ng (Hong Kong University of Science and Technology)Shi Gao (Hong Kong University of Science and Technology),"2118299058,2146437384,2170178419,2425608615","Crowd selection is essential to crowd sourcing applications, since choosing the right workers with particular expertise to carry out crowdsourced tasks is extremely important. The central problem is simple but tricky: given a crowdsourced task, who are the most knowledgable users to ask? In this demo, we show our framework that tackles the problem of crowdsourced task assignment on Twitter according to the social activities of its users. Since user profiles on Twitter do not reveal user interests and skills, we transfer the knowledge from categorized Yahoo! Answers datasets for learning user expertise. Then, we select the right crowd for certain tasks based on user expertise. We study the effectiveness of our system using extensive user evaluation. We further engage the attendees to participate a game called--Whom to Ask on Twitter?. This helps understand our ideas in an interactive manner. Our crowd selection can be accessed by the following url http://webproject2.cse.ust.hk:8034/tcrowd/.",2013,Knowledge Discovery and Data Mining,Fields of study: crowdsourcinginternet privacymultimediaworld wide webdata miningcomputer science
"LAFT-Explorer: inferring, visualizing and predicting how your social network expands",Jun Zhang (Tsinghua University)Chaokun Wang (Tsinghua University)Yuanchi Ning (Tsinghua University)Yichi Liu (Tsinghua University)Jianmin Wang (Tsinghua University)Philip S. Yu (University of Illinois at Chicago),"2664544038,2106340623,2715106501,2229478303,2310637432,2125104194","The study of social network evolution has attracted many attentions from both the industry and academia. In this paper we demonstrate LaFT-Explorer, a general toolkit for explaining and reproducing the network growth process based on the friendship propagation. LaFT-Explorer presents multiple perspectives for analyzing the network evolution process and structure, including LaFT-Tree, LaFT-Trace and LaFT-Flow. Upon that we build LaFT-Rec, a new visualized interactive friend recommendation service based on the friendship propagation. LaFT-Rec not only shows whom one may make friends with, but also tells the user that why you should make friends with him and how you can reach him. We demonstrate our system built upon the academic social network of DBLP.",2013,Knowledge Discovery and Data Mining,Fields of study: world wide web
Analysis of advanced meter infrastructure data of water consumption in apartment buildings,Einat Kermany (IBM)Hanna Mazzawi (IBM)Dorit Baras (IBM)Yehuda Naveh (IBM)Hagai Michaelis,"2016877207,257370261,1631446812,2038724923,2226192487","We present our experience of using machine learning techniques over data originating from advanced meter infrastructure (AMI) systems for water consumption in a medium-size city. We focus on two new use cases that are of special importance to city authorities. One use case is the automatic identification of malfunctioning meters, with a focus on distinguishing them from legitimate non-consumption such as during periods when the household residents are on vacation. The other use case is the identification of leaks or theft in the unmetered common areas of apartment buildings. These two use cases are highly important to city authorities both because of the lost revenue they imply and because of the hassle to the residents in cases of delayed identification. Both cases are inherently complex to analyze and require advanced data mining techniques in order to achieve high levels of correct identification. Our results provide for faster and more accurate detection of malfunctioning meters as well as leaks in the common areas. This results in significant tangible value to the authorities in terms of increase in technician efficiency and a decrease in the amount of wasted, non-revenue, water.",2013,Knowledge Discovery and Data Mining,Fields of study: watercomputer securitymachine learningsimulationcomputer science
Towards never-ending learning from time series streams,"Yuan Hao (University of California, Riverside)Yanping Chen 0005 (University of California, Riverside)Jesin Zakaria (University of California, Riverside)Bing Hu 0001 (University of California, Riverside)Thanawin Rakthanmanon (University of California, Riverside)Eamonn J. Keogh (University of California, Riverside)","2169241512,2472401419,2129110089,2105942667,141314290,2170070822","Time series classification has been an active area of research in the data mining community for over a decade, and significant progress has been made in the tractability and accuracy of learning. However, virtually all work assumes a one-time training session in which labeled examples of all the concepts to be learned are provided. This assumption may be valid in a handful of situations, but it does not hold in most medical and scientific applications where we initially may have only the vaguest understanding of what concepts can be learned. Based on this observation, we propose a never-ending learning framework for time series in which an agent examines an unbounded stream of data and occasionally asks a teacher (which may be a human or an algorithm) for a label. We demonstrate the utility of our ideas with experiments in domains as diverse as medicine, entomology, wildlife monitoring, and human behavior analyses.",2013,Knowledge Discovery and Data Mining,Fields of study: time seriesbiological classificationdata stream miningdata sciencedata miningartificial intelligencemachine learningstatisticscomputer sciencemathematics
Fast structure learning in generalized stochastic processes with latent factors,Mohammad Taha Bahadori (University of Southern California)Yan Liu (University of Southern California)Eric P. Xing (Carnegie Mellon University),"2032867848,2240541904,351197510","Understanding and quantifying the impact of unobserved processes is one of the major challenges of analyzing multivariate time series data. In this paper, we analyze a flexible stochastic process model, the generalized linear auto-regressive process (GLARP) and identify the conditions under which the impact of hidden variables appears as an additive term to the evolution matrix estimated with the maximum likelihood. In particular, we examine three examples, including two popular models for count data, i.e, Poisson and Conwey-Maxwell Poisson vector auto-regressive processes, and one powerful model for extreme value data, i.e., Gumbel vector auto-regressive processes. We demonstrate that the impact of hidden factors can be separated out via convex optimization in these three models. We also propose a fast greedy algorithm based on the selection of composite atoms in each iteration and provide a performance guarantee for it. Experiments on two synthetic datasets, one social network dataset and one climatology dataset demonstrate the the superior performance of our proposed models.",2013,Knowledge Discovery and Data Mining,Fields of study: generalized linear modeltime serieseconometricsdata miningmachine learningstatisticscomputer sciencemathematics
The dataminer's guide to scalable mixed-membership and nonparametric bayesian models,Amr Ahmed (Google)Alexander J. Smola (Carnegie Mellon University),"2259645355,1972291593","Large amounts of data arise in a multitude of situations, ranging from bioinformatics to astronomy, manufacturing, and medical applications. For concreteness our tutorial focuses on data obtained in the context of the internet, such as user generated content (microblogs, e-mails, messages), behavioral data (locations, interactions, clicks, queries), and graphs. Due to its magnitude, much of the challenges are to extract structure and interpretable models without the need for additional labels, i.e. to design effective unsupervised techniques. We present design patterns for hierarchical nonparametric Bayesian models, efficient inference algorithms, and modeling tools to describe salient aspects of the data.",2013,Knowledge Discovery and Data Mining,Fields of study: association rule learningcluster analysisdata sciencedata miningmachine learningstatisticscomputer science
Mining discriminative subgraphs from global-state networks,"Sayan Ranu (IBM)Minh X. Hoang (University of California, Santa Barbara)Ambuj K. Singh (University of California, Santa Barbara)","2096541091,2223201173,2099219664","Global-state networks provide a powerful mechanism to model the increasing heterogeneity in data generated by current systems. Such a network comprises of a series of network snapshots with dynamic local states at nodes, and a global network state indicating the occurrence of an event. Mining discriminative subgraphs from global-state networks allows us to identify the influential sub-networks that have maximum impact on the global state and unearth the complex relationships between the local entities of a network and their collective behavior. In this paper, we explore this problem and design a technique called MINDS to mine minimally discriminative subgraphs from large global-state networks. To combat the exponential subgraph search space, we derive the concept of an edit map and perform Metropolis Hastings sampling on it to compute the answer set. Furthermore, we formulate the idea of network-constrained decision trees to learn prediction models that adhere to the underlying network structure. Extensive experiments on real datasets demonstrate excellent accuracy in terms of prediction quality. Additionally, MINDS achieves a speed-up of at least four orders of magnitude over baseline techniques.",2013,Knowledge Discovery and Data Mining,Fields of study: data miningpattern recognitionmachine learningstatisticscomputer sciencemathematics
Scalable supervised dimensionality reduction using clustering,Troy Raeder (University of Notre Dame)Claudia Perlich (IBM)Brian DalessandroOri StitelmanFoster J. Provost (New York University),"1999864269,164824025,1876520739,2234062319,2158932634","The automated targeting of online display ads at scale requires the simultaneous evaluation of a single prospect against many independent models. When deciding which ad to show to a user, one must calculate likelihood-to-convert scores for that user across all potential advertisers in the system. For modern machine-learning-based targeting, as conducted by Media6Degrees (M6D), this can mean scoring against thousands of models in a large, sparse feature space. Dimensionality reduction within this space is useful, as it decreases scoring time and model storage requirements. To meet this need, we develop a novel algorithm for scalable supervised dimensionality reduction across hundreds of simultaneous classification tasks. The algorithm performs hierarchical clustering in the space of model parameters from historical models in order to collapse related features into a single dimension. This allows us to implicitly incorporate feature and label data across all tasks without operating directly in a massive space. We present experimental results showing that for this task our algorithm outperforms other popular dimensionality-reduction algorithms across a wide variety of ad campaigns, as well as production results that showcase its performance in practice.",2013,Knowledge Discovery and Data Mining,Fields of study: cluster analysisdata miningpattern recognitionmachine learningcomputer science
Proceedings of the ACM SIGKDD Workshop on Interactive Data Exploration and Analytics,Duen Horng Chau (Georgia Institute of Technology)Jilles Vreeken (University of Antwerp)Matthijs van Leeuwen (Katholieke Universiteit Leuven)Christos Faloutsos (Carnegie Mellon University),"2024561599,1971070670,2143928993,2198983026","We have entered the era of big data. Massive datasets, surpassing terabytes and petabytes in size are now commonplace. They arise in numerous settings in science, government, and enterprises, and technology exists by which we can collect and store such massive amounts of information. Yet, making sense of these data remains a fundamental challenge. We lack the means to exploratively analyze databases of this scale. Currently, few technologies allow us to freely ""wander"" around the data, and make discoveries by following our intuition, or serendipity. While standard data mining aims at finding highly interesting results, it is typically computationally demanding and time consuming, thus may not be well-suited for interactive exploration of large datasets. Interactive data mining techniques that aptly integrate human intuition, by means of visualization and intuitive human-computer interaction techniques, and machine computation support have been shown to help people gain significant insights into a wide range of problems. However, as datasets are being generated in larger volumes, higher velocity, and greater variety, creating effective interactive data mining techniques becomes a much harder task.",2013,Knowledge Discovery and Data Mining,Fields of study: data scienceworld wide webdata miningmachine learningcomputer science
Proceedings of the 12th International Workshop on Data Mining in Bioinformatics,Jake Chen (Indiana University School of Informatics)Mohammed Zaki (Rensselaer Polytechnic Institute)Gaurav Pandey (Icahn School of Medicine at Mount Sinai)Huzefa Rangwala (George Mason University)George Karypis (University of Minnesota),"2662611124,2165917828,2345239316,2096698710,219814910","Bioinformatics is the science of managing, mining, and interpreting information from biological data. Various genome projects have contributed to an exponential growth in DNA and protein sequence databases. Rapid advances in high-throughput technologies, such as microarrays, mass spectrometry and new/next-generation sequencing, can monitor quantitatively the presence or activity of thousands of genes, RNAs, proteins, metabolites, and compounds in a given biological state. The ongoing influx of these data, the pressing need to address complex biomedical challenge, and the gap between the two have collectively created exciting opportunities for data mining researchers.",2013,Knowledge Discovery and Data Mining,Fields of study: data sciencebioinformaticsdata mining
Exploring venue-based city-to-city similarity measures,Daniel Preoţiuc-Pietro (University of Sheffield)Justin Cranshaw (Carnegie Mellon University)Tae Yano (Carnegie Mellon University),"1221439708,2100888799,2116796655","In this work we explore the use of incidentally generated social network data for the folksonomic characterization of cities by the types of amenities located within them. Using data collected about venue categories in various cities, we examine the effect of different granularities of spatial aggregation and data normalization when representing a city as a collection of its venues. We introduce three vector-based representations of a city, where aggregations of the venue categories are done within a grid structure, within the city's municipal neighborhoods, and across the city as a whole. We apply our methods to a novel dataset consisting of Foursquare venue data from 17 cities across the United States, totaling over 1 million venues. Our preliminary investigation demonstrates that different assumptions in the urban perception could lead to qualitative, yet distinctive, variations in the induced city description and categorization.",2013,Knowledge Discovery and Data Mining,Fields of study: cluster analysisdata miningmachine learningcomputer science
Model selection in markovian processes,Assaf Hallak (Technion – Israel Institute of Technology)Dotan Di-Castro (Technion – Israel Institute of Technology)Shie Mannor (Technion – Israel Institute of Technology),"1968751253,2231687915,2252608274","When analyzing data that originated from a dynamical system, a common practice is to encompass the problem in the well known frameworks of Markov Decision Processes (MDPs) and Reinforcement Learning (RL). The state space in these solutions is usually chosen in some heuristic fashion and the formed MDP can then be used to simulate and predict data, as well as indicate the best possible action in each state. The model chosen to characterize the data affects the complexity and accuracy of any further action we may wish to apply, yet few methods that rely on the dynamic structure to select such a model were suggested. In this work we address the problem of how to use time series data to choose from a finite set of candidate discrete state spaces, where these spaces are constructed by a domain expert. We formalize the notion of model selection consistency in the proposed setup. We then discuss the difference between our proposed framework and the classical Maximum Likelihood (ML) framework, and give an example where ML fails. Afterwards, we suggest alternative selection criteria and show them to be weakly consistent. We then define weak consistency for a model construction algorithm and show a simple algorithm that is weakly consistent. Finally, we test the performance of the suggested criteria and algorithm on both simulated and real world data.",2013,Knowledge Discovery and Data Mining,Fields of study: markov decision processmodel selectionreinforcement learningdata miningmachine learningmathematical optimizationstatisticscomputer sciencemathematics
Randomly sampling maximal itemsets,Sandy Moens (University of Antwerp)Bart Goethals (University of Antwerp),"1982900222,1992071743","Pattern mining techniques generally enumerate lots of uninteresting and redundant patterns. To obtain less redundant collections, techniques exist that give condensed representations of these collections. However, the proposed techniques often rely on complete enumeration of the pattern space, which can be prohibitive in terms of time and memory. Sampling can be used to filter the output space of patterns without explicit enumeration. We propose a framework for random sampling of maximal itemsets from transactional databases. The presented framework can use any monotonically decreasing measure as interestingness criteria for this purpose. Moreover, we use an approximation measure to guide the search for maximal sets to different parts of the output space. We show in our experiments that the method can rapidly generate small collections of patterns with good quality. The sampling framework has been implemented in the interactive visual data mining tool called MIME 1 , as such enabling users to quickly sample a collection of patterns and analyze the results.",2013,Knowledge Discovery and Data Mining,Fields of study: random walkdiscrete mathematicscombinatoricsdata miningstatisticsmathematics
Drug-target interaction prediction for drug repurposing with probabilistic similarity logic,"Shobeir Fakhraei (University of Maryland, College Park)Louiqa Raschid (University of Maryland, College Park)Lise Getoor (University of Maryland, College Park)","2167285659,285822418,1984940772","The high development cost and low success rate of drug discovery from new compounds highlight the need for methods to discover alternate therapeutic effects for currently approved drugs. Computational methods can be effective in focusing efforts for such drug repurposing. In this paper, we propose a novel drug-target interaction prediction framework based on probabilistic similarity logic (PSL) [5]. Interaction prediction corresponds to link prediction in a bipartite network of drug-target interactions extended with a set of similarities between drugs and between targets. Using probabilistic first-order logic rules in PSL, we show how rules describing link predictions based on triads and tetrads can effectively make use of a variety of similarity measures. We learn weights for the rules based on training data, and report relative importance of each similarity for interaction prediction. We show that the learned rule weights significantly improve prediction precision. We evaluate our results on a dataset of drug-target interactions obtained from Drugbank [27] augmented with five drug-based and three target-based similarities. We integrate domain knowledge in drug-target interaction prediction and match the performance of the state-of-the-art drug-target interaction prediction systems [22] with our model using simple triad-based rules. Furthermore, we apply techniques that make link prediction in PSL more efficient for drug-target interaction prediction.",2013,Knowledge Discovery and Data Mining,Fields of study: drug repositioningtargeted drug deliverydomain knowledgedrug discoverystatistical relational learningdata sciencedata miningartificial intelligencemachine learningcomputer science
Stochastic subspace search for top-k multi-view clustering,Geng Li (Rensselaer Polytechnic Institute)Stephan Günnemann (Carnegie Mellon University)Mohammed J. Zaki (Rensselaer Polytechnic Institute),"2123276876,316694267,2165917828","Finding multiple clustering solutions has recently gained much attention. Based on the observation that data is often multi-faceted, novel clustering methods have been introduced capable of detecting multiple, diverse clusterings. In this work-in-progress paper, we present a novel stochastic subspace search principle that tackles the requirements of multi-view clustering. The main idea is to consider each subspace as a state in a Markov chain and using Monte Carlo methods to sample the multi-view subspaces. By dynamically adapting the underlying probability density function we realize the generation of alternative clustering views. We present preliminary experimental results of our method and we describe future research directions.",2013,Knowledge Discovery and Data Mining,Fields of study: flame clusteringbrown clusteringcanopy clustering algorithmdbscancorrelation clusteringconstrained clusteringdata stream clusteringcure data clustering algorithmfuzzy clusteringclustering high dimensional datacluster analysisconsensus clusteringbiclusteringconceptual clusteringdata sciencedata miningmachine learningstatisticscomputer science
Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining,Inderjit S. Dhillon (University of Texas at Austin)Yehuda Koren (Google)Rayid Ghani (University of Chicago)Ted E. Senator (Science Applications International Corporation)Paul Bradley (Microsoft)Rajesh Parekh (Facebook)Jingrui He (Stevens Institute of Technology)Robert L. Grossman (University of Chicago)Ramasamy Uthurusamy (General Motors),"2033403132,1966367906,2655769349,1935278117,2104534353,2528283674,2716944071,2095279874,941590","It is our great pleasure to welcome you to the 19th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD). The annual ACM SIGKDD conference is the premier international forum for data mining, knowledge discovery and big data. It brings together researchers and practitioners from academia, industry, and government to share their ideas, research results and experiences. KDD-2013 features plenary presentations, paper presentations, poster sessions, workshops, tutorials, exhibits, demonstrations, and the KDD Cup competition. Today, you hear a lot about big data, data science and data intensive computing. The core of this work is extracting knowledge and useful information from data, which for science leads to beautiful insights, and for applications leads to actions, alerts and decisions. The KDD community has always been at the center of this activity and it is clear from this conference that it will continue to drive this broader field of big data. This year there were 726 submissions to the KDD Research Track, and 125 papers were accepted. There were 136 submissions to the KDD Industry and Government Track, and 34 papers were accepted. KDD also has a history of inviting talks that are of broad interest to the KDD community. This year we chose to have 4 plenary talks. A program committee also selected 8 talks to present at the Industry Practice Exposition. A strength of the KDD conference is the number of workshops and tutorials that are co-located with it. This year there were 10 full-day workshops, 5 half-day workshops, and 6 tutorials. We thank all sponsors, who are a very important part of the conference, and the members of the Organizing Committee and our other colleagues who volunteered their time during the past year to make this conference a success. Special thanks goes to the Research Track Co-Chairs and the Industry and Government Track Co-Chairs. Also special thanks are due to the Local Arrangements Chair, the Treasurer, the Proceedings Co-Chairs, and the KDD Cup Committee. We are grateful to the several program committees that provided the advice necessary to put together a quality program - the Research Track Program Committee, the Research Track Senior Program Committee, the Industry and Government Track Program Committee, the Industry Practice Expo Program Committee, the Workshop Program Committee, the Tutorial Program Committee, and the Demo Program Committee. We know that you will find this year's exhibits and demonstrations exciting and remind you that some of the most interesting discussions can be found there. Please join us for KDD-2013 to gain new knowledge and to exchange exciting new research results, leading practices, and high impact applications in big data, knowledge discovery and data mining. We hope that you will find this program interesting and thought-provoking and that the conference will provide you with a valuable opportunity to share ideas with other researchers and practitioners from institutions around the world.",2013,Knowledge Discovery and Data Mining,Fields of study: data scienceoperations researchdata mining
LAICOS: an open source platform for personalized social web search,Mohamed Reda Bouadjenek (French Institute for Research in Computer Science and Automation)Hakim Hacid (Bell Labs)Mokrane Bouzeghoub (French Institute for Research in Computer Science and Automation),"161286830,2306955562,551507098","In this paper, we introduce LAICOS, a social Web search engine as a contribution to the growing area of Social Information Retrieval (SIR). Social information and personalization are at the heart of LAICOS. On the one hand, the social context of documents is added as a layer to their textual content traditionally used for indexing to provide Personalized Social Document Representations. On the other hand, the social context of users is used for the query expansion process using the Personalized Social Query Expansion framework (PSQE) proposed in our earlier works. We describe the different components of the system while relying on social bookmarking systems as a source of social information for personalizing and enhancing the IR process. We show how the internal structure of indexes as well as the query expansion process operated using social information.",2013,Knowledge Discovery and Data Mining,Fields of study: social media optimizationquery expansionpersonalizationsocial networksocial computingworld wide webinformation retrievaldata miningcomputer science
Optimizing parallel belief propagation in junction treesusing regression,Lu Zheng (Carnegie Mellon University)Ole J Mengshoel (Carnegie Mellon University),"2104123728,2008986367","The junction tree approach, with applications in artificial intelligence, computer vision, machine learning, and statistics, is often used for computing posterior distributions in probabilistic graphical models. One of the key challenges associated with junction trees is computational, and several parallel computing technologies - including many-core processors - have been investigated to meet this challenge. Many-core processors (including GPUs) are now programmable, unfortunately their complexities make it hard to manually tune their parameters in order to optimize software performance. In this paper, we investigate a machine learning approach to minimize the execution time of parallel junction tree algorithms implemented on a GPU. By carefully allocating a GPU's threads to different parallel computing opportunities in a junction tree, and treating this thread allocation problem as a machine learning problem, we find in experiments that regression - specifically support vector regression - can substantially outperform manual optimization.",2013,Knowledge Discovery and Data Mining,Fields of study: belief propagationbayesian networkregressiontheoretical computer sciencedistributed computingdata miningmachine learningcomputer science
Predicting the present with search engine data,Hal Varian (Google),125896300,"Many businesses now have almost real time data available about their operations. This data can be helpful in contemporaneous prediction (""nowcasting"") of various economic indicators. We illustrate how one can use Google search data to nowcast economic metrics of interest, and discuss some of the ramifications for research and policy. Our approach combines three Bayesian techniques: Kalman filtering, spike-and-slab regression, and model averaging. We use Kalman filtering to whiten the time series in question by removing the trend and seasonal behavior. Spike-and-slab regression is a Bayesian method for variable selection that works even in cases where the number of predictors is far larger than the number of observations. Finally, we use Markov Chain Monte Carlo methods to sample from the posterior distribution for our model; the final forecast is an average over thousands of draws from the posterior. An advantage of the Bayesian approach is that it allows us to specify informative priors that affect the number and type of predictors in a flexible way.",2013,Knowledge Discovery and Data Mining,Fields of study: econometricsdata miningmachine learningstatisticscomputer science
Direct optimization of ranking measures for learning to rank models,Ming Tan (Wright State University)Tian Xia (Wright State University)Lily Guo (Wright State University)Shaojun Wang (Wright State University),"2258895517,2304900629,2144474685,2167395975","We present a novel learning algorithm, DirectRank, which directly and exactly optimizes ranking measures without resorting to any upper bounds or approximations. Our approach is essentially an iterative coordinate ascent method. In each iteration, we choose one coordinate and only update the corresponding parameter, with all others remaining fixed. Since the ranking measure is a stepwise function of a single parameter, we propose a novel line search algorithm that can locate the interval with the best ranking measure along this coordinate quite efficiently. In order to stabilize our system in small datasets, we construct a probabilistic framework for document-query pairs to maximize the likelihood of the objective permutation of top-$\tau$ documents. This iterative procedure ensures convergence. Furthermore, we integrate regression trees as our weak learners in order to consider the correlation between the different features. Experiments on LETOR datasets and two large datasets, Yahoo challenge data and Microsoft 30K web data, show an improvement over state-of-the-art systems.",2013,Knowledge Discovery and Data Mining,Fields of study: ranking svmlearning to ranksupervised learningdata miningpattern recognitionmachine learningstatisticscomputer sciencemathematics
A time-dependent enhanced support vector machine for time series regression,Goce Ristanoski (University of Melbourne)Wei Liu (University of Melbourne)James Bailey (University of Melbourne),"164497747,2172864801,2131557737","Support Vector Machines (SVMs) are a leading tool in machine learning and have been used with considerable success for the task of time series forecasting. However, a key challenge when using SVMs for time series is the question of how to deeply integrate time elements into the learning process. To address this challenge, we investigated the distribution of errors in the forecasts delivered by standard SVMs. Once we identified the samples that produced the largest errors, we observed their correlation with distribution shifts that occur in the time series. This motivated us to propose a time-dependent loss function which allows the inclusion of the information about the distribution shifts in the series directly into the SVM learning process. We present experimental results which indicate that using a time-dependent loss function is highly promising, reducing the overall variance of the errors, as well as delivering more accurate predictions.",2013,Knowledge Discovery and Data Mining,Fields of study: relevance vector machinesupport vector machineloss functiontime seriesdata miningpattern recognitionmachine learningcomputer science
Algorithmic techniques for modeling and mining large graphs (AMAzING),Alan M. Frieze (Carnegie Mellon University)Aristides Gionis (Aalto University)Charalampos E. Tsourakakis (Harvard University),"2036959358,737311942,750472553","Network science has emerged over the last years as an interdisciplinary area spanning traditional domains including mathematics, computer science, sociology, biology and economics. Since complexity in social, biological and economical systems, and more generally in complex systems, arises through pairwise interactions there exists a surging interest in understanding networks. In this tutorial, we will provide an in-depth presentation of the most popular random-graph models used for modeling real-world networks. We will then discuss efficient algorithmic techniques for mining large graphs, with emphasis on the problems of extracting graph sparsifiers, partitioning graphs into densely connected components, and finding dense subgraphs. We will motivate the problems we will discuss and the algorithms we will present with real-world applications. Our aim is to survey important results in the areas of modeling and mining large graphs, to uncover the intuition behind the key ideas, and to present future research directions.",2013,Knowledge Discovery and Data Mining,Fields of study: association rule learningcluster analysisdata sciencedata miningartificial intelligencemachine learningstatisticscomputer sciencemathematics
A unified search federation system based on online user feedback,Luo Jie (Yahoo!)Sudarshan Lamkhede (Yahoo!)Rochit Sapra (Yahoo!)Evans Hsu (Yahoo!)Helen Song (Yahoo!)Yi Chang (Yahoo!),"2095673574,148984349,2222683383,2225838884,2226440226,2168000538","Today's popular web search engines expand the search process beyond crawled web pages to specialized corpora (""verticals"") like images, videos, news, local, sports, finance, shopping etc., each with its own specialized search engine. Search federation deals with problems of the selection of search engines to query and merging of their results into a single result set. Despite a few recent advances, the problem is still very challenging. First, due to the heterogeneous nature of different verticals, how the system merges the vertical results with the web documents to serve the user's information need is still an open problem. Moreover, the scale of the search engine and the increasing number of vertical properties requires a solution which is efficient and scaleable. In this paper, we propose a unified framework for the search federation problem. We model the search federation as a contextual bandit problem. The system uses reward as a proxy for user satisfaction. Given a query, our system predicts the expected reward for each vertical, then organizes the search result page (SERP) in a way which maximizes the total reward. Instead of relying on human judges, our system leverages implicit user feedback to learn the model. The method is efficient to implement and can be applied to verticals of different nature. We have successfully deployed the system to three different markets, and it handles multiple verticals in each market. The system is now serving hundreds of millions of queries live each day, and has improved user metrics considerably.",2013,Knowledge Discovery and Data Mining,Fields of study: search analyticsbeam searchweb search querysearch engineworld wide webdata miningmachine learningsimulationcomputer science
Estimating sharer reputation via social data calibration,Jaewon Yang (Stanford University)Bee-Chung Chen (LinkedIn)Deepak Agarwal (LinkedIn),"2131921352,2152441490,2116605949","Online social networks have become important channels for users to share content with their connections and diffuse information. Although much work has been done to identify socially influential users, the problem of finding ""reputable"" sharers, who share good content , has received relatively little attention. Availability of such reputation scores can be useful or various applications like recommending people to follow, procuring high quality content in a scalable way, creating a content reputation economy to incentivize high quality sharing, and many more. To estimate sharer reputation, it is intuitive to leverage data that records how recipients respond (through clicking, liking, etc.) to content items shared by a sharer. However, such data is usually biased --- it has a selection bias since the shared items can only be seen and responded to by users connected to the sharer in most social networks, and it has a response bias since the response is usually influenced by the relationship between the sharer and the recipient (which may not indicate whether the shared content is good). To correct for such biases, we propose to utilize an additional data source that provides unbiased goodness estimates for a small set of shared items, and calibrate biased social data through a novel multi-level hierarchical model that describes how the unbiased data and biased data are jointly generated according to sharer reputation scores. The unbiased data also provides the ground truth for quantitative evaluation of different methods. Experiments based on such ground-truth data show that our proposed model significantly outperforms existing methods that estimate social influence using biased social data.",2013,Knowledge Discovery and Data Mining,Fields of study: internet privacyworld wide webdata miningstatisticscomputer science
A fast and scalable clustering-based approach for constructing reliable radiation hybrid maps,Raed I. Seetan (North Dakota State University)Ajay Kumar (North Dakota State University)Anne M. Denton (North Dakota State University)M. Javed Iqbal (North Dakota State University)Omar Al-Azzam (University of Minnesota)Shahryar F. Kianian (North Dakota State University),"1978455809,2311919934,2126706684,2141029284,2182530555,2296924123","The process of mapping markers from radiation hybrid mapping (RHM) experiments is equivalent to the traveling salesman problem and, thereby, has combinatorial complexity. As an additional problem, experiments typically result in some unreliable markers that reduce the overall quality of the map. We propose a clustering approach for addressing both problems efficiently by eliminating unreliable markers without the need for mapping the complete set of markers. Traditional approaches for eliminating markers use resampling of the full data set, which has an even higher computational complexity than the original mapping problem. In contrast, the proposed approach uses a divide and conquer strategy to construct framework maps based on clusters that exclude unreliable markers. Clusters are ordered using parallel processing and are then combined to form the complete map. Using an RHM data set of the human genome, we compare the framework maps from our proposed approaches with published physical maps and with the Carthagene tool. Overall, our approach has a very low computational complexity and produces solid framework maps with good chromosome coverage and high agreement with the physical map marker order.",2013,Knowledge Discovery and Data Mining,Fields of study: cluster analysistheoretical computer sciencebioinformaticsmachine learningcomputer science
Optimization in learning and data analysis,Stephen J. Wright (University of Wisconsin-Madison),2167218018,"Optimization tools are vital to data analysis and learning. The optimization perspective has provided valuable insights, and optimization formulations have led to practical algorithms with good theoretical properties. In turn, the rich collection of problems in learning and data analysis is providing fresh perspectives on optimization algorithms and is driving new fundamental research in the area. We discuss research on several areas in this domain, including signal reconstruction, manifold learning, and regression/classification, describing in each case recent research in which optimization algorithms have been developed and applied successfully. A particular focus is asynchronous parallel algorithms for optimization and linear algebra, and their applications in data analysis and learning.",2013,Knowledge Discovery and Data Mining,Fields of study: probabilistic based design optimizationonline machine learningengineering optimizationhyperparameter optimizationtest functions for optimizationoptimization problemcombinatorial optimizationmanagement sciencedata miningartificial intelligencemachine learningcomputer science
Dynamic memory allocation policies for postings in real-time Twitter search,"Nima Asadi 0001 (University of Maryland, College Park)Jimmy J. Lin (University of Maryland, College Park)Michael Busch (Twitter)","2103777546,2163619555,2310428244","We explore a real-time Twitter search application where tweets are arriving at a rate of several thousands per second. Real-time search demands that they be indexed and searchable immediately, which leads to a number of implementation challenges. In this paper, we focus on one aspect: dynamic postings allocation policies for index structures that are completely held in main memory. The core issue can be characterized as a ""Goldilocks Problem"". Because memory remains today a scare resource, an allocation policy that is too aggressive leads to inefficient utilization, while a policy that is too conservative is slow and leads to fragmented postings lists. We present a dynamic postings allocation policy that allocates memory in increasingly-larger ""slices"" from a small number of large, fixed pools of memory. With an analytical model and experiments, we explore different settings that balance time (query evaluation speed) and space (memory utilization).",2013,Knowledge Discovery and Data Mining,Fields of study: static memory allocationmemory managementmultimediaworld wide websimulationcomputer science
Towards anytime active learning: interrupting experts to reduce annotation costs,Maria Eugenia Ramirez-Loaiza (Illinois Institute of Technology)Aron Culotta (Illinois Institute of Technology)Mustafa Bilgic (Illinois Institute of Technology),"2006774049,2151487642,2010936866","Many active learning methods use annotation cost or expert quality as part of their framework to select the best data for annotation. While these methods model expert quality, availability, or expertise, they have no direct influence on any of these elements. We present a novel framework built upon decision-theoretic active learning that allows the learner to directly control label quality by allocating a time budget to each annotation. We show that our method is able to improve performance efficiency of the active learner through an interruption mechanism trading off the induced error with the cost of annotation. Our simulation experiments on three document classification tasks show that some interruption is almost always better than none, but that the optimal interruption time varies by dataset.",2013,Knowledge Discovery and Data Mining,Fields of study: active learningvalue of informationdata miningmachine learningsimulationcomputer science
Fast and exact network trajectory similarity computation: a case-study on bicycle corridor planning,Michael R. Evans (University of Minnesota)Dev Oliver (University of Minnesota)Shashi Shekhar (University of Minnesota)Francis Harvey (University of Minnesota),"2117207139,2121583490,2134885186,2460558526","Given a set of trajectories on a road network, the goal of the All-Pair Network Trajectory Similarity (APNTS) problem is to calculate the similarity between all trajectories using the Network Hausdorff Distance. This problem is important for a variety of societal applications, such as facilitating greener travel via bicycle corridor identification. The APNTS problem is challenging due to the high cost of computing the exact Network Hausdorff Distance between trajectories in spatial big datasets. Previous work on the APNTS problem takes over 16 hours of computation time on a real-world dataset of bicycle GPS trajectories in Minneapolis, MN. In contrast, this paper focuses on a scalable method for the APNTS problem using the idea of row-wise computation, resulting in a computation time of less than 6 minutes on the same datasets. We provide a case study for transportation services using a data-driven approach to identify primary bicycle corridors for public transportation by leveraging emerging GPS trajectory datasets.",2013,Knowledge Discovery and Data Mining,Fields of study: machine learningsimulationmathematical optimization
Query clustering based on bid landscape for sponsored search auction optimization,Ye Chen (Microsoft)Weiguo Liu (Microsoft)Jeonghee Yi (Microsoft)Anton Schwaighofer (Microsoft)Tak W. Yan (Microsoft),"2628943388,2150598481,2226691631,2618515328,2139394280","In sponsored search auctions, the auctioneer operates the marketplace by setting a number of auction parameters such as reserve prices for the task of auction optimization. The auction parameters may be set for each individual keyword, but the optimization problem becomes intractable since the number of keywords is in the millions. To reduce the dimensionality and generalize well, one wishes to cluster keywords or queries into meaningful groups, and set parameters at the keyword-cluster level. For auction optimization, keywords shall be deemed as interchangeable commodities with respect to their valuations from advertisers, represented as bid distributions or landscapes. Clustering keywords for auction optimization shall thus be based on their bid distributions. In this paper we present a formalism of clustering probability distributions, and its application to query clustering where each query is represented as a probability density of click-through rate (CTR) weighted bid and distortion is measured by KL divergence. We first derive a k-means variant for clustering Gaussian densities, which have a closed-form KL divergence. We then develop an algorithm for clustering Gaussian mixture densities, which generalize a single Gaussian and are typically a more realistic parametric assumption for real-world data. The KL divergence between Gaussian mixture densities is no longer analytically tractable; hence we derive a variational EM algorithm that minimizes an upper bound of the total within-cluster KL divergence. The clustering algorithm has been deployed successfully into production, yielding significant improvement in revenue and clicks over the existing production system. While motivated by the specific setting of query clustering, the proposed clustering method is generally applicable to many real-world applications where an example is better characterized by a distribution than a finite-dimensional feature vector in Euclidean space as in the classical k-means.",2013,Knowledge Discovery and Data Mining,Fields of study: k medians clusteringcanopy clustering algorithmauction algorithmcorrelation clusteringconstrained clusteringdata stream clusteringcure data clustering algorithmfuzzy clusteringbayesian probabilitycluster analysisdata miningmachine learningmathematical optimizationstatisticscomputer sciencemathematics
Mining for geographically disperse communities in social networks by leveraging distance modularity,"Paulo Shakarian (Military Academy)Patrick Roos (University of Maryland, College Park)Devon Callahan (Military Academy)Cory Kirk (Military Academy)","102302551,2667530482,2025179199,2399923717","Social networks where the actors occupy geospatial locations are prevalent in military, intelligence, and policing operations such as counter-terrorism, counter-insurgency, and combating organized crime. These networks are often derived from a variety of intelligence sources. The discovery of communities that are geographically disperse stems from the requirement to identify higher-level organizational structures, such as a logistics group that provides support to various geographically disperse terrorist cells. We apply a variant of Newman-Girvan modularity to this problem known as distance modularity. To address the problem of finding geographically disperse communities, we modify the well-known Louvain algorithm to find partitions of networks that provide near-optimal solutions to this quantity. We apply this algorithm to numerous samples from two real-world social networks and a terrorism network data set whose nodes have associated geospatial locations. Our experiments show this to be an effective approach and highlight various practical considerations when applying the algorithm to distance modularity maximization. Several military, intelligence, and law-enforcement organizations are working with us to further test and field software for this emerging application.",2013,Knowledge Discovery and Data Mining,Fields of study: terrorismcomplex networklogisticsreasonmanagement scienceworld wide webcomputer securitydata miningmachine learningcomputer science
Succinct interval-splitting tree for scalable similarity search of compound-protein pairs with property constraints,Yasuo Tabei (National Presto Industries)Akihiro Kishimoto (IBM)Masaaki Kotera (Kyoto University)Yoshihiro Yamanishi (Kyushu University),"2094849311,2128994182,1985752241,2029139975","Analyzing functional interactions between small compounds and proteins is indispensable in genomic drug discovery. Since rich information on various compound-protein inter- actions is available in recent molecular databases, strong demands for making best use of such databases require to in- vent powerful methods to help us find new functional compound-protein pairs on a large scale. We present the succinct interval-splitting tree algorithm (SITA) that efficiently per- forms similarity search in databases for compound-protein pairs with respect to both binary fingerprints and real-valued properties. SITA achieves both time and space efficiency by developing the data structure called interval-splitting trees, which enables to efficiently prune the useless portions of search space, and by incorporating the ideas behind wavelet tree, a succinct data structure to compactly represent trees. We experimentally test SITA on the ability to retrieve similar compound-protein pairs/substrate-product pairs for a query from large databases with over 200 million compound- protein pairs/substrate-product pairs and show that SITA performs better than other possible approaches.",2013,Knowledge Discovery and Data Mining,Fields of study: wavelet treesuccinct data structuretheoretical computer sciencedata miningmachine learningcomputer sciencemathematics
Quadratic optimization to identify highly heritable quantitative traits from complex phenotypic features,Jiangwen Sun (University of Connecticut)Jinbo Bi (University of Connecticut)Henry R. Kranzler (University of Pennsylvania),"2096599005,2066975796,598558924","Identifying genetic variation underlying a complex disease is important. Many complex diseases have heterogeneous phenotypes and are products of a variety of genetic and environmental factors acting in concert. Deriving highly heritable quantitative traits of a complex disease can improve the identification of genetic risk of the disease. The most sophisticated methods so far perform unsupervised cluster analysis on phenotypic features; and then a quantitative trait is derived based on each resultant cluster. Heritability is estimated to assess the validity of the derived quantitative traits. However, none of these methods explicitly maximize the heritability of the derived traits. We propose a quadratic optimization approach that directly utilizes heritability as an objective during the derivation of quantitative traits of a disease. This method maximizes an objective function that is formulated by decomposing the traditional maximum likelihood method for estimating heritability of a quantitative trait. We demonstrate the effectiveness of the proposed method on both synthetic data and real-world problems. We apply our algorithm to identify highly heritable traits of complex human-behavior disorders including opioid and cocaine use disorders, and highly heritable traits of dairy cattle that are economically important. Our approach outperforms standard cluster analysis and several previous methods.",2013,Knowledge Discovery and Data Mining,Fields of study: genetic architecturegenetic correlationquantitative trait locusheritabilityquadratic programmingbioinformatics
To buy or not to buy: that is the question,Oren Etzioni (University of Washington),57747768,"Shopping can be decomposed into three basic questions: what, where, and when to buy? In this talk, I'll describe how we utilize advanced data-mining and text-mining techniques at Decide.com (and earlier at Farecast) to solve these problems for on-line shoppers. Our algorithms have predicted prices utilizing billions of data points, and ranked products based on millions of reviews.",2013,Knowledge Discovery and Data Mining,Fields of study: data miningcomputer science
Collaborative boosting for activity classification in microblogs,Yangqiu Song (Hong Kong University of Science and Technology)Zhengdong Lu (Huawei)Cane Wing-ki Leung (Huawei)Qiang Yang (Huawei),"2099747503,2693494178,2115678292,2109031554","Users' daily activities, such as dining and shopping, inherently reflect their habits, intents and preferences, thus provide invaluable information for services such as personalized information recommendation and targeted advertising. Users' activity information, although ubiquitous on social media, has largely been unexploited. This paper addresses the task of user activity classification in microblogs, where users can publish short messages and maintain social networks online. We identify the importance of modeling a user's individuality, and that of exploiting opinions of the user's friends for accurate activity classification. In this light, we propose a novel collaborative boosting framework comprising a text-to-activity classifier for each user, and a mechanism for collaboration between classifiers of users having social connections. The collaboration between two classifiers includes exchanging their own training instances and their dynamically changing labeling decisions. We propose an iterative learning procedure that is formulated as gradient descent in learning function space, while opinion exchange between classifiers is implemented with a weighted voting in each learning iteration. We show through experiments that on real-world data from Sina Weibo, our method outperforms existing off-the-shelf algorithms that do not take users' individuality or social connections into account.",2013,Knowledge Discovery and Data Mining,Fields of study: boostingdata sciencedata miningmachine learningcomputer science
Exploratory analysis of highly heterogeneous document collections,Arun S. Maiya (Institute for Defense Analyses)John P. Thompson (Institute for Defense Analyses)Francisco Loaiza-Lemos (Institute for Defense Analyses)Robert M. Rolfe (Institute for Defense Analyses),"2225019251,2288445732,2013904602,2480102029","We present an effective multifaceted system for exploratory analysis of highly heterogeneous document collections. Our system is based on intelligently tagging individual documents in a purely automated fashion and exploiting these tags in a powerful faceted browsing framework. Tagging strategies employed include both unsupervised and supervised approaches based on machine learning and natural language processing. As one of our key tagging strategies, we introduce the KERA algorithm (Keyword Extraction for Reports and Articles). KERA extracts topic-representative terms from individual documents in a purely unsupervised fashion and is revealed to be significantly more effective than state-of-the-art methods. Finally, we evaluate our system in its ability to help users locate documents pertaining to military critical technologies buried deep in a large heterogeneous sea of information.",2013,Knowledge Discovery and Data Mining,Fields of study: tag cloudtopic modelworld wide webinformation retrievaldata miningmachine learningcomputer science
Targeting and influencing at scale: from presidential elections to social good,Rayid Ghani (University of Chicago),2655769349,"If you're still recovering from the barrage of ads, news, emails, Facebook posts, and newspaper articles that were giving you the latest poll numbers, asking you to volunteer, donate money, and vote, this talk will give you a look behind the scenes on why you were seeing what you were seeing. I will talk about how machine learning and data mining along with randomized experiments were used to target and influence tens of millions of people. Beyond the presidential elections, these methodologies for targeting and influence have the power to solve big problems in education, healthcare, energy, transportation, and related areas. I will talk about some recent work we're doing at the University of Chicago Data Science for Social Good summer fellowship program working with non-profits and government organizations to tackle some of these challenges.",2013,Knowledge Discovery and Data Mining,Fields of study: targetingsocial networkdata mining
Massively parallel expectation maximization using graphics processing units,Muzaffer Can Altinigneli (Ludwig Maximilian University of Munich)Claudia Plant (Florida State University)Christian Böhm (Ludwig Maximilian University of Munich),"2230377636,2122910652,2486446532","Composed of several hundreds of processors, the Graphics Processing Unit (GPU) has become a very interesting platform for computationally demanding tasks on massive data. A special hierarchy of processors and fast memory units allow very powerful and efficient parallelization but also demands novel parallel algorithms. Expectation Maximization (EM) is a widely used technique for maximum likelihood estimation. In this paper, we propose an innovative EM clustering algorithm particularly suited for the GPU platform on NVIDIA's Fermi architecture. The central idea of our algorithm is to allow the parallel threads exchanging their local information in an asynchronous way and thus updating their cluster representatives on demand by a technique called Asynchronous Model Updates (Async-EM). Async-EM enables our algorithm not only to accelerate convergence but also to reduce the overhead induced by memory bandwidth limitations and synchronization requirements. We demonstrate (1) how to reformulate the EM algorithm to be able to exchange information using Async-EM and (2) how to exploit the special memory and processor architecture of a modern GPU in order to share this information among threads in an optimal way. As a perspective Async-EM is not limited to EM but can be applied to a variety of algorithms.",2013,Knowledge Discovery and Data Mining,Fields of study: general purpose computing on graphics processing unitsfermi gamma ray space telescopeexpectation maximization algorithmtheoretical computer scienceparallel computingreal time computingmachine learningcomputer science
Network sampling,Mohammad A. Hasan (Indiana University – Purdue University Indianapolis)Jennifer Neville (Purdue University)Nesreen Ahmed (Purdue University),"2430381672,2124572662,2119838086","Network data appears in various domains, including social, communication, and information sciences. Analysis of such data is crucial for making inferences and predictions about these networks, and moreover, for understanding the different processes that drive their evolution. However, a major bottleneck to perform such an analysis is the massive size of real-life networks, which makes modeling and analyzing these networks simply infeasible. Further, many networks, specifically those that belong to social and communication domains, are not visible to the public due to privacy concerns, and other networks, such as the Web, are only accessible via crawling. Therefore, to overcome the above challenges, researchers use network sampling overwhelmingly as a key statistical approach to select a sub-population of interest that can be studied thoroughly. In this tutorial, we aim to cover a diverse collection of methodologies and applications of network sampling. We will begin with a discussion of the problem setting in terms of objectives (such as, sampling a representative subgraph, sampling graphlets, etc.), population of interest (vertices, edges, motifs), and sampling methodologies (such as Metropolis-Hastings, random walk, and snowball sampling). We will then present a number of applications of these methods, and will outline both the resulting opportunities and possible biases of different methods in each application.",2013,Knowledge Discovery and Data Mining,Fields of study: association rule learningcluster analysisdata sciencedata miningartificial intelligencemachine learningstatisticscomputer sciencemathematics
Repetition-aware content placement in navigational networks,Dora Erdos (Boston University)Vatche Ishakian (BBN Technologies)Azer Bestavros (Boston University)Evimaria Terzi (Boston University),"2035702190,2016095920,298297050,2110675235","Arguably, the most effective technique to ensure wide adoption of a concept (or product) is by repeatedly exposing individuals to messages that reinforce the concept (or promote the product). Recognizing the role of repeated exposure to a message, in this paper we propose a novel framework for the effective placement of content: Given the navigational patterns of users in a network, e.g. , web graph, hyperlinked corpus, or road network, and given a model of the relationship between content-adoption and frequency of exposition, we define the repetition-aware content-placement (RACP) problem as that of identifying the set of B nodes on which content should be placed so that the expected number of users adopting that content is maximized. The key contribution of our work is the introduction of memory into the navigation process, by making user conversion dependent on the number of her exposures to that content. This dependency is captured using a conversion model that is general enough to capture arbitrary dependencies. Our solution to this general problem builds upon the notion of absorbing random walks, which we extend appropriately in order to address the technicalities of our definitions. Although we show the RACP problem to be NP-hard, we propose a general and efficient algorithmic solution. Our experimental results demonstrate the efficacy and the efficiency of our methods in multiple real-world datasets obtained from different application domains.",2013,Knowledge Discovery and Data Mining,Fields of study: markov chainmultimediadata miningmachine learningsimulationstatisticscomputer science
Outlier detection in personalized medicine,Raymond Ng (University of British Columbia),2109621001,"Personalized medicine has been hailed as one of the main directions for medical research in this century. In the first half of the talk, we give an overview on our personalized medicine projects that use gene expression, proteomics, DNA and clinical features. In the second half, we give two applications where outlier detection is valuable for the success of our work. The first one focuses on identifying mislabeled patients, and the second one deals with quality control of microarrays.",2013,Knowledge Discovery and Data Mining,Fields of study: bioinformaticsdata mining
Storygraph: extracting patterns from spatio-temporal data,Ayush Shrestha (Georgia State University)Ben Miller (Georgia State University)Ying Zhu (Georgia State University)Yi Zhao (Georgia State University),"1985003857,2146168445,2695604644,2672891910","Analysis of spatio-temporal data often involves correlating different events in time and location to uncover relationships between them. It is also desirable to identify different patterns in the data. Visualizing time and space in the same chart is not trivial. Common methods includes plotting the latitude, longitude and time as three dimensions of a 3D chart. Drawbacks of these 3D charts include not being able to scale well due to cluttering, occlusion and difficulty to track time in case of clustered events. In this paper we present a novel 2D visualization technique called Storygraph which provides an integrated view of time and location to address these issues. We also present storylines based on Storygraph which show movement of the actors over time. Lastly, we present case studies to show the applications of Storygraph.",2013,Knowledge Discovery and Data Mining,Fields of study: data sciencedata miningcomputer science
Improving quality control by early prediction of manufacturing outcomes,Sholom M. Weiss (IBM)Amit Dhurandhar (IBM)Robert J. Baseman (IBM),"2128066252,2061123877,695769674","We describe methods for continual prediction of manufactured product quality prior to final testing. In our most expansive modeling approach, an estimated final characteristic of a product is updated after each manufacturing operation. Our initial application is for the manufacture of microprocessors, and we predict final microprocessor speed. Using these predictions, early corrective manufacturing actions may be taken to increase the speed of expected slow wafers (a collection of microprocessors) or reduce the speed of fast wafers. Such predictions may also be used to initiate corrective supply chain management actions. Developing statistical learning models for this task has many complicating factors: (a) a temporally unstable population (b) missing data that is a result of sparsely sampled measurements and (c) relatively few available measurements prior to corrective action opportunities. In a real manufacturing pilot application, our automated models selected 125 fast wafers in real-time. As predicted, those wafers were significantly faster than average. During manufacture, downstream corrective processing restored 25 nominally unacceptable wafers to normal operation.",2013,Knowledge Discovery and Data Mining,Fields of study: manufacturingquality controlpredictiondata miningstatistics
SEA: a system for event analysis on chinese tweets,Yaqiong Wang (Beihang University)Hongfu Liu (Beihang University)Hao Lin (Beihang University)Junjie Wu (Beihang University)Zhiang Wu (Nanjing University of Finance and Economics)Jie Cao (Nanjing University of Finance and Economics),"2225970081,2647258088,2293401935,2149366604,2701292483,2659082948","Recent years have witnessed the explosive growth of online social media. Weibo, a famous ""Chinese Twitter"", has attracted over 0.5 billion users in less than four years, with more than 1000 tweets generated in every second. These tweets are informative but very fragmented, and thus would be better archived from an event perspective, as done by Weibo itself in the ""Micro-Topic"" program. This effort, however, is yet far from satisfaction for not providing enough analytical power to events. In light of this, in this demo paper, we propose SEA, a System for Event Analysis on Chinese tweets. In general, SEA is an event-centric, multi-functional platform that conducts panoramic analysis on Weibo events from various aspects, including the semantic information of the events, the temporal and spatial trends, the public sentiments, the hidden sub-events, the key users in the event diffusion and their preferences, etc. These functions are enabled by the integration of various analytical models and by the noSQL techniques adopted purposefully for massive tweets management. Finally, a case study on the ""Spring Festival"" event demonstrates the effectiveness of SEA. To our best knowledge, SEA is the first third-party system that provides panoramic analysis to Weibo events.",2013,Knowledge Discovery and Data Mining,Fields of study: internet privacyworld wide webdata mining
Robust sparse estimation of multiresponse regression and inverse covariance matrix via the L2 distance,Aurelie C. Lozano (IBM)Huijing Jiang (IBM)Xinwei Deng (Virginia Tech),"2134160163,2634008210,2703442297","We propose a robust framework to jointly perform two key modeling tasks involving high dimensional data: (i) learning a sparse functional mapping from multiple predictors to multiple responses while taking advantage of the coupling among responses, and (ii) estimating the conditional dependency structure among responses while adjusting for their predictors. The traditional likelihood-based estimators lack resilience with respect to outliers and model misspecification. This issue is exacerbated when dealing with high dimensional noisy data. In this work, we propose instead to minimize a regularized distance criterion, which is motivated by the minimum distance functionals used in nonparametric methods for their excellent robustness properties. The proposed estimates can be obtained efficiently by leveraging a sequential quadratic programming algorithm. We provide theoretical justification such as estimation consistency for the proposed estimator. Additionally, we shed light on the robustness of our estimator through its linearization, which yields a combination of weighted lasso and graphical lasso with the sample weights providing an intuitive explanation of the robustness. We demonstrate the merits of our framework through simulation study and the analysis of real financial and genetics data.",2013,Knowledge Discovery and Data Mining,Fields of study: clustering high dimensional datafeature selectioneconometricsmachine learningstatisticscomputer sciencemathematics
Exact sparse recovery with L0 projections,Ping Li (Cornell University)Cun-Hui Zhang (Rutgers University),"2721445647,2691215079","Many applications (e.g., anomaly detection) concern sparse signals. This paper focuses on the problem of recovering a K -sparse signal x ∈ R /1× N , i.e., K N and ∑ N / i =1 1{ x i ≠ 0} = K . In the mainstream framework of compressed sensing (CS), × is recovered from M linear measurements y = xS ∈ R /1× M , where S ∈ R N × M is often a Gaussian (or Gaussian-like) design matrix. In our proposed method, the design matrix S is generated from an α-stable distribution with α ≈ 0. Our decoding algorithm mainly requires one linear scan of the coordinates, followed by a few iterations on a small number of coordinates which are ""undetermined"" in the previous iteration. Our practical algorithm consists of two estimators. In the first iteration, the (absolute) minimum estimator is able to filter out a majority of the zero coordinates. The gap estimator , which is applied in each iteration, can accurately recover the magnitudes of the nonzero coordinates. Comparisons with linear programming (LP) and orthogonal matching pursuit (OMP) demonstrate that our algorithm can be significantly faster in decoding speed and more accurate in recovery quality, for the task of exact spare recovery. Our procedure is robust against measurement noise. Even when there are no sufficient measurements, our algorithm can still reliably recover a significant portion of the nonzero coordinates.",2013,Knowledge Discovery and Data Mining,Fields of study: compressed sensingtheoretical computer sciencemachine learningmathematical optimizationstatisticscomputer sciencemathematics
Constrained stochastic gradient descent for large-scale least squares problem,"Yang Mu (University of Massachusetts Boston)Wei Ding (University of Massachusetts Boston)Tianyi Zhou (University of Technology, Sydney)Dacheng Tao (University of Technology, Sydney)","2106579702,2116574971,2153604853,2104129307","The least squares problem is one of the most important regression problems in statistics, machine learning and data mining. In this paper, we present the Constrained Stochastic Gradient Descent (CSGD) algorithm to solve the large-scale least squares problem. CSGD improves the Stochastic Gradient Descent (SGD) by imposing a provable constraint that the linear regression line passes through the mean point of all the data points. It results in the best regret bound $O(\log{T})$, and fastest convergence speed among all first order approaches. Empirical studies justify the effectiveness of CSGD by comparing it with SGD and other state-of-the-art approaches. An example is also given to show how to use CSGD to optimize SGD based least squares problems to achieve a better performance.",2013,Knowledge Discovery and Data Mining,Fields of study: total least squaresstochastic gradient descentnon linear least squaresleast squaresstochastic optimizationmachine learningmathematical optimizationstatisticscomputer sciencemathematics
Nonparametric hierarchal bayesian modeling in non-contractual heterogeneous survival data,Shouichi NaganoYusuke IchikawaNoriko Takaya (Nippon Telegraph and Telephone)Tadasu UchiyamaMakoto Abe (University of Tokyo),"2411663119,2129675869,2584052289,2640245979,2167838943","An important problem in the non-contractual marketing domain is discovering the customer lifetime and assessing the impact of customer's characteristic variables on the lifetime. Unfortunately, the conventional hierarchical Bayes model cannot discern the impact of customer's characteristic variables for each customer. To overcome this problem, we present a new survival model using a non-parametric Bayes paradigm with MCMC. The assumption of a conventional model, logarithm of purchase rate and dropout rate with linear regression, is extended to include our assumption of the Dirichlet Process Mixture of regression. The extension assumes that each customer belongs probabilistically to different mixtures of regression, thereby permitting us to estimate a different impact of customer characteristic variables for each customer. Our model creates several customer groups to mirror the structure of the target data set. The effectiveness of our proposal is confirmed by a comparison involving a real e-commerce transaction dataset and an artificial dataset; it generally achieves higher predictive performance. In addition, we show that preselecting the actual number of customer groups does not always lead to higher predictive performance.",2013,Knowledge Discovery and Data Mining,Fields of study: markov chain monte carlocustomer relationship managementeconometricsdata miningmachine learningstatisticscomputer science
"Whose ""city of tomorrow"" is it?: on urban computing, utopianism, and ethics",Justin Cranshaw (Carnegie Mellon University),2100888799,"In this article I discuss some ethical and moral ramifications of the future envisioned by urban computing. In doing so, I make analogies to twentieth century utopian visions of the ""city of tomorrow,"" so that we might see the historical context of a similar field with similar utopian instincts. I hope this context helps us better understand how our work might affect the lives of city dwellers in profound ways that we may never fully foresee. I discuss ethical questions related to using urban computing for policy making, for real-estate development, and for surveillance. I also define the concept of distributed sensing, and discuss some difficult regulatory questions that surround it. I hope this work inspires urban computing researchers to think critically in order to assess societal implications of the technologies they develop.",2013,Knowledge Discovery and Data Mining,Fields of study: ethicsarchitectureurban designmanagement science
Summarizing probabilistic frequent patterns: a fast approach,"Chunyang Liu (University of Technology, Sydney)Ling Chen (University of Technology, Sydney)Chengqi Zhang (University of Technology, Sydney)","2502197411,2633547329,2166080598","Mining probabilistic frequent patterns from uncertain data has received a great deal of attention in recent years due to the wide applications. However, probabilistic frequent pattern mining suffers from the problem that an exponential number of result patterns are generated, which seriously hinders further evaluation and analysis. In this paper, we focus on the problem of mining probabilistic representative frequent patterns (P-RFP), which is the minimal set of patterns with adequately high probability to represent all frequent patterns. Observing the bottleneck in checking whether a pattern can probabilistically represent another, which involves the computation of a joint probability of the supports of two patterns, we introduce a novel approximation of the joint probability with both theoretical and empirical proofs. Based on the approximation, we propose an Approximate P-RFP Mining (APM) algorithm, which effectively and efficiently compresses the set of probabilistic frequent patterns. To our knowledge, this is the first attempt to analyze the relationship between two probabilistic frequent patterns through an approximate approach. Our experiments on both synthetic and real-world datasets demonstrate that the APM algorithm accelerates P-RFP mining dramatically, orders of magnitudes faster than an exact solution. Moreover, the error rate of APM is guaranteed to be very small when the database contains hundreds transactions, which further affirms APM is a practical solution for summarizing probabilistic frequent patterns.",2013,Knowledge Discovery and Data Mining,Fields of study: data sciencedata miningmachine learningstatisticscomputer science
Proceedings of the Second International Workshop on Issues of Sentiment Discovery and Opinion Mining,Erik Cambria (National University of Singapore)Bing Liu (University of Illinois at Chicago)Yongzheng Zhang (eBay)Yunqing Xia (Tsinghua University),"1974519269,2244698799,2705560853,2129890741","The exponential growth of the Social Web is virally infecting more and more critical business processes such as customer support and satisfaction, brand and reputation management, product design and marketing. Because of this global trend, web users already evolved from the era of social relationships, in which they began to get connected and started to share contents, to the era of social functionality, in which they started using social networks as the main platform for communication and dissemination of information. Today, web users are going through the era of social colonization, in which every experience on the Web can be social (e.g., Facebook Like button), and are getting ready for the era of social context, in which web contents will be highly targeted and personalized. The final stage of such Social Web evolution is the so called era of social commerce, in which communities will define future products and services. In such context, the research field of sentiment analysis, which has already been rapidly growing in the last decade, is destined to become more and more important for Web and business dynamics. To this end, the Second International Workshop on Issues of Sentiment Discovery and Opinion Mining (WISDOM 2013: http://sentic.net/wisdom) aims to explore how the wisdom of the crowds is affecting (and will affect) the evolution of the Web and of businesses gravitating around it. In particular, the workshop explores two different stages of sentiment analysis: the former focusing on the identification of opinionated text over the Web, the latter focusing on the classification of such text either in terms of polarity detection or emotion recognition.",2013,Knowledge Discovery and Data Mining,Fields of study: social semantic websocial webdata scienceworld wide webdata mining
Indexed block coordinate descent for large-scale linear classification with limited memory,Ian En-Hsu Yen (National Taiwan University)Chun-Fu Chang (National Taiwan University)Ting-Wei Lin (National Taiwan University)Shan-Wei Lin (National Taiwan University)Shou-De Lin (National Taiwan University),"2162095421,2110228645,2140714400,2300711308,2114357324","Linear Classification has achieved complexity linear to the data size. However, in many applications, data contain large amount of samples that does not help improve the quality of model, but still cost much I/O and memory to process. In this paper, we show how a Block Coordinate Descent method based on Nearest-Neighbor Index can significantly reduce such cost when learning a dual-sparse model. In particular, we employ truncated loss function to induce a series of convex programs with superior dual sparsity, and solve each dual using Indexed Block Coordinate Descent, which makes use of Approximate Nearest Neighbor (ANN) search to select active dual variables without I/O cost on irrelevant samples. We prove that, despite the bias and weak guarantee from ANN query, the proposed algorithm has global convergence to the solution defined on entire dataset, with sublinear complexity each iteration. Experiments in both sufficient and limited memory conditions show that the proposed approach learns many times faster than other state-of-the-art solvers without sacrificing accuracy.",2013,Knowledge Discovery and Data Mining,Fields of study: coordinate descentsearch engine indexingk nearest neighbors algorithmbiological classificationdata miningmachine learningmathematical optimizationstatisticsalgorithmcomputer sciencemathematics
Solving combinatorial optimization problems using relaxed linear programming: a high performance computing perspective,Chen Jin (Northwestern University)Qiang Fu (University of Minnesota)Huahua Wang (University of Minnesota)Ankit Agrawal (Northwestern University)William Hendrix (Northwestern University)Wei-keng Liao (Northwestern University)Md. Mostofa Ali Patwary (Northwestern University)Arindam Banerjee (University of Minnesota)Alok N. Choudhary (Northwestern University),"2151364674,2293941420,2143868435,2160807299,2140799759,2161046949,2305409651,2037585042,2147783234",Several important combinatorial optimization problems can be formulated as maximum a posteriori (MAP) inference in discrete graphical models. We adopt the recently proposed parallel MAP inference algorithm Bethe-ADMM and implement it using message passing interface (MPI) to fully utilize the computing power provided by the modern supercomputers with thousands of cores. The empirical results show that our parallel implementation scales almost linearly even with thousands of cores.,2013,Knowledge Discovery and Data Mining,Fields of study: variable eliminationmessage passing interfacetheoretical computer sciencedistributed computingmachine learningmathematical optimizationcomputer science
Exploiting user clicks for automatic seed set generation for entity matching,Xiao Bai 0002 (Yahoo!)Flavio Paiva Junqueira (Microsoft)Srinivasan H. Sengamedu (Yahoo!),"2104717575,2308249582,1823577077","Matching entities from different information sources is a very important problem in data analysis and data integration. It is, however, challenging due to the number and diversity of information sources involved, and the significant editorial efforts required to collect sufficient training data. In this paper, we present an approach that leverages user clicks during Web search to automatically generate training data for entity matching. The key insight of our approach is that Web pages clicked for a given query are likely to be about the same entity. We use random walk with restart to reduce data sparseness, rely on co-clustering to group queries and Web pages, and exploit page similarity to improve matching precision. Experimental results show that: ( i ) With 360K pages from 6 major travel websites, we obtain 84K matchings (of 179K pages) that refer to the same entities, with an average precision of 0.826; ( ii ) The quality of matching obtained from a classifier trained on the resulted seed data is promising: the performance matches that of editorial data at small size and improves with size.",2013,Knowledge Discovery and Data Mining,Fields of study: random walkbiclusteringworld wide webinformation retrievaldata miningmachine learningstatisticscomputer science
iHR: an online recruiting system for Xiamen Talent Service Center,Wenxing Hong (Xiamen University)Lei Li (Florida International University)Tao Li (Florida International University)Wenfu Pan,"2103986308,2432045905,2472069284,2328488051","Online recruiting systems have gained immense attention in the wake of more and more job seekers searching jobs and enterprises finding candidates on the Internet. A critical problem in a recruiting system is how to maximally satisfy the desires of both job seekers and enterprises with reasonable recommendations or search results. In this paper, we investigate and compare various online recruiting systems from a product perspective. We then point out several key functions that help achieve a win-win situation between job seekers and enterprises for a successful recruiting system. Based on the observations and key functions, we design, implement and deploy a web-based application of recruiting system, named iHR, for Xiamen Talent Service Center. The system utilizes the latest advances in data mining and recommendation technologies to create a user-oriented service for a myriad of audience in job marketing community. Empirical evaluation and online user studies demonstrate the efficacy and effectiveness of our proposed system. Currently, iHR has been deployed at http://i.xmrc.com.cn/XMRCIntel.",2013,Knowledge Discovery and Data Mining,Fields of study: job analysisdata miningsimulation
Panel: a data scientist's guide to making money from start-ups,"Foster J. Provost (New York University)Geoffrey I. Webb (Monash University, Clayton campus)","2158932634,2126304162",-,2013,Knowledge Discovery and Data Mining,Fields of study: moneycomputer science
KDD Cup 2013 - author-paper identification challenge: second place team,Dmitry Efimov (Moscow State University)Lucas SilvaBenjamin Solecki,"2680611598,2236859701,2224151557","This paper describes our submission to the KDD Cup 2013 Track 1 Challenge: Author-Paper Indentification in the Microsoft Academic Search database. Our approach is based on Gradient Boosting Machine (GBM) of Friedman ([5]) and deep feature engineering. The method was second in the final standings with Mean Average Precision (MAP) of 0.98144, while the winning submission scored 0.98259.",2013,Knowledge Discovery and Data Mining,Fields of study: featurecollaborative filteringcross validationdecision treedata sciencedata miningmachine learningcomputer science
Proceedings of the Seventh International Workshop on Data Mining for Online Advertising,Esin Saka (Microsoft)Dou Shen (Baidu)Bin Gao (Microsoft)Jun Yan (Microsoft)Ying Li (Microsoft),"2256446505,2136428695,2616890138,2150635322,2426934046","Online advertising is a key component in the whole internet ecosystem and is growing rapidly with constantly evolving business models and practices. Examples of online advertising include sponsored search, display advertising, Rich Media Ads, interstitial ads, online classified advertising, e-mail marketing and so on. With the rapid growth of social network, social network advertising (as exemplified by Facebook and Groupon) is taking off and playing an important role in the whole landscape. Also, more and more offline ads for both big brands and local businesses are moving online. The online advertising industry is facing tons of challenges. For example, how to understand end users' need and advertisers' goals; what is the right strategy to connect user and ads; what ads should be delivered through which type of of advertising. These challenges bring great opportunities for researchers and data miners to come up with new technologies. Therefore, a forum for researchers and industry practitioners to exchange latest research results and construct collaborations will be of great service to the data mining community and generate value for the industry.",2013,Knowledge Discovery and Data Mining,Fields of study: advertising account executiveshare of voicenative advertisingadvertising campaignadvertising researchcontextual advertisingonline advertisingadvertisingdata mining
Contextual rule-based feature engineering for author-paper identification,Erheng Zhong (Hong Kong University of Science and Technology)Lianghao Li (Hong Kong University of Science and Technology)Naiyan Wang (Hong Kong University of Science and Technology)Ben Tan (Hong Kong University of Science and Technology)Yin Zhu (Hong Kong University of Science and Technology)Lili Zhao (Hong Kong University of Science and Technology)Qiang Yang (Hong Kong University of Science and Technology),"2108094379,2724092001,2626808564,2102551043,2104993999,2500675786,2109031554","We present the ideas and methodologies that we used to address the KDD Cup 2013 challenge on author-paper identification. We firstly formulate the problem as a personalized ranking task and then propose to solve the task through a supervised learning framework. The key point is to eliminate those incorrectly assigned papers of a given author based on existing records. We choose Gradient Boosted Tree as our main classifier. Through our exploration we conclude that the most critical factor to achieve our results is the effective feature engineering. In this paper, we formulate this process as a unified framework that constructs features based on contextual information and combines machine learning techniques with human intelligence. Besides this, we suggest several strategies to parse authors' names, which improve the prediction results significantly. Divide-conquer based model building as well as the model averaging techniques also benefit the prediction precision.",2013,Knowledge Discovery and Data Mining,Fields of study: featuredata miningpattern recognitionmachine learningcomputer science
Data-driven study of urban infrastructure to enable city-wide ubiquitous computing,Gautam S. Thakur (Oak Ridge National Laboratory)Pan Hui (Hong Kong University of Science and Technology)Ahmed Helmy (University of Florida),"2161150550,2100555659,1963774796","Engineering a city-wide ubiquitous computing system requires a comprehensive understanding of urban infrastructure including physical motorways, vehicular traffic, and human activities. Many world cities were built at different time periods and with different purposes that resulted in diversified structures and characteristics, which have to be carefully considered while designing ubiquitous computing facilities. In this paper, we propose a novel technique to study global urban infrastructure, with enabling city-wide ubiquitous computing as the aim, using a massive data-driven network of planet-scale online web-cameras and a location-based online social network service, Foursquare. Our approach examines six metropolitan regions' infrastructure that includes more than 800 locations, 25 million vehicular mobility records, 220k routes, and two million Foursquare check-ins. We evaluate the spatio-temporal correlation in traffic patterns, examine the structure and connectivity in regions, and study the impact of human mobility on vehicular traffic to gain insight for enabling city-wide ubiquitous computing.",2013,Knowledge Discovery and Data Mining,Fields of study: network sciencecomputer securitysimulationstatistics
Hadoop: a view from the trenches,Milind Bhandarkar (LinkedIn),2580859665,"From it's beginnings as a framework for building web crawlers for small-scale search engines to being one of the most promising technologies for building datacenter-scale distributed computing and storage platforms, Apache Hadoop has come far in the last seven years. In this talk I will reminisce about the early days of Hadoop, and will give an overview of the current state of the Hadoop ecosystem, and some real-world use cases of this open source platform. I will conclude with some crystal gazing in the future of Hadoop and associated technologies.",2013,Knowledge Discovery and Data Mining,Fields of study: data scienceworld wide webdatabasecomputer science
Zips: mining compressing sequential patterns in streams,Hoang Thanh Lam (Eindhoven University of Technology)Toon Calders (Eindhoven University of Technology)Jie Yang (Eindhoven University of Technology)Fabian Mörchen (Amazon.com)Dmitriy Fradkin (Siemens),"2118785635,2064105222,2502894806,1886239512,2028188512","We propose a streaming algorithm, based on the minimal description length (MDL) principle, for extracting non-redundant sequential patterns. For static databases, the MDL-based approach that selects patterns based on their capacity to compress data rather than their frequency, was shown to be remarkably effective for extracting meaningful patterns and solving the redundancy issue in frequent itemset and sequence mining. The existing MDL-based algorithms, however, either start from a seed set of frequent patterns, or require multiple passes through the data. As such, the existing approaches scale poorly and are unsuitable for large datasets. Therefore, our main contribution is the proposal of a new, streaming algorithm, called Zips, that does not require a seed set of patterns and requires only one scan over the data. For Zips, we extended the Lempel-Ziv (LZ) compression algorithm in three ways: first, whereas LZ assigns codes uniformly as it builds up its dictionary while scanning the input, Zips assigns codewords according to the usage of the dictionary words; more heaviliy used words get shorter code-lengths. Secondly, Zips exploits also non-consecutive occurences of dictionary words for compression. And, third, the well-known space-saving algorithm is used to evict unpromising words from the dictionary. Experiments on one synthetic and two real-world large-scale datasets show that our approach extracts meaningful compressing patterns with similar quality to the state-of-the-art multi-pass algorithms proposed for static databases of sequences. Moreover, our approach scales linearly with the size of data streams while all the existing algorithms do not.",2013,Knowledge Discovery and Data Mining,Fields of study: visual analyticstheoretical computer sciencedata miningmachine learningcomputer science
Scale-out beyond map-reduce,Raghu Ramakrishnan (Microsoft)Team Members Cisl (Microsoft),"2574082543,2229214340","The amount and variety of data being collected in the enterprise is growing at a staggering pace. The default now is to capture and store any and all data, in anticipation of potential future strategic value, and vast amounts of data are being generated by instrumenting key customer and systems touch points. Until recently, data was gathered for well-defined objectives such as auditing, forensics, reporting and line-of-business operations; now, exploratory and predictive analysis is becoming ubiquitous. These differences in data heterogeneity, scale and usage are leading to a new generation of data management and analytic systems, where the emphasis is on supporting a wide range of large datasets to be stored uniformly and analyzed seamlessly using whatever techniques are most appropriate, including traditional tools like SQL and BI and newer tools, e.g., for machine learning. These new systems are necessarily based on scale-out architectures for both storage and computation. The terms Big Data and data science are often used to refer to this class of systems and applications. Hadoop has become a key building block in the new generation of scale-out systems. Early versions of analytic tools over Hadoop, such as Hive [1] and Pig [2] for SQL-like queries, were implemented by translation into Map-Reduce computations. This approach has inherent limitations, and the emergence of resource managers such as YARN [3] and Mesos [4] has opened the door for newer analytic tools to bypass the Map-Reduce layer. This trend is especially significant for iterative computations such as graph analytics and machine learning, for which Map-Reduce is widely recognized to be a poor fit. In fact, the website of the machine learning toolkit Apache Mahout [5] explicitly warns about the slow performance of some of the algorithms on Hadoop. In this talk, I will examine this architectural trend, and argue that resource managers are a first step in re-factoring the early implementations of Map-Reduce, and that more work is needed if we wish to support a variety of analytic tools on a common scale-out computational fabric. I will then present REEF, which runs on top of resource managers like YARN and provides support for task monitoring and restart, data movement and communications, and distributed state management. Finally, I will illustrate the value of using REEF to implement iterative algorithms for graph analytics and machine learning.",2013,Knowledge Discovery and Data Mining,Fields of study: analyticsreefsqlscalabilitybig datadata sciencedata miningdatabasemachine learningcomputer science
Mining the digital universe of data to develop personalized cancer therapies,Eric E. Schadt (Icahn School of Medicine at Mount Sinai),668066243,"The development of a personalized approach to medical care is now well recognized as an urgent priority. This approach is particularly important in oncology, where it is well understood that each cancer diagnosis is unique at the molecular level, arising from a particular and specific collection of genetic alterations. Furthermore, taking a personalized approach to oncology may expedite the treatment process, pre-empting therapeutic decisions based on fewer data in favor of treatments targeted to an individual's tumor. This directed course may be key to survival for many patients who are terminal or have failed standard therapies.",2013,Knowledge Discovery and Data Mining,Fields of study: rna seqdna sequencingbioinformaticscomputer science
Proceedings of the 2nd ACM SIGKDD International Workshop on Urban Computing,Steven E. Koonin (New York University)Ouri E. Wolfson (University of Illinois at Chicago)Yu Zheng (Microsoft),"2656905796,2175821055,2145115012","Urbanization's rapid progress has led to many big cities, which have modernized people's lives but also engendered big challenges, such as air pollution, increased energy consumption and traffic congestion. Tackling these challenges can seem nearly impossible years ago given the complex and dynamic settings of cities. Nowadays, sensing technologies and large-scale computing infrastructures have produced a variety of big data in urban spaces, e.g. human mobility, air quality, traffic patterns, and geographical data. The big data implies rich knowledge about a city and can help tackle these challenges when used correctly. Urban computing is a process of acquisition, integration, and analysis of big and heterogeneous data generated by a diversity of sources in urban spaces, such as sensors, devices, vehicles, buildings, and human, to tackle the major issues that cities face, e.g. air pollution, increased energy consumption and traffic congestion. Urban computing connects unobtrusive and ubiquitous sensing technologies, advanced data management and analytics models, and novel visualization methods, to create win-win-win solutions that improve urban environment, human life quality, and city operation systems. Urban computing also helps us understand the nature of urban phenomena and even predict the future of cities.",2013,Knowledge Discovery and Data Mining,Fields of study: data miningsimulation
Adaptive adversaries: building systems to fight fraud and cyber intruders,Ari Gesher,2227565132,"Statistical machine learning / knowledge discovery techniques tend to fail when faced with an adaptive adversary attempting to evade detection in the data. Humans do an excellent job of correctly spotting adaptive adversaries given a good way to digest the data. On the other hand, humans are glacially slow and error-prone when it comes to moving through very large volumes of data, a task best left to the machines. Fighting complex fraud and cyber-security threats requires a symbiosis between the computers and teams of human analysts. The computers use algorithmic analysis, heuristics, and/or statistical characterization to find interesting 'simple' patterns in the data. These candidate events are then queued for in-depth human analysis in rich, expressive, interactive analysis environments. In this talk, we'll take a look at case studies of three different systems, using a partnership of automation and human analysis on large scale data to find the clandestine human behavior that these datasets hold, including a discussion of the backend systems architecture and a demo of the interactive analysis environment. The backend systems architecture is a mix of open source technologies, like Cassandra, Lucene, and Hadoop, and some new components that bind them all together. The interactive analysis environment allows seamless pivoting between semantic, geospatial, and temporal analysis with a powerful GUI interface that's usable by non-data scientists. The systems are real systems currently in use by commercial banks, pharmaceutical companies, and governments.",2013,Knowledge Discovery and Data Mining,Fields of study: intelligence amplificationknowledge extractionworld wide webcomputer securitydata miningmachine learningcomputer science
Latent outlier detection and the low precision problem,Fei Wang (University of Sydney)Sanjay Chawla (University of Sydney)Didi Surian (University of Sydney),"2310246644,2201421368,2021959541","The identification of outliers is an intrinsic component of knowledge discovery. However, most outlier detection techniques operate in the observational space, which is often associated with information redundancy and noise. Also, due to the usually high dimensionality of the observational space, the anomalies detected are difficult to comprehend. In this paper we claim that algorithms for discovery of outliers in a latent space will not only lead to more accurate results but potentially provide a natural medium to explain and describe outliers. Specifically, we propose combining Non-Negative Matrix Factorization (NMF) with subspace analysis to discover and interpret outliers. We report on preliminary work towards such an approach.",2013,Knowledge Discovery and Data Mining,Fields of study: anomaly detectiondata miningpattern recognitionmachine learningstatisticscomputer science
A data mining driven risk profiling method for road asset management,Daniel Emerson (Queensland University of Technology)Justin Z. Weligamage (Department of Transport)Richi Nayak (Queensland University of Technology),"2152085522,1972158058,2144864747","Road surface skid resistance has been shown to have a strong relationship to road crash risk, however, applying the current method of using investigatory levels to identify crash prone roads is problematic as they may fail in identifying risky roads outside of the norm. The proposed method analyses a complex and formerly impenetrable volume of data from roads and crashes using data mining. This method rapidly identifies roads with elevated crash-rate, potentially due to skid resistance deficit, for investigation. A hypothetical skid resistance/crash risk curve is developed for each road segment, driven by the model deployed in a novel regression tree extrapolation method. The method potentially solves the problem of missing skid resistance values which occurs during network-wide crash analysis, and allows risk assessment of the major proportion of roads without skid resistance values.",2013,Knowledge Discovery and Data Mining,Fields of study: missing datarisk managementcomputer securitydata miningstatistics
A privacy preserving framework for managing vehicle data in road pricing systems,"Huayu Wu (Agency for Science, Technology and Research)Wee Siong Ng (Agency for Science, Technology and Research)Kian-Lee Tan (National University of Singapore)Wei Wu (Agency for Science, Technology and Research)Shili Xiang (Agency for Science, Technology and Research)Mingqiang Xue (Agency for Science, Technology and Research)","2704714878,2136598690,2142476784,2311660342,2082679073,2303542671","The Electronic Road Pricing (ERP) system was implemented by the Land Transport Authority of Singapore to control traffic by road pricing since 1998. To better understand the traffic condition and improve the pricing scheme, the government initiated the next generation ERP (ERP 2) project, which aims to use the Global Navigation Satellite System (GNSS) collecting positional data from vehicles for analysis. However, most drivers fear of being monitored once the government installs the devices in their vehicles to collect GPS data. The existing data stream management systems (DSMS) centralize both data management and privacy control at server site. This framework assumes DSMS server is secure and trustable, and protects providers' data from illegal access by data users. In ERP 2, the DSMS server is maintained by the government, i.e., data user. Thus, the existing framework is not adoptable. We propose a novel framework in which privacy protection is pushed to data provider site. By doing this, the system could be safer and more efficient. Our framework can be used for the situations such as ERP 2, i.e., data providers would like to control their own privacy policies and/or the workload of DSMS server needs to be reduced.",2013,Knowledge Discovery and Data Mining,Fields of study: internet privacyworld wide webcomputer securitydata mining
Cyber security: how visual analytics unlock insight,Raffael Marty,2665613593,"In the Cyber Security domain, we have been collecting 'big data' for almost two decades. The volume and variety of our data is extremely large, but understanding and capturing the semantics of the data is even more of a challenge. Finding the needle in the proverbial haystack has been attempted from many different angles. In this talk we will have a look at what approaches have been explored, what has worked, and what has not. We will see that there is still a large amount of work to be done and data mining is going to play a central role. We'll try to motivate that in order to successfully find bad guys, we will have to embrace a solution that not only leverages clever data mining, but employs the right mix between human computer interfaces, data mining, and scalable data platforms. Traditionally, cyber security has been having its challenges with data mining. We are different. We will explore how to adopt data mining algorithms to the security domain. Some approaches like predictive analytics are extremely hard, if not impossible. How would you predict the next cyber attack? Others need to be tailored to the security domain to make them work. Visualization and visual analytics seem to be extremely promising to solve cyber security issues. Situational awareness, large-scale data exploration, knowledge capture, and forensic investigations are four top use-cases we will discuss. Visualization alone, however, does not solve security problems. We need algorithms that support the visualizations. For example to reduce the amount of data so an analyst can deal with it, in both volume and semantics.",2013,Knowledge Discovery and Data Mining,Fields of study: internet privacycomputer securitydata miningcomputer science
Mining data from mobile devices: a survey of smart sensing and analytics,Spiros Papadimitriou (Rutgers University)Tina Eliassi-Rad (Rutgers University),"2650391232,218538652","Mobile connected devices, and smartphones in particular, are rapidly emerging as a dominant computing and sensing platform. This poses several unique opportunities for data collection and analysis, as well as new challenges. In this tutorial, we survey the state-of-the-art in terms of mining data from mobile devices across different application areas such as ads, healthcare, geosocial, public policy, etc. Our tutorial has three parts. In part one, we summarize data collection in terms of various sensing modalities. In part two, we present cross-cutting challenges such as real-time analysis, security, and we outline cross cutting methods for mobile data mining such as network inference, streaming algorithms, etc. In the last part, we specifically overview emerging and fast-growing application areas, such as noted above. Concluding, we briefly highlight the opportunities for joint design of new data collection techniques and analysis methods, suggesting additional directions for future research.",2013,Knowledge Discovery and Data Mining,Fields of study: association rule learningcluster analysisdata sciencecomputer securitydata miningmachine learningcomputer science
Experience from hosting a corporate prediction market: benefits beyond the forecasts,Thomas A. Montgomery (Ford Motor Company)Paul M. Stieg (Ford Motor Company)Michael J. Cavaretta (Ford Motor Company)Paul E. Moraal (Ford Motor Company),"2127697135,2125805122,1198499056,2262663807","Prediction markets are virtual stock markets used to gain insight and forecast events by leveraging the wisdom of crowds. Popularly applied in the public to cultural questions (election results, box-office returns), they have recently been applied by corporations to leverage employee knowledge and forecast answers to business questions (sales volumes, products and features, release timing). Determining whether to run a prediction market requires practical experience that is rarely described. Over the last few years, Ford Motor Company obtained practical experience by deploying one of the largest corporate prediction markets known. Business partners in the US, Europe, and South America provided questions on new vehicle features, sales volumes, take rates, pricing, and macroeconomic trends. We describe our experience, including both the strong and weak correlations found between predictions and real world results. Evaluating this methodology goes beyond prediction accuracy, however, since there are many side benefits. In addition to the predictions, we discuss the value of comments, stock price changes over time, the ability to overcome bureaucratic limits, and flexibly filling holes in corporate knowledge, enabling better decision making. We conclude with advice on running prediction markets, including writing good questions, market duration, motivating traders and protecting confidential information.",2013,Knowledge Discovery and Data Mining,Fields of study: social mediaforecastingactuarial sciencedata miningstatisticscomputer science
Finding frequent sub-trajectories with time constraints,Xin Huang (Chinese Academy of Sciences)Jun Luo (Chinese Academy of Sciences)Xin Wang (University of Calgary),"2723816625,2706727915,2646086397","With the advent of location-based social media and location-acquisition technologies, trajectory data are becoming more and more ubiquitous in the real world. Trajectory pattern mining has received a lot of attention in recent years. Frequent sub-trajectories, in particular, might contain very usable knowledge. In this paper, we define a new trajectory pattern called frequent sub-trajectories with time constraints (FSTTC) that requires not only the same continuous location sequence but also the similar staying time in each location. We present a two-phase approach to find FSTTCs based on suffix tree. Firstly, we select the spatial information from the trajectories and generate location sequences. Then the suffix tree is adopted to mine out the frequent location sequences. Secondly, we cluster all sub-trajectories with the same frequent location sequence with respect to the staying time using modified DBSCAN algorithm to find the densest clusters. Accordingly, the frequent sub-trajectories with time constraints, represented by the clusters, are identified. Experimental results show that our approach is efficient and can find useful and interesting information from the spatio-temporal trajectories.",2013,Knowledge Discovery and Data Mining,Fields of study: visualizationanalysisdata miningartificial intelligencemachine learningcomputer science
Speeding up large-scale learning with a social prior,Deepayan Chakrabarti (Facebook)Ralf Herbrich (Amazon.com),"2189524569,2227627541","Slow convergence and poor initial accuracy are two problems that plague efforts to use very large feature sets in online learning. This is especially true when only a few features are ""active"" in any training example, and the frequency of activations of different features is skewed. We show how these problems can be mitigated if a graph of relationships between features is known. We study this problem in a fully Bayesian setting, focusing on the problem of using Facebook user-IDs as features, with the social network giving the relationship structure. Our analysis uncovers significant problems with the obvious regularizations, and motivates a two-component mixture-model ""social prior"" that is provably better. Empirical results on large-scale click prediction problems show that our algorithm can learn as well as the baseline with 12 M fewer training examples, and continuously outperforms it for over 60 M examples. On a second problem using binned features, our model outperforms the baseline even after the latter sees 5x as much data.",2013,Knowledge Discovery and Data Mining,Fields of study: mixture modeldata miningartificial intelligencemachine learningstatisticscomputer science
Distinguishing the unexplainable from the merely unusual: adding explanations to outliers to discover and detect significant complex rare events,Ted E. Senator (Science Applications International Corporation)Henry G. Goldberg (Science Applications International Corporation)Alex Memory (Science Applications International Corporation),"1935278117,2137686017,1964480931","This paper discusses the key role of explanations for applications that discover and detect significant complex rare events. These events are distinguished not necessarily by outliers (i.e., unusual or rare data values), but rather by their inexplicability in terms of appropriate real-world behaviors. Outlier detection techniques are typically part of such applications and may provide useful starting points; however, they are far from sufficient for identifying events of interest and discriminating them from similar but uninteresting events to a degree necessary for operational utility. Other techniques that distinguish anomalies from outliers, and then enable anomalies to be classified as relevant or not to the particular detection problem are also necessary. We argue that explanations are the key to the effectiveness of such complex rare event detection applications, and illustrate this point with examples from several real applications.",2013,Knowledge Discovery and Data Mining,Fields of study: anomaly detectiondata miningmachine learningstatisticscomputer science
Anomaly detection on ITS data via view association,Junaidillah Fadlil (National Taiwan University of Science and Technology)Hsing-Kuo Pao (National Taiwan University of Science and Technology)Yuh-Jye Lee (National Taiwan University of Science and Technology),"346641218,2161189351,2714480662","We focus on detecting anomalous events in transportation systems. In transportation systems, other than normal road situation, anomalous events happen once in a while such as traffic accidents, ambulance car passing, harsh weather conditions, etc. Identifying the anomalous traffic events is essential because the events can lead to critical conditions where immediate investigation and recovery may be necessary. We propose an anomaly detection method for transportation systems where we create a police report automatically after detecting anomalies. Unlike the traditional police report, in this case, some quantitative analysis shall be done as well to provide experts with an advanced, precise and professional description of the anomalous event. For instance, we can provide the moment, the location as well as how severe the accident occurs in the upstream and downstream routes. We present an anomaly detection approach based on view association given multiple feature views on the transportation data if the views are more or less independent from each other. For each single view, anomalies are detected based on a manifold learning and hierarchical clustering procedures and anomalies from different views are associated and detected as anomalies with high confidence. We study two well-known ITS datasets which include the data from Mobile Century project and the PeMS dataset, and we evaluate the proposed method by comparing the automatically generated report and real report from police during the related period.",2013,Knowledge Discovery and Data Mining,Fields of study: nonlinear dimensionality reductiontrajectoryassociationanomaly detectioncomputer securitydata miningmachine learningsimulationcomputer science
A process-centric data mining and visual analytic tool for exploring complex social networks,Denis Dimitrov (Georgetown University)Lisa Singh (Georgetown University)Janet Mann (Georgetown University),"2099393907,2095714368,2137884725","Social scientists and observational scientists have a need to analyze complex network data sets. Examples of such exploratory tasks include: finding communities that exist in the data, comparing results from different graph mining algorithms, identifying regions of similarity or dissimilarity in the data sets, and highlighting nodes with important centrality properties. While many methods, algorithms, and visualizations exist, the capability to apply and combine them for ad-hoc visual exploration or as part of an analytic workflow process is still an open problem that needs to be addressed to help scientists, especially those without extensive programming knowledge. In this paper, we present Invenio-Workflow, a tool that supports exploratory analysis of network data by integrating workflow, querying, data mining, statistics, and visualization to enable scientific inquiry. Invenio-Workflow can be used to create custom exploration tasks, in addition to the standard task templates. After describing the features of the system, we illustrate its utility through several use cases based on networks from different domains.",2013,Knowledge Discovery and Data Mining,Fields of study: visual analyticsdata sciencedata miningmachine learningcomputer science
"Lytic: synthesizing high-dimensional algorithmic analysis with domain-agnostic, faceted visual analytics",Edward Clarkson (Georgia Tech Research Institute)Jaegul Choo (Georgia Institute of Technology)John Turgeson (Georgia Tech Research Institute)Ray Decuir (Georgia Tech Research Institute)Haesun Park (Georgia Institute of Technology),"2433968907,2148380128,2421507084,2231053728,2123241397","We present Lytic , a domain-independent, faceted visual analytic (VA) system for interactive exploration of large datasets. It combines a flexible UI that adapts to arbitrary character-separated value (CSV) datasets with algorithmic preprocessing to compute unsupervised dimension reduction and cluster data from high-dimensional fields. It provides a variety of visualization options that require minimal user effort to configure and a consistent user experience between visualization types and underlying datasets. Filtering, comparison and visualization operations work in concert, allowing users to hop seamlessly between actions and pursue answers to expected and unexpected data hypotheses.",2013,Knowledge Discovery and Data Mining,Fields of study: interactive visual analysisvisual analyticsdata scienceworld wide webdata miningmachine learningcomputer science
The business impact of deep learning,Jeremy Howard,2721103548,"In the last year deep learning has gone from being a special purpose machine learning technique used mainly for image and speech recognition, to becoming a general purpose machine learning tool. This has broad implications for all organizations that rely on data analysis. It represents the latest development in a general trend towards more automated algorithms, and away from domain specific knowledge. For organizations that rely on domain expertise for their competitive advantage, this trend could be extremely disruptive. For start-ups interested in entering established markets, this trend could be a major opportunity. This talk will be a non-technical introduction to general-purpose deep learning, and its potential business impact.",2013,Knowledge Discovery and Data Mining,Fields of study: inductive transfersynchronous learningactive learningdata miningartificial intelligencemachine learningsimulationcomputer science
SAE: social analytic engine for large networks,Yang Yang (Tsinghua University)Jianfei Wang (Tsinghua University)Yutao Zhang (Tsinghua University)Wei Chen (Tsinghua University)Jing Zhang (Tsinghua University)Honglei Zhuang (Tsinghua University)Zhilin Yang (Tsinghua University)Bo Ma (Tsinghua University)Zhanpeng Fang (Tsinghua University)Sen Wu (Tsinghua University)Xiaoxiao Li (Tsinghua University)Debing Liu (Tsinghua University)Jie Tang (Tsinghua University),"2673426936,2645680893,2223115198,2670958046,2635071740,2128637305,2489788272,2607880608,2119723005,2443423181,2724020300,2223203911,2158012360","Online social networks become a bridge to connect our physical daily life and the virtual Web space, which not only provides rich data for mining, but also brings many new challenges. In this paper, we present a novel Social Analytic Engine (SAE) for large online social networks. The key issues we pursue in the analytic engine are concerned with the following problems: 1) at the micro-level, how do people form different types of social ties and how people influence each other? 2) at the meso-level, how do people group into communities? 3) at the macro-level, what are the hottest topics in a social network and how the topics evolve over time? We propose methods to address the above questions. The methods are general and can be applied to various social networking data. We have deployed and validated the proposed analytic engine over multiple different networks and validated the effectiveness and efficiency of the proposed methods.",2013,Knowledge Discovery and Data Mining,Fields of study: social influencesocial networksocial computingworld wide webdata miningmachine learningsimulation
Interactive visual analytics for high dimensional data,Haesun Park (Georgia Institute of Technology),2123241397,"Many modern data sets can be represented in high dimensional vector spaces and have benefited from computational methods that utilize advanced techniques from numerical linear algebra and optimization. Visual analytics approaches have contributed greatly to data understanding and analysis due to utilization of both automated algorithms and human's quick visual perception and interaction. However, visual analytics targeting high dimensional large-scale data has been challenging due to low dimensional screen space with limited pixels to represent data. Among various computational techniques supporting visual analytics, dimension reduction and clustering have played essential roles by reducing the dimension and volume to visually manageable scales. In this talk, we present some of the key foundational methods for supervised dimension reduction such as linear discriminant analysis (LDA), dimension reduction and clustering/topic discovery by nonnegative matrix factorization (NMF), and visual spatial alignment for effective fusion and comparisons by Orthogonal Procrustes. We demonstrate how these methods can effectively support interactive visual analytic tasks that involve large-scale document and image data sets.",2013,Knowledge Discovery and Data Mining,Fields of study: interactive visual analysisvisual analyticscomputer visiondata miningmachine learningcomputer science
Financing lead triggers: empowering sales reps through knowledge discovery and fusion,Kareem S. Aggour (GE Global Research)Bethany Hoogs (GE Global Research),"2280254350,746931223","Sales representatives must have access to meaningful and actionable intelligence about potential customers to be effective in their roles. Historically, GE Capital Americas sales reps identified leads by manually searching through news reports and financial statements either in print or online. Here we describe a system built to automate the collection and aggregation of information on companies, which is then mined to identify actionable sales leads. The Financing Lead Triggers system is comprised of three core components that perform information fusion, knowledge discovery and information visualization. Together these components extract raw data from disparate sources, fuse that data into information, and then automatically mine that information for actionable sales leads driven by a combination of expert-defined and statistically derived triggers. A web-based interface provides sales reps access to the company information and sales leads in a single location. The use of the Lead Triggers system has significantly improved the performance of the sales reps, providing them with actionable intelligence that has improved their productivity by 30-50%. In 2010, Lead Triggers provided leads on opportunities that represented over $44B in new deal commitments for GE Capital.",2013,Knowledge Discovery and Data Mining,Fields of study: hit to leadsales managementknowledge extractiondata miningcomputer science
"Proceedings of the 4th MultiClust Workshop on Multiple Clusterings, Multi-view Data, and Multi-source Knowledge-driven Clustering",Ira Assent (Aarhus University)Carlotta Domeniconi (George Mason University)Francesco Gullo (Yahoo!)Andrea Tagarelli (University of Calabria)Arthur Zimek (Ludwig Maximilian University of Munich),"145164693,45678088,1979201319,273425128,242745652","Cluster detection is a very traditional data analysis task with several decades of research. However, it also includes a large variety of different subtopics investigated by different communities such as data mining, machine learning, statistics, and database systems. ""Multiple Clusterings, Multi-view Data, and Multi-source Knowledge-driven Clustering"" names several challenges around clustering: making sense or even making use of many, possibly redundant clustering results, of different representations and properties of data, of different sources of knowledge. Approaches such as ensemble clustering, semi-supervised clustering, subspace clustering meet around these problems. Yet they tackle these problems with different backgrounds, focus on different details, and include ideas from different research communities. This diversity is a major potential for this emerging field and should be highlighted by this workshop. A core motivation for this workshop series is our believe that these approaches are not just tackling different parts of the problem but that they should benefit from each other and, ultimately, combine the different perspectives and techniques to tackle the clustering problem more effectively. In paper presentations and discussions, we therefore would like to encourage the workshop participants to look at their own research problems from multiple perspectives.",2013,Knowledge Discovery and Data Mining,Fields of study: conceptual clusteringdata sciencedata miningmachine learningcomputer science
An online system with end-user services: mining novelty concepts from tv broadcast subtitles,Mika Rautiainen (University of Oulu)Jouni Sarvanko (University of Oulu)Arto Heikkinen (University of Oulu)Mika Ylianttila (Cleveland Institute of Electronics)Vassilis Kostakos (University of Oulu),"2064479991,243409837,2211459624,2036134916,325537935","Better tools for content-based access of video are needed to improve access to time-continuous video data. Particularly information about linear TV broadcast programs has been available in a form limited to program guides that provide short manually described overviews of the program content. Recent development in digitalization of TV broadcasting and emergence of web-based services for catch-up and on-demand viewing bring out new possibilities to access data. In this paper we introduce our data mining system and accompanying services for summarizing Finnish DVB broadcast streams from seven national channels. We describe how data mining of novelty concepts can be extracted from DVB subtitles to augment web-based ""Catch-Up TV Guide"" and ""Novelty Cloud"" TV services. Furthermore, our system allows accessing media fragments as Picture Quotes via generated word lists and provides content-based recommendations to find new programs that have content similar to the user selected programs. Our index consists of over 180 000 programs that are used to recommend relevant programs. The service has been under development and available online since 2010. It has registered over 5000 user sessions.",2013,Knowledge Discovery and Data Mining,Fields of study: internet privacymultimediaworld wide webdata miningcomputer science
A semi-supervised approach for author disambiguation in KDD CUP 2013,Jianyu Zhao (Southeast University)Peng Wang (Southeast University)Kai Huang (Southeast University),"2701932579,2715197448,2719873448","Name disambiguation, which aims to identify multiple names which correspond to one person and same names which refer to different persons, is one of the most important basic problems in many areas such as natural language processing, information retrieval and digital libraries. Microsoft academic search data in KDD Cup 2013 Track 2 task brings one such challenge to the researchers in the knowledge discovery and data mining community. Besides the real-world and large-scale characteristic, the Track 2 task raises several challenges: (1) Consideration of both synonym and polysemy problems; (2) Existence of huge amount of noisy data with missing attributes; (3) Absence of labeled data that makes this challenge a cold start problem. In this paper, we describe our solution to Track 2 of KDD Cup 2013. The challenge of this track is author disambiguation, which aims at identifying whether authors are the same person by using academic publication data. We propose a multi-phase semi-supervised approach to deal with the challenge. First, we preprocess the dataset and generate features for models, then construct a coauthor-based network and employ community detection to accomplish first-phase disambiguation task, which handles the cold-start problem. Second, using results in first phase, we use support vector machine and various other models to utilize noisy data with missing attributes in the dataset. Further, we propose a self-taught procedure to solve ambiguity in coauthor information, boosting performance of results from other models. Finally, by blending results from different models, we finally achieves 6 th place with 0.98717 mean F-score on public leaderboard and 7 th place with 0.98651 mean F-score on private leaderboard.",2013,Knowledge Discovery and Data Mining,Fields of study: cold startsupport vector machinedata scienceinformation retrievaldata miningmachine learningcomputer science
Feature engineering and tree modeling for author-paper identification challenge,Jiefei Li (Sun Yat-sen University)Xiaocong Liang (Sun Yat-sen University)Weijie Ding (Sun Yat-sen University)Weidong Yang (Sun Yat-sen University)Rong Pan (Sun Yat-sen University),"2647009079,2700212751,2231858249,2671294266,2194641296","The ability to search literature and collect/aggregate metrics around publications is a central tool for modern research. Both academic and industry researchers across hundreds of scientific disciplines, from astronomy to zoology, increasingly rely on search to understand what has been published and by whom. Microsoft Academic Search is an open platform, which provides a variety of metrics and experiences for the research community, in addition to literature search. As the covering data came from many sources, the profile of an author with an ambiguous name tends to contain noise, resulting in papers that are incorrectly assigned to others. KDD Cup 2013 Track 1 challenges participants to determine which papers in an author profile were truly written by the given author. In this work, we present how to use tree-base models to accurately predict the paper author. We incorporate feature engineering into the models with the advantages of them. This paper introduces two kinds of tree-base models (GB-DT [4], RGF [5]) and presents in detail the learning algorithm and how features can be generated for the task. The experimental results show the effectiveness of the proposed approach.",2013,Knowledge Discovery and Data Mining,Fields of study: featuredata sciencedata miningartificial intelligencemachine learningstatisticscomputer sciencemathematics
Proceedings of the ACM SIGKDD Workshop on Outlier Detection and Description,Leman Akoglu (Stony Brook University)Emmanuel Müller (Karlsruhe Institute of Technology)Jilles Vreeken (University of Antwerp),"2288278917,2112493600,1971070670","Traditionally, outlier mining and anomaly discovery focused on the automatic detection of highly deviating objects. It has been studied for several decades in statistics, machine learning, and data mining, and has led to a lot of insight as well as automated systems for the detection of outliers. However, for today's applications to be successful, mere identification of anomalies alone is not enough. With more and more applications using outlier analysis for data exploration and knowledge discovery, the demand for manual verification and understanding of outliers is steadily increasing. Examples include applications such as health surveillance, fraud analysis, or sensor monitoring, where one is particularly interested in why an object seems outlying.",2013,Knowledge Discovery and Data Mining,Fields of study: anomaly detectiondata sciencedata miningartificial intelligencemachine learningcomputer science
The scorecard solution to the author-paper identification challenge,Xing Zhao,2305338394,This paper describes team mb74's solution to Track 1 of KDD Cup 2013. The challenge is to determine whether an author has written a given paper in the Microsoft Academic Search database. The key part of our solution is the feature generation which is described with details. A FICO scorecard model with Bernoulli likelihood as the objective is trained in our solution. Alternative objectives and training methods are discussed. It is shown that the scorecard method effectively helps the feature generation and addresses the mean average precision measure for this problem.,2013,Knowledge Discovery and Data Mining,Fields of study: balanced scorecardlogistic regressiondata sciencedata miningmachine learningstatisticscomputer science
Trial and error in influential social networks,Xiaohui Bei (Nanyang Technological University)Ning Chen (Nanyang Technological University)Liyu Dou (Nanyang Technological University)Xiangru Huang (Shanghai Jiao Tong University)Ruixin Qiang (Shanghai Jiao Tong University),"2014511961,2147735703,2289534399,2643668102,2634756252","In this paper, we introduce a trial-and-error model to study information diffusion in a social network. Specifically, in every discrete period, all individuals in the network concurrently try a new technology or product with certain respective probabilities. If it turns out that an individual observes a better utility, he will then adopt the trial; otherwise, the individual continues to choose his prior selection. We first demonstrate that the trial and error behavior of individuals characterizes certain global community structures of a social network, from which we are able to detect macro-communities through the observation of micro-behavior of individuals. We run simulations on classic benchmark testing graphs, and quite surprisingly, the results show that the trial and error dynamics even outperforms the Louvain method (a popular modularity maximization approach) if individuals have dense connections within communities. This gives a solid justification of the model. We then study the influence maximization problem in the trial-and-error dynamics. We give a heuristic algorithm based on community detection and provide experiments on both testing and large scale collaboration networks. Simulation results show that our algorithm significantly outperforms several well-studied heuristics including degree centrality and distance centrality in almost all of the scenarios. Our results reveal the relation between the budget that an advertiser invests and marketing strategies, and indicate that the mixing parameter, a benchmark evaluating network community structures, plays a critical role for information diffusion.",2013,Knowledge Discovery and Data Mining,Fields of study: trial and errorsocial networkdata miningmachine learningsimulationstatisticscomputer science
"Using ""big data"" to solve ""small data"" problems",Chris Neumann,2227884153,"The brief history of knowledge discovery is filled with products that promised to bring ""BI to the masses"". But how do you build a product that truly bridges the gap between the conceptual simplicity of ""questions and answers"" and the structure needed to query traditional data stores? In this talk, Chris Neumann will discuss how DataHero applied the principles of user-centric design and development over a year and a half to create a product with which more than 95% of new users can get answers on their first attempt. He'll demonstrate the process DataHero uses to determine the best combination of algorithms and user interface concepts needed to create intuitive solutions to potentially complex interactions, including: Determining the structure of files uploaded by users Accurately identifying data types within files Presenting users with an optimal visualization for any combination of data Helping users to ask questions of data when they don't know what to do Chris will also talk about what it's like to start a ""Big Data"" company and how he applied lessons from his time as the first engineer at Aster Data Systems to DataHero.",2013,Knowledge Discovery and Data Mining,Fields of study: analyticsbig datadata scienceworld wide webdata miningcomputer science
When TEDDY meets GrizzLY: temporal dependency discovery for triggering road deicing operations,Céline Robardet (Institut national des sciences Appliquées de Lyon)Vasile-Marian Scuturici (Institut national des sciences Appliquées de Lyon)Marc Plantevit (University of Lyon)Antoine Fraboulet,"1976373341,222118968,107359601,2634232936","Temporal dependencies between multiple sensor data sources link two types of events if the occurrence of one is repeatedly followed by the appearance of the other in a certain time interval. TEDDY algorithm aims at discovering such dependencies, identifying the statically significant time intervals with a chi2 test. We present how these dependencies can be used within the GrizzLY project to tackle an environmental and technical issue: the deicing of the roads. This project aims to wisely organize the deicing operations of an urban area, based on several sensor network measures of local atmospheric phenomena. A spatial and temporal dependency-based model is built from these data to predict freezing alerts.",2013,Knowledge Discovery and Data Mining,Fields of study: computer securitydata miningsimulationcomputer science
KDD Cup 2013: author disambiguation,Benjamin SoleckiLucas SilvaDmitry Efimov (Moscow State University),"2224151557,2236859701,2680611598",This paper describes our team's (BS Man & Dmitry & Leustagos) approach to the KDD Cup 2013 track 2 challenge: Author Disambiguation in the Microsoft Academic Search database.,2013,Knowledge Discovery and Data Mining,Fields of study: world wide webinformation retrievaldata miningcomputer science
"Human-Computer Interaction and Knowledge Discovery in Complex, Unstructured, Big Data: Third International Workshop, HCI-KDD 2013, Held at SouthCHI 2013, Maribor, Slovenia, July 1-3, 2013, Proceedings",Andreas Holzinger (Sofia Medical University)Gabriella Pasi (University of Milan),"2175384922,280681230",-,2013,Knowledge Discovery and Data Mining,Fields of study: data scienceoperations research
"Human-Computer Interaction and Knowledge Discovery in Complex, Unstructured, Big Data: Third International Workshop, HCI-KDD 2013, Held at SouthCHI ... Applications, incl. Internet/Web, and HCI)",Andreas Holzinger (Sofia Medical University)Gabriella Pasi (University of Milan),"2175384922,280681230","This book constitutes the refereed proceedings of the Third Workshop on Human-Computer Interaction and Knowledge Discovery, HCI-KDD 2013, held in Maribor, Slovenia, in July 2013, at SouthCHI 2013. The 20 revised papers presented were carefully reviewed and selected from 68 submissions. The papers are organized in topical sections on human-computer interaction and knowledge discovery, knowledge discovery and smart homes, smart learning environments, and visualization data analytics.",2013,Knowledge Discovery and Data Mining,Fields of study: data scienceworld wide webdata miningcomputer science
"Mining big data: current status, and forecast to the future",Wei Fan (Huawei)Albert Bifet (Yahoo!),"2422054197,307521372","Big Data is a new term used to identify datasets that we can not manage with current methodologies or data mining software tools due to their large size and complexity. Big Data mining is the capability of extracting useful information from these large datasets or streams of data. New mining techniques are necessary due to the volume, variability, and velocity, of such data. The Big Data challenge is becoming one of the most exciting opportunities for the years to come. We present in this issue, a broad overview of the topic, its current status, controversy, and a forecast to the future. We introduce four articles, written by influential scientists in the field, covering the most interesting and state-of-the-art topics on Big Data mining.",2013,Knowledge Discovery and Data Mining,Fields of study: data stream miningdata scienceinformation retrievaldata mining
Mining heterogeneous information networks: a structural analysis approach,Yizhou Sun (Northeastern University)Jiawei Han (University of Illinois at Urbana–Champaign),"2131539564,2121939561","Most objects and data in the real world are of multiple types, interconnected, forming complex, heterogeneous but often semi-structured information networks. However, most network science researchers are focused on homogeneous networks, without distinguishing different types of objects and links in the networks. We view interconnected, multityped data, including the typical relational database data, as heterogeneous information networks, study how to leverage the rich semantic meaning of structural types of objects and links in the networks, and develop a structural analysis approach on mining semi-structured, multi-typed heterogeneous information networks. In this article, we summarize a set of methodologies that can effectively and efficiently mine useful knowledge from such information networks, and point out some promising research directions.",2013,Knowledge Discovery and Data Mining,Fields of study: evolving networksdata sciencedata miningmachine learningcomputer science
Big graph mining: algorithms and discoveries,U. Kang (Carnegie Mellon University)Christos Faloutsos (Carnegie Mellon University),"2426051778,2198983026","How do we find patterns and anomalies in very large graphs with billions of nodes and edges? How to mine such big graphs efficiently? Big graphs are everywhere, ranging from social networks and mobile call networks to biological networks and the World Wide Web. Mining big graphs leads to many interesting applications including cyber security, fraud detection, Web search, recommendation, and many more. In this paper we describe Pegasus, a big graph mining system built on top of MapReduce, a modern distributed data processing platform. We introduce GIM-V, an important primitive that Pegasus uses for its algorithms to analyze structures of large graphs. We also introduce HEigen, a large scale eigensolver which is also a part of Pegasus. Both GIM-V and HEigen are highly optimized, achieving linear scale up on the number of machines and edges, and providing 9.2x and 76x faster performance than their naive counterparts, respectively. Using Pegasus, we analyze very large, real world graphs with billions of nodes and edges. Our findings include anomalous spikes in the connected component size distribution, the 7 degrees of separation in a Web graph, and anomalous adult advertisers in the who-follows-whom Twitter social network.",2013,Knowledge Discovery and Data Mining,Fields of study: data scienceworld wide webdata miningcomputer science
Scaling big data mining infrastructure: the twitter experience,Jimmy J. Lin (Twitter)Dmitriy V. Ryaboy (Twitter),"2163619555,2180476547","The analytics platform at Twitter has experienced tremendous growth over the past few years in terms of size, complexity, number of users, and variety of use cases. In this paper, we discuss the evolution of our infrastructure and the development of capabilities for data mining on ""big data"". One important lesson is that successful big data mining in practice is about much more than what most academics would consider data mining: life ""in the trenches"" is occupied by much preparatory work that precedes the application of data mining algorithms and followed by substantial effort to turn preliminary models into robust solutions. In this context, we discuss two topics: First, schemas play an important role in helping data scientists understand petabyte-scale data stores, but they're insufficient to provide an overall ""big picture"" of the data available to generate insights. Second, we observe that a major challenge in building data analytics platforms stems from the heterogeneity of the various components that must be integrated together into production workflows---we refer to this as ""plumbing"". This paper has two goals: For practitioners, we hope to share our experiences to flatten bumps in the road for those who come after us. For academic researchers, we hope to provide a broader context for data mining in production environments, pointing out opportunities for future work.",2013,Knowledge Discovery and Data Mining,Fields of study: data scienceworld wide webdata mining
Mining large streams of user data for personalized recommendations,Xavier Amatriain (Netflix),2571014644,"The Netflix Prize put the spotlight on the use of data mining and machine learning methods for predicting user preferences. Many lessons came out of the competition. But since then, Recommender Systems have evolved. This evolution has been driven by the greater availability of different kinds of user data in industry and the interest that the area has drawn among the research community. The goal of this paper is to give an up-to-date overview of the use of data mining approaches for personalization and recommendation. Using Netflix personalization as a motivating use case, I will describe the use of different kinds of data and machine learning techniques. After introducing the traditional approaches to recommendation, I highlight some of the main lessons learned from the Netflix Prize. I then describe the use of recommendation and personalization techniques at Netflix. Finally, I pinpoint the most promising current research avenues and unsolved problems that deserve attention in this domain.",2013,Knowledge Discovery and Data Mining,Fields of study: data scienceworld wide webdata miningcomputer science
Outlier ensembles: position paper,Charu C. Aggarwal (IBM),2146335907,"Ensemble analysis is a widely used meta-algorithm for many data mining problems such as classification and clustering. Numerous ensemble-based algorithms have been proposed in the literature for these problems. Compared to the clustering and classification problems, ensemble analysis has been studied in a limited way in the outlier detection literature. In some cases, ensemble analysis techniques have been implicitly used by many outlier analysis algorithms, but the approach is often buried deep into the algorithm and not formally recognized as a general-purpose meta-algorithm. This is in spite of the fact that this problem is rather important in the context of outlier analysis. This paper discusses the various methods which are used in the literature for outlier ensembles and the general principles by which such analysis can be made more effective. A discussion is also provided on how outlier ensembles relate to the ensemble-techniques used commonly for other data mining problems.",2013,Knowledge Discovery and Data Mining,Fields of study: data miningpattern recognitionmachine learningcomputer science
Discovering interesting information with advances in web technology,Richi Nayak (Queensland University of Technology)Pierre Senellart (Institut Mines-Télécom)Fabian M. Suchanek (Max Planck Society)Aparna S. Varde (Montclair State University),"2144864747,2039937633,69603646,74820909","The Web is a steadily evolving resource comprising much more than mere HTML pages. With its ever-growing data sources in a variety of formats, it provides great potential for knowledge discovery. In this article, we shed light on some interesting phenomena of the Web: the deep Web, which surfaces database records as Web pages; the Semantic Web, which defines meaningful data exchange formats; XML, which has established itself as a lingua franca for Web data exchange; and domain-specific markup languages, which are designed based on XML syntax with the goal of preserving semantics in targeted domains. We detail these four developments in Web technology, and explain how they can be used for data mining. Our goal is to show that all these areas can be as useful for knowledge discovery as the HTML-based part of the Web.",2013,Knowledge Discovery and Data Mining,Fields of study: web 2 0website parse templatesemantic web stackweb modelingsocial semantic webdata webweb standardsweb mappingweb developmentweb designweb navigationhtmlsemantic webweb serviceweb intelligenceweb engineeringweb miningworld wide webinformation retrievaldata miningcomputer science
Studying the source code of scientific research,Graham Cormode (University of Warwick)S. Muthukrishnan (Rutgers University)Jinyun Yan (Rutgers University),"2190072679,2190118346,2147854521","Just as inspecting the source code of programs tells us a lot about the process of programming, inspecting the ""source code"" of scientific papers informs on the process of scientific writing. We report on our study of the source of tens of thousands of papers from Computer Science and Mathematics.",2013,Knowledge Discovery and Data Mining,Fields of study: theoretical computer sciencecomputational sciencecomputer science
A Depression Detection Model Based on Sentiment Analysis in Micro-blog Social Network,"Xinyu Wang (Beijing University of Posts and Telecommunications)Chunhong Zhang (Beijing University of Posts and Telecommunications)Yang Ji (Beijing University of Posts and Telecommunications)Li Sun (Beijing University of Posts and Telecommunications)Leijia Wu (University of Technology, Sydney)Zhana Bao (Waseda University)","2502123914,2649500795,2636243028,2424258007,2713736595,2136360010","Datasets originating from social networks are valuable to many fields such as sociology and psychology. But the supports from technical perspective are far from enough, and specific approaches are urgently in need. This paper applies data mining to psychology area for detecting depressed users in social network services. Firstly, a sentiment analysis method is proposed utilizing vocabulary and man-made rules to calculate the depression inclination of each micro-blog. Secondly, a depression detection model is constructed based on the proposed method and 10 features of depressed users derived from psychological research. Then 180 users and 3 kinds of classifiers are used to verify the model, whose precisions are all around 80%. Also, the significance of each feature is analyzed. Lastly, an application is developed within the proposed model for mental health monitoring online. This study is supported by some psychologists, and facilitates them in data-centric aspect in turn.",2013,Knowledge Discovery and Data Mining,Fields of study: psychological researchmicrobloggingsentiment analysismental healthworld wide webdata miningcomputer science
Indirect Information Linkage for OSINT through Authorship Analysis of Aliases,Robert Layton (Federation University Australia)Charles Perez (University of Technology of Troyes)Babiga Birregah (University of Technology of Troyes)Paul Andrew Watters (Federation University Australia)Marc Lemercier (University of Technology of Troyes),"2274713458,2294121795,206624646,1954375491,2138283223","In this paper we examine the problem of automatically linking online accounts for open source intelligence gathering. We specifically aim to determine if two social media accounts are shared by the same author, without the use of direct linking evidence. We profile the accounts using authorship analysis and find the best matching guess. We apply this to a series of Twitter accounts identified as malicious by a methodology named SPOT and find several pairs of accounts that belong to the same author, despite no direct evidence linking the two. Overall, our results show that linking aliases is possible with an accuracy of 84%, and using our automated threshold method improves our accuracy to over 90% by removing incorrectly discovered matches.",2013,Knowledge Discovery and Data Mining,Fields of study: world wide webdata miningcomputer science
Cost-Based Quality Measures in Subgroup Discovery,Rob M. Konijn (Leiden University)Wouter Duivesteijn (Leiden University)Marvin Meeng (Leiden University)Arno J. Knobbe (Leiden University),"2020123811,134040164,30494414,1229146049","In this paper we consider data where examples are not only labeled in the classical sense (positive or negative), but also have costs associated with them. In this sense, each example has two target attributes, and we aim to find clearly defined subsets of the data where the values of these two targets have an unusual distribution. In other words, we are focusing on a Subgroup Discovery task over somewhat unusual data, and investigate possible quality measures that take into account both the binary as well as the cost target. In defining such quality measures, we aim to produce interpretable valuation of subgroups, such that data analysts can directly value the findings, and relate these to monetary gains or losses. Our work is particularly relevant in the domain of health care fraud detection. In this data, the binary target identifies the patients of a specific medical practitioner under investigation, whereas the cost target specifies how much money is spent on each patient. When looking for clear specifications of differences in claim behavior, we clearly need to take into account both the 'positive' examples (patients of the practitioner) and 'negative' examples (other patients), as well as information about costs of all patients. A typical subgroup will now list a number of treatments, and how the patients of our practitioner differ in both the prevalence of the treatments as well as the associated costs. An additional angle considered in this paper is the recently proposed Local Subgroup Discovery, where subgroups are judged according to the difference with a local reference group, rather than the entire dataset. We show how the cost-based analysis of data specifically fits this local focus.",2013,Knowledge Discovery and Data Mining,Fields of study: data miningmathematics
Dynamic Similarity-Aware Inverted Indexing for Real-Time Entity Resolution,Banda Ramadan (Australian National University)Peter Christen (Australian National University)Huizhi Liang (Australian National University)Ross W. Gayler (Veda)David Hawking (Australian National University),"2126170099,2023765750,2152830259,2160811952,37244654","Entity resolution is the process of identifying groups of records in a single or multiple data sources that represent the same real-world entity. It is an important tool in data de-duplication, in linking records across databases, and in matching query records against a database of existing entities. Most existing entity resolution techniques complete the resolution process offline and on static databases. However, real-world databases are often dynamic, and increasingly organizations need to resolve entities in real-time. Thus, there is a need for new techniques that facilitate working with dynamic databases in real-time. In this paper, we propose a dynamic similarity-aware inverted indexing technique (DySimII) that meets these requirements. We also propose a frequency-filtered indexing technique where only the most frequent attribute values are indexed. We experimentally evaluate our techniques on a large real-world voter database. The results show that when the index size grows no appreciable increase is found in the average record insertion time (around 0.1 msec) and in the average query time (less than 0.1 sec). We also find that applying the frequency-filtered approach reduces the index size with only a slight drop in recall.",2013,Knowledge Discovery and Data Mining,Fields of study: record linkageinformation retrievaldata miningdatabasecomputer science
On the Evaluation of the Homogeneous Ensembles with CV-Passports,Vladimir Nikulin (University of Queensland)Aneesha Bakharia (Queensland University of Technology)Tian-Hsiang Huang (National Sun Yat-sen University),"2601969657,2250415102,2310461985","Ensembles are often capable of greater prediction accuracy than any of their individual members. As a consequence of the diversity between individual base-learners, an ensemble will not suffer from overfitting. In this regard, development of a systematic and automatic approach for the evaluation of ensemble solutions is particularly important. Based on the mechanism of homogeneous ensembling (known, also, as bagging), we can construct a passport of the solution as a unified validation trajectory against all available training data. Assuming that passports mimic closely the corresponding test solutions, we can use them for the consideration of many tasks including optimizations of blends and ensembles, calculation of the biases and any other tests as required. The reported results were obtained online during the International PAKDD data mining competition in 2010, where we were awarded a certificate for the fourth best result. We, also, report results from the second most popular contest on the Kaggle platform named ''Credit'', where we demonstrate one of the best results.",2013,Knowledge Discovery and Data Mining,Fields of study: boostingcross validationdecision treebiological classificationdata sciencedata miningmachine learningstatisticscomputer science
EEG-MINE: Mining and Understanding Epilepsy Data,Sunhee Kim (Carnegie Mellon University)Christos Faloutsos (Carnegie Mellon University)Hyung-Jeong Yang (Chonnam National University),"2131938950,2198983026,2295751673","Given electroencephalogram time series data from patients with epilepsy, can we find patterns and regularities? The typical approach thus far is to use tensors or dynamical systems. Here, we present EEG-MINE, a nonlinear, chaos-based ""gray box model"", that blends domain knowledge with data observations. When applied to numerous, real EEG sequences, EEG-MINE (a) can successfully reconstruct the signals with high accuracy; (b) can spot surprising patterns within seizure EEG signals; and (c) may provide early warning of epileptic seizures.",2013,Knowledge Discovery and Data Mining,Fields of study: artificial neural networkartificial intelligencemachine learningcomputer science
A PSO-Based Cost-Sensitive Neural Network for Imbalanced Data Classification,Peng Cao (Northeastern University)Dazhe Zhao (Northeastern University)Osmar R. Zaïane (University of Alberta),"2156204076,2135947625,2308328903","Learning from imbalanced data is an important and common problem. Many methods have been proposed to address and attempt to solve the problem, including sampling and cost-sensitive learning. This paper presents an effective wrapper approach incorporating the evaluation measure directly into the objective function of cost-sensitive neural network to improve the performance of classification, by simultaneously optimizing the best pair of feature subset, intrinsic structure parameters and misclassification costs. The optimization is based on Particle Swarm Optimization. Our designed method can be applied on the binary class and multi-class classification. Experimental results on various standard benchmark datasets show that the proposed method is effective in comparison with commonly used sampling techniques.",2013,Knowledge Discovery and Data Mining,Fields of study: artificial neural networkdata miningpattern recognitionmachine learningcomputer science
A Novel Proposal for Outlier Detection in High Dimensional Space,Zhana Bao (Waseda University)Wataru Kameyama (Waseda University),"2136360010,308872687","Finding rare information behind big data is important and meaningful for outlier detection. However, to find such rare information is extremely difficult when the notorious curse of dimensionality exists in high dimensional space. Most of existing methods fail to obtain good result since the Euclidean distance cannot work well in high dimensional space. In this paper, we first perform a grid division of data for each attribute, and compare the density ratio for every point in each dimension. We then project the points of the same area to other dimensions, and then we calculate the disperse extent with defined cluster density value. At last, we sum up all weight values for each point in two-step calculations. After the process, outliers are those points scoring the largest weight. The experimental results show that the proposed algorithm can achieve high precision and recall on the synthetic datasets with the dimension varying from 100 to 10000.",2013,Knowledge Discovery and Data Mining,Fields of study: econometricsdata miningstatisticsmathematics
A New Feature Selection and Feature Contrasting Approach Based on Quality Metric: Application to Efficient Classification of Complex Textual Data,"Jean-Charles Lamirel (French Institute for Research in Computer Science and Automation)Pascal Cuxac (Centre national de la recherche scientifique)Aneesh Sreevallabh Chivukula (International Institute of Information Technology, Hyderabad)Kafil Hajlaoui (Centre national de la recherche scientifique)","1996598587,171686626,2211092358,1907109315","Feature maximization is a cluster quality metric which favors clusters with maximum feature representation as regard to their associated data. In this paper we go one step further showing that a straightforward adaptation of such metric can provide a highly efficient feature selection and feature contrasting model in the context of supervised classification. We more especially show that this technique can enhance the performance of classification methods whilst very significantly outperforming (+80%) the state-of-the art feature selection techniques in the case of the classification of unbalanced, highly multidimensional and noisy textual data, with a high degree of similarity between the classes.",2013,Knowledge Discovery and Data Mining,Fields of study: linear classifierdimensionality reductionfeaturek nearest neighbors algorithmsupervised learningfeature selectiondata miningpattern recognitionmachine learningcomputer science
CPPG: Efficient Mining of Coverage Patterns Using Projected Pattern Growth Technique,"P. Gowtham Srinivas (International Institute of Information Technology, Hyderabad)P. Krishna Reddy (International Institute of Information Technology, Hyderabad)A. V. Trinath (International Institute of Information Technology, Hyderabad)","2109152216,2143169954,378632874","The knowledge of coverage patterns extracted from the transactional data sets is useful in efficient placement of banner advertisements. The existing algorithm to extract coverage patterns is an apriori-like approach. In this paper, we propose an improved coverage pattern mining method by exploiting the notion of ""non-overlap pattern projection"". The proposed approach improves the performance by efficiently pruning the search space and extracting the complete set of coverage patterns. The performance results show that the proposed approach significantly improves the performance over the existing approach.",2013,Knowledge Discovery and Data Mining,Fields of study: online advertisingworld wide webdata miningsimulationcomputer science
Using Scan-Statistical Correlations for Network Change Analysis,Adriel Cheng (United States Department of Defense)Peter Dickinson (United States Department of Defense),"2515436048,2384943366","Network change detection is a common prerequisite for identifying anomalous behaviours in computer, telecommunication, enterprise and social networks. Data mining of such networks often focus on the most significant change only. However, inspecting large deviations in isolation can lead to other important and associated network behaviours to be overlooked. This paper proposes that changes within the network graph be examined in conjunction with one another, by employing correlation analysis to supplement network-wide change information. Amongst other use-cases for mining network graph data, the analysis examines if multiple regions of the network graph exhibit similar degrees of change, or is it considered anomalous for a local network change to occur independently. Building upon Scan-Statistics network change detection, we extend the change detection technique to correlate localised network changes. Our correlation inspired techniques have been deployed for use on various networks internally. Using real-world datasets, we demonstrate the benefits of our correlation change analysis.",2013,Knowledge Discovery and Data Mining,Fields of study: weighted correlation network analysisdynamic network analysisanomaly detectiondata sciencedata miningmachine learningcomputer science
Ensemble Learning Model for Petroleum Reservoir Characterization: A Case of Feed-Forward Back-Propagation Neural Networks,Fatai Anifowose (Universiti Malaysia Sarawak)Jane Labadin (Universiti Malaysia Sarawak)Abdulazeez Abdulraheem (King Fahd University of Petroleum and Minerals),"2441809772,2190043273,2143801528",Conventional machine learning methods are incapable of handling several hypotheses. This is the main strength of the ensemble learning paradigm. The petroleum industry is in great need of this new learning methodology due to the persistent quest for better prediction accuracies of reservoir properties for improved exploration and production activities. This paper proposes an ensemble model of Artificial Neural Networks (ANN) that incorporates various expert opinions on the optimal number of hidden neurons in the prediction of petroleum reservoir properties. The performance of the ensemble model was evaluated using standard decision rules and compared with those of ANN-Ensemble with the conventional Bootstrap Aggregation method and Random Forest. The results showed that the proposed method outperformed the others with the highest correlation coefficient and the least errors. The study also confirmed that ensemble models perform better than the average performance of individual base learners. This study demonstrated the great potential for the application of ensemble learning paradigm in petroleum reservoir characterization.,2013,Knowledge Discovery and Data Mining,Fields of study: reservoir modelingporosityensemblpermeabilityensemble learningartificial neural networkdata miningartificial intelligencemachine learningcomputer science
Mining Correlated Patterns with Multiple Minimum All-Confidence Thresholds,R. Uday Kiran (University of Tokyo)Masaru Kitsuregawa (University of Tokyo),"2139065608,390930505","Correlated patterns are an important class of regularities that exist in a database. The all-confidence measure has been widely used to discover the patterns in real-world applications. This paper theoretically analyzes the all-confidence measure, and shows that, although the measure satisfies the null-invariant property, mining correlated patterns involving both frequent and rare items with a single minimum all-confidence (minAllConf) threshold value causes the ""rare item problem"" if the items' frequencies in a database vary widely. The problem involves either finding very short length correlated patterns involving rare items at a high minAllConf threshold, or generating a huge number of patterns at a low minAllConf threshold. The cause for the problem is that the single minAllConf threshold was not sufficient to capture the items' frequencies in a database effectively. The paper also introduces an alternative model of correlated patterns using the concept of multiple minAllConf thresholds. The proposed model facilitates the user to specify a different minAllConf threshold for each pattern to reflect the varied frequencies of items within it. Experiment results show that the proposed model is very effective.",2013,Knowledge Discovery and Data Mining,Fields of study: knowledge extractiondata sciencedata miningstatisticscomputer sciencemathematics
Clustering in Conjunction with Quantum Genetic Algorithm for Relevant Genes Selection for Cancer Microarray Data,Manju Sardana (Jawaharlal Nehru University)R. K. Agrawal (Jawaharlal Nehru University)Baljeet Kaur (University of Delhi),"2110764039,2309964180,2435266406","Quantum Genetic Algorithm, which utilizes the principle of quantum computing and genetic operators, allows efficient exploration and exploitation of large search space simultaneously. It has been used recently to determine a reduced set of features for cancer microarray data to improve the performance of the learning system. However, the length of the chromosome used is the original dimension of the feature vector. Hence, despite the use of the quantum variant of GA, it requires huge memory and computation time for high dimensional data like microarrays. In this paper, we propose a two phase approach, ClusterQGA, that determines a minimal set of relevant and non-redundant genes. Experimental results on publicly available cancer microarray datasets demonstrate the effectiveness of the proposed approach in comparison to existing methods in terms of classification accuracy and number of features. Also, the proposed approach takes less computation time in comparison to Genetic quantum algorithm proposed by Abderrahim et al.",2013,Knowledge Discovery and Data Mining,Fields of study: dna microarraycluster analysisquantum computergenetic algorithmfeature selectionbioinformaticsdata miningmachine learningcomputer sciencemathematics
Predicting High Impact Academic Papers Using Citation Network Features,Daniel Mcnamara (Australian National University)Paul Wong (Australian National University)Peter Christen (Australian National University)Kee Siong Ng (Australian National University),"2664223314,2066403021,2023765750,2157476061","Predicting future high impact academic papers is of benefit to a range of stakeholders, including governments, universities, academics, and investors. Being able to predict 'the next big thing' allows the allocation of resources to fields where these rapid developments are occurring. This paper develops a new method for predicting a paper's future impact using features of the paper's neighbourhood in the citation network, including measures of interdisciplinarity. Predictors of high impact papers include high early citation counts of the paper, high citation counts by the paper, citations of and by highly cited papers, and interdisciplinary citations of the paper and of papers that cite it. The Scopus database, consisting of over 24 million publication records from 1996-2010 across a wide range of disciplines, is used to motivate and evaluate the methods presented.",2013,Knowledge Discovery and Data Mining,Fields of study: data scienceworld wide webcomputer science
Visual Data Mining Methods for Kernel Smoothed Estimates of Cox Processes,David Rohde (Federal University of Rio de Janeiro)Ruth Huang (University of Queensland)Jonathan Corcoran (University of Queensland)Gentry White (University of Queensland),"2558108640,2136376908,2131040621,2152556034","Real world planning of complex logistical organisations such as the fire service is a complex task requiring synthesis of many different computational techniques, from artificial intelligence and statistical or machine learning to geographical information systems and visualization. A particularly promising approach is to apply established data mining techniques in order to produce a model and make forecasts. The nature of the forecast can then be rendered using visualization techniques in order to assess operational decisions, simultaneously benefiting from generic and powerful data mining techniques, and using visualization to understand these results in the context of the actual problem of interest which may be very specific. Previous approaches to visualization in similar contexts use iso surfaces to visualize densities, these methods ignore recent improvements in interactive 3D visualization such as volume rendering and cut-planes, these methods also ignore what is often a key problem of interest comparing two different stochastic processes, finally previous methods have not paid sufficient attention to differences between estimation of densities and point processes (or Cox processes). This paper seeks to address all of these shortcomings and make recommendations for the trade-offs between visualization techniques for operational decision making. Finally we also demonstrate the ability to include interactive 3D plots within a paper by rendering an iso surface using 3D portable document format (PDF).",2013,Knowledge Discovery and Data Mining,Fields of study: information visualizationdata sciencecomputer visiondistributed computingcomputer securitydata miningdatabaseartificial intelligencemachine learningsimulationstatisticsalgorithmcomputer sciencemathematics
Parameter Estimation Using Improved Differential Evolution (IDE) and Bacterial Foraging Algorithm to Model Tyrosine Production in Mus Musculus (Mouse),Jia Xing Yeoh (Universiti Teknologi Malaysia)Chuii Khim Chong (Universiti Teknologi Malaysia)Yee Wen Choon (Universiti Teknologi Malaysia)Lian En Chai (Universiti Teknologi Malaysia)Safaai Deris (Universiti Teknologi Malaysia)Rosli Md. Illias (Universiti Teknologi Malaysia)Mohd Saberi Mohamad (Universiti Teknologi Malaysia),"1966080822,2157616592,2006160071,2161638723,738100022,2227999364,2131251652","The hybrid of Differential Evolution algorithm with Kalman Filtering and Bacterial Foraging algorithm is a novel global optimization method that is implemented in this research to obtain the best kinetic parameter value. The proposed algorithm is then used to model tyrosine production in mus musculus (mouse) by using a dataset, JAK/STAT (Janus Kinase Signal Transducer and Activator of Transcription) signal transduction pathway. Global optimization is a method to identify the optimal kinetic parameter using ordinary differential equation. From the ordinary parameter of biomathematical field, there are many unknown parameters and commonly the parameters are in nonlinear form. Global optimization method includes differential evolution algorithm which will be used in this research. Kalman Filter and Bacterial Foraging algorithm help in handling noise data and faster convergences respectively in the conventional Differential Evolution. The results from this experiment show estimatedly optimal kinetic parameters values, shorter computation time, and better accuracy of simulated results compared with other estimation algorithms.",2013,Knowledge Discovery and Data Mining,Fields of study: meta optimizationestimation theorysystems modelingartificial intelligencemachine learningmathematical optimizationstatistics
A Constraint and Rule in an Enhancement of Binary Particle Swarm Optimization to Select Informative Genes for Cancer Classification,Mohd Saberi Mohamad (Universiti Teknologi Malaysia)Sigeru Omatu (Osaka Institute of Technology)Safaai Deris (Universiti Teknologi Malaysia)Michifumi Yoshioka (Osaka Prefecture University),"2131251652,2503872861,738100022,2027542045","Gene expression data have been analyzing by many researchers by using a range of computational intelligence methods. From the gene expression data, selecting a small subset of informative genes can do cancer classification. Nevertheless, many of the computational methods face difficulties in selecting small subset since the small number of samples needs to be compared to the huge number of genes (high-dimension), irrelevant genes and noisy genes. Hence, to choose the small subset of informative genes that is significant for the cancer classification, an enhanced binary particle swarm optimization is proposed. Here, the constraint of the elements of particle velocity vectors is introduced and a rule for updating particle's position is proposed. Experiments were performed on five different gene expression data. As a result, in terms of classification accuracy and the number of selected genes, the performance of the introduced method is superior compared to the conventional version of binary particle swarm optimization (BPSO). The other significant finding is lower running times compared to BPSO for this proposed method.",2013,Knowledge Discovery and Data Mining,Fields of study: multi swarm optimizationgene centered view of evolutionbioinformaticspattern recognitionmachine learningmathematics
Objectively Evaluating Interestingness Measures for Frequent Itemset Mining,Albrecht Zimmermann (Katholieke Universiteit Leuven),2100256487,"Itemset mining approaches, while having been studied for more than 15 years, have been evaluated only on a handful of data sets. In particular, they have never been evaluated on data sets for which the ground truth was known. Thus, it is currently unknown whether itemset mining techniques actually recover underlying patterns. Since the weakness of the algorithmically attractive support/confidence framework became apparent early on, a number of interestingness measures have been proposed. Their utility, however, has not been evaluated, except for attempts to establish congruence with expert opinions. Using an extension of the Quest generator proposed in the original itemset mining paper, we propose to evaluate these measures objectively for the first time, showing how many non-relevant patterns slip through the cracks.",2013,Knowledge Discovery and Data Mining,Fields of study: data sciencedata miningdatabase
Structured Feature Extraction Using Association Rules,Nan Tian (Queensland University of Technology)Yue Xu (Queensland University of Technology)Yuefeng Li (Queensland University of Technology)Gabriella Pasi (University of Milano-Bicocca),"2137621742,2098436618,2122434521,280681230","As of today, opinion mining has been widely used to identify the strength and weakness of products (e.g., cameras) or services (e.g., services in medical clinics or hospitals) based upon people's feedback such as user reviews. Feature extraction is a crucial step for opinion mining which has been used to collect useful information from user reviews. Most existing approaches only find individual features of a product without the structural relationships between the features which usually exists. In this paper, we propose an approach to extract features and feature relationship, represented as tree structure called a feature hierarchy, based on frequent patterns and associations between patterns derived from user reviews. The generated feature hierarchy profiles the product at multiple levels and provides more detailed information about the product. Our experiment results based on some popularly used review datasets show that the proposed feature extraction approach can identify more correct features than the baseline model. Even though the datasets used in the experiment are about cameras, our work can be applied to generate features about a service such as the services in hospitals or clinics.",2013,Knowledge Discovery and Data Mining,Fields of study: feature modelassociation rule learningfeaturefeature extractionsentiment analysisdata scienceworld wide webdata miningdatabasemachine learningcomputer science
FTCRank: Ranking Components for Building Highly Reliable Cloud Applications,Hanze Xu (Zhejiang University)Yanan Xie (Zhejiang University)Dinglong Duan (Zhejiang University)Liang Chen (Zhejiang University)Jian Wu (Zhejiang University),"2528673606,2151674255,2281381729,2248621837,2525822525","With the increasing popularity of cloud computing[2], building highly reliable applications on cloud is very important. However, it's hard to give an optimal solution for large-scale cloud applications. In order to provide an effective solution on this research problem, we propose a component ranking approach named as FTCRank for applying fault-tolerant strategies to the significant components. FTCRank considers not only structure information but also component characteristics to obtain the result. Experiments show that FTCRank achieves better results than other existing algorithms in Top-K fault-tolerant cloud tasks.",2013,Knowledge Discovery and Data Mining,Fields of study: cloud testingtheoretical computer sciencedistributed computingdata miningcomputer science
Real-Time Television ROI Tracking Using Mirrored Experimental Designs,Brendan Kitts (Brandeis University)Dyng AuBrian Burdick,"2159085996,2311596393,2232927457","Real-time conversion tracking is the holy grail of TV advertisers. We show how to use thousands of tiny areas available via commercial cable and satellite systems to create low cost tracking cells. These areas are created as ""mirrors"" of a national campaign, and run in parallel with it. With properly controlled areas, it is possible to calculate national effects due to TV using statistical methods. We show performance of the method on a large-scale TV advertising campaign where it was used successfully to maintain a real-time CPA target of $60 for 179 days.",2013,Knowledge Discovery and Data Mining,Fields of study: return on investmenttelevisioncomputer graphics imagessimulation
Identifying Authoritative and Reliable Contents in Community Question Answering with Domain Knowledge,Lifan Guo (Drexel University)Xiaohua Hu (Drexel University),"2139179961,2207916209","Community Question Answering (CQA) has emerged as a popular forum for users to ask and answer questions. Over the last few years, CQA portals such as Yahoo answersand Baidu Zhidao have exploded in popularity, and now provide a viable alternative to general purpose Web search. A number of answers submitted to address questions on CQA sites compose a valuable knowledge repository, which could be a gold mine for information retrieval as well as text mining. Two important questions in CQA research are focused on the quality of contents and the reputation of the answerers. Previous approaches for retrieving relevant and high quality content have been proposed, but not much work has been done on providing an integrated framework to solve these two problems. Besides, no research work has used both text and link information in their methods via leveraging existing ratings of answers and questions. In this paper, we present a novel approach to analyze questions and answers based on the topic modeling framework with Dirichlet forest priors (LDA-DF)[8]. We utilize information obtained from LDA-DF to construct a joint topical and link model to identify authorities and reliable answers on a CQA site.We evaluate our methods in a dataset obtained from Yahoo! Answers. With the new representation of topical structures on CQA datasets, using a limited amount of web resource, we show significant improvements over the state-of-art methods LDA-DF, LDA, and HLDA on performance of authority identification and answer ranking.",2013,Knowledge Discovery and Data Mining,Fields of study: world wide webinformation retrievaldata miningcomputer science
Incremental Constrained Clustering: A Decision Theoretic Approach,Swapna Raj Prabakara Raj (Indian Institute of Technology Madras)Balaraman Ravindran (Indian Institute of Technology Madras),"2276551007,1956086128","Typical constrained clustering algorithms incorporate a set of must-link and cannot-link constraints into the clustering process. These instance level constraints specify relationships between pairs of data items and are generally derived by a domain expert. Generating these constraints is considered as a cumbersome and expensive task. In this paper we describe an incremental constrained clustering framework to discover clusters using a decision theoretic approach. Our framework is novel since we provide an overall evaluation of the clustering in terms of quality in decision making and use this evaluation to ""generate"" instance level constraints. We do not assume any domain knowledge to start with. We show empirical validation of this approach on several test domains and show that we achieve better performance than a feature selection based approach.",2013,Knowledge Discovery and Data Mining,Fields of study: flame clusteringcorrelation clusteringconstrained clusteringfuzzy clusteringdecision theorycluster analysisconceptual clusteringdata miningpattern recognitionmachine learningstatisticscomputer sciencemathematics
Parallel Sentiment Polarity Classification Method with Substring Feature Reduction,Yaowen Zhang (Nanjing University)Xiaojun Xiang (Nanjing University)Cunyan Yin (Nanjing University)Lin Shang (Nanjing University),"2558908285,2283155364,2277247189,2116332931","Sentiment analysis is an important issue in machine learning, which aims to identify the emotion expressed in corpus. However, sentiment analysis is a difficult task, especially in large-scale data, where feature reduction is needed. In this paper, we propose a parallel feature reduction algorithm for sentiment polarity classification based on a substring method. Specifically, the proposed algorithm is based on parallel computing under the Hadoop platform. The proposed algorithm is examined on a large data set and a K-nearest neighbor algorithm and a Rocchio algorithm are used for classification. Experimental results show that the proposed algorithm outperforms other commonly used methods in terms of the classification performance and the computational cost.",2013,Knowledge Discovery and Data Mining,Fields of study: feature extractiondata miningpattern recognitionmachine learningcomputer science
Effective Evaluation Measures for Subspace Clustering of Data Streams,Marwan Hassani (RWTH Aachen University)Yunsu Kim (RWTH Aachen University)Seungjin Choi (Pohang University of Science and Technology)Thomas Seidl (RWTH Aachen University),"2159472396,2095728525,2128913862,2140301036","Nowadays, most streaming data sources are becoming high-dimensional. Accordingly, subspace stream clustering, which aims at finding evolving clusters within subgroups of dimensions, has gained a significant importance. However, existing subspace clustering evaluation measures are mainly designed for static data, and cannot reflect the quality of the evolving nature of data streams. On the other hand, available stream clustering evaluation measures care only about the errors of the full-space clustering but not the quality of subspace clustering. In this paper we propose, to the first of our knowledge, the first subspace clustering measure that is designed for streaming data, called SubCMM: Subspace Cluster Mapping Measure. SubCMM is an effective evaluation measure for stream subspace clustering that is able to handle errors caused by emerging, moving, or splitting subspace clusters. Additionally, we propose a novel method for using available offline subspace clustering measures for data streams within the Subspace MOA framework.",2013,Knowledge Discovery and Data Mining,Fields of study: k medians clusteringsubclucanopy clustering algorithmcorrelation clusteringconstrained clusteringdata stream clusteringcure data clustering algorithmrandom subspace methodfuzzy clusteringclustering high dimensional datacluster analysisdata miningpattern recognitionmachine learningcomputer science
On the Application of Multi-class Classification in Physical Therapy Recommendation,Jing Zhang (University of Alberta)Douglas Gross (American Physical Therapy Association)Osmar R. Zaïane (University of Alberta),"2704951158,2100853181,2308328903","Recommending optimal rehabilitation intervention for injured workers that would lead to successful return-to-work (RTW) is a challenge for clinicians. Currently, the clinicians are unable to identify with complete confidence which intervention is best for a patient and the referral is often made in trial and error fashion. Only 58% recommendations are successful in our dataset. We aim to develop an interpretable decision support system using machine learning to assist the clinicians. We use various re-sampling techniques to tackle the multi-class imbalance and class overlap problem in real world application data. The final model has shown promising potential in classification compared to human baseline and has been integrated into a web-based decision-support tool that requires additional validation in a clinical sample.",2013,Knowledge Discovery and Data Mining,Fields of study: clinical decision support systemdata miningartificial intelligencemachine learningsimulationcomputer science
Multiclass Prediction for Cancer Microarray Data Using Various Variables Range Selection Based on Random Forest,Kohbalan Moorthy (Universiti Teknologi Malaysia)Mohd Saberi Mohamad (Universiti Teknologi Malaysia)Safaai Deris (Universiti Teknologi Malaysia),"2161091230,2131251652,738100022","Continuous data mining has led to the generation of multi class datasets through microarray technology. New improved algorithms are then required to process and interpret these data. Cancer prediction tailored with variable selection process has shown to improve the overall prediction accuracy. Through variable selection process, the amount of informative genes gathered are much lesser than the initial data, yet the selective subset present in other methods cannot be fine-tuned to suit the necessity for particular number of variables. Hence, an improved technique of various variable range selection based on Random Forest method is proposed to allow selective variable subsets for cancer prediction. Our results indicate improvement in the overall prediction accuracy of cancer data based on the improved various variable range selection technique which allows selective variable selection to create best subset of genes. Moreover, this technique can assist in variable interaction analysis, gene network analysis, gene-ranking analysis and many other related fields.",2013,Knowledge Discovery and Data Mining,Fields of study: random forestmicroarray analysis techniquesgene expressionfeature selectionbioinformaticsdata miningpattern recognitionmachine learningcomputer science
Applying Migrating Birds Optimization to Credit Card Fraud Detection,Ekrem Duman (Özyeğin University)Ilker Elikucuk,"2230257874,227750855","We discuss how the Migrating Birds Optimization algorithm (MBO) is applied to statistical credit card fraud detection problem. MBO is a recently proposed metaheuristic algorithm which is inspired by the V flight formation of the migrating birds and it was shown to perform very well in solving a combinatorial optimization problem, namely the quadratic assignment problem. As analyzed in this study, it has a very good performance in the fraud detection problem also when compared to classical data mining and genetic algorithms. Its performance is further increased by the help of some modified neighborhood definitions and benefit mechanisms.",2013,Knowledge Discovery and Data Mining,Fields of study: genetic algorithmcomputer securitydata miningmachine learningcomputer science
Evaluation of Position-Constrained Association-Rule-Based Classification for Tree-Structured Data,Dang Bach Bui (Curtin University)Fedja Hadzic (Curtin University)Michael Hecker (Curtin University),"2011880139,1224142007,2311216122","Tree-structured data is popular in many domains making structural classification an important task. In this paper, a recently proposed structure preserving flat representation is used to generate association rules using itemset mining techniques. The main difference to traditional techniques is that subtrees are constrained by the position in the original tree, and initial associations prior to subtree reconstruction can be based on disconnected subtrees. Imposing the positional constraint on subtreee typically result in a reduces the number of rules generated, especially with greater structural variation among tree instances. This outcome would be desired in the current status of frequent pattern mining, where excessive patterns hinder the practical use of results. However, the question remains whether this reduction comes at a high cost in accuracy and coverage rate reduction. We explore this aspect and compare the approach with a structural classifier based on same subtree type, but not positional constrained in any way. The experiments using publicly available real-world data reveal important differences between the methods and implications when frequent candidate subtrees on which the association rules are based, are not only equivalent structure and node label wise, but also occur at the same position across the tree instances in the database.",2013,Knowledge Discovery and Data Mining,Fields of study: search treeassociation rule learningdata miningpattern recognitionmachine learningcomputer sciencemathematics
Weight Based Live Migration of Virtual Machines,Baiyou Qiao (Northeastern University)Kai Zhang (Northeastern University)Yanpeng Guo (Northeastern University)Yutong Li (Northeastern University)Yuhai Zhao (Northeastern University)Guoren Wang (Northeastern University),"2710966796,2553271493,2549911998,2555566073,2715071529,2166985210","Due to having many advantages, virtualization has been widely used and become a key technology of cloud computing. Live migration of virtual machines is the core technique of virtualization fields, but the existing pre-copy live migration approaches have problems of low copy efficiency and long total migration time, so we propose a weight based live migration algorithm of virtual machines in this paper, which adds weights to the dirty page information collected, preferentially selects and transfers the dirty pages that are not modified frequently, the dirty pages that are modified frequently are transferred after virtual machine is suspended. The algorithm effectively reduces the amount of memory pages transferred and the total migration time. Experiment results show that when virtual machine is under heavy load, the proposed algorithm considerably reduces the number of transferred memory pages and shortens the total migration time without significantly increment of the virtual machine downtime.",2013,Knowledge Discovery and Data Mining,Fields of study: cloud computingvirtual machineoperating systemdistributed computingreal time computingcomputer science
Querying Compressed XML Data,Olfa Arfaoui (École Normale Supérieure)Minyar Sassi-Hidri (École Normale Supérieure),"2251828568,2312461735","The exploitation of large volume of XML (eXtensible Markup Language) data with a limited storage space implies the development of a special and reliable treatment to compress data and query them. This work studies and treats these processes in order to combine them via a mediator while facilitating querying compressed XML data without recourse to the decompression process. We propose a new technique to compress, re-index and query XML data while improving XMill and B+Tree algorithms. We show the reliability and the speed up of the proposed querying system towards response time and answers' exactitude.",2013,Knowledge Discovery and Data Mining,Fields of study: xml schema editorxml validationefficient xml interchangexml frameworkxml encryptionstreaming xmlxml signaturexml databasedocument structure descriptionb treexml schemaxmlcompressioninformation retrievaldata miningdatabasecomputer science
Identifying Gene Knockout Strategy Using Bees Hill Flux Balance Analysis (BHFBA) for Improving the Production of Succinic Acid and Glycerol in Saccharomyces cerevisiae,Yee Wen Choon (Universiti Teknologi Malaysia)Mohd Saberi Mohamad (Universiti Teknologi Malaysia)Safaai Deris (Universiti Teknologi Malaysia)Rosli Md. Illias (Universiti Teknologi Malaysia)Lian En Chai (Universiti Teknologi Malaysia)Chuii Khim Chong (Universiti Teknologi Malaysia),"2006160071,2131251652,738100022,2227999364,2161638723,2157616592","Strains of Saccharomyces cerevisiae can be manipulated to improve product yield and growth characteristics. Optimization algorithms are developed to identify the effects of gene knockout on the results. However, this process is often faced the problem of being trapped in local minima and slow convergence due to repetitive iterations of algorithm. In this paper, we proposed Bees Hill Flux Balance Analysis (BHFBA) which is a hybrid of Bees Algorithm, Hill Climbing Algorithm and Flux Balance Analysis to solve the problems and improve the performance in predicting optimal sets of gene deletion for maximizing the growth rate and production yield of desired metabolite. Saccharomyces cerevisiae is the model organism in this paper. The list of knockout genes, growth rate and production yield after the deletion are the results from the experiments. BHFBA performed better in term of computational time, stability and production yield.",2013,Knowledge Discovery and Data Mining,Fields of study: bees algorithmflux balance analysishill climbingcomputer science
Research on SaaS Resource Management Method Oriented to Periodic User Behavior,Jun Guo (Northeastern University)Hongle Wu (Northeastern University)Hao Huang (Northeastern University)Fang Liu (Northeastern University)Bin Zhang (Northeastern University),"2586409186,2271109726,2304293789,2649926597,2635448166","With the development of Internet technology, SaaS is gaining popularity as a kind of innovative mode of software applications. In order to meet the needs of the periodic user behavior better, allocate virtual resource more reasonable and achieve the targets of SaaS Service performance optimization and energy conservation, this paper puts forward one SaaS resource management method oriented to periodic user behavior. This method takes the periodic user behavior as the research object, predicts future resource demand by predicting and matching concurrent requests and resource occupancy, then allocates the resource by demands. The results show that this strategy has good usability and validity and it can predict the user future demand for resources accurately. This method also lays a foundation for further performance optimization and energy conservation.",2013,Knowledge Discovery and Data Mining,Fields of study: software as a serviceenergy conservationknowledge managementworld wide websimulationcomputer science
A Semantic-Based Dual Caching System for Nomadic Web Service,Panpan Han (Zhejiang University)Liang Chen (Zhejiang University)Jian Wu (Zhejiang University),"2549615932,2248621837,2525822525","As mobile devices become more widely used, they will emerge as a standard platform for hosting Web Service clients. Since mobile devices tend to be connected via a wireless network, they have to work with significantly less and fluctuating bandwidth as well as sudden and unexpected loss of connectivity. Moreover, mobile devices have other constraints, such as limited CPU, memory, or energy resources. To handle the above problems, we proposed a dual caching architecture and development method of web services for mobile devices. With the caching system between the client and server, we can cache the computing result and data downloaded from the server, which can reduce the latency and network load for workloads of web service. Meanwhile, the client side caching system can store the upload and download data.",2013,Knowledge Discovery and Data Mining,Fields of study: mobile searchfalse sharingmobile webmobile deviceweb serviceworld wide weboperating systemreal time computingcomputer science
Threonine Biosynthesis Pathway Simulation Using IBMDE with Parameter Estimation,Chuii Khim Chong (Universiti Teknologi Malaysia)Mohd Saberi Mohamad (Universiti Teknologi Malaysia)Safaai Deris (Universiti Teknologi Malaysia)Mohd Shahir Shamsir (Universiti Teknologi Malaysia)Yee Wen Choon (Universiti Teknologi Malaysia)Lian En Chai (Universiti Teknologi Malaysia),"2157616592,2131251652,738100022,2021867721,2006160071,2161638723","When analysing a metabolic pathway through mathematical model, it is important that the significant parameters are being correctly estimated. However, this process often comes across problems such aseasily being trapped in local minima, repetitive exposure to worse results during the search process, and occurrence of noisy data. Thus, an improved Bee Memory Differential Evolution algorithm (IBMDE), which is a hybrid of the Differential Evolution algorithm (DE), the Kalman Filter (KF), Artificial Bee Colony algorithm (ABC), and a memory feature is presented this paper. IBMDE is an improved estimation algorithm as previous work only utilised DE. The threonine biosynthesis pathway is the metabolic pathways used in this paper. For metabolite O-Phosphohomoserine production simulation, the IBMDE able to produce the estimated optimal kinetic parameter values with significantly reduced error rate (63.67%) and shows a faster convergence time (71.46%) compared to the Nelder Mead (NM), the Simulated Annealing (SA), the Genetic Algorithm (GA), and DE respectively. In addition, IBMDE demostrates to be a reliable estimation algorithm.",2013,Knowledge Discovery and Data Mining,Fields of study: artificial bee colony algorithmkalman filterestimation theoryartificial intelligencemachine learningmathematical optimizationcomputer science
Mining Approximate Keys Based on Reasoning from XML Data,Liu Yijun (Jiangsu University)Ye Feiyue (Jiangsu University)He Sheng (Jiangsu University),"2193292153,1992239141,2101193447","Keys are very important for data management. Due to the hierarchical and flexible structure of XML, mining keys from XML data is a more complex and difficult task than from relational databases. In this paper, we study mining approximate keys from XML data, and define the support and confidence of a key expression based on the number of null values on key paths. In the mining process, inference rules are used to derive new keys. Through the two-phase reasoning, a target set of approximate keys and its reduced set are obtained. Our research conducted experiments over ten benchmark XML datasets from XMark and four files in the UW XML Repository. The results show that the approach is feasible and efficient, with which effective keys in various XML data can be discovered.",2013,Knowledge Discovery and Data Mining,Fields of study: xml schema editorxml schemaxml catalogxml validationefficient xml interchangexml frameworkxml namespacexml encryptionxml signaturebinary xmlsimple api for xmlxml databasedocument structure descriptionxml schemaxmlkeyinformation retrievaldata miningdatabasecomputer science
Enhancing Textual Data Quality in Data Mining: Case Study and Experiences,Yi Feng (Zhejiang Gongshang University)Chunhua Ju (Zhejiang Gongshang University),"2673206580,2638533119","Dirty data is recognized as a top challenge for data mining. Textual data is one type of data that should be explored more on the topic of data quality, to ensure the discovered knowledge is of quality. In this paper, we focus on the topic of textual data quality (TDQ) in data mining. Based on our data mining experiences for years, three typical TDQ dimensions and related problems are highlighted, including representation granularity, representation consistency, and completeness. Then, to provide a real-world example on how to enhance TDQ in data mining, a case study is demonstrated in detail in this paper, under the background of data mining in traditional Chinese medicine and covers three typical TDQ problems and corresponding solutions. The case study provided in this paper is expected to help data analysts and miners to attach more importance to TDQ issue, and enhance TDQ for more reliable data mining.",2013,Knowledge Discovery and Data Mining,Fields of study: data qualitytraditional chinese medicinedata sciencedata miningdatabasecomputer science
A Hybrid of SVM and SCAD with Group-Specific Tuning Parameters in Identification of Informative Genes and Biological Pathways,Muhammad Faiz Misman (Universiti Teknologi Malaysia)Weng Howe Chan (Universiti Teknologi Malaysia)Mohd Saberi Mohamad (Universiti Teknologi Malaysia)Safaai Deris (Universiti Teknologi Malaysia),"2093919505,2105878598,2131251652,738100022","Advancements in pathway-based microarray classification approach leads to a new era of genomic research. However, it is limited by issues regarding the quality of the pathway data as these data are usually curated from biological literatures and in specific biological experiment (e.g. lung cancer experiment), context free pathway information collection process lead to the presence of uninformative genes in the pathways. Many methods in this approach neglect these limitations by treating all genes in a pathway as significant. In this paper, we propose a hybrid of support vector machine and smoothly clipped absolute deviation with group-specific tuning parameters (gSVM-SCAD) to select informative genes within pathways before the pathway evaluation process. Experiments conducted on gender and lung cancer datasets shows that gSVM-SCAD obtains significant results in identifying significant genes and pathways, and in classification accuracy.",2013,Knowledge Discovery and Data Mining,Fields of study: gene centered view of evolutionsupport vector machinebioinformaticsdata miningmachine learningstatisticscomputer science
Mining Clinical Process in Order Histories Using Sequential Pattern Mining Approach,Shusaku Tsumoto (Shimane University)Hidenao Abe (Shimane University),"122205434,2297977950","In hospital information system, order-entry system is used to transfer the orders from doctors or nurses to other medical stuffs. Thus, since order histories will store the clinincal process of each doctors in a sequential way, reuse of such data will capture the process of each clinician. This paper applied two types of sequential pattern mining approaches to analysis of a sequential process for each care process. The empirical results show the methods enable us to capture the temporal characteristics of behavior of clinicians, which will give a subset of decision making process in clinical environments.",2013,Knowledge Discovery and Data Mining,Fields of study: process miningsequential pattern miningdata sciencedata miningcomputer science
An OLAP Server for Sensor Networks Using Augmented Statistics Trees,Neil Dunstan (University of New England),2137499807,The datacube is a conceptual data structure to support OnLine Analytical Processing (OLAP). It is essentially a series of tables organized according to attributes (called dimensions). Table rows (or cells) contain aggregated information for collections of records that satisfy value constraints for each dimension. The Statistics Tree (ST) uses a tree structure for storing the datacube in memory in order to optimize cell lookup time and handle a variety of types of cell-based queries. An Augmented ST (AST) is proposed with additional list structures within the ST. The additional lists link together the cells that comprise the tables of the datacube. An algorithm that builds table lists requires only a single traversal of the ST. Thus the AST supports both cell-level and table-level queries. Algorithms to build and update datacubes stored as ASTs are shown. A web-based wireless sensor network OLAP server based on the AST is described.,2013,Knowledge Discovery and Data Mining,Fields of study: online analytical processingdata cubewireless sensor networktheoretical computer sciencedata miningdatabasemachine learningcomputer science
On the Optimality of Subsets of Features Selected by Heuristic and Hyper-heuristic Approaches,Kourosh Neshatian (University of Canterbury)Lucianne Varn (University of Canterbury),"198144689,2495250648","The concepts of relevance and redundancy are central to feature selection algorithms that do not use a learning algorithm for subset evaluation. Redundancy is in fact a special form of relevance where there is a correlation (linear or nonlinear) between the input features of a problem. Therefore, having a good heuristic for measuring relevance can also help detect redundancy. In this paper, we show that there is a lack of generality in the solutions found by heuristic measures. Through some counter-examples we show that regardless of the type of heuristic measure and search strategy, filter methods cannot optimise the performance of all learning algorithms. We show how different measures may have different notions of relevance between features and how this could lead to not detecting important features in certain problems. We then propose a hyper-heuristic method that generates an appropriate relevance measure for each problem. The new approach can alleviate problems related to missing relevant features.",2013,Knowledge Discovery and Data Mining,Fields of study: minimum redundancy feature selectiondata miningpattern recognitionmachine learningmathematics
Binary Classification Using Genetic Programming: Evolving Discriminant Functions with Dynamic Thresholds,Jill de Jong (University of Canterbury)Kourosh Neshatian (University of Canterbury),"2290518676,198144689",Binary classification is the problem of predicting which of two classes an input vector belongs to. This problem can be solved by using genetic programming to evolve discriminant functions which have a threshold output value that distinguishes between the two classes. The standard approach is to have a static threshold value of zero that is fixed throughout the evolution process. Items with a positive function output value are identified as one class and items with a negative function output value as the other class. We investigate a different approach where an optimum threshold is dynamically determined for each candidate function during the fitness evaluation. The optimum threshold is the one that achieves the lowest misclassification cost. It has an associated class translation rule for output values either side of the threshold value. The two approaches have been compared experimentally using four different datasets. Results suggest the dynamic threshold approach consistently achieves higher performance levels than the standard approach after equal numbers of fitness calls.,2013,Knowledge Discovery and Data Mining,Fields of study: threshold modelmachine learningmathematical optimizationstatisticsmathematics
Modelling Gene Networks by a Dynamic Bayesian Network-Based Model with Time Lag Estimation,Lian En Chai (Universiti Teknologi Malaysia)Mohd Saberi Mohamad (Universiti Teknologi Malaysia)Safaai Deris (Universiti Teknologi Malaysia)Chuii Khim Chong (Universiti Teknologi Malaysia)Yee Wen Choon (Universiti Teknologi Malaysia),"2161638723,2131251652,738100022,2157616592,2006160071","Due to the needs to discover the immense information and understand the underlying mechanism of gene regulations, modelling gene regulatory networks (GRNs) from gene expression data has attracted the interests of numerous researchers. To this end, the dynamic Bayesian network (DBN) has emerged as a popular method in GRNs modelling as it is able to model time-series gene expression data and feedback loops. Nevertheless, the commonly found missing values in gene expression data, the inability to take account of the transcriptional time lag, and the redundant computation time caused by the large search space, frequently inhibits the effectiveness of DBN in modelling GRNs from gene expression data. This paper proposes a DBN-based model (IST-DBN) with missing values imputation, potential regulators selection, and time lag estimation to tackle the aforementioned problems. To evaluate the performance of IST-DBN, we applied the model on the S. cerevisiae cell cycle time-series expression data. The experimental results revealed IST-DBN has decreased computation time and better accuracy in identifying gene-gene relationships when compared with existing DBN-based model and conventional DBN. Furthermore, we expect the resultant networks from IST-DBN to be applied as a general framework for potential gene intervention research.",2013,Knowledge Discovery and Data Mining,Fields of study: dynamic bayesian networkgene regulatory networkbioinformaticsdata miningmachine learningstatisticscomputer science
A Two-Stage Dual Space Reduction Framework for Multi-label Classification,Eakasit Pacharawongsakda (Sirindhorn International Institute of Technology)Thanaruk Theeramunkong (Sirindhorn International Institute of Technology),"87579199,202049940","Multi-label classification has been increasingly recognized since it can classify objects into multiple classes, simultaneously. However, its effectiveness might be sacrificed due to high dimensionality problem in feature space and sparseness problem in label space. To address these issues, this paper proposes a Two-Stage Dual Space Reduction (2SDSR) framework that transforms both feature space and label space into the lower-dimensional spaces. In our framework, the label space is transformed into reduced label space and then supervised dimensionality reduction method is applied to find a small number of features that maximizing dependency between features and that reduced labels. Using these reduced features and labels, a set of classification models are built. In this framework, we employ two well-known feature reduction methods such as MDDM and CCA, and two widely used label reduction methods i.e., PLST and BMD. However, it is possible to apply various dimensionality reduction methods into the framework. By a set of experiments on five real world datasets, the results indicated that our proposed framework can improve the classification performance, compared to the traditional dimensionality reduction approaches which reduce feature space or label space only.",2013,Knowledge Discovery and Data Mining,Fields of study: canonical correlationsingular value decompositiondimensionality reductioncombinatoricspattern recognitionmachine learningcomputer sciencemathematics
Identifying Dominant Economic Sectors and Stock Markets: A Social Network Mining Approach,Ram Babu Roy (Indian Institute of Technology Kharagpur)Uttam Kumar Sarkar (Indian Institute of Management Calcutta),"2559053926,2344037076","We propose a method to identify dominant economic sectors and stock markets using a social network approach to mining stock market data. Closing price data from January 1998 through January 2011 of 2698 stocks selected from 17 major stock market indices have been used in the analysis. A Minimum Spanning Tree (MST) has been constructed using the cross-correlations between weekly returns of the stocks. The MST has been chosen to obtain a simplified but connected network having linkages among similarly behaving stocks and it constitutes a social network of stocks for our study. The macroscopic interdependence networks among economic sectors as well as among stock markets have been derived from the microscopic linkages among stocks in the MST. The analysis of these derived macroscopic networks demonstrates that the European and the North American stock markets and Financial, Industrials, Materials, and Consumer Discretionary economic sectors dominate in the global stock markets.",2013,Knowledge Discovery and Data Mining,Fields of study: stock market bubbleeconomic sectorcentralityminimum spanning tree
M2LGP: Mining Multiple Level Gradual Patterns,Yogi Satrya Aryadinata (Centre national de la recherche scientifique)Anne Laurent (University of Montpellier)Michel Sala (Centre national de la recherche scientifique),"2090580317,2496334203,2190760782","Gradual patterns have been studied for many years as they contain precious information. They have been integrated in many expert systems and rule-based systems, for instance to reason on knowledge such as {\\em ''the greater the number of turns, the greater the number of car crashes\""}. In many cases, this knowledge has been considered as a rule {\\em ''the greater the number of turns $\\rightarrow$ the greater the number of car crashes\""} Historically, works have thus been focused on the representation of such rules, studying how implication could be defined, especially fuzzy implication. These rules were defined by experts who were in charge to describe the systems they were working on in order to turn them to operate automatically. More recently, approaches have been proposed in order to mine databases for automatically discovering such knowledge. Several approaches have been studied, the main scientific topics being: how to determine what is an relevant gradual pattern, and how to discover them as efficiently as possible (in terms of both memory and CPU usage). However, in some cases, end-users are not interested in raw level knowledge, and are rather interested in trends. Moreover, it may be the case that no relevant pattern can be discovered at a low level of granularity (e.g. city), whereas some can be discovered at a higher level (e.g. county). In this paper, we thus extend gradual pattern approaches in order to consider multiple level gradual patterns. For this purpose, we consider two aggregation policies, namely horizontal and vertical.",2013,Knowledge Discovery and Data Mining,Fields of study: data miningartificial intelligencemachine learningsimulationcomputer science
Applying Methods for Signal Detection in Spontaneous Reports to Electronic Patient Records,Jing Zhao (Stockholm University)Isak Karlsson (Stockholm University)Lars Asker (Stockholm University)Henrik Boström (Stockholm University),"2311812514,2123576956,2089372118,2082592554","Currently, pharmacovigilance relies mainly on disproportionality analysis of spontaneous reports. However, the analysis of spontaneous reports is concerned with several problems, such as reliability, under-reporting and insucient patient information. Longitudinal healthcare data, such as Electronic Patient Records (EPRs) in which comprehensive information of each patient is covered, is a complementary source of information to detect Adverse Drug Events (ADEs). A wide set of disproportionality methods has been developed for analyzing spontaneous reports to assess the risk of reported events being ADEs. This study aims to investigate the use of such methods for detecting ADEs when analyzing EPRs. The data used in this study was extracted from Stockholm EPR Corpus. Four disproportionality methods (proportional reporting rate, reporting odds ratio, Bayesian condence propagation neural network, and Gamma-Poisson shrinker) were applied in two dierent ways to analyze EPRs: creating pseudo spontaneous reports based on all observed drug-event pairs (event-level analysis) or analyzing distinct patients who experienced a drug-event pair (patient-level analysis). The methods were evaluated in a case study on safety surveillance of Celecoxib. The results showed that, among the top 200 signals, more ADEs were detected by the event-level analysis than by the patient-level analysis. Moreover, the event-level analysis also resulted in a higher mean average precision. The main conclusion of this study is that the way in which the disproportionality analysis is applied, the event-level or patient-level analysis, can have a much higher impact on the performance than which disproportionality method is employed.",2013,Knowledge Discovery and Data Mining,Fields of study: information systemdata scienceinformation retrievaldata miningstatisticscomputer science
Fast Entity Recognition in Biomedical Text,Amy Siu (Max Planck Society)Dat Ba Nguyen (Max Planck Society)Gerhard Weikum (Max Planck Society),"2167055687,2098329688,514836396",-,2013,Knowledge Discovery and Data Mining,-
Supervised patient similarity measure of heterogeneous patient records,Jimeng Sun (IBM)Fei Wang (IBM)Jianying Hu (IBM)Shahram Edabollahi (IBM),"2110385854,2465953593,2098665416,349380785","Patient similarity assessment is an important task in the context of patient cohort identif cation for comparative effectiveness studies and clinical decision support applications. The goal is to derive clinically meaningful distance metric to measure the similarity between patients represented by their key clinical indicators. How to incorporate physician feedback with regard to the retrieval results? How to interactively update the underlying similarity measure based on the feedback? Moreover, often different physicians have different understandings of patient similarity based on their patient cohorts. The distance metric learned for each individual physician often leads to a limited view of the true underlying distance metric. How to integrate the individual distance metrics from each physician into a globally consistent unif ed metric? We describe a suite of supervised metric learning approaches that answer the above questions. In particular, we present Locally Supervised Metric Learning (LSML) to learn a generalized Mahalanobis distance that is tailored toward physician feedback. Then we describe the interactive metric learning (iMet) method that can incrementally update an existing metric based on physician feedback in an online fashion. To combine multiple similarity measures from multiple physicians, we present Composite Distance Integration (Comdi) method. In this approach we f rst construct discriminative neighborhoods from each individual metrics, then combine them into a single optimal distance metric. Finally, we present a clinical decision support prototype system powered by the proposed patient similarity methods, and evaluate the proposed methods using real EHR data against several baselines.",2012,Knowledge Discovery and Data Mining,Fields of study: data miningpattern recognitionmachine learningcomputer science
Sparse methods for biomedical data,Jieping Ye (Arizona State University)Jun Liu (Siemens),"2305258894,2676596115","Following recent technological revolutions, the investigation of massive biomedical data with growing scale, diversity, and complexity has taken a center stage in modern data analysis. Although complex, the underlying representations of many biomedical data are often sparse. For example, for a certain disease such as leukemia, even though humans have tens of thousands of genes, only a few genes are relevant to the disease; a gene network is sparse since a regulatory pathway involves only a small number of genes; many biomedical signals are sparse or compressible in the sense that they have concise representations when expressed in a proper basis. Therefore, finding sparse representations is fundamentally important for scientific discovery. Sparse methods based on the '1 norm have attracted a great amount of research efforts in the past decade due to its sparsity-inducing property, convenient convexity, and strong theoretical guarantees. They have achieved great success in various applications such as biomarker selection, biological network construction, and magnetic resonance imaging. In this paper, we review state-of-the-art sparse methods and their applications to biomedical data.",2012,Knowledge Discovery and Data Mining,Fields of study: medical researchmagnetic resonance imagingdata sciencebioinformaticsdata miningmachine learningstatisticscomputer science
Next challenges for adaptive learning systems,Indre Zliobaite (Bournemouth University)Albert Bifet (University of Waikato)Mohamed Medhat Gaber (University of Portsmouth)Bogdan Gabrys (Bournemouth University)João Gama (University of Porto)Leandro L. Minku (University of Birmingham)Katarzyna Musial (King's College London),"2057986279,307521372,1976146371,2092807901,2113857198,1231678246,2026708028","Learning from evolving streaming data has become a 'hot' research topic in the last decade and many adaptive learning algorithms have been developed. This research was stimulated by rapidly growing amounts of industrial, transactional, sensor and other business data that arrives in real time and needs to be mined in real time. Under such circumstances, constant manual adjustment of models is in-efficient and with increasing amounts of data is becoming infeasible. Nevertheless, adaptive learning models are still rarely employed in business applications in practice. In the light of rapidly growing structurally rich 'big data', new generation of parallel computing solutions and cloud computing services as well as recent advances in portable computing devices, this article aims to identify the current key research directions to be taken to bring the adaptive learning closer to application needs. We identify six forthcoming challenges in designing and building adaptive learning (pre-diction) systems: making adaptive systems scalable, dealing with realistic data, improving usability and trust, integrat-ing expert knowledge, taking into account various application needs, and moving from adaptive algorithms towards adaptive tools. Those challenges are critical for the evolving stream settings, as the process of model building needs to be fully automated and continuous.",2012,Knowledge Discovery and Data Mining,Fields of study: computingdata sciencedata miningmachine learningsimulationcomputer science
Data mining methodologies for pharmacovigilance,Mei Liu (New Jersey Institute of Technology)Michael E. Matheny (Vanderbilt University)Yong Hu (Sun Yat-sen University)Hua Xu (Vanderbilt University),"2600077860,2105614844,2247417345,2625138606","Medicines are designed to cure, treat, or prevent diseases; however, there are also risks in taking any medicine - particularly short term or long term adverse drug reactions (ADRs) can cause serious harm to patients. Adverse drug events have been estimated to cause over 700,000 emergency department visits each year in the United States. Thus, for medication safety, ADR monitoring is required for each drug throughout its life cycle, including early stages of drug design, different phases of clinical trials, and postmarketing surveillance. Pharmacovigilance (PhV) is the science that concerns with the detection, assessment, understanding and prevention of ADRs. In the pre-marketing stages of a drug, PhV primarily focuses on predicting potential ADRs using preclinical characteristics of the compounds (e.g., drug targets, chemical structure) or screening data (e.g., bioassay data). In the postmarketing stage, PhV has traditionally involved in mining spontaneous reports submitted to national surveillance systems. The research focus is currently shifting toward the use of data generated from platforms outside the conventional framework such as electronic medical records (EMRs), biomedical literature, and patient-reported data in online health forums. The emerging trend of PhV is to link preclinical data from the experimental platform with human safety information observed in the postmarketing phase. This article provides a general overview of the current computational methodologies applied for PhV at different stages of drug development and concludes with future directions and challenges.",2012,Knowledge Discovery and Data Mining,Fields of study: postmarketing surveillancepharmacovigilancedata miningcomputer science
Cross domain similarity mining: research issues and potential applications including supporting research by analogy,Guozhu Dong (Wright State University),2164298414,"This paper defines the cross domain similarity mining (CDSM) problem, and motivates CDSM with several potential applications. CDSM has big potential in (1) supporting understanding transfer and (2) supporting research by analogy, since similarity is vital to understanding/meaning and to identifying analogy, and since analogy is a fundamental approach frequently used in hypothesis generation and in research. CDSM also has big potential in (3) advancing learning transfer since cross domain similarities can shed light on how to best adapt classifiers/clusterings across given domains and how to avoid negative transfer. CDSM can also be useful for (4) solving the schema/ontology matching problem. Moreover, this paper gives a list of potential research questions for CDSM, and compares CDSM with related studies. One purpose of this paper is to introduce the CDSM problem to the wide KDD community in order to quickly realize the full potential of CDSM.",2012,Knowledge Discovery and Data Mining,Fields of study: data sciencedata miningartificial intelligencecomputer science
Introduction to the special section on clinical data mining,Shipeng Yu (Siemens Healthcare)Bharat Rao (Siemens Healthcare),"2162740804,2570336074","Mining clinical data is a fast-evolving field, ranging from mining patient data of a particular type (e.g., images, genomics) to mining the increased amount of mixed-format information (databases, free text, images, labs, etc) in electronic health records (EHR), to selecting, extracting and synthesizing relevant knowledge from large medical corpuses, to the promise of personalized medicine where therapy and prevention are tailored to smaller and smaller patient subpopulations, down to the individual patient. Clinical data mining can be a key asset in driving vast systemic improvements in healthcare, leading to improved patient outcomes and reduced healthcare costs. In this report we briefly survey the latest advancements in this field, and introduce four selected articles that cover both state-of-the-art data mining techniques for clinical data and discuss emerging clinical data mining applications.",2012,Knowledge Discovery and Data Mining,Fields of study: data sciencedata miningcomputer science
"Mining anatomical, physiological and pathological information from medical images",Xiang Sean Zhou (Siemens Healthcare)Yiqiang Zhan (Siemens Healthcare)Vikas C. Raykar (Siemens Healthcare)Gerardo Hermosillo (Siemens Healthcare)Luca Bogoni (Siemens Healthcare)Zhipang Peng (Siemens Healthcare),"2420712277,2121590643,1764448532,2288628241,1247940802,2287148113","The field of medical imaging has shown substantial growth over the last decade. Even more dramatic increase was observed in the use of machine learning and data mining techniques within this field. In this paper, we discuss three aspects related to information mining in the domain of medical imaging: the target user groups (for whom), the information to mine (what), and technologies to enable mining (how). Specifically, we focus on three types of information: anatomical, physiological and pathological, and present use cases for each one of them. Furthermore, we introduce representative methods and algorithms that are effective for solving these problems. We conclude the paper by discussing some major trends in the related domains for the coming decade.",2012,Knowledge Discovery and Data Mining,Fields of study: data sciencedata miningcomputer science
Triangle listing in massive networks,Shumo Chu (The Chinese University of Hong Kong)James Cheng (The Chinese University of Hong Kong),"2119604566,2304873892","Triangle listing is one of the fundamental algorithmic problems whose solution has numerous applications especially in the analysis of complex networks, such as the computation of clustering coefficients, transitivity, triangular connectivity, trusses, etc. Existing algorithms for triangle listing are mainly in-memory algorithms, whose performance cannot scale with the massive volume of today's fast growing networks. When the input graph cannot fit in main memory, triangle listing requires random disk accesses that can incur prohibitively huge I/O cost. Some streaming, semistreaming, and sampling algorithms have been proposed but these are approximation algorithms. We propose an I/O-efficient algorithm for triangle listing. Our algorithm is exact and avoids random disk access. Our results show that our algorithm is scalable and outperforms the state-of-the-art in-memory and local triangle estimation algorithms.",2012,Knowledge Discovery and Data Mining,Fields of study: discrete mathematicscombinatoricsmathematical optimizationmathematics
Summarizing data succinctly with the most informative itemsets,Michael Mampaey (University of Antwerp)Jilles Vreeken (University of Antwerp)Nikolaj Tatti (University of Antwerp),"2027051129,1971070670,1367500519","Knowledge discovery from data is an inherently iterative process. That is, what we know about the data greatly determines our expectations, and therefore, what results we would find interesting and/or surprising. Given new knowledge about the data, our expectations will change. Hence, in order to avoid redundant results, knowledge discovery algorithms ideally should follow such an iterative updating procedure. With this in mind, we introduce a well-founded approach for succinctly summarizing data with the most informative itemsets; using a probabilistic maximum entropy model, we iteratively find the itemset that provides us the most novel information—that is, for which the frequency in the data surprises us the most—and in turn we update our model accordingly. As we use the maximum entropy principle to obtain unbiased probabilistic models, and only include those itemsets that are most informative with regard to the current model, the summaries we construct are guaranteed to be both descriptive and nonredundant. The algorithm that we present, called mtv, can either discover the top- k most informative itemsets, or we can employ either the Bayesian Information Criterion (bic) or the Minimum Description Length (mdl) principle to automatically identify the set of itemsets that together summarize the data well. In other words, our method will “tell you what you need to know” about the data. Importantly, it is a one-phase algorithm: rather than picking itemsets from a user-provided candidate set, itemsets and their supports are mined on-the-fly. To further its applicability, we provide an efficient method to compute the maximum entropy distribution using Quick Inclusion-Exclusion. Experiments on our method, using synthetic, benchmark, and real data, show that the discovered summaries are succinct, and correctly identify the key patterns in the data. The models they form attain high likelihoods, and inspection shows that they summarize the data well with increasingly specific, yet nonredundant itemsets.",2012,Knowledge Discovery and Data Mining,Fields of study: inclusion exclusion principleprinciple of maximum entropyautomatic summarizationdata miningpattern recognitionmachine learningstatisticscomputer sciencemathematics
Special issue on best of SIGKDD 2011,"Joydeep Ghosh (University of Texas at Austin)Padhraic Smyth (University of California, Irvine)Andrew Tomkins (Yahoo!)Rich Caruana (Cornell University)","2148168557,2137074633,2130754085,2106722673",-,2012,Knowledge Discovery and Data Mining,-
Substantial improvements in the set-covering projection classifier CHIRP (composite hypercubes on iterated random projections),Leland Wilkinson (University of Illinois at Chicago)Anushka Anand (University of Illinois at Chicago)Tuan Nhon Dang (University of Illinois at Chicago),"2098682828,2088213256,2010885274","In Wilkinson et al. [2011] we introduced a new set-covering random projection classifier that achieved average error lower than that of other classifiers in the Weka platform. This classifier was based on an L i norm distance function and exploited an iterative sequence of three stages (projecting, binning, and covering) to deal with the curse of dimensionality, computational complexity, and nonlinear separability. We now present substantial changes that improve robustness and reduce training and testing time by almost an order of magnitude without jeopardizing CHIRP's outstanding error performance.",2012,Knowledge Discovery and Data Mining,Fields of study: pattern recognitionmachine learningstatisticscomputer sciencemathematics
Discovering regions of different functions in a city using human mobility and POIs,Jing Yuan (Microsoft)Yu Zheng (Microsoft)Xing Xie (Microsoft),"2591433979,2145115012,2125800575","The development of a city gradually fosters different functional regions, such as educational areas and business districts. In this paper, we propose a framework (titled DRoF) that Discovers Regions of different Functions in a city using both human mobility among regions and points of interests (POIs) located in a region. Specifically, we segment a city into disjointed regions according to major roads, such as highways and urban express ways. We infer the functions of each region using a topic-based inference model, which regards a region as a document, a function as a topic, categories of POIs (e.g., restaurants and shopping malls) as metadata (like authors, affiliations, and key words), and human mobility patterns (when people reach/leave a region and where people come from and leave for) as words. As a result, a region is represented by a distribution of functions, and a function is featured by a distribution of mobility patterns. We further identify the intensity of each function in different locations. The results generated by our framework can benefit a variety of applications, including urban planning, location choosing for a business, and social recommendations. We evaluated our method using large-scale and real-world datasets, consisting of two POI datasets of Beijing (in 2010 and 2011) and two 3-month GPS trajectory datasets (representing human mobility) generated by over 12,000 taxicabs in Beijing in 2010 and 2011 respectively. The results justify the advantages of our approach over baseline methods solely using POIs or human mobility.",2012,Knowledge Discovery and Data Mining,Fields of study: point of interesturban planningdata miningsimulation
Searching and mining trillions of time series subsequences under dynamic time warping,"Thanawin Rakthanmanon (University of California, Riverside)Bilson J. L. Campana (University of California, Riverside)Abdullah Mueen (University of California, Riverside)Gustavo E. A. P. A. Batista (University of São Paulo)M. Brandon Westover (Brigham and Women's Hospital)Qiang Zhu 0002 (University of California, Riverside)Jesin Zakaria (University of California, Riverside)Eamonn J. Keogh (University of California, Riverside)","141314290,2125777994,2083987245,2165222361,2037513643,2312006013,2129110089,2170070822","Most time series data mining algorithms use similarity search as a core subroutine, and thus the time taken for similarity search is the bottleneck for virtually all time series data mining algorithms. The difficulty of scaling search to large datasets largely explains why most academic work on time series data mining has plateaued at considering a few millions of time series objects, while much of industry and science sits on billions of time series objects waiting to be explored. In this work we show that by using a combination of four novel ideas we can search and mine truly massive time series for the first time. We demonstrate the following extremely unintuitive fact; in large datasets we can exactly search under DTW much more quickly than the current state-of-the-art Euclidean distance search algorithms. We demonstrate our work on the largest set of time series experiments ever attempted. In particular, the largest dataset we consider is larger than the combined size of all of the time series datasets considered in all data mining papers ever published. We show that our ideas allow us to solve higher-level time series data mining problem such as motif discovery and clustering at scales that would otherwise be untenable. In addition to mining massive datasets, we will show that our ideas also have implications for real-time monitoring of data streams, allowing us to handle much faster arrival rates and/or use cheaper and lower powered devices than are currently possible.",2012,Knowledge Discovery and Data Mining,Fields of study: dynamic time warpingeuclidean distancesearch algorithmtime seriesupper and lower boundsdata stream miningdata sciencedata miningmachine learningstatisticscomputer science
Open domain event extraction from twitter,Alan Ritter (University of Washington)Oren Etzioni (University of Washington)Sam Clark (University of Washington),"2154095546,57747768,2116147940","Tweets are the most up-to-date and inclusive stream of in- formation and commentary on current events, but they are also fragmented and noisy, motivating the need for systems that can extract, aggregate and categorize important events. Previous work on extracting structured representations of events has focused largely on newswire text; Twitter's unique characteristics present new challenges and opportunities for open-domain event extraction. This paper describes TwiCal-- the first open-domain event-extraction and categorization system for Twitter. We demonstrate that accurately extracting an open-domain calendar of significant events from Twitter is indeed feasible. In addition, we present a novel approach for discovering important event categories and classifying extracted events based on latent variable models. By leveraging large volumes of unlabeled data, our approach achieves a 14% increase in maximum F1 over a supervised baseline. A continuously updating demonstration of our system can be viewed at http://statuscalendar.com; Our NLP tools are available at http://github.com/aritter/ twitter_nlp.",2012,Knowledge Discovery and Data Mining,Fields of study: social medialatent variable modelinformation extractiondata scienceworld wide webdata miningmachine learningcomputer science
Information diffusion and external influence in networks,Seth A. Myers (Stanford University)Chenguang Zhu (Stanford University)Jure Leskovec (Stanford University),"2146613377,2113736906,1878631932","Social networks play a fundamental role in the diffusion of information. However, there are two different ways of how information reaches a person in a network. Information reaches us through connections in our social networks, as well as through the influence external out-of-network sources, like the mainstream media. While most present models of information adoption in networks assume information only passes from a node to node via the edges of the underlying network, the recent availability of massive online social media data allows us to study this process in more detail. We present a model in which information can reach a node via the links of the social network or through the influence of external sources. We then develop an efficient model parameter fitting technique and apply the model to the emergence of URL mentions in the Twitter network. Using a complete one month trace of Twitter we study how information reaches the nodes of the network. We quantify the external influences over time and describe how these influences affect the information adoption. We discover that the information tends to ""jump"" across the network, which can only be explained as an effect of an unobservable external influence on the network. We find that only about 71% of the information volume in Twitter can be attributed to network diffusion, and the remaining 29% is due to external events and factors outside the network.",2012,Knowledge Discovery and Data Mining,Fields of study: dynamic network analysisnetwork formationsocial networkworld wide websocial scienceartificial intelligencesimulationcomputer science
Towards social user profiling: unified and discriminative influence model for inferring home locations,Rui Li (University of Illinois at Urbana–Champaign)Shengjie Wang (University of Illinois at Urbana–Champaign)Hongbo Deng (University of Illinois at Urbana–Champaign)Rui Wang (University of Illinois at Urbana–Champaign)Kevin Chen Chuan Chang (University of Illinois at Urbana–Champaign),"2244119537,2124458349,2659439019,2680973183,2434208492","Users' locations are important to many applications such as targeted advertisement and news recommendation. In this paper, we focus on the problem of profiling users' home locations in the context of social network (Twitter). The problem is nontrivial, because signals, which may help to identify a user's location, are scarce and noisy. We propose a unified discriminative influence model, named as UDI, to solve the problem. To overcome the challenge of scarce signals, UDI integrates signals observed from both social network (friends) and user-centric data (tweets) in a unified probabilistic framework. To overcome the challenge of noisy signals, UDI captures how likely a user connects to a signal with respect to 1) the distance between the user and the signal, and 2) the influence scope of the signal. Based on the model, we develop local and global location prediction methods. The experiments on a large scale data set show that our methods improve the state-of-the-art methods by 13%, and achieve the best performance.",2012,Knowledge Discovery and Data Mining,Fields of study: social networkinternet privacyworld wide webdata miningmachine learning
Streaming graph partitioning for large distributed graphs,"Isabelle Stanton (University of California, Berkeley)Gabriel Kliot (Microsoft)","1982293450,326634043","Extracting knowledge by performing computations on graphs is becoming increasingly challenging as graphs grow in size. A standard approach distributes the graph over a cluster of nodes, but performing computations on a distributed graph is expensive if large amount of data have to be moved. Without partitioning the graph, communication quickly becomes a limiting factor in scaling the system up. Existing graph partitioning heuristics incur high computation and communication cost on large graphs, sometimes as high as the future computation itself. Observing that the graph has to be loaded into the cluster, we ask if the partitioning can be done at the same time with a lightweight streaming algorithm. We propose natural, simple heuristics and compare their performance to hashing and METIS, a fast, offline heuristic. We show on a large collection of graph datasets that our heuristics are a significant improvement, with the best obtaining an average gain of 76%. The heuristics are scalable in the size of the graphs and the number of partitions. Using our streaming partitioning methods, we are able to speed up PageRank computations on Spark, a distributed computation system, by 18% to 39% for large social networks.",2012,Knowledge Discovery and Data Mining,Fields of study: graph bandwidthstreaming algorithmgraph partitiontheoretical computer sciencecombinatoricsdistributed computingmachine learningcomputer science
Constructing popular routes from uncertain trajectories,Ling-Yin Wei (National Chiao Tung University)Yu Zheng (Microsoft)Wen-Chih Peng (National Chiao Tung University),"2684453606,2145115012,2100961564","The advances in location-acquisition technologies have led to a myriad of spatial trajectories. These trajectories are usually generated at a low or an irregular frequency due to applications' characteristics or energy saving, leaving the routes between two consecutive points of a single trajectory uncertain (called an uncertain trajectory). In this paper, we present a Route Inference framework based on Collective Knowledge (abbreviated as RICK) to construct the popular routes from uncertain trajectories. Explicitly, given a location sequence and a time span, the RICK is able to construct the top-k routes which sequentially pass through the locations within the specified time span, by aggregating such uncertain trajectories in a mutual reinforcement way (i.e., uncertain + uncertain → certain). Our work can benefit trip planning, traffic management, and animal movement studies. The RICK comprises two components: routable graph construction and route inference . First, we explore the spatial and temporal characteristics of uncertain trajectories and construct a routable graph by collaborative learning among the uncertain trajectories. Second, in light of the routable graph, we propose a routing algorithm to construct the top-k routes according to a user-specified query. We have conducted extensive experiments on two real datasets, consisting of Foursquare check-in datasets and taxi trajectories. The results show that RICK is both effective and efficient.",2012,Knowledge Discovery and Data Mining,Fields of study: animal locomotionsocial mediaactive traffic managementcollaborative learningdata miningartificial intelligencemachine learningsimulationcomputer science
Circle-based recommendation in online social networks,Xiwang Yang (Polytechnic Institute of New York University)Harald Steck (Bell Labs)Yong Liu (Polytechnic Institute of New York University),"2161939984,2617630235,2577728620","Online social network information promises to increase recommendation accuracy beyond the capabilities of purely rating/feedback-driven recommender systems (RS). As to better serve users' activities across different domains, many online social networks now support a new feature of ""Friends Circles"", which refines the domain-oblivious ""Friends"" concept. RS should also benefit from domain-specific ""Trust Circles"". Intuitively, a user may trust different subsets of friends regarding different domains. Unfortunately, in most existing multi-category rating datasets, a user's social connections from all categories are mixed together. This paper presents an effort to develop circle-based RS. We focus on inferring category-specific social trust circles from available rating data combined with social network data. We outline several variants of weighting friends within circles based on their inferred expertise levels. Through experiments on publicly available data, we demonstrate that the proposed circle-based recommendation models can better utilize user's social trust information, resulting in increased recommendation accuracy.",2012,Knowledge Discovery and Data Mining,Fields of study: collaborative filteringsocial networkrecommender systemworld wide webdata miningmachine learningcomputer science
Discovering value from community activity on focused question answering sites: a case study of stack overflow,Ashton Anderson (Stanford University)Daniel P. Huttenlocher (Cornell University)Jon M. Kleinberg (Cornell University)Jure Leskovec (Stanford University),"2155788867,677134955,2261367123,1878631932","Question answering (Q&A) websites are now large repositories of valuable knowledge. While most Q&A sites were initially aimed at providing useful answers to the question asker, there has been a marked shift towards question answering as a community-driven knowledge creation process whose end product can be of enduring value to a broad audience. As part of this shift, specific expertise and deep knowledge of the subject at hand have become increasingly important, and many Q&A sites employ voting and reputation mechanisms as centerpieces of their design to help users identify the trustworthiness and accuracy of the content. To better understand this shift in focus from one-off answers to a group knowledge-creation process, we consider a question together with its entire set of corresponding answers as our fundamental unit of analysis, in contrast with the focus on individual question-answer pairs that characterized previous work. Our investigation considers the dynamics of the community activity that shapes the set of answers, both how answers and voters arrive over time and how this influences the eventual outcome. For example, we observe significant assortativity in the reputations of co-answerers, relationships between reputation and answer speed, and that the probability of an answer being chosen as the best one strongly depends on temporal characteristics of answer arrivals. We then show that our understanding of such properties is naturally applicable to predicting several important quantities, including the long-term value of the question and its answers, as well as whether a question requires a better answer. Finally, we discuss the implications of these results for the design of Q&A sites.",2012,Knowledge Discovery and Data Mining,Fields of study: fundamental unitreputationquestion answeringknowledge managementworld wide webdata miningcomputer science
Event-based social networks: linking the online and offline social worlds,Xingjie Liu (Pennsylvania State University)Qi He (IBM)Yuanyuan Tian (IBM)Wang-Chien Lee (Pennsylvania State University)John McPherson (IBM)Jiawei Han (University of Illinois at Urbana–Champaign),"2141742294,2309017108,2129760627,2143778659,2106934193,2121939561","Newly emerged event-based online social services, such as Meetup and Plancast, have experienced increased popularity and rapid growth. From these services, we observed a new type of social network - event-based social network (EBSN). An EBSN does not only contain online social interactions as in other conventional online social networks, but also includes valuable offline social interactions captured in offline activities. By analyzing real data collected from Meetup, we investigated EBSN properties and discovered many unique and interesting characteristics, such as heavy-tailed degree distributions and strong locality of social interactions. We subsequently studied the heterogeneous nature (co-existence of both online and offline social interactions) of EBSNs on two challenging problems: community detection and information flow. We found that communities detected in EBSNs are more cohesive than those in other types of social networks (e.g. location-based social networks). In the context of information flow, we studied the event recommendation problem. By experimenting various information diffusion patterns, we found that a community-based diffusion model that takes into account of both online and offline interactions provides the best prediction power. This paper is the first research to study EBSNs at scale and paves the way for future studies on this new type of social network. A sample dataset of this study can be downloaded from http://www.largenetwork.org/ebsn.",2012,Knowledge Discovery and Data Mining,Fields of study: heterogeneous networkheavy tailed distributiondegree distributionsocial network analysisdiffusioninformation flowsocial behaviorsocial networkdata collectionsocial computingsocial relationinternet privacyworld wide web
MoodLens: an emoticon-based sentiment analysis system for chinese tweets,Jichang Zhao (Beihang University)Li Dong (Beihang University)Junjie Wu (Beihang University)Ke Xu (Beihang University),"2153657934,2649943809,2149366604,2600070012","Recent years have witnessed the explosive growth of online social media. Weibo, a Twitter-like online social network in China, has attracted more than 300 million users in less than three years, with more than 1000 tweets generated in every second. These tweets not only convey the factual information, but also reflect the emotional states of the authors, which are very important for understanding user behaviors. However, a tweet in Weibo is extremely short and the words it contains evolve extraordinarily fast. Moreover, the Chinese corpus of sentiments is still very small, which prevents the conventional keyword-based methods from being used. In light of this, we build a system called MoodLens , which to our best knowledge is the first system for sentiment analysis of Chinese tweets in Weibo. In MoodLens , 95 emoticons are mapped into four categories of sentiments, i.e. angry, disgusting, joyful, and sad, which serve as the class labels of tweets. We then collect over 3.5 million labeled tweets as the corpus and train a fast Naive Bayes classifier, with an empirical precision of 64.3%. MoodLens also implements an incremental learning method to tackle the problem of the sentiment shift and the generation of new words. Using MoodLens for real-time tweets obtained from Weibo, several interesting temporal and spatial patterns are observed. Also, sentiment variations are well captured by MoodLens to effectively detect abnormal events in China. Finally, by using the highly efficient Naive Bayes classifier, MoodLens is capable of online real-time sentiment monitoring. The demo of MoodLens can be found at http://goo.gl/8DQ65.",2012,Knowledge Discovery and Data Mining,Fields of study: social medianaive bayes classifiercommon spatial patternsentiment analysisinternet privacyworld wide webdata miningmachine learningcomputer science
Rise and fall patterns of information diffusion: model and implications,"Yasuko Matsubara (Kyoto University)Yasushi Sakurai (Nippon Telegraph and Telephone)B. Aditya Prakash (Carnegie Mellon University)Lei Li (University of California, Berkeley)Christos Faloutsos (Carnegie Mellon University)","2106416586,1989929707,2124002246,2608982952,2198983026","The recent explosion in the adoption of search engines and new media such as blogs and Twitter have facilitated faster propagation of news and rumors. How quickly does a piece of news spread over these media? How does its popularity diminish over time? Does the rising and falling pattern follow a simple universal law? In this paper, we propose SpikeM, a concise yet flexible analytical model for the rise and fall patterns of influence propagation. Our model has the following advantages: (a) unification power: it generalizes and explains earlier theoretical models and empirical observations; (b) practicality: it matches the observed behavior of diverse sets of real data; (c) parsimony: it requires only a handful of parameters; and (d) usefulness: it enables further analytics tasks such as fore- casting, spotting anomalies, and interpretation by reverse- engineering the system parameters of interest (e.g. quality of news, count of interested bloggers, etc.). Using SpikeM, we analyzed 7.2GB of real data, most of which were collected from the public domain. We have shown that our SpikeM model accurately and succinctly describes all the patterns of the rise-and-fall spikes in these real datasets.",2012,Knowledge Discovery and Data Mining,Fields of study: social networkdata sciencedata miningartificial intelligencemachine learningsimulationstatisticscomputer science
Defining and evaluating network communities based on ground-truth,Jaewon Yang (Stanford University)Jure Leskovec (Stanford University),"2131921352,1878631932","Nodes in real-world networks, such as social, information or technological networks, organize into communities where edges appear with high concentration among the members of the community. Identifying communities in networks has proven to be a challenging task mainly due to a plethora of definitions of a community, intractability of algorithms, issues with evaluation and the lack of a reliable gold-standard ground-truth. We study a set of 230 large social, collaboration and information networks where nodes explicitly define group memberships. We use these groups to define the notion of ground-truth communities. We then propose a methodology which allows us to compare and quantitatively evaluate different definitions of network communities on a large scale. We choose 13 commonly used definitions of network communities and examine their quality, sensitivity and robustness. We show that the 13 definitions naturally group into four classes. We find that two of these definitions, Conductance and Triad-participation-ratio, consistently give the best performance in identifying ground-truth communities.",2012,Knowledge Discovery and Data Mining,Fields of study: gold standardmodularityground truthmanagement sciencesocial psychologydata miningcomputer science
Review spam detection via temporal pattern discovery,Sihong Xie (University of Illinois at Chicago)Guan Wang (University of Illinois at Chicago)Shuyang Lin (University of Illinois at Chicago)Philip S. Yu (University of Illinois at Chicago),"2106011892,2152311907,2096615129,2125104194","Online reviews play a crucial role in today's electronic commerce. It is desirable for a customer to read reviews of products or stores before making the decision of what or from where to buy. Due to the pervasive spam reviews, customers can be misled to buy low-quality products, while decent stores can be defamed by malicious reviews. We observe that, in reality, a great portion (> 90% in the data we study) of the reviewers write only one review (singleton review). These reviews are so enormous in number that they can almost determine a store's rating and impression. However, existing methods did not examine this larger part of the reviews. Are most of these singleton reviews truthful ones? If not, how to detect spam reviews in singleton reviews? We call this problem singleton review spam detection. To address this problem, we observe that the normal reviewers' arrival pattern is stable and uncorrelated to their rating pattern temporally. In contrast, spam attacks are usually bursty and either positively or negatively correlated to the rating. Thus, we propose to detect such attacks via unusually correlated temporal patterns. We identify and construct multidimensional time series based on aggregate statistics, in order to depict and mine such correlations. In this way, the singleton review spam detection problem is mapped to a abnormally correlated pattern detection problem. We propose a hierarchical algorithm to robustly detect the time windows where such attacks are likely to have happened. The algorithm also pinpoints such windows in different time resolutions to facilitate faster human inspection. Experimental results show that the proposed method is effective in detecting singleton review attacks. We discover that singleton review is a significant source of spam reviews and largely affects the ratings of online stores.",2012,Knowledge Discovery and Data Mining,Fields of study: time seriese commerceinternet privacyworld wide webdata miningmachine learningstatisticscomputer science
Inferring land use from mobile phone activity,Jameson L. Toole (Massachusetts Institute of Technology)Michael Ulm (Austrian Institute of Technology)Marta C. González (Massachusetts Institute of Technology)Dietmar Bauer (Austrian Institute of Technology),"2149823502,2042650420,2277322729,2149149383","Understanding the spatiotemporal distribution of people within a city is crucial to many planning applications. Obtaining data to create required knowledge, currently involves costly survey methods. At the same time ubiquitous mobile sensors from personal GPS devices to mobile phones are collecting massive amounts of data on urban systems. The locations, communications, and activities of millions of people are recorded and stored by new information technologies. This work utilizes novel dynamic data, generated by mobile phone users, to measure spatiotemporal changes in population. In the process, we identify the relationship between land use and dynamic population over the course of a typical week. A machine learning classification algorithm is used to identify clusters of locations with similar zoned uses and mobile phone activity patterns. It is shown that the mobile phone data is capable of delivering useful information on actual land use that supplements zoning regulations.",2012,Knowledge Discovery and Data Mining,Fields of study: mobile searchmobile databasesurvey methodologydynamic dataland useinformation technologymobile computinginternet privacydata miningsimulation
Trustworthy online controlled experiments: five puzzling outcomes explained,Ron Kohavi (Microsoft)Alex Deng (Microsoft)Brian Frasca (Microsoft)Roger Longbotham (Microsoft)Toby Walker (Microsoft)Ya Xu (Microsoft),"73615348,2172042952,1598186712,2067073079,2623983410,2310280520","Online controlled experiments are often utilized to make data-driven decisions at Amazon, Microsoft, eBay, Facebook, Google, Yahoo, Zynga, and at many other companies. While the theory of a controlled experiment is simple, and dates back to Sir Ronald A. Fisher's experiments at the Rothamsted Agricultural Experimental Station in England in the 1920s, the deployment and mining of online controlled experiments at scale--thousands of experiments now--has taught us many lessons. These exemplify the proverb that the difference between theory and practice is greater in practice than in theory. We present our learnings as they happened: puzzling outcomes of controlled experiments that we analyzed deeply to understand and explain. Each of these took multiple-person weeks to months to properly analyze and get to the often surprising root cause. The root causes behind these puzzling results are not isolated incidents; these issues generalized to multiple experiments. The heightened awareness should help readers increase the trustworthiness of the results coming out of controlled experiments. At Microsoft's Bing, it is not uncommon to see experiments that impact annual revenue by millions of dollars, thus getting trustworthy results is critical and investing in understanding anomalies has tremendous payoff: reversing a single incorrect decision based on the results of an experiment can fund a whole team of analysts. The topics we cover include: the OEC (Overall Evaluation Criterion), click tracking, effect trends, experiment length and power, and carryover effects.",2012,Knowledge Discovery and Data Mining,Fields of study: randomized experimentoperations researchdata miningartificial intelligencesimulationstatisticscomputer sciencemathematics
Integrating meta-path selection with user-guided object clustering in heterogeneous information networks,"Yizhou Sun (University of Illinois at Urbana–Champaign)Brandon Norick (University of Illinois at Urbana–Champaign)Jiawei Han (University of Illinois at Urbana–Champaign)Xifeng Yan (University of California, Santa Barbara)Philip S. Yu (University of Illinois at Chicago)Xiao Yu (University of Illinois at Urbana–Champaign)","2131539564,2222819066,2121939561,2116657824,2125104194,2160715520","Real-world, multiple-typed objects are often interconnected, forming heterogeneous information networks. A major challenge for link-based clustering in such networks is its potential to generate many different results, carrying rather diverse semantic meanings. In order to generate desired clustering, we propose to use meta-path , a path that connects object types via a sequence of relations, to control clustering with distinct semantics. Nevertheless, it is easier for a user to provide a few examples (""seeds"") than a weighted combination of sophisticated meta-paths to specify her clustering preference. Thus, we propose to integrate meta-path selection with user-guided clustering to cluster objects in networks, where a user first provides a small set of object seeds for each cluster as guidance. Then the system learns the weights for each meta-path that are consistent with the clustering result implied by the guidance, and generates clusters under the learned weights of meta-paths. A probabilistic approach is proposed to solve the problem, and an effective and efficient iterative algorithm, PathSelClus , is proposed to learn the model, where the clustering quality and the meta-path weights are mutually enhancing each other. Our experiments with several clustering tasks in two real networks demonstrate the power of the algorithm in comparison with the baselines.",2012,Knowledge Discovery and Data Mining,Fields of study: flame clusteringhierarchical clustering of networksbrown clusteringcanopy clustering algorithmdetermining the number of clusters in a data setdbscancorrelation clusteringconstrained clusteringdata stream clusteringcure data clustering algorithmaffinity propagationfuzzy clusteringclustering high dimensional dataiterative methodcluster analysisconsensus clusteringconceptual clusteringtheoretical computer sciencedata miningmachine learningcomputer science
Cross-domain collaboration recommendation,Jie Tang (Tsinghua University)Sen Wu (Tsinghua University)Jimeng Sun (IBM)Hang Su (Tsinghua University),"2158012360,2443423181,2110385854,2627558300","Interdisciplinary collaborations have generated huge impact to society. However, it is often hard for researchers to establish such cross-domain collaborations. What are the patterns of cross-domain collaborations? How do those collaborations form? Can we predict this type of collaborations? Cross-domain collaborations exhibit very different patterns compared to traditional collaborations in the same domain: 1) sparse connection : cross-domain collaborations are rare; 2) complementary expertise : cross-domain collaborators often have different expertise and interest; 3) topic skewness : cross-domain collaboration topics are focused on a subset of topics. All these patterns violate fundamental assumptions of traditional recommendation systems. In this paper, we analyze the cross-domain collaboration data from research publications and confirm the above patterns. We propose the Cross-domain Topic Learning (CTL) model to address these challenges. For handling sparse connections, CTL consolidates the existing cross-domain collaborations through topic layers instead of at author layers, which alleviates the sparseness issue. For handling complementary expertise, CTL models topic distributions from source and target domains separately, as well as the correlation across domains. For handling topic skewness, CTL only models relevant topics to the cross-domain collaboration. We compare CTL with several baseline approaches on large publication datasets from different domains. CTL outperforms baselines significantly on multiple recommendation metrics. Beyond accurate recommendation performance, CTL is also insensitive to parameter tuning as confirmed in the sensitivity analysis.",2012,Knowledge Discovery and Data Mining,Fields of study: social influencesensitivity analysissocial networkrecommender systemdata scienceworld wide webdata miningcomputer science
GigaTensor: scaling tensor analysis up by 100 times - algorithms and discoveries,U. Kang (Carnegie Mellon University)Evangelos E. Papalexakis (Carnegie Mellon University)Abhay Harpale (Carnegie Mellon University)Christos Faloutsos (Carnegie Mellon University),"2426051778,1418764031,1967260281,2198983026","Many data are modeled as tensors, or multi dimensional arrays. Examples include the predicates (subject, verb, object) in knowledge bases, hyperlinks and anchor texts in the Web graphs, sensor streams (time, location, and type), social networks over time, and DBLP conference-author-keyword relations. Tensor decomposition is an important data mining tool with various applications including clustering, trend detection, and anomaly detection. However, current tensor decomposition algorithms are not scalable for large tensors with billions of sizes and hundreds millions of nonzeros: the largest tensor in the literature remains thousands of sizes and hundreds thousands of nonzeros. Consider a knowledge base tensor consisting of about 26 million noun-phrases. The intermediate data explosion problem, associated with naive implementations of tensor decomposition algorithms, would require the materialization and the storage of a matrix whose largest dimension would be ≈7 x 10 14 ; this amounts to ~10 Petabytes, or equivalently a few data centers worth of storage, thereby rendering the tensor analysis of this knowledge base, in the naive way, practically impossible. In this paper, we propose GIGATENSOR, a scalable distributed algorithm for large scale tensor decomposition. GIGATENSOR exploits the sparseness of the real world tensors, and avoids the intermediate data explosion problem by carefully redesigning the tensor decomposition algorithm. Extensive experiments show that our proposed GIGATENSOR solves 100 times bigger problems than existing methods. Furthermore, we employ GIGATENSOR in order to analyze a very large real world, knowledge base tensor and present our astounding findings which include discovery of potential synonyms among millions of noun-phrases (e.g. the noun 'pollutant' and the noun-phrase 'greenhouse gases').",2012,Knowledge Discovery and Data Mining,Fields of study: anchor textnoun phrasedata centernountensorsocial networkbig dataanomaly detectiondistributed algorithmknowledge basetheoretical computer sciencedata miningmachine learningcomputer science
Estimating conversion rate in display advertising from past erformance data,Kuang-chih LeeBurkay OrtenAli Dasdan (Yahoo!)Wentong Li,"2676499131,2634182670,4321789,2703517407","In targeted display advertising, the goal is to identify the best opportunities to display a banner ad to an online user who is most likely to take a desired action such as purchasing a product or signing up for a newsletter. Finding the best ad impression, i.e., the opportunity to show an ad to a user, requires the ability to estimate the probability that the user who sees the ad on his or her browser will take an action, i.e., the user will convert. However, conversion probability estimation is a challenging task since there is extreme data sparsity across different data dimensions and the conversion event occurs rarely. In this paper, we present our approach to conversion rate estimation which relies on utilizing past performance observations along user, publisher and advertiser data hierarchies. More specifically, we model the conversion event at different select hierarchical levels with separate binomial distributions and estimate the distribution parameters individually. Then we demonstrate how we can combine these individual estimators using logistic regression to accurately identify conversion events. In our presentation, we also discuss main practical considerations such as data imbalance, missing data, and output probability calibration, which render this estimation problem more difficult but yet need solving for a real-world implementation of the approach. We provide results from real advertising campaigns to demonstrate the effectiveness of our proposed approach.",2012,Knowledge Discovery and Data Mining,Fields of study: user requirements documentbinomial distributionmissing datalogistic regressiondata miningmachine learningstatisticscomputer science
USpan: an efficient algorithm for mining high utility sequential patterns,"Junfu Yin (University of Technology, Sydney)Zhigang Zheng (University of Technology, Sydney)Longbing Cao (University of Technology, Sydney)","2227567020,2169880472,2115085568","Sequential pattern mining plays an important role in many applications, such as bioinformatics and consumer behavior analysis. However, the classic frequency-based framework often leads to many patterns being identified, most of which are not informative enough for business decision-making. In frequent pattern mining, a recent effort has been to incorporate utility into the pattern selection framework, so that high utility (frequent or infrequent) patterns are mined which address typical business concerns such as dollar value associated with each pattern. In this paper, we incorporate utility into sequential pattern mining, and a generic framework for high utility sequence mining is defined. An efficient algorithm, USpan, is presented to mine for high utility sequential patterns. In USpan, we introduce the lexicographic quantitative sequence tree to extract the complete set of high utility sequences and design concatenation mechanisms for calculating the utility of a node and its children with two effective pruning strategies. Substantial experiments on both synthetic and real datasets show that USpan efficiently identifies high utility sequences from large scale data with very low minimum utility.",2012,Knowledge Discovery and Data Mining,Fields of study: sequential pattern miningconsumer behaviourdata sciencebioinformaticsdata miningcomputer science
A probabilistic model for multimodal hash function learning,Yi Zhen (Hong Kong University of Science and Technology)Dit Yan Yeung (Hong Kong University of Science and Technology),"2122755168,2109477161","In recent years, both hashing-based similarity search and multimodal similarity search have aroused much research interest in the data mining and other communities. While hashing-based similarity search seeks to address the scalability issue, multimodal similarity search deals with applications in which data of multiple modalities are available. In this paper, our goal is to address both issues simultaneously. We propose a probabilistic model, called multimodal latent binary embedding (MLBE), to learn hash functions from multimodal data automatically. MLBE regards the binary latent factors as hash codes in a common Hamming space. Given data from multiple modalities, we devise an efficient algorithm for the learning of binary latent factors which corresponds to hash function learning. Experimental validation of MLBE has been conducted using both synthetic data and two realistic data sets. Experimental results show that MLBE compares favorably with two state-of-the-art models.",2012,Knowledge Discovery and Data Mining,Fields of study: k independent hashingfeature hashinguniversal hashinglocality sensitive hashinghash functionsynthetic datastatistical modeldata miningpattern recognitionmachine learningstatisticscomputer science
eTrust: understanding trust evolution in an online world,Jiliang Tang (Arizona State University)Huiji Gao (Arizona State University)Huan Liu (Arizona State University)Atish Das Sarma (eBay),"2147392410,2166899337,2122391114,2266878914","Most existing research about online trust assumes static trust relations between users. As we are informed by social sciences, trust evolves as humans interact. Little work exists studying trust evolution in an online world. Researching online trust evolution faces unique challenges because more often than not, available data is from passive observation. In this paper, we leverage social science theories to develop a methodology that enables the study of online trust evolution. In particular, we propose a framework of evolution trust, eTrust, which exploits the dynamics of user preferences in the context of online product review. We present technical details about modeling trust evolution, and perform experiments to show how the exploitation of trust evolution can help improve the performance of online applications such as rating and trust prediction.",2012,Knowledge Discovery and Data Mining,Fields of study: computational trustinterpersonal relationshipknowledge management
Robust multi-task feature learning,Pinghua Gong (Tsinghua University)Jieping Ye (Biodesign Institute)Changshui Zhang (Tsinghua University),"2152377557,2305258894,2105451568","Multi-task learning (MTL) aims to improve the performance of multiple related tasks by exploiting the intrinsic relationships among them. Recently, multi-task feature learning algorithms have received increasing attention and they have been successfully applied to many applications involving high dimensional data. However, they assume that all tasks share a common set of features, which is too restrictive and may not hold in real-world applications, since outlier tasks often exist. In this paper, we propose a Robust Multi-Task Feature Learning algorithm (rMTFL) which simultaneously captures a common set of features among relevant tasks and identifies outlier tasks. Specifically, we decompose the weight (model) matrix for all tasks into two components. We impose the well-known group Lasso penalty on row groups of the first component for capturing the shared features among relevant tasks. To simultaneously identify the outlier tasks, we impose the same group Lasso penalty but on column groups of the second component. We propose to employ the accelerated gradient descent to efficiently solve the optimization problem in rMTFL, and show that the proposed algorithm is scalable to large-size problems. In addition, we provide a detailed theoretical analysis on the proposed rMTFL formulation. Specifically, we present a theoretical bound to measure how well our proposed rMTFL approximates the true evaluation, and provide bounds to measure the error between the estimated weights of rMTFL and the underlying true weights. Moreover, by assuming that the underlying true weights are above the noise level, we present a sound theoretical result to show how to obtain the underlying true shared features and outlier tasks (sparsity patterns). Empirical studies on both synthetic and real-world data demonstrate that our proposed rMTFL is capable of simultaneously capturing shared features among tasks and identifying outlier tasks.",2012,Knowledge Discovery and Data Mining,Fields of study: multi task learningclustering high dimensional dataempirical researchgradient descentoptimization problemfeature selectiontext miningdata sciencedata miningmachine learningstatisticscomputer science
Integrating community matching and outlier detection for mining evolutionary community outliers,Manish Gupta (University of Illinois at Urbana–Champaign)Jing Gao (State University of New York System)Yizhou Sun (University of Illinois at Urbana–Champaign)Jiawei Han (University of Illinois at Urbana–Champaign),"2664163432,2096731881,2131539564,2121939561","Temporal datasets, in which data evolves continuously, exist in a wide variety of applications, and identifying anomalous or outlying objects from temporal datasets is an important and challenging task. Different from traditional outlier detection, which detects objects that have quite different behavior compared with the other objects, temporal outlier detection tries to identify objects that have different evolutionary behavior compared with other objects. Usually objects form multiple communities, and most of the objects belonging to the same community follow similar patterns of evolution. However, there are some objects which evolve in a very different way relative to other community members, and we define such objects as evolutionary community outliers . This definition represents a novel type of outliers considering both temporal dimension and community patterns. We investigate the problem of identifying evolutionary community outliers given the discovered communities from two snapshots of an evolving dataset. To tackle the challenges of community evolution and outlier detection, we propose an integrated optimization framework which conducts outlier-aware community matching across snapshots and identification of evolutionary outliers in a tightly coupled way. A coordinate descent algorithm is proposed to improve community matching and outlier detection performance iteratively. Experimental results on both synthetic and real datasets show that the proposed approach is highly effective in discovering interesting evolutionary community outliers.",2012,Knowledge Discovery and Data Mining,Fields of study: anomaly detectiondata miningpattern recognitionmachine learningcomputer science
Playlist prediction via metric embedding,Shuo Chen (Cornell University)Joshua L. Moore (Cornell University)Douglas Turnbull (Ithaca College)Thorsten Joachims (Cornell University),"2295814016,2097778451,2227091269,245171893","Digital storage of personal music collections and cloud-based music services (e.g. Pandora, Spotify) have fundamentally changed how music is consumed. In particular, automatically generated playlists have become an important mode of accessing large music collections. The key goal of automated playlist generation is to provide the user with a coherent listening experience. In this paper, we present Latent Markov Embedding (LME), a machine learning algorithm for generating such playlists. In analogy to matrix factorization methods for collaborative filtering, the algorithm does not require songs to be described by features a priori, but it learns a representation from example playlists. We formulate this problem as a regularized maximum-likelihood embedding of Markov chains in Euclidian space, and show how the resulting optimization problem can be solved efficiently. An empirical evaluation shows that the LME is substantially more accurate than adaptations of smoothed n-gram models commonly used in natural language processing.",2012,Knowledge Discovery and Data Mining,Fields of study: user modelingtheoretical computer sciencespeech recognitiondata miningmachine learningstatisticscomputer sciencemathematics
A shapelet transform for time series classification,Jason Lines (University of East Anglia)Luke M. Davis (University of East Anglia)Jon Hills (University of East Anglia)Anthony Bagnall (University of East Anglia),"1984663852,2142105487,2163379274,2171856547","The problem of time series classification (TSC), where we consider any real-valued ordered data a time series, presents a specific machine learning challenge as the ordering of variables is often crucial in finding the best discriminating features. One of the most promising recent approaches is to find shapelets within a data set. A shapelet is a time series subsequence that is identified as being representative of class membership. The original research in this field embedded the procedure of finding shapelets within a decision tree. We propose disconnecting the process of finding shapelets from the classification algorithm by proposing a shapelet transformation. We describe a means of extracting the k best shapelets from a data set in a single pass, and then use these shapelets to transform data by calculating the distances from a series to each shapelet. We demonstrate that transformation into this new data space can improve classification accuracy, whilst retaining the explanatory power provided by shapelets.",2012,Knowledge Discovery and Data Mining,Fields of study: filterdecision treetransformationtime seriesdata miningpattern recognitionmachine learningstatisticscomputer sciencemathematics
Unsupervised feature selection for linked social media data,Jiliang Tang (Arizona State University)Huan Liu (Arizona State University),"2147392410,2122391114","The prevalent use of social media produces mountains of unlabeled, high-dimensional data. Feature selection has been shown effective in dealing with high-dimensional data for efficient data mining. Feature selection for unlabeled data remains a challenging task due to the absence of label information by which the feature relevance can be assessed. The unique characteristics of social media data further complicate the already challenging problem of unsupervised feature selection, (e.g., part of social media data is linked, which makes invalid the independent and identically distributed assumption), bringing about new challenges to traditional unsupervised feature selection algorithms. In this paper, we study the differences between social media data and traditional attribute-value data, investigate if the relations revealed in linked data can be used to help select relevant features, and propose a novel unsupervised feature selection framework, LUFS, for linked social media data. We perform experiments with real-world social media datasets to evaluate the effectiveness of the proposed framework and probe the working of its key components.",2012,Knowledge Discovery and Data Mining,Fields of study: social medialinked dataclustering high dimensional dataindependent and identically distributed random variablesfeature selectiondata miningpattern recognitionmachine learningcomputer science
TM-LDA: efficient online modeling of latent topic transitions in social media,Yu Wang (Emory University)Eugene Agichtein (Emory University)Michele Benzi (Emory University),"2595793754,2283615530,696533411","Latent topic analysis has emerged as one of the most effective methods for classifying, clustering and retrieving textual data. However, existing models such as Latent Dirichlet Allocation (LDA) were developed for static corpora of relatively large documents. In contrast, much of the textual content on the web, and especially social media, is temporally sequenced, and comes in short fragments, including microblog posts on sites such as Twitter and Weibo, status updates on social networking sites such as Facebook and LinkedIn, or comments on content sharing sites such as YouTube. In this paper we propose a novel topic model, Temporal-LDA or TM-LDA, for efficiently mining text streams such as a sequence of posts from the same author, by modeling the topic transitions that naturally arise in these data. TM-LDA learns the transition parameters among topics by minimizing the prediction error on topic distribution in subsequent postings. After training, TM-LDA is thus able to accurately predict the expected topic distribution in future posts. To make these predictions more efficient for a realistic online setting, we develop an efficient updating algorithm to adjust the topic transition parameters, as new documents stream in. Our empirical results, over a corpus of over 30 million microblog posts, show that TM-LDA significantly outperforms state-of-the-art static LDA models for estimating the topic distribution of new documents over time. We also demonstrate that TM-LDA is able to highlight interesting variations of common topic transitions, such as the differences in the work-life rhythm of cities, and factors associated with area-specific problems and complaints.",2012,Knowledge Discovery and Data Mining,Fields of study: social medialatent dirichlet allocationmean squared prediction errorlanguage modeldata scienceworld wide webdata miningmachine learningcomputer science
"Mining large-scale, sparse GPS traces for map inference: comparison of approaches",Xuemei Liu (Shanghai Jiao Tong University)James Biagioni (University of Illinois at Chicago)Jakob Eriksson (University of Illinois at Chicago)Yin Wang (Hewlett-Packard)George Forman (Hewlett-Packard)Yanmin Zhu (Shanghai Jiao Tong University),"2304799252,1984222178,2129658448,2426629451,2195069006,2130173690","We address the problem of inferring road maps from large-scale GPS traces that have relatively low resolution and sampling frequency. Unlike past published work that requires high-resolution traces with dense sampling, we focus on situations with coarse granularity data, such as that obtained from thousands of taxis in Shanghai, which transmit their location as seldom as once per minute. Such data sources can be made available inexpensively as byproducts of existing processes, rather than having to drive every road with high-quality GPS instrumentation just for map building - and having to re-drive roads for periodic updates. Although the challenges in using opportunistic probe data are significant, successful mining algorithms could potentially enable the creation of continuously updated maps at very low cost. In this paper, we compare representative algorithms from two approaches: working with individual reported locations vs. segments between consecutive locations. We assess their trade-offs and effectiveness in both qualitative and quantitative comparisons for regions of Shanghai and Chicago.",2012,Knowledge Discovery and Data Mining,Fields of study: image resolutiondata miningmachine learningcomputer science
RolX: structural role extraction & mining in large graphs,Keith Henderson (Lawrence Livermore National Laboratory)Brian Gallagher (Lawrence Livermore National Laboratory)Tina Eliassi-Rad (Rutgers University)Hanghang Tong (IBM)Sugato Basu (Google)Leman Akoglu (Carnegie Mellon University)Danai Koutra (Carnegie Mellon University)Christos Faloutsos (Carnegie Mellon University)Lei Li (Carnegie Mellon University),"2196117844,2188853992,218538652,2224718883,2169880767,2288278917,1524801041,2198983026,2608982952","Given a network, intuitively two nodes belong to the same role if they have similar structural behavior. Roles should be automatically determined from the data, and could be, for example, ""clique-members,"" ""periphery-nodes,"" etc. Roles enable numerous novel and useful network-mining tasks, such as sense-making, searching for similar nodes, and node classification. This paper addresses the question: Given a graph, how can we automatically discover roles for nodes? We propose RolX (Role eXtraction) , a scalable (linear in the number of edges), unsupervised learning approach for automatically extracting structural roles from general network data. We demonstrate the effectiveness of RolX on several network-mining tasks: from exploratory data analysis to network transfer learning. Moreover, we compare network role discovery with network community discovery. We highlight fundamental differences between the two (e.g., roles generalize across disconnected networks, communities do not); and show that the two approaches are complimentary in nature.",2012,Knowledge Discovery and Data Mining,Fields of study: transfer of learningexploratory data analysisunsupervised learningdata sciencedata miningmachine learningcomputer science
Automatic detection of rumor on Sina Weibo,Fan Yang (Shandong University)Yang Liu (Shandong University)Xiaohui Yu (Shandong University)Min Yang (Shandong University),"2628396664,2651507659,2688662778,2642454346","The problem of gauging information credibility on social networks has received considerable attention in recent years. Most previous work has chosen Twitter, the world's largest micro-blogging platform, as the premise of research. In this work, we shift the premise and study the problem of information credibility on Sina Weibo, China's leading micro-blogging service provider. With eight times more users than Twitter, Sina Weibo is more of a Facebook-Twitter hybrid than a pure Twitter clone, and exhibits several important characteristics that distinguish it from Twitter. We collect an extensive set of microblogs which have been confirmed to be false rumors based on information from the official rumor-busting service provided by Sina Weibo. Unlike previous studies on Twitter where the labeling of rumors is done manually by the participants of the experiments, the official nature of this service ensures the high quality of the dataset. We then examine an extensive set of features that can be extracted from the microblogs, and train a classifier to automatically detect the rumors from a mixed set of true information and false information. The experiments show that some of the new features we propose are indeed effective in the classification, and even the features considered in previous studies have different implications with Sina Weibo than with Twitter. To the best of our knowledge, this is the first study on rumor analysis and detection on Sina Weibo.",2012,Knowledge Discovery and Data Mining,Fields of study: microbloggingservice providersocial networkbiological classificationinternet privacyworld wide webcomputer securitydata mining
Mining top-K high utility itemsets,Cheng Wei Wu (National Cheng Kung University)Bai En Shie (National Cheng Kung University)Vincent S. Tseng (National Cheng Kung University)Philip S. Yu (University of Illinois at Chicago),"2111809384,2575054689,2023431847,2125104194","Mining high utility itemsets from databases is an emerging topic in data mining, which refers to the discovery of itemsets with utilities higher than a user-specified minimum utility threshold min_util . Although several studies have been carried out on this topic, setting an appropriate minimum utility threshold is a difficult problem for users. If min_util is set too low, too many high utility itemsets will be generated, which may cause the mining algorithms to become inefficient or even run out of memory. On the other hand, if min_util is set too high, no high utility itemset will be found. Setting appropriate minimum utility thresholds by trial and error is a tedious process for users. In this paper, we address this problem by proposing a new framework named top-k high utility itemset mining, where k is the desired number of high utility itemsets to be mined. An efficient algorithm named TKU (Top-K Utility itemsets mining) is proposed for mining such itemsets without setting min_util. Several features were designed in TKU to solve the new challenges raised in this problem, like the absence of anti-monotone property and the requirement of lossless results. Moreover, TKU incorporates several novel strategies for pruning the search space to achieve high efficiency. Results on real and synthetic datasets show that TKU has excellent performance and scalability.",2012,Knowledge Discovery and Data Mining,Fields of study: searching the conformational space for dockingdata sciencedata miningdatabasecomputer science
Bid optimizing and inventory scoring in targeted online advertising,Claudia Perlich (IBM)Brian DalessandroRod HookOri StitelmanTroy Raeder (University of Notre Dame)Foster J. Provost (New York University Stern School of Business),"164824025,1876520739,2230241747,2234062319,1999864269,2158932634","Billions of online display advertising spots are purchased on a daily basis through real time bidding exchanges (RTBs). Advertising companies bid for these spots on behalf of a company or brand in order to purchase these spots to display banner advertisements. These bidding decisions must be made in fractions of a second after the potential purchaser is informed of what location (Internet site) has a spot available and who would see the advertisement. The entire transaction must be completed in near real-time to avoid delays loading the page and maintain a good users experience. This paper presents a bid-optimization approach that is implemented in production at Media6Degrees for bidding on these advertising opportunities at an appropriate price. The approach combines several supervised learning algorithms, as well as second price auction theory, to determine the correct price to ensure that the right message is delivered to the right person, at the right time.",2012,Knowledge Discovery and Data Mining,Fields of study: real time biddingauction snipingvickrey auctioncausalitylogistic regressiononline advertisingsupervised learningmachine learningcomputer science
Automatic taxonomy construction from keywords,Xueqing Liu (Microsoft)Yangqiu Song (Microsoft)Shixia Liu (Microsoft)Haixun Wang (Microsoft),"2689074745,2099747503,2111177146,2116756368","Taxonomies, especially the ones in specific domains, are becoming indispensable to a growing number of applications. State-of-the-art approaches assume there exists a text corpus to accurately characterize the domain of interest, and that a taxonomy can be derived from the text corpus using information extraction techniques. In reality, neither assumption is valid, especially for highly focused or fast-changing domains. In this paper, we study a challenging problem: Deriving a taxonomy from a set of keyword phrases. A solution can benefit many real life applications because i) keywords give users the flexibility and ease to characterize a specific domain; and ii) in many applications, such as online advertisements, the domain of interest is already represented by a set of keywords. However, it is impossible to create a taxonomy out of a keyword set itself. We argue that additional knowledge and contexts are needed. To this end, we first use a general purpose knowledgebase and keyword search to supply the required knowledge and context. Then we develop a Bayesian approach to build a hierarchical taxonomy for a given set of keywords. We reduce the complexity of previous hierarchical clustering approaches from O ( n 2 log n ) to O ( n log n ), so that we can derive a domain specific taxonomy from one million keyword phrases in less than an hour. Finally, we conduct comprehensive large scale experiments to show the effectiveness and efficiency of our approach. A real life example of building an insurance-related query taxonomy illustrates the usefulness of our approach for specific domains.",2012,Knowledge Discovery and Data Mining,Fields of study: hierarchical clusteringbayesian probabilityonline advertisinginformation extractiondata scienceinformation retrievaldata miningmachine learningstatisticscomputer science
Entity-centric topic-oriented opinion summarization in twitter,Xinfan Meng (Peking University)Furu Wei (Microsoft)Xiaohua Liu (Microsoft)Ming Zhou (Microsoft)Sujian Li (Peking University)Houfeng Wang (Peking University),"1984684747,2171151462,2614859247,2143584880,2161397473,2113814419","Microblogging services, such as Twitter, have become popular channels for people to express their opinions towards a broad range of topics. Twitter generates a huge volume of instant messages (i.e. tweets) carrying users' sentiments and attitudes every minute, which both necessitates automatic opinion summarization and poses great challenges to the summarization system. In this paper, we study the problem of opinion summarization for entities, such as celebrities and brands, in Twitter. We propose an entity-centric topic-based opinion summarization framework, which aims to produce opinion summaries in accordance with topics and remarkably emphasizing the insight behind the opinions. To this end, we first mine topics from #hashtags, the human-annotated semantic tags in tweets. We integrate the #hashtags as weakly supervised information into topic modeling algorithms to obtain better interpretation and representation for calculating the similarity among them, and adopt Affinity Propagation algorithm to group #hashtags into coherent topics. Subsequently, we use templates generalized from paraphrasing to identify tweets with deep insights, which reveal reasons, express demands or reflect viewpoints. Afterwards, we develop a target (i.e. entity) dependent sentiment classification approach to identifying the opinion towards a given target (i.e. entity) of tweets. Finally, the opinion summary is generated through integrating information from dimensions of topic, opinion and insight, as well as other factors (e.g. topic relevancy, redundancy and language styles) in an unified optimization framework. We conduct extensive experiments on a real-life data set to evaluate the performance of individual opinion summarization modules as well as the quality of the produced summary. The promising experiment results show the effectiveness of the proposed framework and algorithms.",2012,Knowledge Discovery and Data Mining,Fields of study: affinity propagationmulti document summarizationsentiment analysisautomatic summarizationdata scienceworld wide webdata miningmachine learningcomputer science
DEMON: a local-first discovery method for overlapping communities,Michele Coscia (Harvard University)Giulio Rossetti (Istituto di Scienza e Tecnologie dell'Informazione)Fosca Giannotti (Istituto di Scienza e Tecnologie dell'Informazione)Dino Pedreschi (University of Pisa),"2105560137,2120671011,1205765909,7769909","Community discovery in complex networks is an interesting problem with a number of applications, especially in the knowledge extraction task in social and information networks. However, many large networks often lack a particular community organization at a global level. In these cases, traditional graph partitioning algorithms fail to let the latent knowledge embedded in modular structure emerge, because they impose a top-down global view of a network. We propose here a simple local-first approach to community discovery, able to unveil the modular organization of real complex networks. This is achieved by democratically letting each node vote for the communities it sees surrounding it in its limited view of the global system, i.e. its ego neighborhood, using a label propagation algorithm; finally, the local communities are merged into a global collection. We tested this intuition against the state-of-the-art overlapping and non-overlapping community discovery methods, and found that our new method clearly outperforms the others in the quality of the obtained communities, evaluated by using the extracted communities to predict the metadata about the nodes of several real world networks. We also show how our method is deterministic, fully incremental, and has a limited time complexity, so that it can be used on web-scale real networks.",2012,Knowledge Discovery and Data Mining,Fields of study: complex networkknowledge extractiondata sciencebioinformaticsdata miningcomputer science
Differentially private transit data publication: a case study on the montreal transportation system,Rui Chen (Concordia University)Benjamin C.M. Fung (Concordia University)Bipin C. Desai (Concordia University)Nériah M. Sossou,"2690718549,2015549731,2117781371,2228535187","With the wide deployment of smart card automated fare collection (SCAFC) systems, public transit agencies have been benefiting from huge volume of transit data, a kind of sequential data, collected every day. Yet, improper publishing and use of transit data could jeopardize passengers' privacy. In this paper, we present our solution to transit data publication under the rigorous differential privacy model for the Societe de transport de Montreal (STM). We propose an efficient data-dependent yet differentially private transit data sanitization approach based on a hybrid-granularity prefix tree structure. Moreover, as a post-processing step, we make use of the inherent consistency constraints of a prefix tree to conduct constrained inferences, which lead to better utility. Our solution not only applies to general sequential data, but also can be seamlessly extended to trajectory data. To our best knowledge, this is the first paper to introduce a practical solution for publishing large volume of sequential data under differential privacy. We examine data utility in terms of two popular data analysis tasks conducted at the STM, namely count queries and frequent sequential pattern mining. Extensive experiments on real-life STM datasets confirm that our approach maintains high utility and is scalable to large datasets.",2012,Knowledge Discovery and Data Mining,Fields of study: differential privacysequential pattern miningsmart cardtree structuredata collectiondata analysisworld wide webdata miningdatabasecomputer science
"Vertex neighborhoods, low conductance cuts, and good seeds for local community methods",David F. Gleich (Purdue University)C. Seshadhri (Sandia National Laboratories),"2148810670,1214261146","The communities of a social network are sets of vertices with more connections inside the set than outside. We theoretically demonstrate that two commonly observed properties of social networks, heavy-tailed degree distributions and large clustering coefficients, imply the existence of vertex neighborhoods (also known as egonets) that are themselves good communities. We evaluate these neighborhood communities on a range of graphs. What we find is that the neighborhood communities can exhibit conductance scores that are as good as the Fiedler cut. Also, the conductance of neighborhood communities shows similar behavior as the network community profile computed with a personalized PageRank community detection method. Neighborhood communities give us a simple and powerful heuristic for speeding up local partitioning methods. Since finding good seeds for the PageRank clustering method is difficult, most approaches involve an expensive sweep over a great many starting vertices. We show how to use neighborhood communities to quickly generate a small set of seeds.",2012,Knowledge Discovery and Data Mining,Fields of study: conductanceclustering coefficientheavy tailed distributiondegree distributionsocial networkcombinatoricsmachine learningmathematical optimizationmathematics
The untold story of the clones: content-agnostic factors that impact YouTube video popularity,Youmna Borghol (NICTA)Sebastien Ardon (NICTA)Niklas Carlsson (Linköping University)Derek L. Eager (University of Saskatchewan)Anirban Mahanti (NICTA),"162925452,2590868965,2083072794,721713163,2166267501","Video dissemination through sites such as YouTube can have widespread impacts on opinions, thoughts, and cultures. Not all videos will reach the same popularity and have the same impact. Popularity differences arise not only because of differences in video content, but also because of other ""content-agnostic"" factors. The latter factors are of considerable interest but it has been difficult to accurately study them. For example, videos uploaded by users with large social networks may tend to be more popular because they tend to have more interesting content, not because social network size has a substantial direct impact on popularity. In this paper, we develop and apply a methodology that is able to accurately assess, both qualitatively and quantitatively, the impacts of various content-agnostic factors on video popularity. When controlling for video content, we observe a strong linear ""rich-get-richer"" behavior, with the total number of previous views as the most important factor except for very young videos. The second most important factor is found to be video age. We analyze a number of phenomena that may contribute to rich-get-richer, including the first-mover advantage, and search bias towards popular videos. For young videos we find that factors other than the total number of previous views, such as uploader characteristics and number of keywords, become relatively more important. Our findings also confirm that inaccurate conclusions can be reached when not controlling for content.",2012,Knowledge Discovery and Data Mining,Fields of study: first mover advantagecloningsocial networkinternet privacymultimediaworld wide websocial science
On the separability of structural classes of communities,Bruno D. Abrahao (Cornell University)Sucheta Soundarajan (Cornell University)John E. Hopcroft (Cornell University)Robert Kleinberg (Cornell University),"2094811419,1846143951,2079447303,2006104002","Three major factors govern the intricacies of community extraction in networks: (1) the application domain includes a wide variety of networks of fundamentally different natures, (2) the literature offers a multitude of disparate community detection algorithms, and (3) there is no consensus characterizing how to discriminate communities from non-communities. In this paper, we present a comprehensive analysis of community properties through a class separability framework. Our approach enables the assessement of the structural dissimilarity among the output of multiple community detection algorithms and between the output of algorithms and communities that arise in practice. To demostrate this concept, we furnish our method with a large set of structural properties and multiple community detection algorithms. Applied to a diverse collection of large scale network datasets, the analysis reveals that (1) the different detection algorithms extract fundamentally different structures; (2) the structure of communities that arise in practice is closest to that of communities that random-walk-based algorithms extract, although still siginificantly different from that of the output of all the algorithms; and (3) a small subset of the properties are nearly as discriminative as the full set, while making explicit the ways in which the algorithms produce biases. Our framework enables an informed choice of the most suitable community detection method for a given purpose and network and allows for a comparison of existing community detection algorithms while guiding the design of new ones.",2012,Knowledge Discovery and Data Mining,Fields of study: community structurerandom walkdata miningartificial intelligencemachine learningstatisticsmathematics
Finding trendsetters in information networks,Diego Sáez-Trumper (Pompeu Fabra University)Giovanni Comarela (Universidade Federal de Minas Gerais)Virgílio A. F. Almeida (Universidade Federal de Minas Gerais)Ricardo A. Baeza-Yates (Yahoo!)Fabrício Benevenuto (Universidade Federal de Ouro Preto),"2084963834,2178546446,2100108167,528588921,1976666824","Influential people have an important role in the process of information diffusion. However, there are several ways to be influential, for example, to be the most popular or the first that adopts a new idea. In this paper we present a methodology to find trendsetters in information networks according to a specific topic of interest. Trendsetters are people that adopt and spread new ideas influencing other people before these ideas become popular. At the same time, not all early adopters are trendsetters because only few of them have the ability of propagating their ideas by their social contacts through word-of-mouth. Differently from other influence measures, a trendsetter is not necessarily popular or famous, but the one whose ideas spread over the graph successfully. Other metrics such as node in-degree or even standard Pagerank focus only in the static topology of the network. We propose a ranking strategy that focuses on the ability of some users to push new ideas that will be successful in the future. To that end, we combine temporal attributes of nodes and edges of the network with a Pagerank based algorithm to find the trendsetters for a given topic. To test our algorithm we conduct innovative experiments over a large Twitter dataset. We show that nodes with high in-degree tend to arrive late for new trends, while users in the top of our ranking tend to be early adopters that also influence their social contacts to adopt the new trend.",2012,Knowledge Discovery and Data Mining,Fields of study: social networkworld wide webdata miningartificial intelligencemachine learningcomputer science
Feature grouping and selection over an undirected graph,Sen Yang (Arizona State University)Lei Yuan (Arizona State University)Ying Cheng Lai (Arizona State University)Xiaotong Shen (University of Minnesota)Peter Wonka (Arizona State University)Jieping Ye (Arizona State University),"2617528270,2243895322,2476563404,2125069230,729125449,2305258894","High-dimensional regression/classification continues to be an important and challenging problem, especially when features are highly correlated. Feature selection, combined with additional structure information on the features has been considered to be promising in promoting regression/classification performance. Graph-guided fused lasso (GFlasso) has recently been proposed to facilitate feature selection and graph structure exploitation, when features exhibit certain graph structures. However, the formulation in GFlasso relies on pairwise sample correlations to perform feature grouping, which could introduce additional estimation bias. In this paper, we propose three new feature grouping and selection methods to resolve this issue. The first method employs a convex function to penalize the pairwise l ∞ norm of connected regression/classification coefficients, achieving simultaneous feature grouping and selection. The second method improves the first one by utilizing a non-convex function to reduce the estimation bias. The third one is the extension of the second method using a truncated l 1 regularization to further reduce the estimation bias. The proposed methods combine feature grouping and feature selection to enhance estimation accuracy. We employ the alternating direction method of multipliers (ADMM) and difference of convex functions (DC) programming to solve the proposed formulations. Our experimental results on synthetic data and two real datasets demonstrate the effectiveness of the proposed methods.",2012,Knowledge Discovery and Data Mining,Fields of study: convex functionsynthetic dataregressiondirect methodfeaturebiological classificationfeature selectiontext miningdata miningpattern recognitionmachine learningstatisticscomputer science
GetJar mobile application recommendations with very sparse datasets,"Kent ShiKamal Ali (University of California, Irvine)","2627832973,2129105142","The Netflix competition of 2006 [2] has spurred significant activity in the recommendations field, particularly in approaches using latent factor models [3,5,8,12] However, the near ubiquity of the Netflix and the similar MovieLens datasets 1 may be narrowing the generality of lessons learned in this field. At GetJar, our goal is to make appealing recommendations of mobile applications (apps). For app usage, we observe a distribution that has higher kurtosis (heavier head and longer tail) than that for the aforementioned movie datasets. This happens primarily because of the large disparity in resources available to app developers and the low cost of app publication relative to movies. In this paper we compare a latent factor (PureSVD) and a memory-based model with our novel PCA-based model, which we call Eigenapp. We use both accuracy and variety as evaluation metrics. PureSVD did not perform well due to its reliance on explicit feedback such as ratings, which we do not have. Memory-based approaches that perform vector operations in the original high dimensional space over-predict popular apps because they fail to capture the neighborhood of less popular apps. They have high accuracy due to the concentration of mass in the head, but did poorly in terms of variety of apps exposed. Eigenapp, which exploits neighborhood information in low dimensional spaces, did well both on precision and variety, underscoring the importance of dimensionality reduction to form quality neighborhoods in high kurtosis distributions.",2012,Knowledge Discovery and Data Mining,Fields of study: sparse matrixprincipal component analysisevaluationrecommender systemdata sciencedata miningmachine learningsimulationstatisticscomputer science
Rank-loss support instance machines for MIML instance annotation,Forrest Briggs (Oregon State University)Xiaoli Z. Fern (Oregon State University)Raviv Raich (Oregon State University),"2165737930,2018589324,2150540218","Multi-instance multi-label learning (MIML) is a framework for supervised classification where the objects to be classified are bags of instances associated with multiple labels. For example, an image can be represented as a bag of segments and associated with a list of objects it contains. Prior work on MIML has focused on predicting label sets for previously unseen bags. We instead consider the problem of predicting instance labels while learning from data labeled only at the bag level. We propose Rank-Loss Support Instance Machines, which optimize a regularized rank-loss objective and can be instantiated with different aggregation models connecting instance-level predictions with bag-level predictions. The aggregation models that we consider are equivalent to defining a ""support instance"" for each bag, which allows efficient optimization of the rank-loss objective using primal sub-gradient descent. Experiments on artificial and real-world datasets show that the proposed methods achieve higher accuracy than other loss functions used in prior work, e.g., Hamming loss, and recent work in ambiguous label classification.",2012,Knowledge Discovery and Data Mining,Fields of study: bioacousticssupport vector machineloss functiongradient descentautomatic image annotationinstance based learningdata miningpattern recognitionmachine learningcomputer science
Exploiting large-scale check-in data to recommend time-sensitive routes,Hsun-Ping Hsieh (National Taiwan University)Cheng-Te Li (National Taiwan University)Shou-De Lin (National Taiwan University),"2109618838,2139086518,2114357324","Location-based services allow users to perform geo-spatial check-in actions, which facilitates the mining of the moving activities of human beings. This paper proposes to recommend time-sensitive trip routes, consisting of a sequence of locations with associated time stamps, based on knowledge extracted from large-scale check-in data. Given a query location with the starting time, our goal is to recommend a time-sensitive route. We argue a good route should consider (a) the popularity of places, (b) the visiting order of places, (c) the proper visiting time of each place, and (d) the proper transit time from one place to another. By devising a statistical model, we integrate these four factors into a goodness function which aims to measure the quality of a route. Equipped with the goodness measure, we propose a greedy method to construct the time-sensitive route for the query. Experiments on Gowalla datasets demonstrate the effectiveness of our model on detecting real routes and cloze test of routes, comparing with other baseline methods. We also develop a system TripRouter as a real-time demo platform.",2012,Knowledge Discovery and Data Mining,Fields of study: location based servicestatistical modelknowledge extractionworld wide webdata miningmachine learningsimulationstatisticscomputer science
Query-driven discovery of semantically similar substructures in heterogeneous networks,Xiao Yu (University of Illinois at Urbana–Champaign)Yizhou Sun (University of Illinois at Urbana–Champaign)Peixiang Zhao (University of Illinois at Urbana–Champaign)Jiawei Han (University of Illinois at Urbana–Champaign),"2160715520,2131539564,2169163720,2121939561","Heterogeneous information networks that contain multiple types of objects and links are ubiquitous in the real world, such as bibliographic networks, cyber-physical networks, and social media networks. Although researchers have studied various data mining tasks in information networks, interactive query-based network exploration techniques have not been addressed systematically, which, in fact, are highly desirable for exploring large-scale information networks. In this demo, we introduce and demonstrate our recent research project on query-driven discovery of semantically similar substructures in heterogeneous networks. Given a subgraph query, our system searches a given large information network and finds efficiently a list of subgraphs that are structurally identical and semantically similar. Since data mining methods are used to obtain semantically similar entities (nodes), we use discovery as a term to describe this process. In order to achieve high efficiency and scalability, we design and implement a filter-and verification search framework, which can first generate promising subgraph candidates using off line indices built by data mining results, and then verify candidates with a recursive pruning matching process. The proposed system demonstrates the effectiveness of our query-driven semantic similarity search framework and the efficiency of the proposed methodology on multiple real-world heterogeneous information networks.",2012,Knowledge Discovery and Data Mining,Fields of study: social mediaheterogeneous networksemantic similarityinformation retrievaldata miningdatabasemachine learningcomputer science
A framework for summarizing and analyzing twitter feeds,Xintian Yang (Ohio State University)Amol Ghoting (IBM)Yiye Ruan (Ohio State University)Srinivasan Parthasarathy (Ohio State University),"2131691483,2102101334,2145305802,2106796124","The firehose of data generated by users on social networking and microblogging sites such as Facebook and Twitter is enormous. Real-time analytics on such data is challenging with most current efforts largely focusing on the efficient querying and retrieval of data produced recently. In this paper, we present a dynamic pattern driven approach to summarize data produced by Twitter feeds. We develop a novel approach to maintain an in-memory summary while retaining sufficient information to facilitate a range of user-specific and topic-specific temporal analytics. We empirically compare our approach with several state-of-the-art pattern summarization approaches along the axes of storage cost, query accuracy, query flexibility, and efficiency using real data from Twitter. We find that the proposed approach is not only scalable but also outperforms existing approaches by a large margin.",2012,Knowledge Discovery and Data Mining,Fields of study: analyticssocial networkinternet privacyworld wide webdata miningcomputer science
Capacitated team formation problem on social networks,Anirban Majumder (Bell Labs)Samik Datta (Bell Labs)K.V.M. Naidu (Walmart Labs),"2042129406,2249925385,2181822518","In a team formation problem, one is required to find a group of users that can match the requirements of a collaborative task. Example of such collaborative tasks abound, ranging from software product development to various participatory sensing tasks in knowledge creation. Due to the nature of the task, team members are often required to work on a co-operative basis. Previous studies [1, 2] have indicated that co-operation becomes effective in presence of social connections. Therefore, effective team selection requires the team members to be socially close as well as a division of the task among team members so that no user is overloaded by the assignment. In this work, we investigate how such teams can be formed on a social network. Since our team formation problems are proven to be NP-hard, we design efficient approximate algorithms for finding near optimum teams with provable guarantees. As traditional data-sets from on-line social networks (e.g. Twitter, Facebook etc) typically do not contain instances of large scale collaboration, we have crawled millions of software repositories spanning a period of four years and hundreds of thousands of developers from GitHub, a popular open-source social coding network . We perform large scale experiments on this data-set to evaluate the accuracy and efficiency of our algorithms. Experimental results suggest that our algorithms achieve significant improvement in finding effective teams, as compared to naive strategies and scale well with the size of the data. Finally, we provide a validation of our techniques by comparing with existing software teams.",2012,Knowledge Discovery and Data Mining,Fields of study: team software processsocial networkmanagement scienceknowledge managementworld wide websocial sciencemachine learningsimulation
Incorporating heterogeneous information for personalized tag recommendation in social tagging systems,Wei Feng (Tsinghua University)Jianyong Wang (Tsinghua University),"2647481813,2105625159","A social tagging system provides users an effective way to collaboratively annotate and organize items with their own tags. A social tagging system contains heterogeneous information like users' tagging behaviors, social networks, tag semantics and item profiles. All the heterogeneous information helps alleviate the cold start problem due to data sparsity. In this paper, we model a social tagging system as a multi-type graph. To learn the weights of different types of nodes and edges, we propose an optimization framework, called OptRank. OptRank can be characterized as follows:(1) Edges and nodes are represented by features. Different types of edges and nodes have different set of features. (2) OptRank learns the best feature weights by maximizing the average AUC (Area Under the ROC Curve) of the tag recommender. We conducted experiments on two publicly available datasets, i.e., Delicious and Last.fm. Experimental results show that: (1) OptRank outperforms the existing graph based methods when only (user, tag, item) relation is available. (2) OptRank successfully improves the results by incorporating social network, tag semantics and item profiles.",2012,Knowledge Discovery and Data Mining,Fields of study: receiver operating characteristicdifference setsocial networkrecommender systemworld wide webinformation retrievaldata miningmachine learningcomputer science
PatentMiner: topic-driven patent analysis and mining,Jie Tang (Tsinghua University)Bo Wang (Tsinghua University)Yang Yang (Tsinghua University)Po Hu (Tsinghua University)Yanting Zhao (Tsinghua University)Xinyu Yan (Tsinghua University)Bo Gao (Tsinghua University)Minlie Huang (Tsinghua University)Peng Xu (ExxonMobil)Weichang Li (ExxonMobil)Adam K. Usadi (ExxonMobil),"2158012360,2703712228,2673426936,2139057371,2231433081,2227582908,2649313175,2660412917,2302742212,2131301603,1999889971","Patenting is one of the most important ways to protect company's core business concepts and proprietary technologies. Analyzing large volume of patent data can uncover the potential competitive or collaborative relations among companies in certain areas, which can provide valuable information to develop strategies for intellectual property (IP), R&D, and marketing. In this paper, we present a novel topic-driven patent analysis and mining system. Instead of merely searching over patent content, we focus on studying the heterogeneous patent network derived from the patent database, which is represented by several types of objects (companies, inventors, and technical content) jointly evolving over time. We design and implement a general topic-driven framework for analyzing and mining the heterogeneous patent network. Specifically, we propose a dynamic probabilistic model to characterize the topical evolution of these objects within the patent network. Based on this modeling framework, we derive several patent analytics tools that can be directly used for IP and R&D strategy planning, including a heterogeneous network co-ranking method, a topic-level competitor evolution analysis algorithm, and a method to summarize the search results. We evaluate the proposed methods on a real-world patent database. The experimental results show that the proposed techniques clearly outperform the corresponding baseline methods.",2012,Knowledge Discovery and Data Mining,Fields of study: patent visualisationheterogeneous networkintellectual propertysocial networkstatistical modelcompetitor analysisdata sciencedata mining
Using smart card data to extract passenger's spatio-temporal density and train's trajectory of MRT system,Lijun Sun (National University of Singapore)Der-Horng Lee (National University of Singapore)Alex Erath (ETH Zurich)Xianfeng Huang,"2163287186,2638532736,2336167859,2434640664","Rapid tranit systems are the most important public transportation service modes in many large cities around the world. Hence, its service reliability is of high importance for government and transit agencies. Despite taking all the necessary precautions, disruptions cannot be entirely prevented but what transit agencies can do is to prepare to respond to failure in a timely and effective manner. To this end, information about daily travel demand patterns are crucial to develop efficient failure response strategies. To the extent of urban computing, smart card data offers us the opportunity to investigate and understand the demand pattern of passengers and service level from transit operators. In this present study, we present a methodology to analyze smart card data collected in Singapore, to describe dynamic demand characteristics of one case mass rapid transit (MRT) service. The smart card reader registers passengers when they enter and leave an MRT station. Between tapping in and out of MRT stations, passengers are either walking to and fro the platform as they alight and board on the trains or they are traveling in the train. To reveal the effective position of the passengers, a regression model based on the observations from the fastest passengers for each origin destination pair has been developed. By applying this model to all other observations, the model allows us to divide passengers in the MRT system into two groups, passengers on the trains and passengers waiting in the stations. The estimation model provides the spatio-temporal density of passengers. From the density plots, trains' trajectories can be identified and passengers can be assigned to single trains according to the estimated location. Thus, with this model, the location of a certain train and the number of onboard passengers can be estimated, which can further enable transit agencies to improve their response to service disruptions. Since the respective final destination can also be derived from the data set, one can develop effective failure response scenarios such as the planning of contingency buses that bring passengers directly to their final destinations and thus relieves the bridging buses that are typically made available in such situations.",2012,Knowledge Discovery and Data Mining,Fields of study: service levelpublic transportsmart cardpredictiondata collectionregression analysismobile computingsimulationstatisticscomputer science
Interacting viruses in networks: can both survive?,Alex Beutel (Carnegie Mellon University)B. Aditya Prakash (Carnegie Mellon University)Roni Rosenfeld (Carnegie Mellon University)Christos Faloutsos (Carnegie Mellon University),"2045447989,2124002246,2117063646,2198983026","Suppose we have two competing ideas/products/viruses, that propagate over a social or other network. Suppose that they are strong/virulent enough, so that each, if left alone, could lead to an epidemic. What will happen when both operate on the network? Earlier models assume that there is perfect competition: if a user buys product 'A' (or gets infected with virus 'X'), she will never buy product 'B' (or virus 'Y'). This is not always true: for example, a user could install and use both Firefox and Google Chrome as browsers. Similarly, one type of flu may give partial immunity against some other similar disease. In the case of full competition, it is known that 'winner takes all,' that is the weaker virus/product will become extinct. In the case of no competition, both viruses survive, ignoring each other. What happens in-between these two extremes? We show that there is a phase transition: if the competition is harsher than a critical level, then 'winner takes all;' otherwise, the weaker virus survives. These are the contributions of this paper (a) the problem definition, which is novel even in epidemiology literature (b) the phase-transition result and (c) experiments on real data, illustrating the suitability of our results.",2012,Knowledge Discovery and Data Mining,Fields of study: perfect competitioncompetitionphase transitionoperations researchsimulation
Learning personal + social latent factor model for social recommendation,Yelong Shen (Kent State University)Ruoming Jin (Kent State University),"2665888088,2119237514","Social recommendation, which aims to systematically leverage the social relationships between users as well as their past behaviors for automatic recommendation, attract much attention recently. The belief is that users linked with each other in social networks tend to share certain common interests or have similar tastes ( homophily principle ); such similarity is expected to help improve the recommendation accuracy and quality. There have been a few studies on social recommendations; however, they almost completely ignored the heterogeneity and diversity of the social relationship. In this paper, we develop a joint personal and social latent factor (PSLF) model for social recommendation. Specifically, it combines the state-of-the-art collaborative filtering and the social network modeling approaches for social recommendation. Especially, the PSLF extracts the social factor vectors for each user based on the state-of-the-art mixture membership stochastic blockmodel, which can explicitly express the varieties of the social relationship. To optimize the PSLF model, we develop a scalable expectation-maximization (EM) algorithm, which utilizes a novel approximate mean-field technique for fast expectation computation. We compare our approach with the latest social recommendation approaches on two real datasets, Flixter and Douban (both with large social networks). With similar training cost, our approach has shown a significant improvement in terms of prediction accuracy criteria over the existing approaches.",2012,Knowledge Discovery and Data Mining,Fields of study: social heuristicsexpectation maximization algorithmrecommender systemdata miningmachine learningcomputer science
Online learning to diversify from implicit feedback,Karthik Raman (Cornell University)Pannaga Shivaswamy (Cornell University)Thorsten Joachims (Cornell University),"2289464261,1788475625,245171893","In order to minimize redundancy and optimize coverage of multiple user interests, search engines and recommender systems aim to diversify their set of results. To date, these diversification mechanisms are largely hand-coded or relied on expensive training data provided by experts. To overcome this problem, we propose an online learning model and algorithms for learning diversified recommendations and retrieval functions from implicit feedback. In our model, the learning algorithm presents a ranking to the user at each step, and uses the set of documents from the presented ranking, which the user reads, as feedback. Even for imperfect and noisy feedback, we show that the algorithms admit theoretical guarantees for maximizing any submodular utility measure under approximately rational user behavior. In addition to the theoretical results, we find that the algorithm learns quickly, accurately, and robustly in empirical evaluations on two datasets.",2012,Knowledge Discovery and Data Mining,Fields of study: search enginerecommender systemtheoretical computer sciencedata miningmachine learningcomputer science
Urban point-of-interest recommendation by mining user check-in behaviors,Josh Jia-Ching Ying (National Cheng Kung University)Eric Hsueh-Chan Lu (National Cheng Kung University)Wen-Ning Kuo (National Cheng Kung University)Vincent S. Tseng (National Cheng Kung University),"2159909716,2005838078,2223798530,2023431847","In recent years, researches on recommendation of urban Points-Of-Interest (POI), such as restaurants, based on social information have attracted a lot of attention. Although a number of social-based recommendation techniques have been proposed in the literature, most of their concepts are only based on the individual or friends' check-in behaviors. It leads to that the recommended POIs list is usually constrained within the users' or friends' living area. Furthermore, since context-aware and environmental information changes quickly, especially in urban areas, how to extract appropriate features from such kind of heterogeneous data to facilitate the recommendation is also a critical and challenging issue. In this paper, we propose a novel approach named Urban POI-Mine (UPOI-Mine) that integrates location-based social networks (LBSNs) for recommending users urban POIs based on the user preferences and location properties simultaneously. The core idea of UPOI-Mine is to build a regression-tree-based predictor in the normalized check-in space, so as to support the prediction of interestingness of POI related to each user's preference. Based on the LBSN data, we extract the features of places in terms of i) Social Factor , ii) Individual Preference , and iii) POI Popularity for model building. To our best knowledge, this is the first work on urban POI recommendation that considers social factor, individual preference and POI popularity in LBSN data, simultaneously. Through comprehensive experimental evaluations on a real dataset from Gowalla, the proposed UPOI-Mine is shown to deliver excellent performance.",2012,Knowledge Discovery and Data Mining,Fields of study: point of interestmodel buildingdecision treesocial networkworld wide webdata miningcomputer science
Overlapping community detection via bounded nonnegative matrix tri-factorization,Yu Zhang (Hong Kong University of Science and Technology)Dit Yan Yeung (Hong Kong University of Science and Technology),"2648094648,2109477161","Complex networks are ubiquitous in our daily life, with the World Wide Web, social networks, and academic citation networks being some of the common examples. It is well understood that modeling and understanding the network structure is of crucial importance to revealing the network functions. One important problem, known as community detection, is to detect and extract the community structure of networks. More recently, the focus in this research topic has been switched to the detection of overlapping communities. In this paper, based on the matrix factorization approach, we propose a method called bounded nonnegative matrix tri-factorization (BNMTF). Using three factors in the factorization, we can explicitly model and learn the community membership of each node as well as the interaction among communities. Based on a unified formulation for both directed and undirected networks, the optimization problem underlying BNMTF can use either the squared loss or the generalized KL-divergence as its loss function. In addition, to address the sparsity problem as a result of missing edges, we also propose another setting in which the loss function is defined only on the observed edges. We report some experiments on real-world datasets to demonstrate the superiority of BNMTF over other related matrix factorization methods.",2012,Knowledge Discovery and Data Mining,Fields of study: nonnegative matrixnon negative matrix factorizationcommunity structurecomplex networkmatrix decompositionnetwork analysisloss functionsocial networkoptimization problemcombinatoricsmachine learningmathematical optimizationcomputer sciencemathematics
Intelligent advertising framework for digital signage,Phil Tian (Intel)Addicam V. Sanjay (Intel)Kunapareddy Chiranjeevi (Intel)Shahzad Malik Malik (Intel),"2232421280,2132332169,2108493130,2617797738","How to realize targeted advertising in digital signage is an interesting question. This paper proposed an Intelligent Advertising Framework (IAF), which pioneers the integration of Anonymous Viewer Analytics (AVA) and Data Mining technologies to achieve Targeted and interactive Advertising. IAF correlates AVA viewership information with point-of-sale (POS) data, and establishes a link between the response time to an ad by a certain demographic group and the effect on the sale of the advertised product. With the advertising models learned based on this correlation, IAF can provide advertisers and retailers with intelligence to show the right ads to right audience in right location at right time. Preliminary results indicate that IAF will greatly improve the effect and utility of advertising and maximize the Return on Investment (ROI) of advertisers and retailers. The demo shows Intel's leadership regarding intelligent advertising in the Digital Signage industry.",2012,Knowledge Discovery and Data Mining,Fields of study: native advertisingsearch advertisingpoint of salereturn on investmentadvertising researchcontextual advertisingonline advertisinginternet privacyadvertisingcomputer science
Avoiding the crowds: understanding Tube station congestion patterns from trip data,Irina Ceapa (University College London)Chris Smith (University College London)Licia Capra (University College London),"2231333077,2421220884,2105995467","For people travelling using public transport, overcrowding is one of the major causes of discomfort. However, most Advanced Traveller Information Systems (ATIS) do not take crowdedness into account, suggesting routes either based on number of interchanges or overall travel time, regardless of how comfortable (in terms of crowdedness) the trip might be. Identifying times when public transport is overcrowded could help travellers change their travel patterns, by either travelling slightly earlier or later, or by travelling from/to a different but geographically close station. In this paper, we illustrate how historical automated fare collection systems data can be mined in order to reveal station crowding patterns. In particular, we study one such dataset of travel history on the London underground (known colloquially as the ""Tube""). Our spatio-temporal analysis demonstrates that crowdedness is a highly regular phenomenon during the working week, with large spikes occurring in short time intervals. We then illustrate how crowding levels can be accurately predicted, even with simple techniques based on historic averages. These results demonstrate that information regarding crowding levels can be incorporated within ATIS, so as to provide travellers with more personalised travel plans.",2012,Knowledge Discovery and Data Mining,Fields of study: public transportinformation systempredictionmobile computingoperations researchtelecommunicationsdata miningsimulationstatisticscomputer science
Mining recent temporal patterns for event detection in multivariate time series data,Iyad Batal (University of Pittsburgh)Dmitriy Fradkin (Siemens)H James Harrison (University of Virginia)Fabian Moerchen (Siemens)Milos Hauskrecht (University of Pittsburgh),"1841724547,2028188512,2510947214,1991190249,85581826","Improving the performance of classifiers using pattern mining techniques has been an active topic of data mining research. In this work we introduce the recent temporal pattern mining framework for finding predictive patterns for monitoring and event detection problems in complex multivariate time series data. This framework first converts time series into time-interval sequences of temporal abstractions. It then constructs more complex temporal patterns backwards in time using temporal operators. We apply our framework to health care data of 13,558 diabetic patients and show its benefits by efficiently finding useful patterns for detecting and diagnosing adverse medical conditions that are associated with diabetes.",2012,Knowledge Discovery and Data Mining,Fields of study: health caretime seriestext miningdata scienceinformation retrievaldata miningcomputer science
Fast mining and forecasting of complex time-stamped events,Yasuko Matsubara (Kyoto University)Yasushi Sakurai (Nippon Telegraph and Telephone)Christos Faloutsos (Carnegie Mellon University)Tomoharu Iwata (Nippon Telegraph and Telephone)Masatoshi Yoshikawa (Kyoto University),"2106416586,1989929707,2198983026,2108993706,2307105024","Given huge collections of time-evolving events such as web-click logs, which consist of multiple attributes (e.g., URL, userID, times- tamp), how do we find patterns and trends? How do we go about capturing daily patterns and forecasting future events? We need two properties: (a) effectiveness, that is, the patterns should help us understand the data, discover groups, and enable forecasting, and (b) scalability, that is, the method should be linear with the data size. We introduce TriMine, which performs three-way mining for all three attributes, namely, URLs, users, and time. Specifically TriMine discovers hidden topics, groups of URLs, and groups of users, simultaneously. Thanks to its concise but effective summarization, it makes it possible to accomplish the most challenging and important task, namely, to forecast future events. Extensive experiments on real datasets demonstrate that TriMine discovers meaningful topics and makes long-range forecasts, which are notoriously difficult to achieve. In fact, TriMine consistently outperforms the best state-of-the-art existing methods in terms of accuracy and execution speed (up to 74x faster).",2012,Knowledge Discovery and Data Mining,Fields of study: topic modeltensorforecastingdata scienceworld wide webdata miningmachine learningstatisticscomputer science
Mining event periodicity from incomplete observations,Zhenhui Li (University of Illinois at Urbana–Champaign)Jingjing Wang (University of Illinois at Urbana–Champaign)Jiawei Han (University of Illinois at Urbana–Champaign),"2098136913,2628903998,2121939561","Advanced technology in GPS and sensors enables us to track physical events, such as human movements and facility usage. Periodicity analysis from the recorded data is an important data mining task which provides useful insights into the physical events and enables us to report outliers and predict future behaviors. To mine periodicity in an event, we have to face real-world challenges of inherently complicated periodic behaviors and imperfect data collection problem. Specifically, the hidden temporal periodic behaviors could be oscillating and noisy, and the observations of the event could be incomplete. In this paper, we propose a novel probabilistic measure for periodicity and design a practical method to detect periods. Our method has thoroughly considered the uncertainties and noises in periodic behaviors and is provably robust to incomplete observations. Comprehensive experiments on both synthetic and real datasets demonstrate the effectiveness of our method.",2012,Knowledge Discovery and Data Mining,Fields of study: data collectionoscillationdata sciencedata miningmachine learningstatistics
On socio-spatial group query for location-based social networks,De-Nian Yang (Academia Sinica)Chih-Ya Shen (National Taiwan University)Wang-Chien Lee (Pennsylvania State University)Ming-Syan Chen (National Taiwan University),"2096343151,2122520210,2143778659,2122365371","Challenges faced in organizing impromptu activities are the requirements of making timely invitations in accordance with the locations of candidate attendees and the social relationship among them. It is desirable to find a group of attendees close to a rally point and ensure that the selected attendees have a good social relationship to create a good atmosphere in the activity. Therefore, this paper proposes Socio-Spatial Group Query (SSGQ) to select a group of nearby attendees with tight social relation. Efficient processing of SSGQ is very challenging due to the tradeoff in the spatial and social domains. We show that the problem is NP-hard via a proof and design an efficient algorithm SSGSelect, which includes effective pruning techniques to reduce the running time for finding the optimal solution. We also propose a new index structure, Social R-Tree to further improve the efficiency. User study and experimental results demonstrate that SSGSelect significantly outperforms manual coordination in both solution quality and efficiency.",2012,Knowledge Discovery and Data Mining,Fields of study: social networksocial relationworld wide webdata miningartificial intelligencemachine learningcomputer science
PageRank on an evolving graph,Bahman Bahmani (Stanford University)Ravi Kumar (Yahoo!)Mohammad Mahdian (Google)Eli Upfal (Brown University),"2001310754,2232709231,2161106313,265232414","One of the most important features of the Web graph and social networks is that they are constantly evolving. The classical computational paradigm, which assumes a fixed data set as an input to an algorithm that terminates, is inadequate for such settings. In this paper we study the problem of computing PageRank on an evolving graph. We propose an algorithm that, at any moment in the time and by crawling a small portion of the graph, provides an estimate of the PageRank that is close to the true PageRank of the graph at that moment. We will also evaluate our algorithm experimentally on real data sets and on randomly generated inputs. Under a stylized model of graph evolution, we show that our algorithm achieves a provable performance guarantee that is significantly better than the naive algorithm that crawls the nodes in a round-robin fashion.",2012,Knowledge Discovery and Data Mining,Fields of study: distance hereditary graphstrength of a graphvoltage graphcomplement graphgraph bandwidthmoral graphnull graphclique widthrandom geometric graphgraphrandom walktheoretical computer sciencecombinatoricsdistributed computingstatisticscomputer science
Learning from crowds in the presence of schools of thought,Yuandong Tian (Carnegie Mellon University)Jun Zhu (Tsinghua University),"2112566403,2305755055","Crowdsourcing has recently become popular among machine learning researchers and social scientists as an effective way to collect large-scale experimental data from distributed workers. To extract useful information from the cheap but potentially unreliable answers to tasks, a key problem is to identify reliable workers as well as unambiguous tasks. Although for objective tasks that have one correct answer per task, previous works can estimate worker reliability and task clarity based on the single gold standard assumption, for tasks that are subjective and accept multiple reasonable answers that workers may be grouped into, a phenomenon called schools of thought , existing models cannot be trivially applied. In this work, we present a statistical model to estimate worker reliability and task clarity without resorting to the single gold standard assumption. This is instantiated by explicitly characterizing the grouping behavior to form schools of thought with a rank-1 factorization of a worker-task groupsize matrix. Instead of performing an intermediate inference step, which can be expensive and unstable, we present an algorithm to analytically compute the sizes of different groups. We perform extensive empirical studies on real data collected from Amazon Mechanical Turk. Our method discovers the schools of thought, shows reasonable estimation of worker reliability and task clarity, and is robust to hyperparameter changes. Furthermore, our estimated worker reliability can be used to improve the gold standard prediction for objective tasks.",2012,Knowledge Discovery and Data Mining,Fields of study: crowdsourcinggold standardgroup dynamicsempirical researchdata collectionstatistical modeldata miningartificial intelligencemachine learningstatisticscomputer sciencemathematics
Modeling disease progression via fused sparse group lasso,Jiayu Zhou (Arizona State University)Jun Liu (Arizona State University)Vaibhav A. Narayan (Johnson & Johnson Pharmaceutical Research and Development)Jieping Ye (Arizona State University),"2097879502,2610591563,2690485612,2305258894","Alzheimer's Disease (AD) is the most common neurodegenerative disorder associated with aging. Understanding how the disease progresses and identifying related pathological biomarkers for the progression is of primary importance in Alzheimer's disease research. In this paper, we develop novel multi-task learning techniques to predict the disease progression measured by cognitive scores and select biomarkers predictive of the progression. In multi-task learning, the prediction of cognitive scores at each time point is considered as a task, and multiple prediction tasks at different time points are performed simultaneously to capture the temporal smoothness of the prediction models across different time points. Specifically, we propose a novel convex fused sparse group Lasso (cFSGL) formulation that allows the simultaneous selection of a common set of biomarkers for multiple time points and specific sets of biomarkers for different time points using the sparse group Lasso penalty and in the meantime incorporates the temporal smoothness using the fused Lasso penalty. The proposed formulation is challenging to solve due to the use of several non-smooth penalties. We show that the proximal operator associated with the proposed formulation exhibits a certain decomposition property and can be computed efficiently; thus cFSGL can be solved efficiently using the accelerated gradient method. To further improve the model, we propose two non-convex formulations to reduce the shrinkage bias inherent in the convex formulation. We employ the difference of convex programming technique to solve the non-convex formulations. Our extensive experiments using data from the Alzheimer's Disease Neuroimaging Initiative demonstrate the effectiveness of the proposed progression models in comparison with existing methods for disease progression. We also perform longitudinal stability selection to identify and analyze the temporal patterns of biomarkers in disease progression.",2012,Knowledge Discovery and Data Mining,Fields of study: multi task learningstabilizing selectiongradient methodregressionpredictive modellingtext miningdata sciencedata miningmachine learningstatisticscomputer science
Active spectral clustering via iterative uncertainty reduction,"Fabian L. Wauthier (University of California, Berkeley)Nebojsa Jojic (Microsoft)Michael I. Jordan (University of California, Berkeley)","1762337056,2092023269,2435751034","Spectral clustering is a widely used method for organizing data that only relies on pairwise similarity measurements. This makes its application to non-vectorial data straight-forward in principle, as long as all pairwise similarities are available. However, in recent years, numerous examples have emerged in which the cost of assessing similarities is substantial or prohibitive. We propose an active learning algorithm for spectral clustering that incrementally measures only those similarities that are most likely to remove uncertainty in an intermediate clustering solution. In many applications, similarities are not only costly to compute, but also noisy. We extend our algorithm to maintain running estimates of the true similarities, as well as estimates of their accuracy. Using this information, the algorithm updates only those estimates which are relatively inaccurate and whose update would most likely remove clustering uncertainty. We compare our methods on several datasets, including a realistic example where similarities are expensive and noisy. The results show a significant improvement in performance compared to the alternatives.",2012,Knowledge Discovery and Data Mining,Fields of study: canopy clustering algorithmcorrelation clusteringconstrained clusteringcure data clustering algorithmspectral clusteringfuzzy clusteringactive learningcluster analysisdata miningpattern recognitionmachine learningstatisticscomputer sciencemathematics
ComSoc: adaptive transfer of user behaviors over composite social network,Erheng Zhong (Hong Kong University of Science and Technology)Wei Fan (IBM)Junwei Wang (Tencent)Lei Xiao (Tencent)Yong Li (Tencent),"2108094379,2491264568,2231289907,2295231835,2704805915","Accurate prediction of user behaviors is important for many social media applications, including social marketing, personalization and recommendation, etc. A major challenge lies in that, the available behavior data or interactions between users and items in a given social network are usually very limited and sparse (e.g., >= 99.9% empty). Many previous works model user behavior from only historical user logs. We observe that many people are members of several social networks in the same time, such as Facebook, Twitter and Tencent's QQ. Importantly, their behaviors and interests in different networks influence one another. This gives us an opportunity to leverage the knowledge of user behaviors in different networks, in order to alleviate the data sparsity problem, and enhance the predictive performance of user modeling. Combining different networks ""simply and naively"" does not work well. Instead, we formulate the problem to model multiple networks as ""composite network knowledge transfer"". We first select the most suitable networks inside a composite social network via a hierarchical Bayesian model, parameterized for individual users, and then build topic models for user behavior prediction using both the relationships in the selected networks and related behavior data. To handle big data, we have implemented the algorithm using Map/Reduce. We demonstrate that the proposed composite network-based user behavior model significantly improve the predictive accuracy over a number of existing approaches on several real world applications, such as a very large social-networking dataset from Tencent Inc.",2012,Knowledge Discovery and Data Mining,Fields of study: evolving networksdynamic network analysistransfer of learningsocial mediauser modelingsocial network analysissocial networkdata scienceworld wide webdata miningmachine learningcomputer science
Large-scale learning of word relatedness with constraints,Guy Halawi (Tel Aviv University)Gideon Dror (Yahoo!)Evgeniy Gabrilovich (Yahoo!)Yehuda Koren (Yahoo!),"2222038715,1992641199,1804802447,1966367906","Prior work on computing semantic relatedness of words focused on representing their meaning in isolation , effectively disregarding inter-word affinities. We propose a large-scale data mining approach to learning word-word relatedness, where known pairs of related words impose constraints on the learning process. We learn for each word a low-dimensional representation, which strives to maximize the likelihood of a word given the contexts in which it appears. Our method, called CLEAR, is shown to significantly outperform previously published approaches. The proposed method is based on first principles, and is generic enough to exploit diverse types of text corpora, while having the flexibility to impose constraints on the derived word similarities. We also make publicly available a new labeled dataset for evaluating word relatedness algorithms, which we believe to be the largest such dataset to date.",2012,Knowledge Discovery and Data Mining,Fields of study: semantic similarityfirst principlecomputational semanticsnatural language processingdata miningpattern recognitionmachine learningcomputer science
Low rank modeling of signed networks,Cho-Jui Hsieh (University of Texas at Austin)Kai-Yang Chiang (University of Texas at Austin)Inderjit S. Dhillon (University of Texas at Austin),"2148022289,2129170864,2033403132","Trust networks, where people leave trust and distrust feedback, are becoming increasingly common. These networks may be regarded as signed graphs, where a positive edge weight captures the degree of trust while a negative edge weight captures the degree of distrust. Analysis of such signed networks has become an increasingly important research topic. One important analysis task is that of sign inference, i.e., infer unknown (or future) trust or distrust relationships given a partially observed signed network. Most state-of-the-art approaches consider the notion of structural balance in signed networks, building inference algorithms based on information about links, triads, and cycles in the network. In this paper, we first show that the notion of weak structural balance in signed networks naturally leads to a global low-rank model for the network. Under such a model, the sign inference problem can be formulated as a low-rank matrix completion problem. We show that we can perfectly recover missing relationships, under certain conditions, using state-of-the-art matrix completion algorithms. We also propose the use of a low-rank matrix factorization approach with generalized loss functions as a practical method for sign inference - this approach yields high accuracy while being scalable to large signed networks, for instance, we show that this analysis can be performed on a synthetic graph with 1.1 million nodes and 120 million edges in 10 minutes. We further show that the low-rank model can be used for other analysis tasks on signed networks, such as user segmentation through signed graph clustering, with theoretical guarantees. Experiments on synthetic as well as real data show that our low rank model substantially improves accuracy of sign inference as well as clustering. As an example, on the largest real dataset available to us (Epinions data with 130K nodes and 840K edges), our matrix factorization approach yields 94.6% accuracy on the sign inference task as compared to 90.8% accuracy using a state-of-the-art cycle-based method - moreover, our method runs in 40 seconds as compared to 10,000 seconds for the cycle-based method.",2012,Knowledge Discovery and Data Mining,Fields of study: matrix decompositionloss functiondiscrete mathematicscombinatoricsdata miningmachine learningstatisticscomputer sciencemathematics
Where to wait for a taxi,Xudong Zheng (Beihang University)Xiao Liang (Beihang University)Ke Xu (Beihang University),"2148494609,2656928116,2600070012","People often have the demand to decide where to wait for a taxi in order to save their time. In this paper, to address this problem, we employ the non-homogeneous Poisson process (NHPP) to model the behavior of vacant taxis. According to the statistics of the parking time of vacant taxis on the roads and the number of the vacant taxis leaving the roads in history, we can estimate the waiting time at different times on road segments. We also propose an approach to make recommendations for potential passengers on where to wait for a taxi based on our estimated waiting time. Then we evaluate our approach through the experiments on simulated passengers and actual trajectories of 12,000 taxis in Beijing. The results show that our estimation is relatively accurate and could be regarded as a reliable upper bound of the waiting time in probability. And our recommendation is a tradeoff between the waiting time and walking distance, which would bring practical assistance to potential passengers. In addition, we develop a mobile application TaxiWaiter on Android OS to help the users wait for taxis based on our approach and historical data.",2012,Knowledge Discovery and Data Mining,Fields of study: inhomogeneous poisson processpoisson distributionupper and lower boundssystems modelingsimulationstatistics
HySAD: a semi-supervised hybrid shilling attack detector for trustworthy product recommendation,"Zhiang Wu (Nanjing University of Finance and Economics)Junjie Wu (Beihang University)Jie Cao (Nanjing University of Finance and Economics)Dacheng Tao (University of Technology, Sydney)","2701292483,2149366604,2659082948,2104129307","Shilling attackers apply biased rating profiles to recommender systems for manipulating online product recommendations. Although many studies have been devoted to shilling attack detection, few of them can handle the hybrid shilling attacks that usually happen in practice, and the studies for real-life applications are rarely seen. Moreover, little attention has yet been paid to modeling both labeled and unlabeled user profiles, although there are often a few labeled but numerous unlabeled users available in practice. This paper presents a Hybrid Shilling Attack Detector, or HySAD for short, to tackle these problems. In particular, HySAD introduces MC-Relief to select effective detection metrics, and Semi-supervised Naive Bayes (SNB_lambda) to precisely separate Random-Filler model attackers and Average-Filler model attackers from normal users. Thorough experiments on MovieLens and Netflix datasets demonstrate the effectiveness of HySAD in detecting hybrid shilling attacks, and its robustness for various obfuscated strategies. A real-life case study on product reviews of Amazon.cn is also provided, which further demonstrates that HySAD can effectively improve the accuracy of a collaborative-filtering based recommender system, and provide interesting opportunities for in-depth analysis of attacker behaviors. These, in turn, justify the value of HySAD for real-world applications.",2012,Knowledge Discovery and Data Mining,Fields of study: collaborative filteringnaive bayes classifierselection biasrecommender systemworld wide webcomputer securitydata miningmachine learningcomputer science
Discovering urban spatial-temporal structure from human activity patterns,Shan Jiang (Massachusetts Institute of Technology)Joseph Ferreira (Massachusetts Institute of Technology)Marta C. Gonzalez (Massachusetts Institute of Technology),"2674653191,2147702459,2277322729","Urban geographers, planners, and economists have long been studying urban spatial structure to understand the development of cities. Statistical and data mining techniques, as proposed in this paper, go a long way in improving our knowledge about human activities extracted from travel surveys. As of today, most urban simulators have not yet incorporated the various types of individuals by their daily activities. In this work, we detect clusters of individuals by daily activity patterns, integrated with their usage of space and time, and show that daily routines can be highly predictable, with clear differences depending on the group, e.g. students vs. part time workers. This analysis presents the basis to capture collective activities at large scales and expand our perception of urban structure from the spatial dimension to spatial-temporal dimension. It will be helpful for planers to understand how individuals utilize time and interact with urban space in metropolitan areas and crucial for the design of sustainable cities in the future.",2012,Knowledge Discovery and Data Mining,Fields of study: kernel density estimationcomputational sociologydata miningsimulationstatistics
Inductive multi-task learning with multiple view data,Jintao Zhang (University of Kansas)Jun Huan (University of Kansas),"2309818640,2139058963","In many real-world applications, it is becoming common to have data extracted from multiple diverse sources, known as ""multi-view"" data. Multi-view learning (MVL) has been widely studied in many applications, but existing MVL methods learn a single task individually. In this paper, we study a new direction of multi-view learning where there are multiple related tasks with multi-view data (i.e. multi-view multi-task learning, or MVMT Learning). In our MVMT learning methods, we learn a linear mapping for each view in each task. In a single task, we use co-regularization to obtain functions that are in-agreement with each other on the unlabeled samples and achieve low classification errors on the labeled samples simultaneously. Cross different tasks, additional regularization functions are utilized to ensure the functions that we learn in each view are similar. We also developed two extensions of the MVMT learning algorithm. One extension handles missing views and the other handles non-uniformly related tasks. Experimental studies on three real-world data sets demonstrate that our MVMT methods significantly outperform the existing state-of-the-art methods.",2012,Knowledge Discovery and Data Mining,Fields of study: multi task learninggeneralization erroractive learningfeature learningsemi supervised learninginstance based learningunsupervised learningdata miningpattern recognitionmachine learningcomputer science
Active sampling for entity matching,Kedar Bellare (Yahoo!)Suresh Iyengar (Yahoo!)Aditya G. Parameswaran (Stanford University)Vibhor Rastogi (Yahoo!),"1829235141,2230643152,2077695977,2184633580","In entity matching, a fundamental issue while training a classifier to label pairs of entities as either duplicates or non-duplicates is the one of selecting informative training examples. Although active learning presents an attractive solution to this problem, previous approaches minimize the misclassification rate (0-1 loss) of the classifier, which is an unsuitable metric for entity matching due to class imbalance (i.e., many more non-duplicate pairs than duplicate pairs). To address this, a recent paper [1] proposes to maximize recall of the classifier under the constraint that its precision should be greater than a specified threshold. However, the proposed technique requires the labels of all n input pairs in the worst-case. Our main result is an active learning algorithm that approximately maximizes recall of the classifier while respecting a precision constraint with provably sub-linear label complexity (under certain distributional assumptions). Our algorithm uses as a black-box any active learning module that minimizes 0-1 loss. We show that label complexity of our algorithm is at most log n times the label complexity of the black-box, and also bound the difference in the recall of classifier learnt by our algorithm and the recall of the optimal classifier satisfying the precision constraint. We provide an empirical evaluation of our algorithm on several real-world matching data sets that demonstrates the effectiveness of our approach.",2012,Knowledge Discovery and Data Mining,Fields of study: data deduplicationactive learningsatisfiabilitydata miningpattern recognitionmachine learningstatisticscomputer sciencemathematics
An integrated data mining approach to real-time clinical monitoring and deterioration warning,Yi Mao (Washington University in St. Louis)Wenlin Chen (Washington University in St. Louis)Yixin Chen (Washington University in St. Louis)Chenyang Lu (Washington University in St. Louis)Marin Kollef (Washington University in St. Louis)Thomas C. Bailey (Washington University in St. Louis),"2466982642,2231133452,2295009377,2099764636,1145070569,2172679573","Clinical study found that early detection and intervention are essential for preventing clinical deterioration in patients, for patients both in intensive care units (ICU) as well as in general wards but under real-time data sensing (RDS). In this paper, we develop an integrated data mining approach to give early deterioration warnings for patients under real-time monitoring in ICU and RDS. Existing work on mining real-time clinical data often focus on certain single vital sign and specific disease. In this paper, we consider an integrated data mining approach for general sudden deterioration warning. We synthesize a large feature set that includes first and second order time-series features, detrended fluctuation analysis (DFA), spectral analysis, approximative entropy, and cross-signal features. We then systematically apply and evaluate a series of established data mining methods, including forward feature selection, linear and nonlinear classification algorithms, and exploratory undersampling for class imbalance. An extensive empirical study is conducted on real patient data collected between 2001 and 2008 from a variety of ICUs. Results show the benefit of each of the proposed techniques, and the final integrated approach significantly improves the prediction quality. The proposed clinical warning system is currently under integration with the electronic medical record system at Barnes-Jewish Hospital in preparation for a clinical trial. This work represents a promising step toward general early clinical warning which has the potential to significantly improve the quality of patient care in hospitals.",2012,Knowledge Discovery and Data Mining,Fields of study: detrended fluctuation analysisreal time dataclinical trialempirical researchdata collectiontime seriessecond order logicfeature selectiondata scienceoperations researchdata miningstatisticscomputer science
Metro maps of science,Dafna Shahaf (Carnegie Mellon University)Carlos Guestrin (Carnegie Mellon University)Eric Horvitz (Microsoft),"1275334909,1988556028,1970391018","As the number of scientific publications soars, even the most enthusiastic reader can have trouble staying on top of the evolving literature. It is easy to focus on a narrow aspect of one's field and lose track of the big picture. Information overload is indeed a major challenge for scientists today, and is especially daunting for new investigators attempting to master a discipline and scientists who seek to cross disciplinary borders. In this paper, we propose metrics of influence, coverage and connectivity for scientific literature. We use these metrics to create structured summaries of information, which we call metro maps . Most importantly, metro maps explicitly show the relations between papers in a way which captures developments in the field. Pilot user studies demonstrate that our method helps researchers acquire new knowledge efficiently: map users achieved better precision and recall scores and found more seminal papers while performing fewer searches.",2012,Knowledge Discovery and Data Mining,Fields of study: information overloadinformationautomatic summarizationworld wide webdata miningmachine learningsimulationcomputer science
A framework for robust discovery of entity synonyms,Kaushik Chakrabarti (Microsoft)Surajit Chaudhuri (Microsoft)Tao Cheng (Microsoft)Dong Xin (Google),"2124676925,2163909284,2285779824,1991372327","Entity synonyms are critical for many applications like information retrieval and named entity recognition in documents. The current trend is to automatically discover entity synonyms using statistical techniques on web data. Prior techniques suffer from several limitations like click log sparsity and inability to distinguish between entities of different concept classes. In this paper, we propose a general framework for robustly discovering entity synonym with two novel similarity functions that overcome the limitations of prior techniques. We develop efficient and scalable techniques leveraging the MapReduce framework to discover synonyms at large scale. To handle long entity names with extraneous tokens, we propose techniques to effectively map long entity names to short queries in query log. Our experiments on real data from different entity domains demonstrate the superior quality of our synonyms as well as the efficiency of our algorithms. The entity synonyms produced by our system is in production in Bing Shopping and Video search, with experiments showing the significance it brings in improving search experience.",2012,Knowledge Discovery and Data Mining,Fields of study: entity linkinginformation retrievaldata miningdatabasecomputer science
Fast algorithms for maximal clique enumeration with limited memory,"James Cheng (Nanyang Technological University)Linhong Zhu (Agency for Science, Technology and Research)Yiping Ke (Agency for Science, Technology and Research)Shumo Chu (Nanyang Technological University)","2304873892,2114458094,2164055299,2119604566","Maximal clique enumeration (MCE) is a long-standing problem in graph theory and has numerous important applications. Though extensively studied, most existing algorithms become impractical when the input graph is too large and is disk-resident. We first propose an efficient partition-based algorithm for MCE that addresses the problem of processing large graphs with limited memory. We then further reduce the high cost of CPU computation of MCE by a careful nested partition based on a cost model. Finally, we parallelize our algorithm to further reduce the overall running time. We verified the efficiency of our algorithms by experiments in large real-world graphs.",2012,Knowledge Discovery and Data Mining,Fields of study: clique problemmaximal independent settreewidthindependent setparallel algorithmgraph theorydiscrete mathematicscombinatoricsmathematical optimizationcomputer sciencemathematics
A system for extracting top-K lists from the web,Zhixian Zhang (Shanghai Jiao Tong University)Kenny Qili Zhu (Shanghai Jiao Tong University)Haixun Wang (Microsoft),"2283903961,2106081732,2116756368","List data is an important source of structured data on the web. This paper is concerned with ""top-k"" pages, which are web pages that describe a list of k instances of a particular topic or concept. Examples include ""the 10 tallest persons in the world"" and ""the 50 hits of 2010 you don't want to miss"". Compared to normal web list data, ""top-k"" lists contain richer information and are easier to understand. Therefore the extraction of such lists can help enrich existing knowledge bases about general concepts, or act as a preprocessing step to produce facts for a fact answering engine. We present an efficient system that extracts the target lists from web pages with high accuracy. We have used the system to process up to 160 million, or 1/10 of a high-frequency web snapshot from Bing, and obtained over 140,000 lists with 90.4% precision.",2012,Knowledge Discovery and Data Mining,Fields of study: web 2 0page viewsocial semantic webstatic web pagedata website mapweb developmentweb designweb serverweb serviceweb pagedata modelhigh frequencyweb miningweb search engineknowledge baseworld wide webinformation retrievaldata miningcomputer science
SPF-GMKL: generalized multiple kernel learning with a million kernels,Ashesh Jain (Indian Institute of Technology Delhi)S.V.N. Vishwanathan (Purdue University)Manik Varma (Microsoft),"2229498112,2584127737,2130043413","Multiple Kernel Learning (MKL) aims to learn the kernel in an SVM from training data. Many MKL formulations have been proposed and some have proved effective in certain applications. Nevertheless, as MKL is a nascent field, many more formulations need to be developed to generalize across domains and meet the challenges of real world applications. However, each MKL formulation typically necessitates the development of a specialized optimization algorithm. The lack of an efficient, general purpose optimizer capable of handling a wide range of formulations presents a significant challenge to those looking to take MKL out of the lab and into the real world. This problem was somewhat alleviated by the development of the Generalized Multiple Kernel Learning (GMKL) formulation which admits fairly general kernel parameterizations and regularizers subject to mild constraints. However, the projected gradient descent GMKL optimizer is inefficient as the computation of the step size and a reasonably accurate objective function value or gradient direction are all expensive. We overcome these limitations by developing a Spectral Projected Gradient (SPG) descent optimizer which: a) takes into account second order information in selecting step sizes; b) employs a non-monotone step size selection criterion requiring fewer function evaluations; c) is robust to gradient noise, and d) can take quick steps when far away from the optimum. We show that our proposed SPG-GMKL optimizer can be an order of magnitude faster than projected gradient descent on even small and medium sized datasets. In some cases, SPG-GMKL can even outperform state-of-the-art specialized optimization algorithms developed for a single MKL formulation. Furthermore, we demonstrate that SPG-GMKL can scale well beyond gradient descent to large problems involving a million kernels or half a million data points. Our code and implementation are available publically.",2012,Knowledge Discovery and Data Mining,Fields of study: stochastic gradient descentsupport vector machinedata miningmachine learningmathematical optimizationalgorithmcomputer sciencemathematics
Efficient personalized pagerank with accuracy assurance,Yasuhiro Fujiwara (Nippon Telegraph and Telephone)Makoto Nakatsuji (Nippon Telegraph and Telephone)Takeshi Yamamuro (Nippon Telegraph and Telephone)Hiroaki Shiokawa (Nippon Telegraph and Telephone)Makoto Onizuka (Nippon Telegraph and Telephone),"2157865440,2040281177,2307575184,2139345634,2270636005","Personalize PageRank (PPR) is an effective relevance (proximity) measure in graph mining. The goal of this paper is to efficiently compute single node relevance and top-k/highly relevant nodes without iteratively computing the relevances of all nodes. Based on a ""random surfer model"", PPR iteratively computes the relevances of all nodes in a graph until convergence for a given user preference distribution. The problem with this iterative approach is that it cannot compute the relevance of just one or a few nodes. The heart of our solution is to compute single node relevance accurately in non-iterative manner based on sparse matrix representation, and to compute top-k/highly relevant nodes exactly by pruning unnecessary relevance computations based on upper/lower relevance estimations. Our experiments show that our approach is up to seven orders of magnitude faster than the existing alternatives.",2012,Knowledge Discovery and Data Mining,Fields of study: sparse matrixtheoretical computer sciencedata miningmachine learningcomputer science
"BC-PDM: data mining, social network analysis and text mining system based on cloud computing",Le Yu (Peking University)Jian Zheng (Peking University)Wei Chong Shen (Peking University)Bin Wu (Peking University)Bai Wang (Peking University)Long Qian (Peking University)Bo Ren Zhang (Peking University),"2525444618,2709790083,2235274038,2646895658,2688233564,2652079640,2110644986","Telecom BI(Business Intelligence) system consists of a set of application programs and technologies for gathering, storing, analyzing and providing access to data, which contribute to manage business information and make decision precisely. However, traditional analysis algorithms meet new challenges as the continued exponential growth in both the volume and the complexity of telecom data. With the Cloud Computing development, some parallel data analysis systems have been emerging. However, existing systems have rarely comprehensive function, either providing data analysis service or providing social network analysis. We need a comprehensive tool to store and analysis large scale data efficiently. In response to the challenge, the SaaS (Software-as-a-Service) BI system, BC-PDM (Big Cloud-Parallel Data Mining), are proposed. BC-PDM supports parallel ETL process, statistical analysis, data mining, text mining and social network analysis which are based on Hadoop. This demo introduces three tasks: business recommendation, customer community detection and user preference classification by employing a real telecom data set. Experimental results show BC-PDM is very efficient and effective for intelligence data analysis.",2012,Knowledge Discovery and Data Mining,Fields of study: software as a servicecloud computingexponential growthsocial network analysisdata analysisdata stream miningtext miningbusiness intelligencedata sciencedata miningdatabasecomputer science
From user comments to on-line conversations,Chunyan Wang (Stanford University)Mao Ye (HP Labs)Bernardo A. Huberman (HP Labs),"2172233713,2617455009,2343374067","We present an analysis of user conversations in on-line social media and their evolution over time. We propose a dynamic model that predicts the growth dynamics and structural properties of conversation threads. The model reconciles the differing observations that have been reported in existing studies. By separating artificial factors from user behavior, we show that there are actually underlying rules in common for on-line conversations in different social media websites. Results of our model are supported by empirical measurements throughout a number of different social media websites.",2012,Knowledge Discovery and Data Mining,Fields of study: social mediasocial networkmultimediasocial psychologyworld wide webcomputer science
A near-linear time approximation algorithm for angle-based outlier detection in high-dimensional data,Ninh Pham (IT University of Copenhagen)Rasmus Pagh (IT University of Copenhagen),"2162985535,1864519460","Outlier mining in d -dimensional point sets is a fundamental and well studied data mining task due to its variety of applications. Most such applications arise in high-dimensional domains. A bottleneck of existing approaches is that implicit or explicit assessments on concepts of distance or nearest neighbor are deteriorated in high-dimensional data. Following up on the work of Kriegel et al. (KDD '08), we investigate the use of angle-based outlier factor in mining high-dimensional outliers. While their algorithm runs in cubic time (with a quadratic time heuristic), we propose a novel random projection-based technique that is able to estimate the angle-based outlier factor for all data points in time near-linear in the size of the data. Also, our approach is suitable to be performed in parallel environment to achieve a parallel speedup. We introduce a theoretical analysis of the quality of approximation to guarantee the reliability of our estimation algorithm. The empirical experiments on synthetic and real world data sets demonstrate that our approach is efficient and scalable to very large high-dimensional data sets.",2012,Knowledge Discovery and Data Mining,Fields of study: clustering high dimensional datatime complexityk nearest neighbors algorithmanomaly detectiondata miningmachine learningstatisticscomputer science
Mining coherent subgraphs in multi-layer graphs with edge labels,Brigitte Boden (RWTH Aachen University)Stephan Günnemann (RWTH Aachen University)Holger Hoffmann (RWTH Aachen University)Thomas Seidl (RWTH Aachen University),"2117690819,316694267,2224264343,2140301036","Mining dense subgraphs such as cliques or quasi-cliques is an important graph mining problem and closely related to the notion of graph clustering. In various applications, graphs are enriched by additional information. For example, we can observe graphs representing different types of relations between the vertices. These multiple edge types can also be viewed as different ""layers"" of the same graph, which is denoted as a ""multi-layer graph"" in this work. Additionally, each edge might be annotated by a label characterizing the given relation in more detail. By exploiting all these different kinds of information, the detection of more interesting clusters in the graph can be supported. In this work, we introduce the multi-layer coherent subgraph (MLCS) model, which defines clusters of vertices that are densely connected by edges with similar labels in a subset of the graph layers. We avoid redundancy in the result by selecting only the most interesting, non-redundant clusters for the output. Based on this model, we introduce the best-first search algorithm MiMAG. In thorough experiments we demonstrate the strengths of MiMAG in comparison with related approaches on synthetic as well as real-world datasets.",2012,Knowledge Discovery and Data Mining,Fields of study: factor critical graphdistance hereditary graphsimplex graphstrength of a graphvoltage graphcomplement graphforbidden graph characterizationgraph bandwidthcoxeter graphgraph powergraph factorizationbutterfly graphstring graphcomparability graphbiconnected graphnull graphgraph labelingclique widthsymmetric graphline graphclustering coefficientgraphsearch algorithmcluster analysisdiscrete mathematicscombinatoricsmachine learningcomputer sciencemathematics
Intrusion as (anti)social communication: characterization and detection,Qi Ding (Boston University)Natallia Katenka (Boston University)Paul Barford (University of Wisconsin-Madison)Eric D. Kolaczyk (Boston University)Mark Crovella (Boston University),"2102365199,2068216664,2078767370,274731230,91397146","A reasonable definition of intrusion is: entering a community to which one does not belong. This suggests that in a network, intrusion attempts may be detected by looking for communication that does not respect community boundaries. In this paper, we examine the utility of this concept for identifying malicious network sources. In particular, our goal is to explore whether this concept allows a core-network operator using flow data to augment signature-based systems located at network edges. We show that simple measures of communities can be defined for flow data that allow a remarkably effective level of intrusion detection simply by looking for flows that do not respect those communities. We validate our approach using labeled intrusion attempt data collected at a large number of edge networks. Our results suggest that community-based methods can offer an important additional dimension for intrusion detection systems.",2012,Knowledge Discovery and Data Mining,Fields of study: anomaly based intrusion detection systemintrusion prevention systemintrusion detection systemcomputer securitydata miningcomputer science
SmartDispatch: enabling efficient ticket dispatch in an IT service environment,Shivali Agarwal (IBM)Renuka Sindhgatta (IBM)Bikram Sengupta (IBM),"2154300049,1977082244,2113952558","In an IT service delivery environment, the speedy dispatch of a ticket to the correct resolution group is the crucial first step in the problem resolution process. The size and complexity of such environments make the dispatch decision challenging, and incorrect routing by a human dispatcher can lead to significant delays that degrade customer satisfaction, and also have adverse financial implications for both the customer and the IT vendor. In this paper, we present SmartDispatch, a learning-based tool that seeks to automate the process of ticket dispatch while maintaining high accuracy levels. SmartDispatch comes with two classification approaches - the well-known SVM method, and a discriminative term-based approach that we designed to address some of the issues in SVM classification that were empirically observed. Using a combination of these approaches, SmartDispatch is able to automate the dispatch of a ticket to the correct resolution group for a large share of the tickets, while for the rest, it is able to suggest a short list of 3-5 groups that contain the correct resolution group with a high probability. Empirical evaluation of SmartDispatch on data from 3 large service engagement projects in IBM demonstrate the efficacy and practical utility of the approach.",2012,Knowledge Discovery and Data Mining,Fields of study: service delivery frameworkcustomer satisfactioncomputer securitydata miningmachine learningsimulation
Maximum inner-product search using cone trees,Parikshit Ram (Georgia Institute of Technology)Alexander G. Gray (Georgia Institute of Technology),"2293981250,2112810595","The problem of efficiently finding the best match for a query in a given set with respect to the Euclidean distance or the cosine similarity has been extensively studied. However, the closely related problem of efficiently finding the best match with respect to the inner-product has never been explored in the general setting to the best of our knowledge. In this paper we consider this problem and contrast it with the previous problems considered. First, we propose a general branch-and-bound algorithm based on a (single) tree data structure. Subsequently, we present a dual-tree algorithm for the case where there are multiple queries. Our proposed branch-and-bound algorithms are based on novel inner-product bounds. Finally we present a new data structure, the cone tree, for increasing the efficiency of the dual-tree algorithm. We evaluate our proposed algorithms on a variety of data sets from various applications, and exhibit up to five orders of magnitude improvement in query time over the naive search technique in some cases.",2012,Knowledge Discovery and Data Mining,Fields of study: branch and bounddata structurediscrete mathematicscombinatoricsmathematical optimizationcomputer sciencemathematics
The contextual focused topic model,Xu Chen (Duke University)Mingyuan Zhou (Duke University)Lawrence Carin (Duke University),"2304058330,2170042945,657437189","A nonparametric Bayesian contextual focused topic model (cFTM) is proposed. The cFTM infers a sparse (""focused"") set of topics for each document, while also leveraging contextual information about the author(s) and document venue. The hierarchical beta process, coupled with a Bernoulli process, is employed to infer the focused set of topics associated with each author and venue; the same construction is also employed to infer those topics associated with a given document that are unusual (termed ""random effects""), relative to topics that are inferred as probable for the associated author(s) and venue. To leverage statistical strength and infer latent interrelationships between authors and venues, the Dirichlet process is utilized to cluster authors and venues. The cFTM automatically infers the number of topics needed to represent the corpus, the number of author and venue clusters, and the probabilistic importance of the author, venue and random-effect information on word assignment for a given document. Efficient MCMC inference is presented. Example results and interpretations are presented for two real datasets, demonstrating promising performance, with comparison to other state-of-the-art methods.",2012,Knowledge Discovery and Data Mining,Fields of study: topic modelrandom effects modelcluster analysisdata sciencedata miningpattern recognitionmachine learningstatisticscomputer science
Multi-label hypothesis reuse,Sheng-Jun Huang (Nanjing University)Yang Yu (Nanjing University)Zhi-Hua Zhou (Nanjing University),"2145988500,2501979622,2286237009","Multi-label learning arises in many real-world tasks where an object is naturally associated with multiple concepts. It is well-accepted that, in order to achieve a good performance, the relationship among labels should be exploited. Most existing approaches require the label relationship as prior knowledge, or exploit by counting the label co-occurrence. In this paper, we propose the MAHR approach, which is able to automatically discover and exploit label relationship. Our basic idea is that, if two labels are related, the hypothesis generated for one label can be helpful for the other label. MAHR implements the idea as a boosting approach with a hypothesis reuse mechanism. In each boosting round, the base learner for a label is generated by not only learning on its own task but also reusing the hypotheses from other labels, and the amount of reuse across labels provides an estimate of the label relationship. Extensive experimental results validate that MAHR is able to achieve superior performance and discover reasonable label relationship. Moreover, we disclose that the label relationship is usually asymmetric.",2012,Knowledge Discovery and Data Mining,Fields of study: data miningartificial intelligencecomputer science
Chromatic correlation clustering,Francesco Bonchi (Yahoo!)Aristides Gionis (Yahoo!)Francesco Gullo (Yahoo!)Antti Ukkonen (Yahoo!),"2176652147,737311942,1979201319,2635975366","We study a novel clustering problem in which the pairwise relations between objects are categorical . This problem can be viewed as clustering the vertices of a graph whose edges are of different types ( colors ). We introduce an objective function that aims at partitioning the graph such that the edges within each cluster have, as much as possible, the same color. We show that the problem is NP-hard and propose a randomized algorithm with approximation guarantee proportional to the maximum degree of the input graph. The algorithm iteratively picks a random edge as pivot, builds a cluster around it, and removes the cluster from the graph. Although being fast, easy-to-implement, and parameter free, this algorithm tends to produce a relatively large number of clusters. To overcome this issue we introduce a variant algorithm, which modifies how the pivot is chosen and and how the cluster is built around the pivot. Finally, to address the case where a fixed number of output clusters is required, we devise a third algorithm that directly optimizes the objective function via a strategy based on the alternating minimization paradigm. We test our algorithms on synthetic and real data from the domains of protein-interaction networks, social media, and bibliometrics. Experimental evidence show that our algorithms outperform a baseline algorithm both in the task of reconstructing a ground-truth clustering and in terms of objective function value.",2012,Knowledge Discovery and Data Mining,Fields of study: k medians clusteringcanopy clustering algorithmcomplete linkage clusteringdetermining the number of clusters in a data setk medoidscorrelation clusteringconstrained clusteringdata stream clusteringcure data clustering algorithmsingle linkage clusteringaffinity propagationclustering coefficientsocial mediafuzzy clusteringground truthclustering high dimensional datarandomized algorithmmatchingcluster analysiscombinatoricsdata miningmachine learningmathematical optimizationstatisticscomputer sciencemathematics
Selecting a characteristic set of reviews,Theodoros Lappas (Boston University)Mark Crovella (Boston University)Evimaria Terzi (Boston University),"2028397797,91397146,2110675235","Online reviews provide consumers with valuable information that guides their decisions on a variety of fronts: from entertainment and shopping to medical services. Although the proliferation of online reviews gives insights about different aspects of a product, it can also prove a serious drawback: consumers cannot and will not read thousands of reviews before making a purchase decision. This need to extract useful information from large review corpora has spawned considerable prior work, but so far all have drawbacks. Review summarization (generating statistical descriptions of review sets) sacrifices the immediacy and narrative structure of reviews. Likewise, review selection (identifying a subset of 'helpful' or 'important' reviews) leads to redundant or non-representative summaries. In this paper, we fill the gap between existing review-summarization and review-selection methods by selecting a small subset of reviews that together preserve the statistical properties of the entire review corpus. We formalize this task as a combinatorial optimization problem and show that it NP-hard both tosolve and approximate. We also design effective algorithms that prove to work well in practice. Our experiments with real review corpora on different types of products demonstrate the utility of our methods, and our user studies indicate that our methods provide a better summary than prior approaches.",2012,Knowledge Discovery and Data Mining,Fields of study: narrative structuremanagement sciencedata sciencedata miningmachine learningcomputer science
The long and the short of it: summarising event sequences with serial episodes,Nikolaj Tatti (University of Antwerp)Jilles Vreeken (University of Antwerp),"1367500519,1971070670","An ideal outcome of pattern mining is a small set of informative patterns, containing no redundancy or noise, that identifies the key structure of the data at hand. Standard frequent pattern miners do not achieve this goal, as due to the pattern explosion typically very large numbers of highly redundant patterns are returned. We pursue the ideal for sequential data, by employing a pattern set mining approach - an approach where, instead of ranking patterns individually, we consider results as a whole. Pattern set mining has been successfully applied to transactional data, but has been surprisingly understudied for sequential data. In this paper, we employ the MDL principle to identify the set of sequential patterns that summarises the data best. In particular, we formalise how to encode sequential data using sets of serial episodes, and use the encoded length as a quality score. As search strategy, we propose two approaches: the first algorithm selects a good pattern set from a large candidate set, while the second is a parameter-free any-time algorithm that mines pattern sets directly from the data. Experimentation on synthetic and real data demonstrates we efficiently discover small sets of informative patterns.",2012,Knowledge Discovery and Data Mining,Fields of study: transaction datadata miningpattern recognitionmachine learningcomputer sciencemathematics
Differential identifiability,Jaewoo Lee (Purdue University)Chris Clifton (Purdue University),"2319471130,2158488542","A key challenge in privacy-preserving data mining is ensuring that a data mining result does not inherently violate privacy. e-Differential Privacy appears to provide a solution to this problem. However, there are no clear guidelines on how to set e to satisfy a privacy policy. We give an alternate formulation, Differential Identifiability , parameterized by the probability of individual identification. This provides the strong privacy guarantees of differential privacy, while letting policy makers set parameters based on the established privacy concept of individual identifiability.",2012,Knowledge Discovery and Data Mining,Fields of study: differential privacyidentifiabilityinternet privacydata miningmachine learningstatisticscomputer sciencemathematics
Identifying users profiles from mobile calls habits,Barbara Furletti (Istituto di Scienza e Tecnologie dell'Informazione)Lorenzo Gabrielli (Istituto di Scienza e Tecnologie dell'Informazione)Chiara Renso (Istituto di Scienza e Tecnologie dell'Informazione)Salvatore Rinzivillo (Istituto di Scienza e Tecnologie dell'Informazione),"2716014144,2157886307,55173896,1263076414","The huge quantity of positioning data registered by our mobile phones stimulates several research questions, mainly originating from the combination of this huge quantity of data with the extreme heterogeneity of the tracked user and the low granularity of the data. We propose a methodology to partition the users tracked by GSM phone calls into profiles like resident, commuters, in transit and tourists. The methodology analyses the phone calls with a combination of top-down and bottom up techniques where the top-down phase is based on a sequence of queries that identify some behaviors. The bottom-up is a machine learning phase to find groups of similar call behavior, thus refining the previous step. The integration of the two steps results in the partitioning of mobile traces into these four user categories that can be deeper analyzed, for example to understand the tourist movements in city or the traffic effects of commuters. An experiment on the identification of user profiles on a real dataset collecting call records from one month in the city of Pisa illustrates the methodology.",2012,Knowledge Discovery and Data Mining,Fields of study: top down and bottom up designworld wide webdata miningmachine learningcomputer science
Estimation of urban commuting patterns using cellphone network data,Vanessa Frias-Martinez (Telefónica)Cristina Soguero (Telefónica)Enrique Frias-Martinez (Telefónica),"225175796,2225907337,603370153","Commuting matrices are key for a variety of fields, including transportation engineering and urban planning. Up to now, these matrices have been typically generated from data obtained from surveys. Nevertheless, such approaches typically involve high costs which limits the frequency of the studies. Cell phones can be considered one of the main sensors of human behavior due to its ubiquity, and as a such, a pervasive source of mobility information at a large scale. In this paper we propose a new technique for the estimation of commuting matrices using the data collected from the pervasive infrastructure of a cell phone network. Our goal is to show that we can construct cell-phone generated matrices that capture the same patterns as traditional commuting matrices. In order to do so we use optimization techniques in combination with a variation of Temporal Association Rules. Our validation results show that it is possible to construct commuting matrices from call detail records with a high degree of accuracy, and as a result our technique is a cost-effective solution to complement traditional approaches.",2012,Knowledge Discovery and Data Mining,Fields of study: association rule learningcost effectiveness analysisdata collectionurban planninghuman behaviordata miningsimulationstatisticscomputer sciencemathematics
Multi-domain active learning for text classification,"Lianghao Li (Tsinghua University)Xiaoming Jin (Tsinghua University)Sinno Jialin Pan (Agency for Science, Technology and Research)Jian-Tao Sun (Microsoft)","2136197840,2127078753,2120836466,2131116857","Active learning has been proven to be effective in reducing labeling efforts for supervised learning. However, existing active learning work has mainly focused on training models for a single domain. In practical applications, it is common to simultaneously train classifiers for multiple domains. For example, some merchant web sites (like Amazon.com) may need a set of classifiers to predict the sentiment polarity of product reviews collected from various domains (e.g., electronics, books, shoes). Though different domains have their own unique features, they may share some common latent features. If we apply active learning on each domain separately, some data instances selected from different domains may contain duplicate knowledge due to the common features. Therefore, how to choose the data from multiple domains to label is crucial to further reducing the human labeling efforts in multi-domain learning. In this paper, we propose a novel multi-domain active learning framework to jointly select data instances from all domains with duplicate information considered. In our solution, a shared subspace is first learned to represent common latent features of different domains. By considering the common and the domain-specific features together, the model loss reduction induced by each data instance can be decomposed into a common part and a domain-specific part. In this way, the duplicate information across domains can be encoded into the common part of model loss reduction and taken into account when querying. We compare our method with the state-of-the-art active learning approaches on several text classification tasks: sentiment classification, newsgroup classification and email spam filtering. The experiment results show that our method reduces the human labeling efforts by 33.2%, 42.9% and 68.7% on the three tasks, respectively.",2012,Knowledge Discovery and Data Mining,Fields of study: multi task learningtransfer of learningactive learningsingle domainactive learningsemi supervised learningsupervised learningdata miningpattern recognitionmachine learningcomputer science
Towards heterogeneous temporal clinical event pattern discovery: a convolutional approach,Fei Wang (IBM)Noah Lee (IBM)Jianying Hu (IBM)Jimeng Sun (IBM)Shahram Ebadollahi (IBM),"2465953593,2105571861,2098665416,2110385854,363122970","Large collections of electronic clinical records today provide us with a vast source of information on medical practice. However, the utilization of those data for exploratory analysis to support clinical decisions is still limited. Extracting useful patterns from such data is particularly challenging because it is longitudinal , sparse and heterogeneous . In this paper, we propose a Nonnegative Matrix Factorization (NMF) based framework using a convolutional approach for open-ended temporal pattern discovery over large collections of clinical records. We call the method One-Sided Convolutional NMF (OSC-NMF). Our framework can mine common as well as individual shift-invariant temporal patterns from heterogeneous events over different patient groups, and handle sparsity as well as scalability problems well. Furthermore, we use an event matrix based representation that can encode quantitatively all key temporal concepts including order, concurrency and synchronicity. We derive efficient multiplicative update rules for OSC-NMF, and also prove theoretically its convergence. Finally, the experimental results on both synthetic and real world electronic patient data are presented to demonstrate the effectiveness of the proposed method.",2012,Knowledge Discovery and Data Mining,Fields of study: non negative matrix factorizationtheoretical computer sciencedata miningmachine learningstatisticscomputer science
"Design principles of massive, robust prediction systems",Troy Raeder (University of Notre Dame)Ori StitelmanBrian DalessandroClaudia Perlich (IBM)Foster J. Provost (New York University),"1999864269,2234062319,1876520739,164824025,2158932634","Most data mining research is concerned with building high-quality classification models in isolation. In massive production systems, however, the ability to monitor and maintain performance over time while growing in size and scope is equally important. Many external factors may degrade classification performance including changes in data distribution, noise or bias in the source data, and the evolution of the system itself. A well-functioning system must gracefully handle all of these. This paper lays out a set of design principles for large-scale autonomous data mining systems and then demonstrates our application of these principles within the m6d automated ad targeting system. We demonstrate a comprehensive set of quality control processes that allow us monitor and maintain thousands of distinct classification models automatically, and to add new models, take on new data, and correct poorly-performing models without manual intervention or system disruption.",2012,Knowledge Discovery and Data Mining,Fields of study: production systemquality controldata stream miningdata sciencedata miningdatabasecomputer science
RainMon: an integrated approach to mining bursty timeseries monitoring data,Ilari Shafer (Carnegie Mellon University)Kai Ren (Carnegie Mellon University)Vishnu Naresh Boddeti (Carnegie Mellon University)Yoshihisa Abe (Carnegie Mellon University)Gregory R. Ganger (Carnegie Mellon University)Christos Faloutsos (Carnegie Mellon University),"2109788593,2680969559,1748409980,2485713002,2791808,2198983026","Metrics like disk activity and network traffic are widespread sources of diagnosis and monitoring information in datacenters and networks. However, as the scale of these systems increases, examining the raw data yields diminishing insight. We present RainMon, a novel end-to-end approach for mining timeseries monitoring data designed to handle its size and unique characteristics. Our system is able to (a) mine large, bursty, real-world monitoring data, (b) find significant trends and anomalies in the data, (c) compress the raw data effectively, and (d) estimate trends to make forecasts. Furthermore, RainMon integrates the full analysis process from data storage to the user interface to provide accessible long-term diagnosis. We apply RainMon to three real-world datasets from production systems and show its utility in discovering anomalous machines and time periods.",2012,Knowledge Discovery and Data Mining,Fields of study: system monitoringproduction systemcomputer data storageuser interfaceprincipal component analysisdata sciencedata miningreal time computingcomputer science
Storytelling in entity networks to support intelligence analysts,M. Shahriar Hossain (Virginia Tech)Patrick Butler (Virginia Tech)Arnold P. Boedihardjo (United States Army Corps of Engineers)Naren Ramakrishnan (Virginia Tech),"2120864034,2293049679,240228327,2199255697","Intelligence analysts grapple with many challenges, chief among them is the need for software support in storytelling, i.e., automatically 'connecting the dots' between disparate entities (e.g., people, organizations) in an effort to form hypotheses and suggest non-obvious relationships. We present a system to automatically construct stories in entity networks that can help form directed chains of relationships, with support for co-referencing, evidence marshaling, and imposing syntactic constraints on the story generation process. A novel optimization technique based on concept lattice mining enables us to rapidly construct stories on massive datasets. Using several public domain datasets, we illustrate how our approach overcomes many limitations of current systems and enables the analyst to efficiently narrow down to hypotheses of interest and reason about alternative explanations.",2012,Knowledge Discovery and Data Mining,Fields of study: intelligence analysispublic domaindata miningartificial intelligencemachine learningcomputer science
LIEGE:: link entities in web lists with knowledge base,Wei Shen (Tsinghua University)Jianyong Wang (Tsinghua University)Ping Luo (HP Labs)Min Wang (HP Labs),"2667078396,2105625159,2291210646,2467205710","A critical step in bridging the knowledge base with the huge corpus of semi-structured Web list data is to link the entity mentions that appear in the Web lists with the corresponding real world entities in the knowledge base, which we call list linking task. This task can facilitate many different tasks such as knowledge base population, entity search and table annotation. However, the list linking task is challenging because a Web list has almost no textual context, and the only input for this task is a list of entity mentions extracted from the Web pages. In this paper, we propose LIEGE, the first general framework to Link the entities in web lists with the knowledge base to the best of our knowledge. Our assumption is that entities mentioned in a Web list can be any collection of entities that have the same conceptual type that people have in mind. To annotate the list items in a Web list with entities that they likely mention, we leverage the prior probability of an entity being mentioned and the global coherence between the types of entities in the Web list. The interdependence between different entity assignments in a Web list makes the optimization of this list linking problem NP-hard. Accordingly, we propose a practical solution based on the iterative substitution to jointly optimize the identification of the mapping entities for the Web list items. We extensively evaluated the performance of our proposed framework over both manually annotated real Web lists extracted from the Web pages and two public data sets, and the experimental results show that our framework significantly outperforms the baseline method in terms of accuracy.",2012,Knowledge Discovery and Data Mining,Fields of study: association listdifference listself organizing listlist update problemsocial semantic webdata webweb pageknowledge baseinformation retrievaldata miningdatabasecomputer science
Mining traffic incidents to forecast impact,Mahalia Miller (Stanford University)Chetan Gupta (Hewlett-Packard),"2340290829,2126857182","Using sensor data from fixed highway traffic detectors, as well as data from highway patrol logs and local weather stations, we aim to answer the domain problem: ""A traffic incident just occurred. How severe will its impact be?"" In this paper we show a practical system for predicting the cost and impact of highway incidents using classification models trained on sensor data and police reports. Our models are built on an understanding of the spatial and temporal patterns of the expected state of traffic at different times of day and locations and past incidents. With high accuracy, our model can predict false reports of incidents that are made to the highway patrol and classify the duration of the incident-induced delays and the magnitude of the incident impact, measured as a function of vehicles delayed, the spatial and temporal extent of the incident. Equipped with our predictions of traffic incident costs and relative impacts, highway operators and first responders will be able to more effectively respond to reports of highway incidents, ultimately improving drivers' welfare and reducing urban congestion.",2012,Knowledge Discovery and Data Mining,Fields of study: cyber physical systemcomputer securitydata miningsimulationcomputer science
Model mining for robust feature selection,Adam Woznica (University of Geneva)Phong Nguyen (University of Geneva)Alexandros Kalousis (University of Geneva),"2150257347,2223944382,1835498038","A common problem with most of the feature selection methods is that they often produce feature sets--models--that are not stable with respect to slight variations in the training data. Different authors tried to improve the feature selection stability using ensemble methods which aggregate different feature sets into a single model. However, the existing ensemble feature selection methods suffer from two main shortcomings: (i) the aggregation treats the features independently and does not account for their interactions, and (ii) a single feature set is returned, nevertheless, in various applications there might be more than one feature sets, potentially redundant, with similar information content. In this work we address these two limitations. We present a general framework in which we mine over different feature models produced from a given dataset in order to extract patterns over the models. We use these patterns to derive more complex feature model aggregation strategies that account for feature interactions, and identify core and distinct feature models. We conduct an extensive experimental evaluation of the proposed framework where we demonstrate its effectiveness over a number of high-dimensional problems from the fields of biology and text-mining.",2012,Knowledge Discovery and Data Mining,Fields of study: minimum redundancy feature selectionkanade lucas tomasi feature trackerfeature modelclustering high dimensional datadimensionality reductionfeature vectorfeatureself informationfeature extractionk nearest neighbors algorithmstabilitybiological classificationfeature selectiontext miningdata miningpattern recognitionmachine learningstatisticscomputer science
Position-normalized click prediction in search advertising,Ye Chen (Microsoft)Tak W. Yan (Microsoft),"2628943388,2139394280","Click-through rate (CTR) prediction plays a central role in search advertising. One needs CTR estimates unbiased by positional effect in order for ad ranking, allocation, and pricing to be based upon ad relevance or quality in terms of click propensity. However, the observed click-through data has been confounded by positional bias, that is, users tend to click more on ads shown in higher positions than lower ones, regardless of the ad relevance. We describe a probabilistic factor model as a general principled approach to studying these exogenous and often overwhelming phenomena. The model is simple and linear in nature, while empirically justified by the advertising domain. Our experimental results with artificial and real-world sponsored search data show the soundness of the underlying model assumption, which in turn yields superior prediction accuracy.",2012,Knowledge Discovery and Data Mining,Fields of study: factor analysisdata miningmachine learningsimulationstatisticscomputer science
Empowering authors to diagnose comprehension burden in textbooks,Rakesh Agrawal (Microsoft)Sunandan Chakraborty (New York University)Sreenivas Gollapudi (Microsoft)Anitha Kannan (Microsoft)Krishnaram Kenthapadi (Microsoft),"2537924216,2168126670,2023254819,2146153601,2088122068",Good textbooks are organized in a systematically progressive fashion so that students acquire new knowledge and learn new concepts based on known items of information. We provide a diagnostic tool for quantitatively assessing the comprehension burden that a textbook imposes on the reader due to non-sequential presentation of concepts. We present a formal definition of comprehension burden and propose an algorithmic approach for computing it. We apply the tool to a corpus of high school textbooks from India and empirically examine its effectiveness in helping authors identify sections of textbooks that can benefit from reorganizing the material presented.,2012,Knowledge Discovery and Data Mining,Fields of study: conceptsknowledge extractiondata sciencemathematics educationdata miningartificial intelligencecomputer science
SHALE: an efficient algorithm for allocation of guaranteed display advertising,Vijay Bharadwaj (Netflix)Peiji Chen (Yahoo!)Wenjing Ma (Yahoo!)Chandrashekhar Nagarajan (Yahoo!)John Tomlin (Yahoo!)Sergei Vassilvitskii (Google)Erik Vee (Facebook)Jian Yang (Yahoo!),"2616915638,2151945453,2166412753,2004947813,2163586335,2156675704,2134018118,2304113540","Motivated by the problem of optimizing allocation in guaranteed display advertising, we develop an efficient, lightweight method of generating a compact allocation plan that can be used to guide ad server decisions. The plan itself uses just O (1) state per guaranteed contract, is robust to noise, and allows us to serve (provably) nearly optimally. The optimization method we develop is scalable, with a small in-memory footprint, and working in linear time per iteration. It is also ""stop-anytime"", meaning that time-critical applications can stop early and still get a good serving solution. Thus, it is particularly useful for optimizing the large problems arising in the context of display advertising. We demonstrate the effectiveness of our algorithm using actual Yahoo! data.",2012,Knowledge Discovery and Data Mining,Fields of study: time complexityonline advertisingreal time computingsimulationmathematical optimizationalgorithmcomputer science
Mining contentions from discussions and debates,Arjun Mukherjee (University of Illinois at Chicago)Bing Liu (University of Illinois at Chicago),"2154911241,2244698799","Social media has become a major source of information for many applications. Numerous techniques have been proposed to analyze network structures and text contents. In this paper, we focus on fine-grained mining of contentions in discussion/debate forums. Contentions are perhaps the most important feature of forums that discuss social, political and religious issues. Our goal is to discover contention and agreement indicator expressions, and contention points or topics both at the discussion collection level and also at each individual post level. To the best of our knowledge, limited work has been done on such detailed analysis. This paper proposes three models to solve the problem, which not only model both contention/agreement expressions and discussion topics, but also, more importantly, model the intrinsic nature of discussions/debates, i.e., interactions among discussants or debaters and topic sharing among posts through quoting and replying relations. Evaluation results using real-life discussion/debate posts from several domains demonstrate the effectiveness of the proposed models.",2012,Knowledge Discovery and Data Mining,Fields of study: social mediacontent analysisworld wide webdata miningartificial intelligencecomputer science
HeteRecom: a semantic-based recommendation system in heterogeneous networks,Chuan Shi (Beijing University of Posts and Telecommunications)Chong Zhou (Beijing University of Posts and Telecommunications)Xiangnan Kong (University of Illinois at Chicago)Philip S. Yu (University of Illinois at Chicago)Gang Liu (Beijing University of Posts and Telecommunications)Bai Wang (Beijing University of Posts and Telecommunications),"2252461150,2096226728,2204127537,2125104194,2638019270,2582794223","Making accurate recommendations for users has become an important function of e-commerce system with the rapid growth of WWW. Conventional recommendation systems usually recommend similar objects, which are of the same type with the query object without exploring the semantics of different similarity measures. In this paper, we organize objects in the recommendation system as a heterogeneous network. Through employing a path-based relevance measure to evaluate the relatedness between any-typed objects and capture the subtle semantic containing in each path, we implement a prototype system (called HeteRecom ) for semantic based recommendation. HeteRecom has the following unique properties: (1) It provides the semantic-based recommendation function according to the path specified by users. (2) It recommends the similar objects of the same type as well as related objects of different types. We demonstrate the effectiveness of our system with a real-world movie data set.",2012,Knowledge Discovery and Data Mining,Fields of study: heterogeneous networksemantic similaritysimilaritysemantic searchrecommender systemworld wide webinformation retrievaldata miningcomputer science
Intelligible models for classification and regression,Yin Lou (Cornell University)Rich Caruana (Microsoft)Johannes Gehrke (Cornell University),"2111965133,2680453709,2083845045","Complex models for regression and classification have high accuracy, but are unfortunately no longer interpretable by users. We study the performance of generalized additive models (GAMs), which combine single-feature models called shape functions through a linear function. Since the shape functions can be arbitrarily complex, GAMs are more accurate than simple linear models. But since they do not contain any interactions between features, they can be easily interpreted by users. We present the first large-scale empirical comparison of existing methods for learning GAMs. Our study includes existing spline and tree-based methods for shape functions and penalized least squares, gradient boosting, and backfitting for learning GAMs. We also present a new method based on tree ensembles with an adaptive number of leaves that consistently outperforms previous work. We complement our experimental results with a bias-variance analysis that explains how different shape models influence the additive model. Our experiments show that shallow bagged trees with gradient boosting distinguish itself as the best method on low- to medium-dimensional datasets.",2012,Knowledge Discovery and Data Mining,Fields of study: generalized additive model for location scale and shapeadditive modelgeneralized additive modelregressionlinear modelbiological classificationeconometricsdata miningpattern recognitionmachine learningstatisticsmathematics
UFIMT: an uncertain frequent itemset mining toolbox,Yongxin Tong (Hong Kong University of Science and Technology)Lei Chen (Hong Kong University of Science and Technology)Philip S. Yu (University of Illinois at Chicago),"2114386387,2318776917,2125104194","In recent years, mining frequent itemsets over uncertain data has attracted much attention in the data mining community. Unlike the corresponding problem in deterministic data, the frequent itemset under uncertain data has two different definitions: the expected support-based frequent itemset and the probabilistic frequent itemset. Most existing works only focus on one of the definitions and no comprehensive study is conducted to compare the two different definitions. Moreover, due to lacking the uniform implementation platform, existing solutions for the same definition even generate inconsistent results. In this demo, we present a demonstration called as UFIMT (underline Uncertain Frequent Itemset Mining Toolbox) which not only discovers frequent itemsets over uncertain data but also compares the performance of different algorithms and demonstrates the relationship between different definitions. In this demo, we firstly present important techniques and implementation skills of the mining problem, secondly, we show the system architecture of UFIMT , thirdly, we report an empirical analysis on extensive both real and synthetic benchmark data sets, which are used to compare different algorithms and to show the close relationship between two different frequent itemset definitions, and finally we discuss some existing challenges and new findings.",2012,Knowledge Discovery and Data Mining,Fields of study: correspondence problemsystems architecturedata sciencedata miningdatabasecomputer science
Practical collapsed variational bayes inference for hierarchical dirichlet process,Issei Sato (University of Tokyo)Kenichi Kurihara (Google)Hiroshi Nakagawa (University of Tokyo),"2148390776,2033931387,2094248864","We propose a novel collapsed variational Bayes (CVB) inference for the hierarchical Dirichlet process (HDP). While the existing CVB inference for the HDP variant of latent Dirichlet allocation (LDA) is more complicated and harder to implement than that for LDA, the proposed algorithm is simple to implement, does not require variance counts to be maintained, does not need to set hyper-parameters, and has good predictive performance.",2012,Knowledge Discovery and Data Mining,Fields of study: variational message passinghierarchical dirichlet processlatent dirichlet allocationeconometricspattern recognitionmachine learningstatisticscomputer sciencemathematics
Joint optimization of bid and budget allocation in sponsored search,Weinan Zhang (Shanghai Jiao Tong University)Ying Zhang (Nankai University)Bin Gao (Microsoft)Yong Yu (Shanghai Jiao Tong University)Xiaojie Yuan (Nankai University)Tie-Yan Liu (Microsoft),"2527611484,2681697379,2616890138,2119244895,2673126507,2108341226","This paper is concerned with the joint allocation of bid price and campaign budget in sponsored search. In this application, an advertiser can create a number of campaigns and set a budget for each of them. In a campaign, he/she can further create several ad groups with bid keywords and bid prices. Data analysis shows that many advertisers are dealing with a very large number of campaigns, bid keywords, and bid prices at the same time, which poses a great challenge to the optimality of their campaign management. As a result, the budgets of some campaigns might be too low to achieve the desired performance goals while those of some other campaigns might be wasted; the bid prices for some keywords may be too low to win competitive auctions while those of some other keywords may be unnecessarily high. In this paper, we propose a novel algorithm to automatically address this issue. In particular, we model the problem as a constrained optimization problem, which maximizes the expected advertiser revenue subject to the constraints of the total budget of the advertiser and the ranges of bid price change. By solving this optimization problem, we can obtain an optimal budget allocation plan as well as an optimal bid price setting. Our simulation results based on the sponsored search log of a commercial search engine have shown that by employing the proposed method, we can effectively improve the performances of the advertisers while at the same time we also see an increase in the revenue of the search engine. In addition, the results indicate that this method is robust to the second-order effects caused by the bid fluctuations from other advertisers.",2012,Knowledge Discovery and Data Mining,Fields of study: unique bid auctionbid shadingsearch engineoptimization problemsecond order logicdata analysiscomputer science
Active learning for online bayesian matrix factorization,Jorge G. Silva (Duke University)Lawrence Carin (Duke University),"2437071335,657437189","The problem of large-scale online matrix completion is addressed via a Bayesian approach. The proposed method learns a factor analysis (FA) model for large matrices, based on a small number of observed matrix elements, and leverages the statistical model to actively select which new matrix entries/observations would be most informative if they could be acquired, to improve the model; the model inference and active learning are performed in an online setting. In the context of online learning, a greedy, fast and provably near-optimal algorithm is employed to sequentially maximize the mutual information between past and future observations, taking advantage of submodularity properties. Additionally, a simpler procedure, which directly uses the posterior parameters learned by the Bayesian approach, is shown to achieve slightly lower estimation quality, with far less computational effort. Inference is performed using a computationally efficient online variational Bayes (VB) procedure. Competitive results are obtained in a very large collaborative filtering problem, namely the Yahoo! Music ratings dataset.",2012,Knowledge Discovery and Data Mining,Fields of study: wake sleep algorithmactive learningcollaborative filteringmutual informationmatrix decompositionfactor analysisbayesian probabilitystatistical modeldata miningpattern recognitionmachine learningstatisticscomputer science
Different slopes for different folks: mining for exceptional regression models with cook's distance,Wouter Duivesteijn (Leiden University)Ad Feelders (Utrecht University)Arno J. Knobbe (Leiden University),"134040164,2467559149,1229146049","Exceptional Model Mining (EMM) is an exploratory data analysis technique that can be regarded as a generalization of subgroup discovery. In EMM we look for subgroups of the data for which a model fitted to the subgroup differs substantially from the same model fitted to the entire dataset. In this paper we develop methods to mine for exceptional regression models. We propose a measure for the exceptionality of regression models (Cook's distance), and explore the possibilities to avoid having to fit the regression model to each candidate subgroup. The algorithm is evaluated on a number of real life datasets. These datasets are also used to illustrate the results of the algorithm. We find interesting subgroups with deviating models on datasets from several different domains. We also show that under certain circumstances one can forego fitting regression models on up to 40% of the subgroups, and these 40% are the relatively expensive regression models to compute.",2012,Knowledge Discovery and Data Mining,Fields of study: proper linear modelcook s distanceregression diagnosticexploratory data analysislinear regressionregression analysiseconometricsdata miningmachine learningstatisticscomputer sciencemathematics
Divide-and-conquer and statistical inference for big data,"Michael I. Jordan (University of California, Berkeley)",2435751034,"I present some recent work on statistical inference for Big Data. Divide-and-conquer is a natural computational paradigm for approaching Big Data problems, particularly given recent developments in distributed and parallel computing, but some interesting challenges arise when applying divide-and-conquer algorithms to statistical inference problems. One interesting issue is that of obtaining confidence intervals in massive datasets. The bootstrap principle suggests resampling data to obtain fluctuations in the values of estimators, and thereby confidence intervals, but this is infeasible with massive data. Subsampling the data yields fluctuations on the wrong scale, which have to be corrected to provide calibrated statistical inferences. I present a new procedure, the ""bag of little bootstraps,"" which circumvents this problem, inheriting the favorable theoretical properties of the bootstrap but also having a much more favorable computational profile. Another issue that I discuss is the problem of large-scale matrix completion. Here divide-and-conquer is a natural heuristic that works well in practice, but new theoretical problems arise when attempting to characterize the statistical performance of divide-and-conquer algorithms. Here the theoretical support is provided by concentration theorems for random matrices, and I present a new approach to this problem based on Stein's method 1 .",2012,Knowledge Discovery and Data Mining,Fields of study: algorithmic inferenceconfidence distributionfiducial inferencestatistical theoryconfidence intervalbig dataeconometricsdata miningmachine learningstatisticscomputer sciencemathematics
Adversarial support vector machine learning,Yan Zhou (University of Texas at Dallas)Murat Kantarcioglu (University of Texas at Dallas)Bhavani M. Thuraisingham (University of Texas at Dallas)Bowei Xi (Purdue University),"2620242970,332400322,2298201484,2060668786","Many learning tasks such as spam filtering and credit card fraud detection face an active adversary that tries to avoid detection. For learning problems that deal with an active adversary, it is important to model the adversary's attack strategy and develop robust learning models to mitigate the attack. These are the two objectives of this paper. We consider two attack models: a free-range attack model that permits arbitrary data corruption and a restrained attack model that anticipates more realistic attacks that a reasonable adversary would devise under penalties. We then develop optimal SVM learning strategies against the two attack models. The learning algorithms minimize the hinge loss while assuming the adversary is modifying data to maximize the loss. Experiments are performed on both artificial and real data sets. We demonstrate that optimal solutions may be overly pessimistic when the actual attacks are much weaker than expected. More important, we demonstrate that it is possible to develop a much more resilient SVM learning model while making loose assumptions on the data corruption models. When derived under the restrained attack model, our optimal SVM learning strategy provides more robust overall performance under a wide range of attack parameters.",2012,Knowledge Discovery and Data Mining,Fields of study: pre play attackonline machine learningadversary modeladvantagegeneralization errorsupport vector machineactive learningsemi supervised learningcomputer securitydata miningmachine learningcomputer science
Online allocation of display ads with smooth delivery,Anand Bhalgat (University of Pennsylvania)Jon Feldman (Google)Vahab S. Mirrokni (Google),"32712699,2123466731,2331823467","Display ads on the Internet are often sold in bundles of thousands or millions of impressions over a particular time period, typically weeks or months. Ad serving systems that assign ads to pages on behalf of publishers must satisfy these contracts, but at the same time try to maximize overall quality of placement. This is usually modeled in the literature as an online allocation problem , where contracts are represented by overall delivery constraints over a finite time horizon. However this model misses an important aspect of ad delivery: time homogeneity. Advertisers who buy these packages expect their ad to be shown smoothly throughout the purchased time period, in order to reach a wider audience, to have a sustained impact, and to support the ads they are running on other media (e.g., television). In this paper we formalize this problem using several nested packing constraints, and develop a tight (1-1/ e )-competitive online algorithm for this problem. Our algorithms and analysis require novel techniques as they involve online computation of multiple dual variables per ad. We then show the effectiveness of our algorithms through exhaustive simulation studies on real data sets.",2012,Knowledge Discovery and Data Mining,Fields of study: online algorithmsatisfiabilitydata miningmachine learningsimulationcomputer sciencemathematics
Linear support vector machines via dual cached loops,Shin Matsushima (University of Tokyo)S.V.N. Vishwanathan (Purdue University)Alexander J. Smola (Yahoo!),"2153313846,2584127737,1972291593","Modern computer hardware offers an elaborate hierarchy of storage subsystems with different speeds, capacities, and costs associated with them. Furthermore, processors are now inherently parallel offering the execution of several diverse threads simultaneously. This paper proposes StreamSVM, the first algorithm for training linear Support Vector Machines (SVMs) which takes advantage of these properties by integrating caching with optimization. StreamSVM works by performing updates in the dual, thus obviating the need to rebalance frequently visited examples. Furthermore we trade off file I/O with data expansion on the fly by generating features on demand. This significantly increases throughput. Experiments show that StreamSVM outperforms other linear SVM solvers, including the award winning work of [38], by orders of magnitude and produces more accurate solutions within a shorter amount of time.",2012,Knowledge Discovery and Data Mining,Fields of study: support vector machinetheoretical computer sciencedata miningreal time computingmachine learningcomputer science
Random forests for metric learning with implicit pairwise position dependence,Caiming Xiong (University at Buffalo)David M. Johnson (University at Buffalo)Ran Xu (University at Buffalo)Jason J. Corso (University at Buffalo),"2095665791,2126929149,2108055350,2073062517","Metric learning makes it plausible to learn semantically meaningful distances for complex distributions of data using label or pairwise constraint information. However, to date, most metric learning methods are based on a single Mahalanobis metric, which cannot handle heterogeneous data well. Those that learn multiple metrics throughout the feature space have demonstrated superior accuracy, but at a severe cost to computational efficiency. Here, we adopt a new angle on the metric learning problem and learn a single metric that is able to implicitly adapt its distance function throughout the feature space. This metric adaptation is accomplished by using a random forest-based classifier to underpin the distance function and incorporate both absolute pairwise position and standard relative position into the representation. We have implemented and tested our method against state of the art global and multi-metric methods on a variety of data sets. Overall, the proposed method outperforms both types of method in terms of accuracy (consistently ranked first) and is an order of magnitude faster than state of the art multi-metric methods (16x faster in the worst case).",2012,Knowledge Discovery and Data Mining,Fields of study: equivalence of metricsintrinsic metricrandom forestpattern recognitionmachine learningmathematical optimizationstatisticscomputer sciencemathematics
Mining regular routes from GPS data for ridesharing recommendations,Wen HeDeyi Li (Tsinghua University)Tianlei ZhangLifeng An (Tsinghua University)Mu Guo (Tsinghua University)Guisheng Chen (Tsinghua University),"2652353609,2658529290,2679404997,2235150000,2222678830,2710627700","The widely use of GPS-enabled devices has provided us amount of trajectories related to individuals' activities. This gives us an opportunity to learn more about the users' daily lives and offer optimized suggestions to improve people's trip styles. In this paper, we mine regular routes from a users' historical trajectory dataset, and provide ridesharing recommendations to a group of users who share similar routes. Here, regular route means a complete route where a user may frequently pass through approximately in the same time of day. In this paper, we first divide users' GPS data into individual routes, and a group of routes which occurred in a similar time of day are grouped together by a sliding time window. A frequency-based regular route mining algorithm is proposed, which is robust to slight disturbances in trajectory data. A feature of Fixed Stop Rate ( FSR ) is used to distinguish the different types of transport modes. Finally, based on the mined regular routes and transport modes, a grid-based route table is constructed for a quick ride matching. We evaluate our method using a large GPS dataset collected by 178 users over a period of four years. The experiment results demonstrate that the proposed method can effectively extract the regular routes and generate rideshare plan among users. This work may help ridesharing to become more efficient and convenient.",2012,Knowledge Discovery and Data Mining,Fields of study: data miningreal time computing
Siren: an interactive tool for mining and visualizing geospatial redescriptions,Esther Galbrun (University of Helsinki)Pauli Miettinen (Max Planck Society),"2276244909,2015634213","We present SIREN, an interactive tool for mining and visualizing geospatial redescriptions. Redescription mining is a powerful data analysis tool that aims at finding alternative descriptions of the same entities. For example, in biology, an important task is to identify the bioclimatic constraints that allow some species to survive, that is, to describe geographical regions in terms of both the fauna that inhabits them and their bioclimatic conditions. Using SIREN, users can explore geospatial data of their interest by visualizing the redescriptions on a map, interactively edit, extend and filter them. To demonstrate the use of the tool, we focus on climatic niche-finding over Europe, as an example task. Yet, SIREN is by no means limited to a particular dataset or application.",2012,Knowledge Discovery and Data Mining,Fields of study: parallel coordinatesvisualizationdata analysisgeospatial analysisdata scienceworld wide webdata miningcomputer science
EventSearch: a system for event discovery and retrieval on multi-type historical data,Dongdong Shan (Peking University)Wayne Xin Zhao (Peking University)Rishan Chen (Peking University)Baihan Shu (Peking University)Ziqi Wang (Peking University)Junjie Yao (Peking University)Hongfei Yan (Peking University)Xiaoming Li (Peking University),"2112634141,2307999729,2700393277,2134590780,2159797654,2502161765,2096609276,2633873560","We present EventSearch, a system for event extraction and retrieval on four types of news-related historical data, i.e., Web news articles, newspapers, TV news program, and micro-blog short messages. The system incorporates over 11 million web pages extracted from ""Web InfoMall"", the Chinese Web Archive since 2001. The newspaper and TV news video clips also span from 2001 to 2011. The system, upon a user query, returns a list of event snippets from multiple data sources. A novel burst model is used to discover events from time-stamped texts. In addition to offline event extraction, our system also provides online event extraction to further meet the user needs. EventSearch provides meaningful analytics that synthesize an accurate description of events. Users interact with the system by ranking the identified events using different criteria (scale, recency and relevance) and submitting their own information needs in different input fields.",2012,Knowledge Discovery and Data Mining,Fields of study: complex event processinginformation needsweb pageworld wide webinformation retrievaldata miningcomputer science
Locally-scaled spectral clustering using empty region graphs,Carlos D. Correa (Lawrence Livermore National Laboratory)Peter Lindstrom (Lawrence Livermore National Laboratory),"2164010038,2278655031","This paper introduces a new method for estimating the local neighborhood and scale of data points to improve the robustness of spectral clustering algorithms. We employ a subset of empty region graphs - the β-skeleton - and non-linear diffusion to define a locally-adapted affinity matrix, which, as we demonstrate, provides higher quality clustering than conventional approaches based on κ nearest neighbors or global scale parameters. Moreover, we show that the clustering quality is far less sensitive to the choice of β and other algorithm parameters, and to transformations such as geometric distortion and random perturbation. We summarize the results of an empirical study that applies our method to a number of 2D synthetic data sets, consisting of clusters of arbitrary shape and scale, and to real multi-dimensional classification examples from benchmarks, including image segmentation.",2012,Knowledge Discovery and Data Mining,Fields of study: k medians clusteringcanopy clustering algorithmcorrelation clusteringconstrained clusteringdata stream clusteringcure data clustering algorithmsingle linkage clusteringspectral clusteringfuzzy clusteringsynthetic dataimage segmentationempirical researchcluster analysisk nearest neighbors algorithmdata miningpattern recognitionmachine learningstatisticscomputer sciencemathematics
Sensing places' life to make city smarter,Stéphane Roche (Laval University)Abbas Rajabifard (University of Melbourne),"2230147616,415517593","This paper explores the smart city concept and proposes an innovative way of sensing urban places' life using aggregation of devices sensors (cameras...) and human sensors (VGI, geosocial networks) datasets. The paper also discusses the need of an enabling geospatial information platform to facilitate data discovery and access in order to support smart cities' operations. Indeed, in this context, Spatial Data Infrastructure plays an important role and acts as an enabling platform linking governments authoritative spatial information with crowd sourced, voluntary information initiatives.",2012,Knowledge Discovery and Data Mining,Fields of study: strategic defense initiativevolunteered geographic informationspatial data infrastructurecrowdsourcingspatial analysislocationsensorinternet privacyworld wide webdata mining
Anonymizing set-valued data by nonreciprocal recoding,Mingqiang Xue (National University of Singapore)Panagiotis Karras (Rutgers–Newark)Chedy Raïssi (French Institute for Research in Computer Science and Automation)Jaideep Vaidya (Rutgers–Newark)Kian-Lee Tan (National University of Singapore),"2303542671,2306017430,2065278996,2164601541,2142476784","Today there is a strong interest in publishing set-valued data in a privacy-preserving manner. Such data associate individuals to sets of values (e.g., preferences, shopping items, symptoms, query logs). In addition, an individual can be associated with a sensitive label (e.g., marital status, religious or political conviction). Anonymizing such data implies ensuring that an adversary should not be able to (1) identify an individual's record, and (2) infer a sensitive label, if such exists. Existing research on this problem either perturbs the data, publishes them in disjoint groups disassociated from their sensitive labels, or generalizes their values by assuming the availability of a generalization hierarchy. In this paper, we propose a novel alternative. Our publication method also puts data in a generalized form, but does not require that published records form disjoint groups and does not assume a hierarchy either; instead, it employs generalized bitmaps and recasts data values in a nonreciprocal manner; formally, the bipartite graph from original to anonymized records does not have to be composed of disjoint complete subgraphs. We configure our schemes to provide popular privacy guarantees while resisting attacks proposed in recent research, and demonstrate experimentally that we gain a clear utility advantage over the previous state of the art.",2012,Knowledge Discovery and Data Mining,Fields of study: bipartite graphprivacyinternet privacyworld wide webdata miningcomputer science
Fast bregman divergence NMF using taylor expansion and coordinate descent,Liangda Li (Georgia Institute of Technology)Guy Lebanon (Georgia Institute of Technology)Haesun Park (Georgia Institute of Technology),"2628459440,2312000221,2123241397","Non-negative matrix factorization (NMF) provides a lower rank approximation of a matrix. Due to nonnegativity imposed on the factors, it gives a latent structure that is often more physically meaningful than other lower rank approximations such as singular value decomposition (SVD). Most of the algorithms proposed in literature for NMF have been based on minimizing the Frobenius norm. This is partly due to the fact that the minimization problem based on the Frobenius norm provides much more flexibility in algebraic manipulation than other divergences. In this paper we propose a fast NMF algorithm that is applicable to general Bregman divergences. Through Taylor series expansion of the Bregman divergences, we reveal a relationship between Bregman divergences and Euclidean distance. This key relationship provides a new direction for NMF algorithms with general Bregman divergences when combined with the scalar block coordinate descent method. The proposed algorithm generalizes several recently proposed methods for computation of NMF with Bregman divergences and is computationally faster than existing alternatives. We demonstrate the effectiveness of our approach with experiments conducted on artificial as well as real world data.",2012,Knowledge Discovery and Data Mining,Fields of study: bregman divergencetaylor seriescalculusmathematical analysismachine learningmathematical optimizationmathematics
SympGraph: a framework for mining clinical notes through symptom relation graphs,Parikshit Sondhi (University of Illinois at Urbana–Champaign)Jimeng Sun (IBM)Hanghang Tong (IBM)ChengXiang Zhai (University of Illinois at Urbana–Champaign),"2019386676,2110385854,2224718883,2152766206","As an integral part of Electronic Health Records (EHRs), clinical notes pose special challenges for analyzing EHRs due to their unstructured nature. In this paper, we present a general mining framework SympGraph for modeling and analyzing symptom relationships in clinical notes. A SympGraph has symptoms as nodes and co-occurrence relations between symptoms as edges, and can be constructed automatically through extracting symptoms over sequences of clinical notes for a large number of patients. We present an important clinical application of SympGraph: symptom expansion, which can expand a given set of symptoms to other related symptoms by analyzing the underlying SympGraph structure. We further propose a matrix update algorithm which provides a significant computational saving for dynamic updates to the graph. Comprehensive evaluation on 1 million longitudinal clinical notes over 13K patients shows that static symptom expansion can successfully expand a set of known symptoms to a disease with high agreement rate with physician input (average precision 0.46), a 31% improvement over baseline co-occurrence based methods. The experimental results also show that the expanded symptoms can serve as useful features for improving AUC measure for disease diagnosis prediction, thus confirming the potential clinical value of our work.",2012,Knowledge Discovery and Data Mining,Fields of study: random walkdata sciencedata miningstatistics
Magnet community identification on social networks,Guan Wang (University of Illinois at Chicago)Yuchen Zhao (University of Illinois at Chicago)Xiaoxiao Shi (University of Illinois at Chicago)Philip S. Yu (University of Illinois at Chicago),"2152311907,2147878578,2134782470,2125104194","Social communities connect people of similar interests together and play essential roles in social network applications. Examples of such communities include people who like the same objects on Facebook, follow common subjects on Twitter, or join similar groups on LinkedIn. Among communities, we notice that some of them are {\em magnetic} to people. A {\em magnet community} is such a community that attracts significantly more people's interests and attentions than other communities of similar topics. With the explosive number of self-formed communities in social networks, one important demand is to identify magnet communities for users. This can not only track attractive communities, but also help improve user experiences and increase their engagements, e.g., the login frequencies and user-generated-content qualities. In this paper, we initiate the study of magnet community identification problem. First we observe several properties of magnet communities, such as attention flow, attention qualify, and attention persistence. Second, we formalize these properties with the combination of community feature extraction into a graph ranking formulation based on constraint quadratic programming. In details, we treat communities of a network as super nodes, and their interactions as links among those super nodes. Therefore, a network of communities is defined. We extract community's magnet features from heterogeneous sources, i.e., a community's standalone features and its dependency features with other communities. A graph ranking model is formulated given these features. Furthermore, we define constraints reflecting communities' magnet properties to regularize the model. We demonstrate the effectiveness of our framework on real world social network data.",2012,Knowledge Discovery and Data Mining,Fields of study: social networkworld wide webdata miningmachine learningsimulation
Multimedia features for click prediction of new ads in display advertising,Haibin Cheng (Yahoo!)Roelof van Zwol (Yahoo!)Javad Azimi (Oregon State University)Eren Manavoglu (Yahoo!)Ruofei Zhang (Yahoo!)Yang Zhou (Yahoo!)Vidhya Navalpakkam (Yahoo!),"2653230632,281648438,2157691694,26108554,2162372842,2667044502,322804077","Non-guaranteed display advertising (NGD) is a multi-billion dollar business that has been growing rapidly in recent years. Advertisers in NGD sell a large portion of their ad campaigns using performance dependent pricing models such as cost-per-click (CPC) and cost-per-action (CPA). An accurate prediction of the probability that users click on ads is a crucial task in NGD advertising because this value is required to compute the expected revenue. State-of-the-art prediction algorithms rely heavily on historical information collected for advertisers, users and publishers. Click prediction of new ads in the system is a challenging task due to the lack of such historical data. The objective of this paper is to mitigate this problem by integrating multimedia features extracted from display ads into the click prediction models. Multimedia features can help us capture the attractiveness of the ads with similar contents or aesthetics. In this paper we evaluate the use of numerous multimedia features (in addition to commonly used user, advertiser and publisher features) for the purposes of improving click prediction in ads with no history. We provide analytical results generated over billions of samples and demonstrate that adding multimedia features can significantly improve the accuracy of click prediction for new ads, compared to a state-of-the-art baseline model.",2012,Knowledge Discovery and Data Mining,Fields of study: generalized method of momentsimagepredictive modellingfeature extractionmultimediaworld wide webdata miningmachine learningcomputer science
Parallel field ranking,Ming Ji (University of Illinois at Urbana–Champaign)Binbin Lin (Zhejiang University)Xiaofei He (Zhejiang University)Deng Cai (Zhejiang University)Jiawei Han (University of Illinois at Urbana–Champaign),"2060104901,2654162087,2157503142,2141500565,2121939561","Recently, ranking data with respect to the intrinsic geometric structure (manifold ranking) has received considerable attentions, with encouraging performance in many applications in pattern recognition, information retrieval and recommendation systems. Most of the existing manifold ranking methods focus on learning a ranking function that varies smoothly along the data manifold. However, beyond smoothness, a desirable ranking function should vary monotonically along the geodesics of the data manifold, such that the ranking order along the geodesics is preserved. In this article, we aim to learn a ranking function that varies linearly and therefore monotonically along the geodesics of the data manifold. Recent theoretical work shows that the gradient field of a linear function on the manifold has to be a parallel vector field. Therefore, we propose a novel ranking algorithm on the data manifolds, called Parallel Field Ranking. Specifically, we try to learn a ranking function and a vector field simultaneously. We require the vector field to be close to the gradient field of the ranking function, and the vector field to be as parallel as possible. Moreover, we require the value of the ranking function at the query point to be the highest, and then decrease linearly along the manifold. Experimental results on both synthetic data and real data demonstrate the effectiveness of our proposed algorithm.",2012,Knowledge Discovery and Data Mining,Fields of study: rankingranking svmrankingmanifoldsynthetic datavector fieldrecommender systempattern recognitionmachine learningmathematical optimizationstatisticscomputer sciencemathematics
"On ""one of the few"" objects",You Wu (Duke University)Pankaj K. Agarwal (Duke University)Chengkai Li (University of Texas at Arlington)Jun Yang (Duke University)Cong Yu (Google),"2225215397,2128565744,2145831560,2589753862,2687370976","Objects with multiple numeric attributes can be compared within any ""subspace"" (subset of attributes). In applications such as computational journalism, users are interested in claims of the form: Karl Malone is one of the only two players in NBA history with at least 25,000 points, 12,000 rebounds, and 5,000 assists in one's career . One challenge in identifying such ""one-of-the- k "" claims ( k = 2 above) is ensuring their ""interestingness"". A small k is not a good indicator for interestingness, as one can often make such claims for many objects by increasing the dimensionality of the subspace considered. We propose a uniqueness-based interestingness measure for one-of-the-few claims that is intuitive for non-technical users, and we design algorithms for finding all interesting claims (across all subspaces) from a dataset. Sometimes, users are interested primarily in the objects appearing in these claims. Building on our notion of interesting claims, we propose a scheme for ranking objects and an algorithm for computing the top-ranked objects. Using real-world datasets, we evaluate the efficiency of our algorithms as well as the advantage of our object-ranking scheme over popular methods such as Kemeny optimal rank aggregation and weighted-sum ranking.",2012,Knowledge Discovery and Data Mining,Fields of study: rankingtheoretical computer sciencediscrete mathematicsdata miningmachine learningstatisticsmathematics
Accelerated singular value thresholding for matrix completion,Yao Hu (Zhejiang University)Debing Zhang (Zhejiang University)Jun Liu (Siemens)Jieping Ye (Arizona State University)Xiaofei He (Zhejiang University),"2627173959,2097018285,2676596115,2305258894,2157503142","Recovering a large matrix from a small subset of its entries is a challenging problem arising in many real world applications, such as recommender system and image in-painting. These problems can be formulated as a general matrix completion problem. The Singular Value Thresholding (SVT) algorithm is a simple and efficient first-order matrix completion method to recover the missing values when the original data matrix is of low rank. SVT has been applied successfully in many applications. However, SVT is computationally expensive when the size of the data matrix is large, which significantly limits its applicability. In this paper, we propose an Accelerated Singular Value Thresholding (ASVT) algorithm which improves the convergence rate from O(1/N) for SVT to O(1/N 2 ), where N is the number of iterations during optimization. Specifically, the dual problem of the nuclear norm minimization problem is derived and an adaptive line search scheme is introduced to solve this dual problem. Consequently, the optimal solution of the primary problem can be readily obtained from that of the dual problem. We have conducted a series of experiments on a synthetic dataset, a distance matrix dataset and a large movie rating dataset. The experimental results have demonstrated the efficiency and effectiveness of the proposed algorithm.",2012,Knowledge Discovery and Data Mining,Fields of study: eight point algorithmline searchdistance matrixsingular valuedualityrate of convergencefirst order logicrecommender systemdiscrete mathematicscombinatoricsmathematical optimizationcomputer sciencemathematics
Detecting changes of clustering structures using normalized maximum likelihood coding,So Hirai (University of Tokyo)Kenji Yamanishi (University of Tokyo),"2103917120,2302798453","We are concerned with the issue of detecting changes of clustering structures from multivariate time series. From the viewpoint of the minimum description length(MDL) principle, we propose an algorithm that tracks changes of clustering structures so that the sum of the code-length for data and that for clustering changes is minimum. Here we employ a Gaussian mixture model(GMM) as representation of clustering, and compute the code-length for data sequences using the normalized maximum likelihood (NML) coding. The proposed algorithm enables us to deal with clustering dynamics including merging, splitting, emergence, disappearance of clusters from a unifying view of the MDL principle. We empirically demonstrate using artificial data sets that our proposed method is able to detect cluster changes significantly more accurately than an existing statistical-test based method and AIC/BIC-based methods. We further use real customers' transaction data sets to demonstrate the validity of our algorithm in market analysis. We show that it is able to detect changes of customer groups, which correspond to changes of real market environments.",2012,Knowledge Discovery and Data Mining,Fields of study: k medians clusteringcanopy clustering algorithmcorrelation clusteringconstrained clusteringdata stream clusteringcure data clustering algorithmtransaction datamarket analysisfuzzy clusteringminimum description lengthclustering high dimensional datamixture modelcluster analysisstatistical hypothesis testingdata miningpattern recognitionmachine learningstatisticscomputer sciencemathematics
Navigating information facets on twitter (NIF-T),Shamanth Kumar (Arizona State University)Fred Morstatter (Arizona State University)Grant Marshall (Arizona State University)Huan Liu (Arizona State University)Ullas Nambiar (IBM),"2166799513,107712532,2610970978,2122391114,1241982450","Recent years have seen an exponential increase in the number of users of social media sites. As the number of users of these sites continues to grow at an extraordinary rate, the amount of data produced follows in magnitude. With this deluge of social media data, the need for comprehensive tools to analyze user interactions is ever increasing. In this paper, we present a novel tool, Navigating Information Facets on Twitter (NIF-T), which helps users to explore data generated on social media sites. Using the three dimensions or facets: time, location, and topic as an example of the many possible facets, we enable the users to explore large social media datasets. With the help of a large corpus of tweets collected from the Occupy Wall Street movement on the Twitter platform we show how our system can be used to identify important aspects of the event along these facets.",2012,Knowledge Discovery and Data Mining,Fields of study: social media optimizationsocial mediathree dimensional spacevisual analyticsinternet privacymultimediaworld wide webcomputer science
Efficient and domain-invariant competitor mining,Theodoros Lappas (Boston University)George Valkanas (National and Kapodistrian University of Athens)Dimitrios Gunopulos (National and Kapodistrian University of Athens),"2028397797,2032326962,2712250546","In any competitive business, success is based on the ability to make an item more appealing to customers than the competition. A number of questions arise in the context of this task: how do we formalize and quantify the competitiveness relationship between two items? Who are the true competitors of a given item? What are the features of an item that most affect its competitiveness? Despite the impact and relevance of this problem to many domains, only a limited amount of work has been devoted toward an effective solution. In this paper, we present a formal definition of the competitiveness between two items. We present efficient methods for evaluating competitiveness in large datasets and address the natural problem of finding the top-k competitors of a given item. Our methodology is evaluated against strong baselines via a user study and experiments on multiple datasets from different domains.",2012,Knowledge Discovery and Data Mining,Fields of study: competitor analysisdata mining
Batch mode active sampling based on marginal probability distribution matching,"Rita Chattopadhyay (Arizona State University)Zheng Wang (Arizona State University)Wei Fan (IBM)Ian Davidson (University of California, Davis)Sethuraman Panchanathan (Arizona State University)Jieping Ye (Arizona State University)","2112113198,2701179173,2491264568,2560595684,2055909567,2305258894","Active Learning is a machine learning and data mining technique that selects the most informative samples for labeling and uses them as training data; it is especially useful when there are large amount of unlabeled data and labeling them is expensive. Recently, batch-mode active learning, where a set of samples are selected concurrently for labeling, based on their collective merit, has attracted a lot of attention. The objective of batch-mode active learning is to select a set of informative samples so that a classifier learned on these samples has good generalization performance on the unlabeled data. Most of the existing batch-mode active learning methodologies try to achieve this by selecting samples based on certain criteria. In this article we propose a novel criterion which achieves good generalization performance of a classifier by specifically selecting a set of query samples that minimize the difference in distribution between the labeled and the unlabeled data, after annotation. We explicitly measure this difference based on all candidate subsets of the unlabeled data and select the best subset. The proposed objective is an NP-hard integer programming optimization problem. We provide two optimization techniques to solve this problem. In the first one, the problem is transformed into a convex quadratic programming problem and in the second method the problem is transformed into a linear programming problem. Our empirical studies using publicly available UCI datasets and two biomedical image databases demonstrate the effectiveness of the proposed approach in comparison with the state-of-the-art batch-mode active learning methods. We also present two extensions of the proposed approach, which incorporate uncertainty of the predicted labels of the unlabeled data and transfer learning in the proposed formulation. In addition, we present a joint optimization framework for performing both transfer and active learning simultaneously unlike the existing approaches of learning in two separate stages, that is, typically, transfer learning followed by active learning. We specifically minimize a common objective of reducing distribution difference between the domain adapted source, the queried and labeled samples and the rest of the unlabeled target domain data. Our empirical studies on two biomedical image databases and on a publicly available 20 Newsgroups dataset show that incorporation of uncertainty information and transfer learning further improves the performance of the proposed active learning based classifier. Our empirical studies also show that the proposed transfer-active method based on the joint optimization framework performs significantly better than a framework which implements transfer and active learning in two separate stages.",2012,Knowledge Discovery and Data Mining,Fields of study: online machine learningstabilityinductive transfermulti task learningtransfer of learningmarginal distributionactive learninggeneralization errorempirical researchprobability distributionoptimization problemactive learningsemi supervised learninginstance based learningmedical imagingunsupervised learninglinear programmingdata sciencedata miningmachine learningstatisticscomputer sciencemathematics
Transductive multi-label ensemble classification for protein function prediction,Guoxian Yu (South China University of Technology)Carlotta Domeniconi (George Mason University)Huzefa Rangwala (George Mason University)Guoji Zhang (South China University of Technology)Zhiwen Yu (South China University of Technology),"2126069887,45678088,2096698710,2144418109,2163362956","Advances in biotechnology have made available multitudes of heterogeneous proteomic and genomic data. Integrating these heterogeneous data sources, to automatically infer the function of proteins, is a fundamental challenge in computational biology. Several approaches represent each data source with a kernel (similarity) function. The resulting kernels are then integrated to determine a composite kernel, which is used for developing a function prediction model. Proteins are also found to have multiple roles and functions. As such, several approaches cast the protein function prediction problem within a multi-label learning framework. In our work we develop an approach that takes advantage of several unlabeled proteins, along with multiple data sources and multiple functions of proteins. We develop a graph-based transductive multi-label classifier (TMC) that is evaluated on a composite kernel, and also propose a method for data integration using the ensemble framework, called transductive multi-label ensemble classifier (TMEC). The TMEC approach trains a graph-based multi-label classifier for each individual kernel, and then combines the predictions of the individual models. Our contribution is the use of a bi-relational directed graph that captures relationships between pairs of proteins, between pairs of functions, and between proteins and functions. We evaluate the ability of TMC and TMEC to predict the functions of proteins by using two yeast datasets. We show that our approach performs better than recently proposed protein function prediction methods on composite and multiple kernels.",2012,Knowledge Discovery and Data Mining,Fields of study: protein function predictiondata miningpattern recognitionmachine learningcomputer sciencemathematics
Community discovery and profiling with social messages,Wenjun Zhou (University of Tennessee)Hongxia Jin (IBM)Yan Liu (University of Southern California),"2097769770,2104742879,2240541904","Discovering communities from social media and collaboration systems has been of great interest in recent years. Existing work show prospects of modeling contents and social links, aiming at discovering social communities, whose definition varies by application. We believe that a community depends not only on the group of people who actively participate, but also the topics they communicate about or collaborate on. This is especially true for workplace email communications. Within an organization, it is not uncommon that employees multifunction, and groups of employees collaborate on multiple projects at the same time. In this paper, we aim to automatically discovering and profiling users' communities by taking into account both the contacts and the topics. More specifically, we propose a community profiling model called COCOMP, where the communities labels are latent, and each social document corresponds to an information sharing activity among the most probable community members regarding the most relevant community issues. Experiment results on several social communication datasets, including emails and Twitter messages, demonstrate that the model can discover users' communities effectively, and provide concrete semantics.",2012,Knowledge Discovery and Data Mining,Fields of study: social mediacollaborationknowledge managementworld wide webdata miningcomputer science
Mining heterogeneous information networks: the next frontier,Jaiwei Han (University of Illinois at Urbana–Champaign),2652120071,"Real world physical and abstract data objects are interconnected, forming gigantic, interconnected networks. By structuring these data objects into multiple types, such networks become semi-structured heterogeneous information networks . Most real world applications that handle big data, including interconnected social media and social networks, scientific, engineering, or medical information systems, online e-commerce systems, and most database systems, can be structured into heterogeneous information networks. For example, in a medical care network, objects of multiple types, such as patients, doctors, diseases, medication, and links such as visits, diagnosis, and treatments are intertwined together, providing rich information and forming heterogeneous information networks. Effective analysis of large-scale heterogeneous information networks poses an interesting but critical challenge. In this talk, we present a set of data mining scenarios in heterogeneous information networks and show that mining heterogeneous information networks is a new and promising research frontier in data mining research. Departing from many existing network models that view data as homogeneous graphs or networks, the semi-structured heterogeneous information network model leverages the rich semantics of typed nodes and links in a network and can uncover surprisingly rich knowledge from interconnected data. This heterogeneous network modeling will lead to the discovery of a set of new principles and methodologies for mining interconnected data. The examples to be used in this discussion include (1) meta path-based similarity search, (2) rank-based clustering, (3) rank-based classification, (4) meta path-based link/relationship prediction, (5) relation strength-aware mining, as well as a few other recent developments. We will also point out some promising research directions and provide convincing arguments on that mining heterogeneous information networks is the next frontier in data mining.",2012,Knowledge Discovery and Data Mining,Fields of study: evolving networkssocial mediaheterogeneous networkinformation integrationnetwork modelsocial networknetwork effectdata sciencedata miningdatabasemachine learningcomputer science
Social sampling,Anirban Dasgupta (Yahoo!)Ravi Kumar (Yahoo!)D. Sivakumar (Yahoo!),"2138105752,2232709231,2558452699","We investigate a class of methods that we call ""social sampling,"" where participants in a poll respond with a summary of their friends' putative responses to the poll. Social sampling leads to a novel trade-off question: the savings in the number of samples(roughly the average degree of the network of participants) vs. the systematic bias in the poll due to the network structure. We provide precise analyses of estimators that result from this idea. With non-uniform sampling of nodes and non-uniform weighting of neighbors' responses, we devise an ideal unbiased estimator. We show that the variance of this estimator is controlled by the second eigenvalue of the normalized Laplacian of the network (the network structure penalty) and the correlation between node degrees and the property being measured (the effective savings factor). In addition, we present a sequence of approximate estimators that are simpler or more realistic or both, and analyze their performance. Experiments on large real-world networks show that social sampling is a powerful paradigm in obtaining accurate estimates with very few samples. At the same time, our results urge caution in interpreting recent results about ""expectation vs. intent polling"".",2012,Knowledge Discovery and Data Mining,Fields of study: pollingsocial networkeconometricsdata miningmachine learningstatisticscomputer sciencemathematics
Linear space direct pattern sampling using coupling from the past,Mario Boley (Fraunhofer Society)Sandy Moens (University of Antwerp)Thomas Gärtner (Fraunhofer Society),"2074964178,1982900222,1891115510","This paper shows how coupling from the past (CFTP) can be used to avoid time and memory bottlenecks in direct local pattern sampling procedures. Such procedures draw controlled amounts of suitably biased samples directly from the pattern space of a given dataset in polynomial time. Previous direct pattern sampling methods can produce patterns in rapid succession after some initial preprocessing phase. This preprocessing phase, however, turns out to be prohibitive in terms of time and memory for many datasets. We show how CFTP can be used to avoid any super-linear preprocessing and memory requirements. This allows to simulate more complex distributions, which previously were intractable. We show for a large number of public real-world datasets that these new algorithms are fast to execute and their pattern collections outperform previous approaches both in unsupervised as well as supervised contexts.",2012,Knowledge Discovery and Data Mining,Fields of study: samplingdata miningmachine learningstatisticscomputer science
Characterizing large-scale population's indoor spatio-temporal interactive behaviors,Yi-Qing Zhang (Fudan University)Xiang Li (Fudan University),"2679861097,2658581426","Human activity behaviors in urban areas mostly occur in interior places, such as department stores, office buildings, and museums. Understanding and characterizing human spatio-temporal interactive behaviors in these indoor areas can help us evaluate the efficiency of social contacts, monitor the frequently asymptomatic diseases transmissions, and design better internal structures of buildings. In this paper, we propose a new temporal quantity: 'Participation Activity Potential' (P PA ) to feature the critical roles of individuals in the populations instead of their degrees in the corresponding complex networks. Especially for the people with high degrees (hubs in the network), Participation Activity Potential which is directly from the statistics of their daily interactions, can easily feature the rank of their degree centrality and achieve as high as 100% accuracy rating without building the corresponding networks by high-complexity algorithms. The effectiveness and efficiency of our new defined quantity is validated in all three empirical data sets collected from a Chinese university campus by the WiFi technology, a small conference and an exhibitions by the RFID technology.",2012,Knowledge Discovery and Data Mining,Fields of study: complex networkhuman dynamicssimulationcomputer science
Learning in non-stationary environments with class imbalance,Thomas Ryan Hoens (University of Notre Dame)Nitesh V. Chawla (University of Notre Dame),"2229603609,1979796846","Learning in non-stationary environments is an increasingly important problem in a wide variety of real-world applications. In non-stationary environments data arrives incrementally, however the underlying generating function may change over time. In addition to the environments being non-stationary, they also often exhibit class imbalance. That is one class (the majority class) vastly outnumbers the other class (the minority class). This combination of class imbalance with non-stationary environments poses significant and interesting practical problems for classification. To overcome these issues, we introduce a novel instance selection mechanism, as well as provide a modification to the Heuristic Updatable Weighted Random Subspaces (HUWRS) method for the class imbalance problem. We then compare our modifications of HUWRS (called HUWRS.IP) to other state of the art algorithms, concluding that HUWRS. IP often achieves vastly superior performance.",2012,Knowledge Discovery and Data Mining,Fields of study: concept driftgenerating functionartificial intelligencemachine learningsimulationcomputer sciencemathematics
Panel on Mining the Big Data,"Michael I. Jordan (University of California, Berkeley)Christos Faloutsos (Carnegie Mellon University)Wen Gao (Peking University)Jiawei Han (University of Illinois at Urbana–Champaign)Zijian Zheng (Microsoft)Usuama Fayyad","2435751034,2198983026,2121069363,2121939561,2096053174,2229606405",-,2012,Knowledge Discovery and Data Mining,-
Multi-source learning for joint analysis of incomplete multi-modality neuroimaging data,"Lei Yuan (Arizona State University)Yalin Wang (Arizona State University)Paul M. Thompson (University of California, Los Angeles)Vaibhav A. Narayan (Johnson & Johnson)Jieping Ye (Arizona State University)","2243895322,2607855732,2114138198,2668609491,2305258894","Incomplete data present serious problems when integrating large-scale brain imaging data sets from different imaging modalities. In the Alzheimer's Disease Neuroimaging Initiative (ADNI), for example, over half of the subjects lack cerebrospinal fluid (CSF) measurements; an independent half of the subjects do not have fluorodeoxyglucose positron emission tomography (FDG-PET) scans; many lack proteomics measurements. Traditionally, subjects with missing measures are discarded, resulting in a severe loss of available information. We address this problem by proposing two novel learning methods where all the samples (with at least one available data source) can be used. In the first method, we divide our samples according to the availability of data sources, and we learn shared sets of features with state-of-the-art sparse learning methods. Our second method learns a base classifier for each data source independently, based on which we represent each source using a single column of prediction scores; we then estimate the missing prediction scores, which, combined with the existing prediction scores, are used to build a multi-source fusion model. To illustrate the proposed approaches, we classify patients from the ADNI study into groups with Alzheimer's disease (AD), mild cognitive impairment (MCI) and normal controls, based on the multi-modality data. At baseline, ADNI's 780 participants (172 AD, 397 MCI, 211 Normal), have at least one of four data types: magnetic resonance imaging (MRI), FDG-PET, CSF and proteomics. These data are used to test our algorithms. Comprehensive experiments show that our proposed methods yield stable and promising results.",2012,Knowledge Discovery and Data Mining,Fields of study: multi task learningdata typeneuroimagingmagnetic resonance imagingdata miningartificial intelligencemachine learningcomputer science
Efficient event pattern matching with match windows,Bruno Cadonna (Free University of Bozen-Bolzano)Johann Gamper (Free University of Bozen-Bolzano)Michael H. Böhlen (University of Zurich),"222821311,1786981052,2300834035","In event pattern matching a sequence of input events is matched against a complex query pattern that specifies constraints on extent, order, values, and quantification of matching events. In this paper we propose a general pattern matching strategy that consists of a pre-processing step and a pattern matching step. Instead of eagerly matching incoming events, the pre-processing step buffers events in a match window to apply different pruning techniques (filtering, partitioning, and testing for necessary match conditions). In the second step, an event pattern matching algorithm, A , is called only for match windows that satisfy the necessary match conditions. This two-phase strategy with a lazy call of the matching algorithm significantly reduces the number of events that need to be processed by A as well as the number of calls to A . This is important since pattern matching algorithms tend to be expensive in terms of runtime and memory complexity, whereas the pre-processing can be done very efficiently. We conduct extensive experiments using real-world data with pattern matching algorithms for, respectively, automata and join trees. The experimental results confirm the effectiveness of our strategy for both types of pattern matching algorithms.",2012,Knowledge Discovery and Data Mining,Fields of study: state patternoptimal matchingapproximate string matchingtemplate matchingpattern matchingsatisfiabilitypattern recognitionmachine learningalgorithmcomputer sciencemathematics
Towards fine-grained urban traffic knowledge extraction using mobile sensing,Xuegang (Jeff) Ban (Center for Excellence in Education)Marco Gruteser (Rutgers University),"2129636270,593974664","We introduce our vision for mining fine-grained urban traffic knowledge from mobile sensing, especially GPS location traces. Beyond characterizing human mobility patterns and measuring traffic congestion, we show how mobile sensing can also reveal details such as intersection performance statistics that are useful for optimizing the timing of a traffic signal. Realizing such applications requires co-designing privacy protection algorithms and novel traffic modeling techniques so that the needs for privacy preserving and traffic modeling can be simultaneously satisfied. We explore privacy algorithms based on the virtual trip lines (VTL) concept to regulate where and when the mobile data should be collected. The traffic modeling techniques feature an integration of traffic principles and learning/optimization techniques. The proposed methods are illustrated using two case studies for extracting traffic knowledge for urban signalized intersection.",2012,Knowledge Discovery and Data Mining,Fields of study: floating car datathree phase traffic theorytraffic generation modeltraffic shapingsatisfiabilityknowledge extractioninternet privacycomputer securitydata miningcomputer science
Semi-supervised learning with mixed knowledge information,Fanhua Shang (Xidian University)Licheng Jiao (Xidian University)Fei Wang (IBM),"1972696148,2166558591,2465953593","Integrating new knowledge sources into various learning tasks to improve their performance has recently become an interesting topic. In this paper we propose a novel semi-supervised learning (SSL) approach, called semi-supervised learning with Mixed Knowledge Information (SSL-MKI) which can simultaneously handle both sparse labeled data and additional pairwise constraints together with unlabeled data. Specifically, we first construct a unified SSL framework to combine the manifold assumption and the pairwise constraints assumption for classification tasks. Then we present a Modified Fixed Point Continuation (MFPC) algorithm with an eigenvalue thresholding (EVT) operator to learn the enhanced kernel matrix. Finally, we develop a two-stage optimization strategy and provide an efficient SSL approach that takes advantage of Laplacian spectral regularization: semi-supervised learning with Enhanced Spectral Kernel (ESK). Experimental results on a variety of synthetic and real-world datasets demonstrate the effectiveness of the proposed ESK approach.",2012,Knowledge Discovery and Data Mining,Fields of study: laplacian matrixfixed pointeigenvalues and eigenvectorssemi supervised learningunsupervised learningpattern recognitionmachine learningmathematical optimizationmathematics
Efficient frequent item counting in multi-core hardware,Pratanu Roy (ETH Zurich)Jens Teubner (ETH Zurich)Gustavo Alonso (ETH Zurich),"2127437822,2077338896,2122271048","The increasing number of cores and the rich instruction sets of modern hardware are opening up new opportunities for optimizing many traditional data mining tasks. In this paper we demonstrate how to speed up the performance of the computation of frequent items by almost one order of magnitude over the best published results by matching the algorithm to the underlying hardware architecture. We start with the observation that frequent item counting, like other data mining tasks, assumes certain amount of skew in the data. We exploit this skew to design a new algorithm that uses a pre-filtering stage that can be implemented in a highly efficient manner through SIMD instructions. Using pipelining, we then combine this pre-filtering stage with a conventional frequent item algorithm ( Space-Saving ) that will process the remainder of the data. The resulting operator can be parallelized with a small number of cores, leading to a parallel implementation that does not suffer any of the overheads of existing parallel solutions when querying the results and offers significantly higher throughput.",2012,Knowledge Discovery and Data Mining,Fields of study: data flow diagramhardware architecturetheoretical computer scienceparallel computingdata miningreal time computingcomputer science
Scalable misbehavior detection in online video chat services,Xinyu Xing (Georgia Institute of Technology)Yu-li Liang (University of Colorado Boulder)Sui Huang (Ohio State University)Hanqiang Cheng (McGill University)Richard Han (University of Colorado Boulder)Qin Lv (University of Colorado Boulder)Xue Liu (McGill University)Shivakant Mishra (University of Colorado Boulder)Yi Zhu (Tsinghua University),"2282059511,2132264088,2102655769,2231295513,2128661168,2154485217,2229430579,2110219690,2649413189","The need for highly scalable and accurate detection and filtering of misbehaving users and obscene content in online video chat services has grown as the popularity of these services has exploded in popularity. This is a challenging problem because processing large amounts of video is compute intensive, decisions about whether a user is misbehaving or not must be made online and quickly, and moreover these video chats are characterized by low quality video, poorly lit scenes, diversity of users and their behaviors, diversity of the content, and typically short sessions. This paper presents EMeralD, a highly scalable system for accurately detecting and filtering misbehaving users in online video chat applications. EMeralD substantially improves upon the state-of-the-art filtering mechanisms by achieving much lower computational cost and higher accuracy. We demonstrate EMeralD's improvement via experimental evaluations on real-world data sets obtained from Chatroulette.com.",2012,Knowledge Discovery and Data Mining,Fields of study: video trackinginternet privacymultimediaworld wide webcomputer science
RecMax: exploiting recommender systems for fun and profit,Amit Goyal (University of British Columbia)Laks V.S. Lakshmanan (University of British Columbia),"2675938765,2659892814","In recent times, collaborative filtering based Recommender Systems (RS) have become extremely popular. While research in recommender systems has mostly focused on improving the accuracy of recommendations, in this paper, we look at the ""flip"" side of a RS. That is, instead of improving existing recommender algorithms, we ask whether we can use an existing operational RS to launch a targeted marketing campaign. To this end, we propose a novel problem called RecMax that aims to select a set of ""seed"" users for a marketing campaign for a new product, such that if they endorse the product by providing relatively high ratings, the number of other users to whom the product is recommended by the underlying RS algorithm is maximum. We motivate RecMax with real world applications. We show that seeding can make a substantial difference, if done carefully. We prove that RecMax is not only NP-hard to solve optimally, it is NP-hard to even approximate within any reasonable factor. Given this hardness, we explore several natural heuristics on 3 real world datasets - Movielens, Yahoo! Music and Jester Joke and report our findings. We show that even though RecMax is hard to approximate, simple natural heuristics may provide impressive gains, for targeted marketing using RS.",2012,Knowledge Discovery and Data Mining,Fields of study: collaborative filteringprofitability indexrecommender systemdata miningmachine learningcomputer science
Smarter outlier detection and deeper understanding of large-scale taxi trip records: a case study of NYC,Jianting Zhang (City College of New York),2269847788,"Outlier detection in large-scale taxi trip records has imposed significant technical challenges due to huge data volumes and complex semantics. In this paper, we report our preliminary work on detecting outliers from 166 millions taxi trips in the New York City (NYC) in 2009 through efficient spatial analysis and network analysis using a NAVTEQ street network with half a million edges. As a byproduct of large-scale shortest path computation in outlier detection, betweenness centralities of street network edges are computed and mapped. The techniques can be used to help better understand the connection strengths among different parts of NYC using the large-scale taxi trip records.",2012,Knowledge Discovery and Data Mining,Fields of study: betweenness centralityshortest path problemspatial analysisnetwork analysisanomaly detectiondata sciencedata miningstatisticscomputer science
Delta-SimRank computing on MapReduce,Liangliang Cao (IBM)Brian Cho (University of Illinois at Urbana–Champaign)Hyun Duk Kim (University of Illinois at Urbana–Champaign)Zhen Li (University of Illinois at Urbana–Champaign)Min-Hsuan Tsai (University of Illinois at Urbana–Champaign)Indranil Gupta (University of Illinois at Urbana–Champaign),"2165330392,2127336427,2148203772,2306064866,2227891643,2127422067","Based on the intuition that ""two objects are similar if they are related to similar objects"", SimRank (proposed by Jeh and Widom in 2002) has become a famous measure to compare the similarity between two nodes using network structure. Although SimRank is applicable to a wide range of areas such as social networks, citation networks, link prediction, etc., it suffers from heavy computational complexity and space requirements. Most existing efforts to accelerate SimRank computation work only for static graphs and on single machines. This paper considers the problem of computing SimRank efficiently in a distributed system while handling dynamic networks which grow with time. We first consider an abstract model called Harmonic Field on Node-pair Graph. We use this model to derive SimRank and the proposed Delta-SimRank, which is demonstrated to fit the nature of distributed computing and can be efficiently implemented using Google's MapReduce paradigm. Delta-SimRank can effectively reduce the computational cost and can also benefit the applications with non-static network structures. Our experimental results on four real world networks show that Delta-SimRank is much more efficient than the distributed SimRank algorithm, and leads to up to 30 times speed-up in the best case1.",2012,Knowledge Discovery and Data Mining,Fields of study: social networkcomputational complexity theorytheoretical computer sciencedistributed computingdata miningmachine learningcomputer science
Following the electrons: methods for power management in commercial buildings,Gowtham Bellala (HP Labs)Manish Marwah (HP Labs)Martin F. Arlitt (HP Labs)Geoff Lyon (HP Labs)Cullen Bash (HP Labs),"2250187273,1950715462,2168209810,2598698548,1004593497","Commercial buildings are significant consumers of electricity. The first step towards better energy management in commercial buildings is monitoring consumption. However, instrumenting every electrical panel in a large commercial building is expensive and wasteful. In this paper, we propose a greedy meter (sensor) placement algorithm based on maximization of information gained, subject to a cost constraint. The algorithm provides a near-optimal solution guarantee. Furthermore, to identify power saving opportunities, we use an unsupervised anomaly detection technique based on a low-dimensional embedding. Further, to better manage resources such as lighting and HVAC, we propose a semi-supervised approach combining hidden Markov models (HMM) and a standard classifier to model occupancy based on readily available port-level network statistics.",2012,Knowledge Discovery and Data Mining,Fields of study: energy managementkullback leibler divergenceanomaly detectionhidden markov modelreal time computingmachine learningsimulationcomputer science
Proceedings of the ACM SIGKDD Workshop on Mining Data Semantics,Ying Ding (Indiana University)Jiawei Han (University of Illinois at Urbana–Champaign)Jie Tang (Tsinghua University)Philip S. Yu (University of Illinois at Chicago),"2161065209,2121939561,2158012360,2125104194",-,2012,Knowledge Discovery and Data Mining,Fields of study: information retrieval
Mining discriminative components with low-rank and sparsity constraints for face recognition,Qiang Zhang (Arizona State University)Baoxin Li (Arizona State University),"2611141307,2128754310","This paper introduces a novel image decomposition approach for an ensemble of correlated images, using low-rank and sparsity constraints. Each image is decomposed as a combination of three components: one common component, one condition component, which is assumed to be a low-rank matrix, and a sparse residual. For a set of face images of N subjects, the decomposition finds N common components, one for each subject, K low-rank components, each capturing a different global condition of the set (e.g., different illumination conditions), and a sparse residual for each input image. Through this decomposition, the proposed approach recovers a clean face image (the common component) for each subject and discovers the conditions (the condition components and the sparse residuals) of the images in the set. The set of N + K images containing only the common and the low-rank components form a compact and discriminative representation for the original images. We design a classifier using only these N + K images. Experiments on commonly-used face data sets demonstrate the effectiveness of the approach for face recognition through comparing with the leading state-of-the-art in the literature. The experiments further show good accuracy in classifying the condition of an input image, suggesting that the components from the proposed decomposition indeed capture physically meaningful features of the input.",2012,Knowledge Discovery and Data Mining,Fields of study: low rank approximationsparse matrixfacial recognition systemcomputer visionpattern recognitionmachine learningcomputer sciencemathematics
Learning binary codes for collaborative filtering,Ke Zhou (Georgia Institute of Technology)Hongyuan Zha (Georgia Institute of Technology),"2311638158,2099091510","This paper tackles the efficiency problem of making recommendations in the context of large user and item spaces. In particular, we address the problem of learning binary codes for collaborative filtering, which enables us to efficiently make recommendations with time complexity that is independent of the total number of items. We propose to construct binary codes for users and items such that the preference of users over items can be accurately preserved by the Hamming distance between their respective binary codes. By using two loss functions measuring the degree of divergence between the training and predicted ratings, we formulate the problem of learning binary codes as a discrete optimization problem. Although this optimization problem is intractable in general, we develop effective relaxations that can be efficiently solved by existing methods. Moreover, we investigate two methods to obtain the binary codes from the relaxed solutions. Evaluations are conducted on three public-domain data sets and the results suggest that our proposed method outperforms several baseline alternatives.",2012,Knowledge Discovery and Data Mining,Fields of study: block codeoptimization problemrecommender systemtheoretical computer sciencedata miningmachine learningmathematical optimizationstatisticscomputer sciencemathematics
GeoSearch: georeferenced video retrieval system,Youngwoo Kim (Pohang University of Science and Technology)Jinha Kim (Pohang University of Science and Technology)Hwanjo Yu (Pohang University of Science and Technology),"2428366802,2603150764,2257366035","Conventional video search systems, to find relevant videos, rely on textual data such as video titles, annotations, and text around the video. Nowadays, video recording devices such as ameras, smartphones and car blackboxes are equipped with GPS sensors and able to capture videos with spatiotemporal information such as time, location and camera direction. We call such videos georeferenced videos . This paper presents a georeferenced video retrieval system, geosearch , which efficiently retrieves videos containing a certain point or range in the map. To enable a fast search of georeferenced videos, geosearch adopts a novel data structure MBTR (Minimum Bounding Tilted Rectangle) in the leaf nodes of R-Tree. New algorithms are developed to build MBTRs from georeferenced videos and to efficiently process point and range queries on MB T Rs. We demonstrate our system on real georeferenced videos, and show that, compared to previous methods, geosearch substantially reduces the index size and also improves the search speed for georeferenced video data. Our online demo is available at ""http://dm.hwanjoyu.org/geosearch"".",2012,Knowledge Discovery and Data Mining,Fields of study: georeferencerange queryvideo trackingspatial databasedata structureworld wide webinformation retrievalcomputer visioncomputer science
Finding trending local topics in search queries for personalization of a recommendation system,Ziad Al Bawab (Yahoo!)George H. Mills (Yahoo!)Jean-Francois Crespo (Google),"2305737265,2686000852,2229874075","In this paper, we present our approach for geographic personalization of a content recommendation system. More specifically, our work focuses on recommending query topics to users. We do this by mining the search query logs to detect trending local topics. For a set of queries we compute their counts and what we call buzz scores, which is a metric for detecting trending behavior. We also compute the entropy of the geographic distribution of the queries as means of detecting their location affinity. We cluster the queries into trending topics and assign the topics to their corresponding location. Human editors then select a subset of these local topics and enter them into a recommendation system. In turn the recommendation system optimizes a pool of trending local and global topics by exploiting user feedback. We present some editorial evaluation of the technique and results of a live experiment. Inclusion of local topics in selected locations into the global pool of topics resulted in more than 6% relative increase in user engagement with the recommendation system compared to using the global topics exclusively.",2012,Knowledge Discovery and Data Mining,Fields of study: personalizationcluster analysisrecommender systemworld wide webinformation retrievaldata miningmachine learningcomputer science
City-scale traffic simulation from digital footprints,Gavin McArdle (Maynooth University)Aonghus Lawlor (Maynooth University)Eoghan Furey (National University of Ireland)Alexei Pozdnoukhov (Maynooth University),"1986668592,2097544373,2231665373,2043078777","This paper introduces a micro-simulation of urban traffic flows within a large scale scenario implemented for the Greater Dublin region in Ireland. Traditionally, the data available for traffic simulations come from a population census and dedicated road surveys which only partly cover shopping, leisure or recreational trips. To account for the latter, the presented traffic modelling framework exploits the digital footprints of city inhabitants on services such as Twitter and Foursquare. We enriched the model with findings from our previous studies on geographical layout of communities in a country-wide mobile phone network to account for socially related journeys. These datasets were used to calibrate a variant of a radiation model of spatial choice, which we introduced in order to drive individuals' decisions on trip destinations within an assigned daily activity plan. We observed that given the distribution of population, the workplace locations, a comprehensive set of urban facilities and a list of typical activity sequences of city dwellers collected within a national road survey, the developed micro-simulation reproduces not only the journey statistics but also the traffic volumes at main road segments with surprising accuracy.",2012,Knowledge Discovery and Data Mining,Fields of study: traffic flowsocial networksocial relationsimulation
User oriented trajectory similarity search,Haibo Wang (University of Queensland)Kuien Liu (Chinese Academy of Sciences),"2715980991,2159549209","Trajectory similarity search studies the problem of finding a trajectory from the database such the found trajectory most similar to the query trajectory. Past research mainly focused on two aspects: shape similarity search and semantic similarity search, leaving personalized similarity search untouched. In this paper, we propose a new query which takes user's preference into consideration to provide personalized searching. We define a new data model for this query and identify the efficiency issue as the key challenge: given a user specified trajectory, how to efficiently retrieve the most similar trajectory from the database. By taking advantage of the spatial localities, we develop a two-phase algorithm to tame this challenge. Two optimized strategies are also developed to speed up the query process. Both the theoretical analysis and the experiments demonstrate the high efficiency of the proposed method.",2012,Knowledge Discovery and Data Mining,Fields of study: web search querysemantic similaritykernel density estimationdata modelcomputational sociologyinformation retrievaldata miningdatabasestatisticscomputer science
Multi-view clustering using mixture models in subspace projections,Stephan Günnemann (RWTH Aachen University)Ines Färber (RWTH Aachen University)Thomas Seidl (RWTH Aachen University),"316694267,2015027442,2140301036","Detecting multiple clustering solutions is an emerging research field. While data is often multi-faceted in its very nature, traditional clustering methods are restricted to find just a single grouping. To overcome this limitation, methods aiming at the detection of alternative and multiple clustering solutions have been proposed. In this work, we present a Bayesian framework to tackle the problem of multi-view clustering. We provide multiple generalizations of the data by using multiple mixture models . Each mixture describes a specific view on the data by using a mixture of Beta distributions in subspace projections . Since a mixture summarizes the clusters located in similar subspace projections, each view highlights specific aspects of the data. In addition, our model handles overlapping views , where the mixture components compete against each other in the data generation process. For efficiently learning the distributions, we propose the algorithm MVGen that exploits the ICM principle and uses Bayesian model selection to trade-off the cluster model's complexity against its goodness of fit. With experiments on various real-world data sets, we demonstrate the high potential of MVGen to detect multiple, overlapping clustering views in subspace projections of the data.",2012,Knowledge Discovery and Data Mining,Fields of study: k medians clusteringbrown clusteringcanopy clustering algorithmdetermining the number of clusters in a data setcorrelation clusteringconstrained clusteringdata stream clusteringcure data clustering algorithmgenerative modelbeta distributionfuzzy clusteringclustering high dimensional datagraphical modelmixture modelgoodness of fitcluster analysisconceptual clusteringdata miningpattern recognitionmachine learningstatisticscomputer sciencemathematics
Testing the significance of spatio-temporal teleconnection patterns,Jaya Kawale (University of Minnesota)Snigdhansu Chatterjee (University of Minnesota)Dominick Ormsby (University of Minnesota)Karsten Steinhaeuser (University of Minnesota)Stefan Liess (University of Minnesota)Vipin Kumar (University of Minnesota),"2091655785,2125500003,2228508263,258494835,146640362,2161062602","Dipoles represent long distance connections between the pressure anomalies of two distant regions that are negatively correlated with each other. Such dipoles have proven important for understanding and explaining the variability in climate in many regions of the world, e.g., the El Nino climate phenomenon is known to be responsible for precipitation and temperature anomalies over large parts of the world. Systematic approaches for dipole detection generate a large number of candidate dipoles, but there exists no method to evaluate the significance of the candidate teleconnections. In this paper, we present a novel method for testing the statistical significance of the class of spatio-temporal teleconnection patterns called as dipoles. One of the most important challenges in addressing significance testing in a spatio-temporal context is how to address the spatial and temporal dependencies that show up as high autocorrelation. We present a novel approach that uses the wild bootstrap to capture the spatio-temporal dependencies, in the special use case of teleconnections in climate data. Our approach to find the statistical significance takes into account the autocorrelation, the seasonality and the trend in the time series over a period of time. This framework is applicable to other problems in spatio-temporal data mining to assess the significance of the patterns.",2012,Knowledge Discovery and Data Mining,Fields of study: use caseel ninoseasonalitytime seriesstatistical significanceeconometricsstatistics
Dark Web portal overlapping community detection based on topic models,Sebastián A. Ríos (University of Chile)Ricardo Muñoz (University of Chile),"2115760643,2166446117","A hot research topic is the study and monitoring of online communities. Of course, homeland security institutions from many countries are using data mining techniques to perform this task, aiming to anticipate and avoid a possible menace to local peace. Tools such as social networks analysis and text mining have contributed to the understanding of these kinds of groups in order to develop counter-terrorism applications. A key application is the discovery of sub-communities of interests which main topic could be a possible homeland security threat. However, most algorithms detect disjoint communities, which means that every community member belongs to a single community. Thus, final conclusions can be omitting valuable information which leads to wrong results interpretations. In this paper, we propose a novel approach to combine traditional network analysis methods for overlapping community detection with topic-model based text mining techniques. Afterwards, we developed a sub-community detection algorithm that allow each member belong more than one sub-community. Experiments were performed using an English language based forum available in the Dark Web portal (islamicAwakening).",2012,Knowledge Discovery and Data Mining,Fields of study: homeland securitylatent dirichlet allocationsocial network analysisnetwork analysisenglishtext mininginternet privacyworld wide webdata miningmachine learningcomputer science
Transparent user models for personalization,Khalid El-Arini (Carnegie Mellon University)Ulrich Paquet (Microsoft)Ralf Herbrich (Facebook)Jurgen Van Gael (Microsoft)Blaise Agüera y Arcas (Microsoft),"1452243399,1773006169,2693067968,74155146,2231011657","Personalization is a ubiquitous phenomenon in our daily online experience. While such technology is critical for helping us combat the overload of information we face, in many cases, we may not even realize that our results are being tailored to our personal tastes and preferences. Worse yet, when such a system makes a mistake, we have little recourse to correct it. In this work, we propose a framework for addressing this problem by developing a new user-interpretable feature set upon which to base personalized recommendations. These features, which we call badges, represent fundamental traits of users (e.g., ""vegetarian"" or ""Apple fanboy"") inferred by modeling the interplay between a user's behavior and self-reported identity. Specifically, we consider the microblogging site Twitter, where users provide short descriptions of themselves in their profiles, as well as perform actions such as tweeting and retweeting. Our approach is based on the insight that we can define badges using high precision, low recall rules (e.g., ""Twitter profile contains the phrase 'Apple fanboy'""), and with enough data, generalize to other users by observing shared behavior. We develop a fully Bayesian, generative model that describes this interaction, while allowing us to avoid the pitfalls associated with having positive-only data. Experiments on real Twitter data demonstrate the effectiveness of our model at capturing rich and interpretable user traits that can be used to provide transparency for personalization.",2012,Knowledge Discovery and Data Mining,Fields of study: graphical modelmultimediaworld wide webdata miningmachine learningcomputer science
Learning approach for domain-independent linked data instance matching,Khai Nguyen (Graduate University for Advanced Studies)Ryutaro Ichise (National Institute of Informatics)Hoai-Bac Le,"2493390968,23099875,2536546855","Because almost all linked data sources are currently published by different providers, interlinking homogeneous instances of these sources is an important problem in data integration. Recently, instance matching has been used to identify owl: sameAs links between linked datasets. Previous approaches primarily use predefined maps of corresponding attributes and most of them are limited to matching in specific domains. In this paper, we propose the LFM, a learning-based instant matching system, which is designed for achieving a reliable domain-independent matcher. First, we compute the similarity vectors between labeled pairs of instances without specifying the meaning of the RDF predicates. Then a learning process is applied to learn a tree classifier for predicting whether the new pairs of instances are identical. Experiments demonstrate that our method achieves a 4% improvement in precision and recall against recent top-ranked matchers, if we use a small amount of labeled data for learning.",2012,Knowledge Discovery and Data Mining,Fields of study: name resolutionlinked datadata integritysemi supervised learninginstance based learningsupervised learningdata miningpattern recognitionmachine learningcomputer science
TourViz: interactive visualization of connection pathways in large graphs,Duen Horng Chau (Carnegie Mellon University)Leman Akoglu (Carnegie Mellon University)Jilles Vreeken (University of Antwerp)Hanghang Tong (IBM)Christos Faloutsos (Carnegie Mellon University),"2024561599,2288278917,1971070670,2224718883,2198983026","We present TourViz, a system that helps its users to interactively visualize and make sense in large network datasets. In particular, it takes as input a set of nodes the user specifies as of interest and presents the user with a visualization of connection subgraphs around these input nodes. Each connection subgraph contains good pathways that highlight succinct connections among a ""close-by"" group of input nodes. TourViz combines visualization with rich user interaction to engage and help the user to further understand the relations among the nodes of interest,by exploring their neighborhood on demand as well as modifying the set of interest nodes. We demonstrate TourViz's usage and benefits using the DBLP graph, consisting of authors and their co-authorship relations, while our system is designed generally to work with any kind of graph data. We will invite the audience to experiment with our system and comment on its usability, usefulness, and how our system can help with their research and improve the understanding of data in other domains.",2012,Knowledge Discovery and Data Mining,Fields of study: interactive visualizationtheoretical computer scienceworld wide webdistributed computingdata miningcomputer science
Large-scale distributed non-negative sparse coding and sparse dictionary learning,Vikas Sindhwani (IBM)Amol Ghoting (IBM),"281476361,2102101334","We consider the problem of building compact, unsupervised representations of large, high-dimensional, non-negative data using sparse coding and dictionary learning schemes, with an emphasis on executing the algorithm in a Map-Reduce environment. The proposed algorithms may be seen as parallel optimization procedures for constructing sparse non-negative factorizations of large, sparse matrices. Our approach alternates between a parallel sparse coding phase implemented using greedy or convex ( l 1 ) regularized risk minimization procedures, and a sequential dictionary learning phase where we solve a set of l 0 optimization problems exactly. These two-fold sparsity constraints lead to better statistical performance on text analysis tasks and at the same time make it possible to implement each iteration in a single Map-Reduce job. We detail our implementations and optimizations that lead to the ability to factor matrices with more than 100 million rows and billions of non-zero entries in just a few hours on a small commodity cluster.",2012,Knowledge Discovery and Data Mining,Fields of study: k svdnon negative matrix factorizationneural codingsparse approximationsparse matrixoptimization problemtext miningtheoretical computer sciencepattern recognitionmachine learningcomputer science
Harnessing the wisdom of the crowds for accurate web page clipping,Lei Zhang (University of Science and Technology of China)Linpeng Tang (Shanghai Jiao Tong University)Ping Luo (HP Labs)Enhong Chen (University of Science and Technology of China)Limei Jiao (HP Labs)Min Wang (HP Labs)Guiquan Liu (University of Science and Technology of China),"2646931153,2119198671,2291210646,2136372366,2102080791,2467205710,2106106071","Clipping Web pages, namely extracting the informative clips (areas) from Web pages, has many applications, such as Web printing and e-reading on small handheld devices. Although many existing methods attempt to address this task, most of them can either work only on certain types of Web pages (e.g., news- and blog-like web pages), or perform semi-automatically where extra user efforts are required in adjusting the outputs. The problem of clipping any types of Web pages accurately in a totally automatic way remains pretty much open. To this end in this study we harness the wisdom of the crowds to provide accurate recommendation of informative clips on any given Web pages. Specifically, we leverage the knowledge on how previous users clip similar Web pages, and this knowledge repository can be represented as a transaction database where each transaction contains the clips selected by a user on a certain Web page. Then, we formulate a new pattern mining problem, mining top-1 qualified pattern , on transaction database for this recommendation. Here, the recommendation considers not only the pattern support but also the pattern occupancy (proposed in this work). High support requires that patterns appear frequently in the database, while high occupancy requires that patterns occupy a large portion of the transactions they appear in. Thus, it leads to both precise and complete recommendation. Additionally, we explore the properties on occupancy to further prune the search space for high-efficient pattern mining. Finally, we show the effectiveness of the proposed algorithm on a human-labeled ground truth dataset consisting of 2000 web pages from 100 major Web sites, and demonstrate its efficiency on large synthetic datasets.",2012,Knowledge Discovery and Data Mining,Fields of study: website parse templateweb analyticsstatic web pagedata website mapweb mappingmashuphits algorithmoccupancyweb navigationweb servermobile deviceweb serviceground truthweb pageworld wide webinformation retrievaldata miningcomputer science
Discovering lag intervals for temporal dependencies,Liang Tang (Florida International University)Tao Li (Florida International University)Larisa Shwartz (IBM),"2128176332,2472069284,2038295021","Time lag is a key feature of hidden temporal dependencies within sequential data. In many real-world applications, time lag plays an essential role in interpreting the cause of discovered temporal dependencies. Traditional temporal mining methods either use a predefined time window to analyze the item sequence, or employ statistical techniques to simply derive the time dependencies among items. Such paradigms cannot effectively handle varied data with special properties, e.g., the interleaved temporal dependencies. In this paper, we study the problem of finding lag intervals for temporal dependency analysis. We first investigate the correlations between the temporal dependencies and other temporal patterns, and then propose a generalized framework to resolve the problem. By utilizing the sorted table in representing time lags among items, the proposed algorithm achieves an elegant balance between the time cost and the space cost. Extensive empirical evaluation on both synthetic and real data sets demonstrates the efficiency and effectiveness of our proposed algorithm in finding the temporal dependencies with lag intervals in sequential data.",2012,Knowledge Discovery and Data Mining,Fields of study: dependence analysisdata miningreal time computingstatisticscomputer science
Bootstrapped language identification for multi-site internet domains,Uwe F. Mayer (eBay),2106807687,"We present an algorithm for language identification, in particular of short documents, for the case of an Internet domain with sites in multiple countries with differing languages. The algorithm is significantly faster than standard language identification methods, while providing state-of-the-art identification. We bootstrap the algorithm based on the language identification based on the site alone, a methodology suitable for any supervised language identification algorithm. We demonstrate the bootstrapping and algorithm on eBay email data and on Twitter status updates data. The algorithm is deployed at eBay as part of the back-office development data repository.",2012,Knowledge Discovery and Data Mining,Fields of study: language identificationboostingstatistical modelspeech recognitiondata miningmachine learningstatisticscomputer science
Fast algorithms for comprehensive n-point correlation estimates,William B. March (Georgia Institute of Technology)Andrew J. Connolly (University of Washington)Alexander G. Gray (Georgia Institute of Technology),"2152176922,2020153005,2112810595","The n -point correlation functions (npcf) are powerful spatial statistics capable of fully characterizing any set of multidimensional points. These functions are critical in key data analyses in astronomy and materials science, among other fields, for example to test whether two point sets come from the same distribution and to validate physical models and theories. For example, the npcf has been used to study the phenomenon of dark energy, considered one of the major breakthroughs in recent scientific discoveries. Unfortunately, directly estimating the continuous npcf at a single value requires O ( N n ) time for $N$ points, and n may be 2, 3, 4 or even higher, depending on the sensitivity required. In order to draw useful conclusions about real scientific problems, we must repeat this expensive computation both for many different scales in order to derive a smooth estimate and over many different subsamples of our data in order to bound the variance. We present the first comprehensive approach to the entire n -point correlation function estimation problem, including fast algorithms for the computation at multiple scales and for many subsamples. We extend the current state-of-the-art tree-based approach with these two algorithms. We show an order-of-magnitude speedup over the current best approach with each of our new algorithms and show that they can be used together to obtain over 500x speedups over the state-of-the-art in order to enable much larger datasets and more accurate scientific analyses than were possible previously.",2012,Knowledge Discovery and Data Mining,Fields of study: dark energyjackknife resamplingspatial analysisphysical modelcorrelation functioneconometricsdata miningmathematical optimizationstatisticsmathematics
A simple methodology for soft cost-sensitive classification,Te-Kang Jan (Academia Sinica)Da-Wei Wang (Academia Sinica)Chi-Hung Lin (National Yang-Ming University)Hsuan-Tien Lin (National Taiwan University),"2222186951,2113178572,2671355912,2127632057","Many real-world data mining applications need varying cost for different types of classification errors and thus call for cost-sensitive classification algorithms. Existing algorithms for cost-sensitive classification are successful in terms of minimizing the cost, but can result in a high error rate as the trade-off. The high error rate holds back the practical use of those algorithms. In this paper, we propose a novel cost-sensitive classification methodology that takes both the cost and the error rate into account. The methodology, called soft cost-sensitive classification, is established from a multicriteria optimization problem of the cost and the error rate, and can be viewed as regularizing cost-sensitive classification with the error rate. The simple methodology allows immediate improvements of existing cost-sensitive classification algorithms. Experiments on the benchmark and the real-world data sets show that our proposed methodology indeed achieves lower test error rates and similar (sometimes lower) test costs than existing cost-sensitive classification algorithms.",2012,Knowledge Discovery and Data Mining,Fields of study: bayes error rateone class classificationregularizationword error ratebiological classificationdata miningpattern recognitionmachine learningstatisticscomputer science
Mining emerging patterns by streaming feature selection,Kui Yu (Hefei University of Technology)Wei Ding (University of Massachusetts Boston)Dan A. Simovici (University of Massachusetts Boston)Xindong Wu (Hefei University of Technology),"2567944696,2116574971,114567545,2123651450","Building an accurate emerging pattern classifier with a high-dimensional dataset is a challenging issue. The problem becomes even more difficult if the whole feature space is unavailable before learning starts. This paper presents a new technique on mining emerging patterns using streaming feature selection. We model high feature dimensions with streaming features, that is, features arrive and are processed one at a time. As features flow in one by one, we online evaluate each coming feature to determine whether it is useful for mining predictive emerging patterns (EPs) by exploiting the relationship between feature relevance and EP discriminability (the predictive ability of an EP). We employ this relationship to guide an online EP mining process. This new approach can mine EPs from a high-dimensional dataset, even when its entire feature set is unavailable before learning. The experiments on a broad range of datasets validate the effectiveness of the proposed approach against other well-established methods, in terms of predictive accuracy, pattern numbers and running time.",2012,Knowledge Discovery and Data Mining,Fields of study: feature vectorfeaturefeature selectiondata miningpattern recognitionmachine learningcomputer science
Coupled behavior analysis for capturing coupling relationships in group-based market manipulations,"Yin Song (University of Technology, Sydney)Longbing Cao (University of Technology, Sydney)Xindong Wu (University of Vermont)Gang WeiWu YeWei Ding (University of Massachusetts Boston)","2695495730,2115085568,2123651450,2687710263,2226973153,2116574971","In stock markets, an emerging challenge for surveillance is that a group of hidden manipulators collaborate with each other to manipulate the price movement of securities. Recently, the coupled hidden Markov model (CHMM)-based coupled behavior analysis (CBA) has been proposed to consider the coupling relationships in the above group-based behaviors for manipulation detection. From the modeling perspective, however, this requires overall aggregation of the behavioral data to cater for the CHMM modeling, which does not differentiate the coupling relationships presented in different forms within the aggregated behaviors and degrade the capability for further anomaly detection. Thus, this paper suggests a general CBA framework for detecting group-based market manipulation by capturing more comprehensive couplings and proposes two variant implementations, which are hybrid coupling (HC)-based and hierarchical grouping (HG)-based respectively. The proposed framework consists of three stages. The first stage, qualitative analysis, generates possible qualitative coupling relationships between behaviors with or without domain knowledge. In the second stage, quantitative representation of coupled behaviors is learned via proper methods. For the third stage, anomaly detection algorithms are proposed to cater for different application scenarios. Experimental results on data from a major Asian stock market show that the proposed framework outperforms the CHMM-based analysis in terms of detecting abnormal collaborative market manipulations. Additionally, the two different implementations are compared with their effectiveness for different application scenarios.",2012,Knowledge Discovery and Data Mining,Fields of study: domain knowledgehierarchical clusteringqualitative researchanomaly detectiondata miningartificial intelligencemachine learningsimulationcomputer science
Web image prediction using multivariate point processes,Gunhee Kim (Carnegie Mellon University)Li Fei-Fei (Stanford University)Eric P. Xing (Carnegie Mellon University),"2210743192,1984838606,351197510","In this paper, we investigate a problem of predicting what images are likely to appear on the Web at a future time point , given a query word and a database of historical image streams that potentiates learning of uploading patterns of previous user images and associated metadata. We address such a Web image prediction problem at both a collective group level and an individual user level. We develop a predictive framework based on the multivariate point process, which employs a stochastic parametric model to solve the relations between image occurrence and the covariates that influence it, in a flexible, scalable, and globally optimal way. Using Flickr datasets of more than ten million images of 40 topics, our empirical results show that the proposed algorithm is more successful in predicting unseen Web images than other candidate methods, including forecasting on semantic meanings only, a PageRank-based image retrieval, and a generative author-time topic model.",2012,Knowledge Discovery and Data Mining,Fields of study: personalizationpoint processpoisson regressionparametric modelimage retrievalglobal optimizationdata miningpattern recognitionmachine learningstatisticscomputer science
Factoring past exposure in display advertising targeting,"Neha Gupta (University of Maryland, College Park)Abhimanyu Das (Yahoo!)Sandeep Pandey (Yahoo!)Vijay K. Narayanan (Yahoo!)","2106213192,2223518471,2137133237,2146547657","Online advertising is becoming more and more performance oriented where the decision to show an advertisement to a user is made based on the user's propensity to respond to the ad in a positive manner, (e.g., purchasing a product, subscribing to an email list). The user response depends on how well the ad campaign matches to the user's interest, as well as the amount of user's past exposure to the campaign - a factor shown to be impactful in controlled experimental studies. Past exposure builds brand-awareness and familiarity with the user, which in turn leads to a higher propensity of the user to buy/convert on the ad impression. In this paper we propose a model of the user response to an ad campaign as a function of both the interest match and the past exposure, where the interest match is estimated using historical search/browse activities of the user. The goal of this paper is two-fold. First, we demonstrate the role played by the user interest and the past exposure in modeling user response by jointly estimating the parameters of these factors. We test this response model over hundreds of real ad campaigns. Second, we use the findings from this joint model to identify more relevant target users for ad campaigns. In particular, we show that on real advertising data this model combines past exposure together with the user profile to identify better target users over the conventional targeting models.",2012,Knowledge Discovery and Data Mining,Fields of study: computer user satisfactionuseruser modelingonline advertisingmultimediadata miningcomputer science
Aggregating web offers to determine product prices,Rakesh Agrawal (Microsoft)Samuel Ieong (Microsoft),"2537924216,2267310192","Historical prices are important information that can help consumers decide whether the time is right to buy a product. They provide both a context to the users, and facilitate the use of prediction algorithms for forecasting future prices. To produce a representative price history, one needs to consider all offers for the product. However, matching offers to a product is a challenging problem, and mismatches could lead to glaring errors in price history. We propose a principled approach to filter out erroneous matches based on a probabilistic model of prices. We give an efficient algorithm for performing inference that takes advantage of the structure of the problem. We evaluate our results empirically using merchant offers collected from a search engine, and measure the proximity of the price history generated by our approach to the true price history. Our method outperforms alternatives based on robust statistics both in tracking the true price levels and the true price trends.",2012,Knowledge Discovery and Data Mining,Fields of study: price leveldata aggregatorrobust statisticssearch enginetime seriesstatistical modeldata miningstatistics
Social media data analysis for revealing collective behaviors,Aoying Zhou (East China Normal University)Weining QianHaixin Ma (East China Normal University),"2111470091,2637493249,2123758205","Along with the development of Web 2.0 applications, social media services has attracted many users and become their hands-on toolkits for recording life, sharing ideas, and social networking. Though social media services are essentially web or mobile applications and services, they combine user-generated content and social networks together, so that information can be created, transmitted, transformed, and consumed in the cyberspace. Thus, social media somehow could be regarded as a kind of sensor to the real life of its users. In general, the data from social media is of low quality. Pieces of information in social media are usually short, with informal presentation, and in some specific context that is highly related to the physical world. Therefore, it is challenging to extract semantics from social media data. However, we argue that given sufficient social media data, users' collective behaviors could be sensed, studied, and even predicted in a certain circumstance. Our study is conducted on data from two services, i.e. Twitter, and Sina Weibo, the most popular microblogging services all over the world and in China, respectively. Collective behaviors are actions of a large amount of various people, which are neither conforming nor deviant. Various collective behaviors are studied in the context of social media. Our studies show that there are various information flow patterns in social media, some of which are similar to traditional media such as newspapers, while others are embedded deep in the social network structure. The evolution of hotspots is highly affected by external stimulation, the social network structure, and individual user's activities. Furthermore, social media tends to be immune to some repeated similar external stimulations. Last but not the least, there is considerable difference in users' behavior between Twitter and Sina Weibo.",2012,Knowledge Discovery and Data Mining,Fields of study: media lab europe s social robotssocial heuristicsmedia system dependency theorysocial media optimizationsocial mediauser generated contentcollective behaviorinformation flowsocial networkdata analysismedia relationssocial computinginternet privacymultimediaworld wide webcomputer science
Summarization-based mining bipartite graphs,Jing Feng (Ludwig Maximilian University of Munich)Xiao He (Ludwig Maximilian University of Munich)Bettina Konte (Ludwig Maximilian University of Munich)Christian Böhm (Ludwig Maximilian University of Munich)Claudia Plant (Florida State University),"2302555519,2280202269,1907870702,2486446532,2122910652","How to extract the truly relevant information from a large relational data set? The answer of this paper is a technique integrating graph summarization, graph clustering, link prediction and the discovery of the hidden structure on the basis of data compression. Our novel algorithm SCMiner (for Summarization-Compression Miner) reduces a large bipartite input graph to a highly compact representation which is very useful for different data mining tasks: 1) Clustering: The compact summary graph contains the truly relevant clusters of both types of nodes of a bipartite graph. 2) Link prediction: The compression scheme of SCMiner reveals suspicious edges which are probably erroneous as well as missing edges, i.e. pairs of nodes which should be connected by an edge. 3) Discovery of the hidden structure: Unlike traditional co-clustering methods, the result of SCMiner is not limited to row- and column-clusters. Besides the clusters, the summary graph also contains the essential relationships between both types of clusters and thus reveals the hidden structure of the data. Extensive experiments on synthetic and real data demonstrate that SCMiner outperforms state-of-the-art techniques for clustering and link prediction. Moreover, SCMiner discovers the hidden structure and reports it in an interpretable way to the user. Based on data compression, our technique does not rely on any input parameters which are difficult to estimate.",2012,Knowledge Discovery and Data Mining,Fields of study: strength of a graphcomplement graphnull graphcorrelation clusteringgraph databasepower graph analysisaffinity propagationclustering coefficientgraphbipartite graphrelational databasecluster analysisdata compressionautomatic summarizationdata miningpattern recognitionmachine learningcomputer sciencemathematics
Question routing in community based QA: incorporating answer quality and answer content,Lei Fang (Tsinghua University)Minlie Huang (Tsinghua University)Xiaoyan Zhu (Tsinghua University),"2125249401,2162268045,2147746173","Community based question and answering (cQA) services provide a convenient way for online users to share and exchange information and knowledge, which is highly valuable for information seeking. User interest and dedication act as the motivation to promote the interactive process of question and answering. In this paper, we aim to address a key issue about cQA systems: routing newly asked questions to appropriate users that may potentially provide answer with high quality. We incorporate answer quality and answer content to build a probabilistic question routing model. Our proposed model is capable of 1) differentiating and quantifying the authority of users for different topic or category; 2) routing questions to users with expertise. Experimental results based on a large collection of data from Wenwen demonstrate that our model is effective and has promising performance.",2012,Knowledge Discovery and Data Mining,Fields of study: service providerquestion answeringworld wide webinformation retrievaldata miningcomputer science
Optimal exact least squares rank minimization,Shuo Xiang (Arizona State University)Yunzhang Zhu (University of Minnesota)Xiaotong Shen (University of Minnesota)Jieping Ye (Arizona State University),"2121591154,2167670508,2125069230,2305258894","In multivariate analysis, rank minimization emerges when a low-rank structure of matrices is desired as well as a small estimation error. Rank minimization is nonconvex and generally NP-hard, imposing one major challenge. In this paper, we consider a nonconvex least squares formulation, which seeks to minimize the least squares loss function with the rank constraint. Computationally, we develop efficient algorithms to compute a global solution as well as an entire regularization solution path. Theoretically, we show that our method reconstructs the oracle estimator exactly from noisy data. As a result, it recovers the true rank optimally against any method and leads to sharper parameter estimation over its counterpart. Finally, the utility of the proposed method is demonstrated by simulations and image reconstruction from noisy background.",2012,Knowledge Discovery and Data Mining,Fields of study: low rank approximationloss functioniterative reconstructionmultivariate analysisestimation theoryleast squaresglobal optimizationtext miningdata miningmachine learningmathematical optimizationstatisticscomputer sciencemathematics
Finding minimum representative pattern sets,Guimei Liu (National University of Singapore)Haojun Zhang (National University of Singapore)Limsoon Wong (National University of Singapore),"2016309378,2310035076,2263195242","Frequent pattern mining often produces an enormous number of frequent patterns, which imposes a great challenge on understanding and further analysis of the generated patterns. This calls for finding a small number of representative patterns to best approximate all other patterns. An ideal approach should 1) produce a minimum number of representative patterns; 2) restore the support of all patterns with error guarantee; and 3) have good efficiency. Few existing approaches can satisfy all the three requirements. In this paper, we develop two algorithms, MinRPset and FlexRPset, for finding minimum representative pattern sets. Both algorithms provide error guarantee. MinRPset produces the smallest solution that we can possibly have in practice under the given problem setting, and it takes a reasonable amount of time to finish. FlexRPset is developed based on MinRPset. It provides one extra parameter K to allow users to make a trade-off between result size and efficiency. Our experiment results show that MinRPset and FlexRPset produce fewer representative patterns than RPlocal---an efficient algorithm that is developed for solving the same problem. FlexRPset can be slightly faster than RPlocal when K is small.",2012,Knowledge Discovery and Data Mining,Fields of study: satisfiabilitydata miningmachine learningalgorithmmathematics
Two approaches to understanding when constraints help clustering,"Ian Davidson (University of California, Davis)",2560595684,"Most algorithm work in data mining focuses on designing algorithms to address a learning problem. Here we focus our attention on designing algorithms to determine the ease or difficulty of a problem instance. The area of clustering under constraints has recently received much attention in the data mining community. We can view the constraints as restricting (either directly or indirectly) the search space of a clustering algorithm to just feasible clusterings. However, to our knowledge no work explores methods to count the feasible clusterings or other measures of difficulty nor the importance of these measures. We present two approaches to efficiently characterize the difficulty of satisfying must-link (ML) and cannot-link (CL) constraints: calculating the fractional chromatic polynomial of the constraint graph using LP and approximately counting the number of feasible clusterings using MCMC samplers. We show that these measures are correlated to the classical performance measures of constrained clustering algorithms. From these insights and our algorithms we construct new methods of generating and pruning constraints and empirically demonstrate their usefulness.",2012,Knowledge Discovery and Data Mining,Fields of study: constrained clusteringchromatic polynomialcluster analysissatisfiabilitysemi supervised learningdata miningpattern recognitionmachine learningmathematical optimizationstatisticscomputer sciencemathematics
Stratified k-means clustering over a deep web data source,Tantan Liu (Ohio State University)Gagan Agrawal (Ohio State University),"2122352814,2154982890","This paper focuses on the problem of clustering data from a {\em hidden} or a deep web data source. A key characteristic of deep web data sources is that data can only be accessed through the limited query interface they support. Because the underlying data set cannot be accessed directly, data mining must be performed based on sampling of the datasets. The samples, in turn, can only be obtained by querying the deep web databases with specific inputs. We have developed a new stratified clustering method addressing this problem for a deep web data source. Specifically, we have developed a stratified k-means clustering method. In our approach, the space of input attributes of a deep web data source is stratified for capturing the relationship between the input and the output attributes. The space of output attributes of a deep web data source is partitioned into sub-spaces. Three representative sampling methods are developed in this paper, with the goal of achieving a good estimation of the statistics , including proportions and centers, within the sub-spaces of the output attributes. We have evaluated our methods using two synthetic and two real datasets. Our comparison shows significant gains in estimation accuracy from both the novel aspects of our work, i.e., the use of stratification(5%-55%), and our and representative sampling methods(up to 54%).",2012,Knowledge Discovery and Data Mining,Fields of study: correlation clusteringdata stream clusteringcure data clustering algorithmfuzzy clusteringcluster analysissamplingdeep webdata sciencedata miningdatabasemachine learningcomputer science
Who will be participating next?: predicting the participation of Dark Web community,Xuning Tang (Drexel University)Christopher C. Yang (Drexel University)Mi Zhang (Drexel University),"2150790750,2128034517,2275500813","Predicting whether a user will be participating in a thread has broad applications, such as thread recommendation and ranking. In an extremist forum, knowing which user will be interested to join a particular thread with sensitive or threatening information is also important for security agent to prevent or prepare for any potential outbreak of crisis. Traditional methods employed a bipartite graph to represent user-thread relationships and predict potential users for a new coming thread based on user similarities. In this paper, we propose a User Interest and Topic Detection model to extract topics and trends from a document corpus and also discover users' interests toward these trends. Information of user interest is then used to predict potential information consumers for a given thread. Experiments conducted in the Dark Web dataset showed the effectiveness of our approach; especially when we have limited information about who have already participated in an existing new thread.",2012,Knowledge Discovery and Data Mining,Fields of study: rankingbipartite graphpredictionmultimediaworld wide webdata miningmachine learningstatisticscomputer science
Estimating entity importance via counting set covers,Aristides Gionis (Yahoo!)Theodoros Lappas (Boston University)Evimaria Terzi (Boston University),"737311942,2028397797,2110675235","The data-mining literature is rich in problems asking to assess the importance of entities in a given dataset. At a high level, existing work identifies important entities either by ranking or by selection. Ranking methods assign a score to every entity in the population, and then use the assigned scores to create a ranked list. The major shortcoming of such approaches is that they ignore the redundancy between high-ranked entities, which may in fact be very similar or even identical. Therefore, in scenarios where diversity is desirable, such methods perform poorly. Selection methods overcome this drawback by evaluating the importance of a group of entities collectively. To achieve this, they typically adopt a set-cover formulation, which identifies the entities in the minimum set cover as the important ones. However, this dichotomy of entities conceals the fact that, even though an entity may not be in the reported cover, it may still participate in many other optimal or near-optimal solutions. In this paper, we propose a framework that overcomes the above drawbacks by integrating the ranking and selection paradigms. Our approach assigns importance scores to entities based on both the number and the quality of set-cover solutions that they participate. Our algorithmic contribution lies with the design of an efficient algorithm for approximating the number of high-quality set covers that each entity participates. Our methodology applies to a wide range of applications. In a user study and an experimental evaluation on real data, we demonstrate that our framework is efficient and provides useful and intuitive results.",2012,Knowledge Discovery and Data Mining,Fields of study: countingset cover problemimportance samplingtheoretical computer sciencedata miningmachine learningstatisticsmathematics
AssocExplorer: an association rule visualization system for exploratory data analysis,"Guimei Liu (National University of Singapore)Andre Suchitra (National University of Singapore)Haojun Zhang (National University of Singapore)Mengling Feng (Agency for Science, Technology and Research)See-Kiong Ng (Agency for Science, Technology and Research)Limsoon Wong (National University of Singapore)","2016309378,2597801766,2310035076,2115837136,2433676728,2263195242","We present a system called AssocExplorer to support exploratory data analysis via association rule visualization and exploration. AssocExplorer is designed by following the visual information-seeking mantra: overview first, zoom and filter, then details on demand. It effectively uses coloring to deliver information so that users can easily detect things that are interesting to them. If users find a rule interesting, they can explore related rules for further analysis, which allows users to find interesting phenomenon that are difficult to detect when rules are examined separately. Our system also allows users to compare rules and inspect rules with similar item composition but different statistics so that the key factors that contribute to the difference can be isolated.",2012,Knowledge Discovery and Data Mining,Fields of study: three click ruleexploratory data analysisassociation rule learningvisual systemdata scienceworld wide webdata miningcomputer science
Exploration of ground truth from raw GPS data,Huajian Mao (National University of Defense Technology)Wuman Luo (Hong Kong University of Science and Technology)Haoyu Tan (Hong Kong University of Science and Technology)Lionel M. Ni (Hong Kong University of Science and Technology)Nong Xiao (National University of Defense Technology),"2672570764,2170038787,2117574360,2135854099,2698794177","To enable smart transportation, a large volume of vehicular GPS trajectory data has been collected in the metropolitan-scale Shanghai Grid project. The collected raw GPS data, however, suffers from various errors. Thus, it is inappropriate to use the raw GPS dataset directly for many potential smart transportation applications. Map matching, a process to align the raw GPS data onto the corresponding road network, is a commonly used technique to calibrate the raw GPS data. In practice, however, there is no ground truth data to validate the calibrated GPS data. It is necessary and desirable to have ground truth data to evaluate the effectiveness of various map matching algorithms, especially in complex environments. In this paper, we propose truthFinder, an interactive map matching system for ground truth data exploration. It incorporates traditional map matching algorithms and human intelligence in a unified manner. The accuracy of truthFinder is guaranteed by the observation that a vehicular trajectory can be correctly identified by human-labeling with the help of a period of historical GPS dataset. To the best of our knowledge, truthFinder is the first interactive map matching system trying to explore the ground truth from historical GPS trajectory data. To measure the cost of human interactions, we design a cost model that classifies and quantifies user operations. Having the guaranteed accuracy, truthFinder is evaluated in terms of operation cost. The results show that truthFinder makes the cost of map matching process up to two orders of magnitude less than the pure human-labeling approach.",2012,Knowledge Discovery and Data Mining,Fields of study: ground truthsysteminterpersonal relationshipcomputer visiondata miningcomputer science
U 2 SOD-DB: a database system to manage large-scale u biquitous u rban s ensing o rigin- d estination data,Jianting Zhang (City University of New York)Camille Kamga (City University of New York)Hongmian Gong (City University of New York)Le Gruenwald (University of Oklahoma),"2269847788,2426923376,2705972311,2188709943","Volumes of urban sensing data captured by consumer electronic devices are growing exponentially and current disk-resident database systems are becoming increasingly incapable of handling such large-scale data efficiently. In this paper, we report our design and implementation of U 2 SOD-DB, a column-oriented, Graphics Processing Unit (GPU)-accelerated, in-memory data management system targeted at large-scale ubiquitous urban sensing origin-destination data. Experiment results show that U 2 SOD-DB is capable of handling hundreds of millions of taxi-trip records with GPS recorded pickup and drop-off locations and times efficiently. Spatial and temporal aggregations on 150 million pickup locations and times in middle-town and downtown Manhattan areas in the New York City (NYC) can be completed in a fraction of a second. This is 10-30X faster than a serial CPU implementation due to GPU accelerations. Spatially joining the 150 million taxi pickup locations with 43 thousand polygons in identifying trip purposes has reduced the runtime from 30.5 hours to around 1,000 seconds and achieved a two orders (100X) speedup using a hybrid CPU-GPU approach.",2012,Knowledge Discovery and Data Mining,Fields of study: general purpose computing on graphics processing unitsautomatic identification and data capturecomputer graphics imagesdata miningreal time computingsimulationcomputer science
Overlapping decomposition for causal graphical modeling,Lei Han (Peking University)Guojie Song (Peking University)Gao Cong (Nanyang Technological University)Kunqing Xie (Peking University),"2531318857,2101713283,2295915604,2144426229","Causal graphical models are developed to detect the dependence relationships between random variables and provide intuitive explanations for the relationships in complex systems. Most of existing work focuses on learning a single graphical model for all the variables. However, a single graphical model cannot accurately characterize the complicated causal relationships for a relatively large graph. In this paper, we propose the problem of estimating an overlapping decomposition for Gaussian graphical models of a large scale to generate overlapping sub-graphical models. Specifically, we formulate an objective function for the overlapping decomposition problem and propose an approximate algorithm for it. A key theory of the algorithm is that the problem of solving a κ+1 node graphical model can be reduced to the problem of solving a one-step regularization based on a solved κ node graphical model. Based on this theory, a greedy expansion algorithm is proposed to generate the overlapping subgraphs. We evaluate the effectiveness of our model on both synthetic datasets and real traffic dataset, and the experimental results show the superiority of our method.",2012,Knowledge Discovery and Data Mining,Fields of study: graphical modelcausalityrandom variablecomplex systemstheoretical computer sciencedata miningmachine learningstatisticscomputer science
EigenBot: foiling spamming botnets with matrix algebra,Ching-Hao Mao (National Taiwan University of Science and Technology)Chang-Cheng Lin (Institute for Information Industry)Jia-Yu (Tim) Pan (Google)Kai-Chi Chang (Institute for Information Industry)Christos Faloutsos (Carnegie Mellon University)Hahn-Ming Lee (National Taiwan University of Science and Technology),"2473922946,2343374977,2159823517,2168206110,2198983026,2169372550","We present EigenBot, a spamming botnet clustering and tracking mechanism that identifies a botnet-based spamming email campaigns. EigenBot extracts the key concepts among the spam emails, despite the high dimensionality, and the noise in the input. We evaluated EigenBot using real spamming botnet data on the Internet: more than one million spam emails, collected during the period from May 2011 from Internet service providers (ISPs) in Taiwan. EigenBot successfully identified spamming botnet groups at a high true positive rate of 82%, thereby improving the detection rate of baseline approaches by 10 absolute percentage points. EigenBot is now employed by the Taiwanese government to support cyber spamming activity alleviation and has already reported 389 spamming sources to the National Communication Commission (the government regulatory agency in Taiwan) in 2011 .",2012,Knowledge Discovery and Data Mining,Fields of study: srizbi botnetbotnetspamlatent semantic indexingindependent component analysissingular value decompositioninternet privacyworld wide webcomputer securitydata miningmachine learningcomputer science
Experiences and lessons in developing industry-strength machine learning and data mining software,Chih-Jen Lin (National Taiwan University),2168176072,"Traditionally academic machine learning and data mining researchers focus on proposing new algorithms. The task of implementing these methods is often left to companies that are developing software packages. However, the gap between the two sides has caused some problems. First, the practical deployment of new algorithms still involves some challenging issues that need to be studied by researchers. Second, without further investigation after publishing their papers, researchers have neither the opportunity to work with real problems nor see how their methods are used. We discuss the experiences in developing two machine learning packages LIBSVM and LIBLINEAR, that are widely used in both academia and industry. We demonstrate that the interaction with users leads us to identify some important research problems. For example, the decision to study and then support multi-class SVM was essential in the early stage of developing LIBSVM. The birth of LIBLINEAR was driven by the need to classify large-scale documents in Internet companies. For fast training of large-scale problems, we had to create new algorithms other than those used in LIBSVM for kernel SVM. We present some practical use of LIBLINEAR for Internet applications. Finally, we give lessons learned and future perspectives for developing industry-strength machine learning and data mining software.",2012,Knowledge Discovery and Data Mining,Fields of study: support vector machinedata sciencedata miningmachine learningcomputer science
User-sentiment topic model: refining user's topics with sentiment information,Tong Zhao (Tsinghua University)Chunping Li (Tsinghua University)Qiang Ding (Huawei)Li Li (Huawei),"2685736861,2147389682,2623124594,2303777625","In large social networks, users feel free to share their feelings about anything they are interested in and many research works have focused on modeling users' interests on social network for product recommendations or personal services. Unfortunately, there are fewer works about finding why users like or dislike something. More specifically, there are many researches about sentiment analysis of users' opinion toward products or topics, but fewer are focused on why they hold this feeling and which aspects or factors of the product (topic) lead to users' different opinions about it. In this paper, we present a hierarchical generative model, called user-sentiment topic model (USTM), which captures users' topics with sentiment information. Our aim is to use USTM to refine users' topics with different sentiment trends including positive, negative and neutral, which can be further used in social network analysis to find influential users on topic level with sentiment information. The experiment results on three datasets show that our proposed USTM can capture user's interests with their sentiment well, making it useful for social network analysis.",2012,Knowledge Discovery and Data Mining,Fields of study: topic modelsocial network analysissocial networksentiment analysisworld wide webinformation retrievaldata miningcomputer science
Latent association analysis of document pairs,"Gengxin Miao (University of California, Santa Barbara)Ziyu Guan (University of California, Santa Barbara)Louise E. Moser (University of California, Santa Barbara)Xifeng Yan (University of California, Santa Barbara)Shu Tao (IBM)Nikos Anerousis (IBM)Jimeng Sun (IBM)","2140287697,2163043385,2154501644,2116657824,2509613010,2011461587,2110385854","This paper presents Latent Association Analysis (LAA), a generative model that analyzes the topics within two document sets simultaneously, as well as the correlations between the two topic structures, by considering the semantic associations among document pairs. LAA defines a correlation factor that represents the connection between two documents, and considers the topic proportion of paired documents based on this factor. Words in the documents are assumed to be randomly generated by particular topic assignments and topic-to-word probability distributions. The paper also presents a new ranking algorithm, based on LAA, that can be used to retrieve target documents that are potentially associated with a given source document. The ranking algorithm uses the latent factor in LAA to rank target documents by the strength of their semantic associations with the source document. We evaluate the LAA algorithm with real datasets, specifically, the IT-Change and the IT-Solution document sets from the IBM IT service environment and the Symptom-Treatment document sets from Google Health. Experimental results demonstrate that the LAA algorithm significantly outperforms existing algorithms.",2012,Knowledge Discovery and Data Mining,Fields of study: topic modelgenetic associationprobability distributiondocument clusteringinformation retrievaldata miningpattern recognitionmachine learningstatisticscomputer science
"Proceedings of the 1st International Workshop on Big Data, Streams and Heterogeneous Source Mining: Algorithms, Systems, Programming Models and Applications",Wei Fan (IBM)Albert Bifet (University of Waikato)Qiang Yang (Hong Kong University of Science and Technology)Philip Yu (University of Illinois at Chicago),"2491264568,307521372,2109031554,2125104194","The aim of this workshop is to bring together people from both academia and industry to present their most recent work related to big-data issues, and exchange ideas and thoughts in order to advance this big-data challenge, which has been considered as one of the most exciting opportunities in the past 10 years. Recent years have witnessed a dramatic increase in our ability to collect data from various sensors, devices, in different formats, from independent or connected applications. This data flood has outpaced our capability to process, analyze, store and understand these datasets. Consider the Internet data. The web pages indexed by Google were around one million in 1998, but quickly reached 1 billion in 2000 and have already exceeded 1 trillion in 2008. This rapid expansion is accelerated by the dramatic increase in acceptance of social networking applications, such as Facebook, Twitter, Weibo, etc., that allow users to create contents freely and amplify the already huge Web volume. Furthermore, with mobile phones becoming the sensory gateway to get real-time data on people from different aspects, the vast amount of data that mobile carrier can potentially process to improve our daily life has significantly outpaced our past CDR (call data record)- based processing for billing purposes only. It can be foreseen that Internet of things (IoT) applications will raise the scale of data to an unprecedented level. People and devices (from home coffee machines to cars, to buses, railway stations and airports) are all loosely connected. Trillions of such connected components will generate a huge data ocean, and valuable information must be discovered from the data to help improve quality of life and make our world a better place. For example, after we get up every morning, in order to optimize our commute time to work and complete the optimization before we arrive at office, the system needs to process information from traffic, weather, construction, police activities to our calendar schedules, and perform deep optimization under the tight time constraints. In all these applications, we are facing significant challenges in leveraging the vast amount of data, including challenges in (1) system capabilities (2) algorithmic design (3) business models.",2012,Knowledge Discovery and Data Mining,Fields of study: world wide webdata miningsimulationcomputer science
A sparsity-inducing formulation for evolutionary co-clustering,Shuiwang Ji (Old Dominion University)Wenlu Zhang (Old Dominion University)Jun Liu (Siemens),"2149659377,2303814361,2676596115","Traditional co-clustering methods identify block structures from static data matrices. However, the data matrices in many applications are dynamic; that is, they evolve smoothly over time. Consequently, the hidden block structures embedded into the matrices are also expected to vary smoothly along the temporal dimension. It is therefore desirable to encourage smoothness between the block structures identified from temporally adjacent data matrices. In this paper, we propose an evolutionary co-clustering formulation for identifying co-cluster structures from time-varying data. The proposed formulation encourages smoothness between temporally adjacent blocks by employing the fused Lasso type of regularization. Our formulation is very flexible and allows for imposing smoothness constraints over only one dimension of the data matrices, thereby enabling its applicability to a large variety of settings. The optimization problem for the proposed formulation is non-convex, non-smooth, and non-separable. We develop an iterative procedure to compute the solution. Each step of the iterative procedure involves a convex, but non-smooth and non-separable problem. We propose to solve this problem in its dual form, which is convex and smooth. This leads to a simple gradient descent algorithm for computing the dual optimal solution. We evaluate the proposed formulation using the Allen Developing Mouse Brain Atlas data. Results show that our formulation consistently outperforms methods without the temporal smoothness constraints.",2012,Knowledge Discovery and Data Mining,Fields of study: gradient descentoptimization problemcombinatoricsdata miningmachine learningmathematical optimizationstatisticscomputer sciencemathematics
CluChunk: clustering large scale user-generated content incorporating chunklet information,Yu Cheng (Northwestern University)Yusheng Xie (Northwestern University)Kunpeng Zhang (Northwestern University)Ankit Agrawal (Northwestern University)Alok N. Choudhary (Northwestern University),"2544119651,2123854923,2144435392,2160807299,2147783234","The exponential rise of online content in the form of blogs, microblogs, forums, and multimedia sharing sites has raised an urgent demand for efficient and high-quality text clustering algorithms for fast navigation and browsing of users based on better document organization. For several kinds of these user-generated content, it is much easier to obtain the input in small sets, where the data in each set belongs to the same class but with unknown class labels. Such data is viewed as weakly-labeled data and the inherent chunklet information is very useful for improving clustering performance. In this paper, we propose a system - CluChunk (clustering chunklet data) to cluster unlabeled web data which incorporates chunklet information. We try to transfer the original feature space by a discriminatively learning linear transformation such that simple unsupervised learning techniques (such as K-Means) in the transformed space can achieve good clustering accuracy. Using larger scale data from some web applications (social media and online forums), we demonstrate that the clustering performance can get significantly improved by: 1)incorporating the inherent weakly-labeled information into the clustering framework; 2)enriching the representation of short text with additional features extracted from the chunklet subset. The proposed approach can be applied to other mining tasks with large scale user-generated content, like product review summarizing and blog content clustering/classification task.",2012,Knowledge Discovery and Data Mining,Fields of study: flame clusteringbrown clusteringcanopy clustering algorithmdbscancorrelation clusteringconstrained clusteringdata stream clusteringcure data clustering algorithmaffinity propagationsocial mediauser generated contentfuzzy clusteringdata transformationdiscrimination learningk means clusteringclustering high dimensional datafeature vectorlinear mapfeature extractioncluster analysisconsensus clusteringbiclusteringconceptual clusteringunsupervised learninginformation retrievaldata miningpattern recognitionmachine learningcomputer science
Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining,Qiang Yang (Hong Kong University of Science and Technology)Deepak Agarwal (LinkedIn)Jian Pei (Simon Fraser University),"2109031554,2116605949,2126330539","The KDD conference has seen remarkable growth since its origins as an IJCAI workshop in Detroit in 1989, evolving into a full-fledged research conference in 1995, underscoring the important role data mining as a field has played in extracting knowledge and actionable insights from vast troves of data that is being generated in the digital world around us. This year we received a record 755 submissions to the research program, from which 133 papers were accepted, for an aggregate acceptance rate of 17.6% (quite similar to recent years). Among the academic conferences, the KDD conference has typically more of an emphasis on research motivated by real-world applications. It is important to keep in mind that it is this synergy of research in areas like algorithms, computational geometry, database, graph theory, machine learning, natural language processing, statistics, visualization and many others when applied to problems arising in diverse fields such as web, medicine, climatology, marketing that drives our field forward, makes it vibrant and fun - who would know that ideas in computational geometry can be adapted to construct fast algorithms to improve online advertising and movie recommendations? The breadth of topics covered in this year's research program is truly comprehensive, including social networks, privacy, text mining, predictive modeling, time-series forecasting, spatial data analysis, geometry, and more. We are very fortunate to have 4 world-class keynote speakers this year spanning industry and academia, providing inspirational talks on cutting-edge techniques and issues in web mining, information networks, statistical inference for big data, and social computing. The process of whittling down the initial 734 submissions to the final set of 133 accepted papers required the coordination and time of a large number of willing volunteers. The program committee (PC) consisted of over 350 reviewers (PC members) and 50 senior PC members. In the first phase each submitted paper was automatically assigned to 3 reviewers (after a bidding process). Once the reviews from each of the 3 reviewers were completed, the program chairs rejected papers that did not receive much support from any of the reviewers. We rejected 259 papers at this stage. Special care was taken to minimize the error of rejecting a potentially good paper at this stage. The papers that survived the first phase were assigned to the senior PC members based on their bids, they had the option of initiating a discussion for any of their papers, e.g., if there was significant divergence in scores among reviewers, or if a paper was on the borderline of being accepted. Following the discussion phase, the senior PC members provided a recommendation score and a detailed meta-review for each paper. In the final phase, we (the program chairs) analyzed all of this information, starting with the obvious accept and reject decisions, and then gradually focusing in more detail on the papers near the borderline, seeking additional reviews and input from the PC and senior PC members where appropriate. We also initiated a shepherding phase with 15 papers having the opportunity of fixing mild issues we thought would be possible to address before they can be accepted. 13 of them were accepted after thorough revisions. Finally, it is quite likely that in hindsight some worthy papers may have been rejected as part of this process - these errors are an unfortunate reality of modern computer science conferences, and hard to avoid when a very large number of decisions have to be made over a short time span based on a subjective reviewing process. Nevertheless, we, the PC chairs, are responsible for those unfortunate errors and welcome suggestions on the matter.",2012,Knowledge Discovery and Data Mining,Fields of study: operations researchdata miningartificial intelligencemachine learningcomputer science
Coordinated clustering algorithms to support charging infrastructure design for electric vehicles,Marjan Momtazpour (Virginia Tech)Patrick Butler (Virginia Tech)M. Shahriar Hossain (Virginia Tech)Mohammad Chehreghani Bozchalui (NEC)Naren Ramakrishnan (Virginia Tech)Ratnesh Sharma (NEC),"1429481628,2293049679,2120864034,2015281268,2199255697,2120320196","The confluence of several developments has created an opportune moment for energy system modernization. In the past decade, smart grids have attracted many research activities in different domains. To realize the next generation of smart grids, we must have a comprehensive understanding of interdependent networks and processes. Next-generation energy systems networks cannot be effectively designed, analyzed, and controlled in isolation from the social, economic, sensing, and control contexts in which they operate. In this paper, we develop coordinated clustering techniques to work with network models of urban environments to aid in placement of charging stations for an electrical vehicle deployment scenario. We demonstrate the multiple factors that can be simultaneously leveraged in our framework in order to achieve practical urban deployment. Our ultimate goal is to help realize sustainable energy system management in urban electrical infrastructure by modeling and analyzing networks of interactions between electric systems and urban populations.",2012,Knowledge Discovery and Data Mining,Fields of study: smart gridnetwork modelcluster analysisdata miningsimulationcomputer science
An enhanced relevance criterion for more concise supervised pattern discovery,Henrik Großkreutz (Fraunhofer Society)Daniel Paurat (Fraunhofer Society)Stefan Rüping (Fraunhofer Society),"2228062695,717148855,2289977704","Supervised local pattern discovery aims to find subsets of a database with a high statistical unusualness in the distribution of a target attribute. Local pattern discovery is often used to generate a human-understandable representation of the most interesting dependencies in a data set. Hence, the more crisp and concise the output is, the better. Unfortunately, standard algorithm often produce very large and redundant outputs. In this paper, we introduce delta-relevance, a definition of a more strict criterion of relevance. It will allow us to significantly reduce the output space, while being able to guarantee that every local pattern has a delta-relevant representative which is almost as good in a clearly defined sense. We show empirically that delta-relevance leads to a considerable reduction of the amount of returned patterns. We also demonstrate that in a top- k setting, the removal of not delta-relevant patterns improves the quality of the result set.",2012,Knowledge Discovery and Data Mining,Fields of study: data miningpattern recognitionmachine learningmathematics
Discriminative clustering for market segmentation,Peter Haider (University of Potsdam)Luca Chiarandini (Pompeu Fabra University)Ulf Brefeld (University of Bonn),"1539679250,2403961532,158112295","We study discriminative clustering for market segmentation tasks. The underlying problem setting resembles discriminative clustering, however, existing approaches focus on the prediction of univariate cluster labels. By contrast, market segments encode complex (future) behavior of the individuals which cannot be represented by a single variable. In this paper, we generalize discriminative clustering to structured and complex output variables that can be represented as graphical models. We devise two novel methods to jointly learn the classifier and the clustering using alternating optimization and collapsed inference, respectively. The two approaches jointly learn a discriminative segmentation of the input space and a generative output prediction model for each segment. We evaluate our methods on segmenting user navigation sequences from Yahoo! News. The proposed collapsed algorithm is observed to outperform baseline approaches such as mixture of experts. We showcase exemplary projections of the resulting segments to display the interpretability of the solutions.",2012,Knowledge Discovery and Data Mining,Fields of study: market segmentationdiscriminative modelgraphical modelpredictive modellingdata miningpattern recognitionmachine learningcomputer science
Sampling minimal frequent boolean (DNF) patterns,Geng Li (Rensselaer Polytechnic Institute)Mohammed J. Zaki (Rensselaer Polytechnic Institute),"2123276876,2165917828","We tackle the challenging problem of mining the simplest Boolean patterns from categorical datasets. Instead of complete enumeration, which is typically infeasible for this class of patterns, we develop effective sampling methods to extract a representative subset of the minimal Boolean patterns (in disjunctive normal form - DNF). We make both theoretical and practical contributions, which allow us to prune the search space based on provable properties. Our approach can provide a near-uniform sample of the minimal DNF patterns. We also show that the mined minimal DNF patterns are very effective when used as features for classification.",2012,Knowledge Discovery and Data Mining,Fields of study: boolean expressionsamplingdiscrete mathematicscombinatoricsstatisticsalgorithmcomputer sciencemathematics
SeqiBloc: mining multi-time spanning blockmodels in dynamic graphs,Jeffrey Chan (University of Melbourne)Wei Liu (University of Melbourne)Christopher Leckie (University of Melbourne)James Bailey (University of Melbourne)Kotagiri Ramamohanarao (University of Melbourne),"2408228308,2172864801,2111831791,2131557737,123309386","Blockmodelling is an important technique for decomposing graphs into sets of roles. Vertices playing the same role have similar patterns of interactions with vertices in other roles. These roles, along with the role to role interactions, can succinctly summarise the underlying structure of the studied graphs. As the underlying graphs evolve with time, it is important to study how their blockmodels evolve too. This will enable us to detect role changes across time, detect different patterns of interactions, for example, weekday and weekend behaviour, and allow us to study how the structure in the underlying dynamic graph evolves. To date, there has been limited research on studying dynamic blockmodels. They focus on smoothing role changes between adjacent time instances. However, this approach can overfit during stationary periods where the underling structure does not change but there is random noise in the graph. Therefore, an approach to a) find blockmodels across spans of time and b) to find the stationary periods is needed. In this paper, we propose an information theoretic framework, SeqiBloc, combined with a change point detection approach to achieve a) and b). In addition, we propose new vertex equivalence definitions that include time, and show how they relate back to our information theoretic approach. We demonstrate their usefulness and superior accuracy over existing work on synthetic and real datasets.",2012,Knowledge Discovery and Data Mining,Fields of study: minimum description lengthchange detectiondiscrete mathematicscombinatoricsdata miningmachine learningstatisticsmathematics
Subspace correlation clustering: finding locally correlated dimensions in subspace projections of the data,Stephan Günnemann (RWTH Aachen University)Ines Färber (RWTH Aachen University)Kittipat Virochsiri (RWTH Aachen University)Thomas Seidl (RWTH Aachen University),"316694267,2015027442,2224991758,2140301036","The necessity to analyze subspace projections of complex data is a well-known fact in the clustering community. While the full space may be obfuscated by overlapping patterns and irrelevant dimensions, only certain subspaces are able to reveal the clustering structure. Subspace clustering discards irrelevant dimensions and allows objects to belong to multiple, overlapping clusters due to individual subspace projections for each set of objects. As we will demonstrate, the observations, which originate the need to consider subspace projections for traditional clustering, also apply for the task of correlation analysis. In this work, we introduce the novel paradigm of subspace correlation clustering: we analyze subspace projections to find subsets of objects showing linear correlations among this subset of dimensions. In contrast to existing techniques, which determine correlations based on the full-space, our method is able to exclude locally irrelevant dimensions, enabling more precise detection of the correlated features. Since we analyze subspace projections, each object can contribute to several correlations. Our model allows multiple overlapping clusters in general but simultaneously avoids redundant clusters deducible from already known correlations. We introduce the algorithm SSCC that exploits different pruning techniques to efficiently generate a subspace correlation clustering. In thorough experiments we demonstrate the strength of our novel paradigm in comparison to existing methods.",2012,Knowledge Discovery and Data Mining,Fields of study: correlation clusteringrandom subspace methodcorrelation dimensioncomplex data typecluster analysisdata miningpattern recognitionmachine learningcomputer sciencemathematics
A dynamic learning framework to thoroughly extract structured data from web pages without human efforts,Dandan Song (Beijing Institute of Technology)Yunpeng Wu (Beijing Institute of Technology)Lejian Liao (Beijing Institute of Technology)Long Li (Beijing Institute of Technology)Fei Sun (Beijing Institute of Technology),"2647528830,2506238651,2652518449,2441998154,2476332389","Tremendous concrete and comprehensive information is contained in structured data of web pages. Attributes and their corresponding values of entities are precious resources for automatic semantic annotation, knowledge discovery, and information utilization. However, various displaying styles and formats of web pages make it a challenging task to extract them. Based on our observation, despite the lack of information in a single page, different web pages and different web sites illustrating similar entities can provide adequate knowledge for computers to learn. This paper presents a dynamic learning framework to effectively extract structured information from enormous websites in various verticals ( e.g ., books, cameras, jobs). Different with other existing approaches that are static, require manually labeling samples and can not be flexible to unseen attributes, our approach aims at dynamically, automatically and thoroughly extracting structured data from web pages. Experiments with totally 17,850 web pages in 4 verticals demonstrated the effectiveness of our framework.",2012,Knowledge Discovery and Data Mining,Fields of study: web 2 0website parse templatesemantic web stackweb modelingsocial semantic webdata webweb standardsweb mappingweb developmentweb serversemantic webweb serviceweb pagedata modelweb intelligenceinformation extractionworld wide webinformation retrievaldata miningcomputer science
On nested palindromes in clickstream data,Michel Speiser (IBM)Gianluca Antonini (IBM)Abderrahim Labbi (IBM)Juliana Sutanto (ETH Zurich),"2158439887,2124314778,2704512326,2120707417","In this paper we discuss an interesting and useful property of clickstream data. Often a visit includes repeated views of the same page. We show that in three real datasets, sampled from the websites of technology and consulting groups and a news broadcaster, page repetitions occur for the majority as a very specific structure, namely in the form of nested palindromes. This can be explained by the widespread use of features which are available in any web browser: the ""refresh"" and ""back"" buttons. Among the types of patterns which can be mined from sequence data, many either stumble if symbol repetitions are involved, or else fail to capture interesting aspects related to symbol repetitions. In an attempt to remedy this, we characterize the palindromic structures, and discuss possible ways of making use of them. One way is to pre-process the sequence data by explicitly inserting these structures, in order to obtain a richer output from conventional mining algorithms. Another application we discuss is to use the information directly, in order to analyze certain aspects of the website under study. We also provide the simple linear-time algorithm which we developed to identify and extract the structures from our data.",2012,Knowledge Discovery and Data Mining,Fields of study: palindromeweb miningworld wide webdata miningdatabasecomputer science
Semantic search and a new moore's law effect in knowledge engineering,Wei-Ying Ma (Microsoft),2134693834,"In history, the Moore's law effect has been used to describe phenomena of exponential improvement in technology when it has a virtuous cycle that makes technology improvement proportional to technology itself. For example, chip performance had doubled every 18-24 months because better processors support the development of better layout tools that support the development of even better processors. I will describe a new Moore's law effect that is being created in knowledge engineering and is driven by the self-reinforcing nature of three trends and technical advancements: big data, machine learning, and Internet economics. I will explain how we can take advantage of this new effect to develop a new generation of semantic and knowledge-based search engines. Specifically, my presentation will cover the following three areas: Knowledge acquisition - our goal is to build a comprehensive entity graph and knowledge graph to complement the web and social graphs. I will introduce techniques for entity extraction and knowledgebase construction through automatic and interactive mining and crowdsourcing. Managing knowledge - our goal is to support advanced analytical queries by combining probabilistic knowledge with a distributed platform. I will focus on technology for both online knowledge serving and offline knowledge inference. Knowledge-empowered search and applications - the knowledge we have acquired and curated enables applications like query understanding, entity-centric search experiences, and answers to natural language queries.",2012,Knowledge Discovery and Data Mining,Fields of study: open knowledge base connectivityprocedural knowledgebody of knowledgedomain knowledgenatural languagesearch enginechippersonal knowledge managementsemantic searchknowledge integrationknowledge engineeringknowledge extractionknowledge baseknowledge based systemsdata scienceknowledge managementdata miningartificial intelligencemachine learningcomputer science
Quantitative analysis for privacy leak software with privacy Petri net,Lejun Fan (Chinese Academy of Sciences)Yuanzhuo Wang (Chinese Academy of Sciences)Xueqi Cheng (Chinese Academy of Sciences)Shuyuan Jin (Chinese Academy of Sciences),"2124070383,2120380447,2129598186,2666346548","Nowadays, lots of private information are collected and spread without proper protection. privacy leak behavior has been widely discovered in many malwares and suspicious applications. We refer to such software as privacy leak software (PLS). In this paper we present an abstract model called Privacy Petri Net (PPN) for privacy leaks analysis. We build PPN modules of different privacy leak behavior sub procedure and give four indicators: possibility, severity, crypticity and manipulability for quantitative analysis. We apply our approach on real-world PLS and the case study shows that we can not only identifies the tested software as PLS, just like which is reported by AVS as malicious, but also calculate the severity, crypticity and manipulability of it. We can also evaluate the suspicious behavior in the applications which the AVSs simply treat as benign.",2012,Knowledge Discovery and Data Mining,Fields of study: privacy softwareprivate information retrievalpetri netquantitative analysisinternet privacycomputer securitydata miningcomputer science
A structural cluster kernel for learning on graphs,Madeleine Seeland (Technische Universität München)Andreas Karwath (University of Mainz)Stefan Kramer (University of Mainz),"1973984780,2307886830,2272282606","In recent years, graph kernels have received considerable interest within the machine learning and data mining community. Here, we introduce a novel approach enabling kernel methods to utilize additional information hidden in the structural neighborhood of the graphs under consideration. Our novel structural cluster kernel (SCK) incorporates similarities induced by a structural clustering algorithm to improve state-of-the-art graph kernels. The approach taken is based on the idea that graph similarity can not only be described by the similarity between the graphs themselves, but also by the similarity they possess with respect to their structural neighborhood. We applied our novel kernel in a supervised and a semi-supervised setting to regression and classification problems on a number of real-world datasets of molecular graphs. Our results show that the structural cluster similarity information can indeed leverage the prediction performance of the base kernel, particularly when the dataset is structurally sparse and consequently structurally diverse. By additionally taking into account a large number of unlabeled instances the performance of the structural cluster kernel can further be improved.",2012,Knowledge Discovery and Data Mining,Fields of study: kernel embedding of distributionstree kernelgraph kernelpolynomial kernelstring kernelradial basis function kernelquantitative structure activity relationshipclustering coefficientkernel methodcheminformaticsdata miningpattern recognitionmachine learningcomputer sciencemathematics
Dependency clustering across measurement scales,Claudia Plant (Florida State University),2122910652,"How to automatically spot the major trends in large amounts of heterogeneous data? Clustering can help. However, most existing techniques suffer from one or more of the following drawbacks: 1) Many techniques support only one particular data type, most commonly numerical attributes. 2) Other techniques do not support attribute dependencies which are prevalent in real data. 3) Some approaches require input parameters which are difficult to estimate. 4) Most clustering approaches lack in interpretability. To address these challenges, we present the algorithm Scenic for dependency clustering across measurement scales. Our approach seamlessly integrates heterogenous data types measured at different scales, most importantly continuous numerical and discrete categorical data. Scenic clusters by arranging objects and attributes in a cluster-specific low-dimensional space. The embedding serves as a compact cluster model allowing to reconstruct the original heterogenous attributes with high accuracy. Thereby embedding reveals the major cluster-specific mixed-type attribute dependencies. Following the Minimum Description Length (MDL) principle, the cluster-specific embedding serves as a codebook for effective data compression. This compression-based view automatically balances goodness-of-fit and model complexity, making input parameters redundant. Finally, the embedding serves as a visualization enhancing the interpretability of the clustering result. Extensive experiments demonstrate the benefits of Scenic.",2012,Knowledge Discovery and Data Mining,Fields of study: data stream clusteringcure data clustering algorithmfuzzy clusteringminimum description lengthdata typegoodness of fitcategorical variablecluster analysisdata compressiondata miningmachine learningstatisticscomputer sciencemathematics
Information propagation game: a tool to acquire humanplaying data for multiplayer influence maximization on social networks,Hung-Hsuan Chen (Pennsylvania State University)Yan-Bin Ciou (National Taiwan University)Shou-De Lin (National Taiwan University),"2136299089,2606039752,2114357324","With the popularity of online social network services, influence maximization on social networks has drawn much attention in recent years. Most of these studies approximate a greedy based sub-optimal solution by proving the submodular nature of the utility function. Instead of using the analytical techniques, we are interested in solving the diffusion competition and influence maximization problem by a data-driven approach. We propose Information Propagation Game (IPG), a framework that can collect a large number of seed picking strategies for analysis. Through the IPG framework, human players are not only having fun but also helping contributing the seed picking strategies. Preliminary experiment suggests that centrality based heuristics are too simple for seed selection in a multiple player environment.",2012,Knowledge Discovery and Data Mining,Fields of study: gamessocial networkartificial intelligencemachine learningsimulationcomputer science
The missing models: a data-driven approach for learning how networks grow,"Robert Patro (University of Maryland, College Park)Geet Duggal (University of Maryland, College Park)Emre Sefer (University of Maryland, College Park)Hao Wang (University of Maryland, College Park)Darya Filippova (University of Maryland, College Park)Carl Kingsford (University of Maryland, College Park)","2123906542,1973757372,1069722141,2310309979,1854063621,2236568161","Probabilistic models of network growth have been extensively studied as idealized representations of network evolution. Models, such as the Kronecker model, duplication-based models, and preferential attachment models, have been used for tasks such as representing null models, detecting anomalies, algorithm testing, and developing an understanding of various mechanistic growth processes. However, developing a new growth model to fit observed properties of a network is a difficult task, and as new networks are studied, new models must constantly be developed. Here, we present a framework, called GrowCode, for the automatic discovery of novel growth models that match user-specified topological features in undirected graphs. GrowCode introduces a set of basic commands that are general enough to encode several previously developed models. Coupling this formal representation with an optimization approach, we show that GrowCode is able to discover models for protein interaction networks, autonomous systems networks, and scientific collaboration networks that closely match properties such as the degree distribution, the clustering coefficient, and assortativity that are observed in real networks of these classes. Additional tests on simulated networks show that the models learned by GrowCode generate distributions of graphs with similar variance as existing models for these classes.",2012,Knowledge Discovery and Data Mining,Fields of study: variable order bayesian networkhierarchical network modelnetwork motifclustering coefficientnull modeldegree distributioncomplex networkstatistical modeldata miningartificial intelligencemachine learningstatisticscomputer science
PubMed search and exploration with real-time semantic network construction,Jinoh Oh (Pohang University of Science and Technology)Taehoon Kim (Pohang University of Science and Technology)Sun Park (Pohang University of Science and Technology)Hwanjo Yu (Pohang University of Science and Technology),"2164587646,2424910887,2119290733,2257366035","Exploring PubMed to find relevant information is challenging and time-consuming because PubMed typically returns a long list of articles as a result of query. Semantic network helps users to explore a large document collection and to capture key concepts and relationships among the concepts. The semantic network also serves to broaden the user's knowledge and extend query keyword by detecting and visualizing new related concepts or relations hidden in the retrieved documents. The problem of existing semantic network techniques is that they typically produce many redundant relationships, which prevents users from quickly capturing the underlying relationships among concepts. This paper develops an online PubMed search system, which displays semantic networks having no redundant relationships in real-time as a result of query. To do so, we propose an efficient semantic network construction algorithm, which prevents producing redundant relationships during the network construction. Our extensive experiments on actual PubMed data show that the proposed method is significantly faster than the method removing redundant relationships afterward. Our method is implemented and integrated into a relevance feedback PubMed search engine, called RefMed, ""http://dm.postech.ac.kr/refmed"", and will be demonstrated through the website.",2012,Knowledge Discovery and Data Mining,Fields of study: semantic computingsemantic similaritysemantic networksearch engineworld wide webinformation retrievaldata miningmachine learningcomputer science
Key lessons learned building recommender systems for large-scale social networks,Christian Posse (LinkedIn),2288969707,"By helping members to connect, discover and share relevant content or find a new career opportunity, recommender systems have become a critical component of user growth and engagement for social networks. The multidimensional nature of engagement and diversity of members on large-scale social networks have generated new infrastructure and modeling challenges and opportunities in the development, deployment and operation of recommender systems. This presentation will address some of these issues, focusing on the modeling side for which new research is much needed while describing a recommendation platform that enables real-time recommendation updates at scale as well as batch computations, and cross-leverage between different product recommendations. Topics covered on the modeling side will include optimizing for multiple competing objectives, solving contradicting business goals, modeling user intent and interest to maximize placement and timeliness of the recommendations, utility metrics beyond CTR that leverage both real-time tracking of explicit and implicit user feedback, gathering training data for new product recommendations, virality preserving online testing and virtual profiling.",2012,Knowledge Discovery and Data Mining,Fields of study: multi objective optimizationsocial networkrecommender systemknowledge managementworld wide webdata miningmachine learningsimulationcomputer science
DAGger: clustering correlated uncertain data (to predict asset failure in energy networks),Dan Olteanu (University of Oxford)Sebastiaan J. van Schaik (University of Oxford),"2081189480,1977384620","DAGger is a clustering algorithm for uncertain data. In contrast to prior work, DAGger can work on arbitrarily correlated data and can compute both exact and approximate clusterings with error guarantees. We demonstrate DAGger using a real-world scenario in which partial discharge data from UK Power Networks is clustered to predict asset failure in the energy network.",2012,Knowledge Discovery and Data Mining,Fields of study: partial dischargecluster analysisbiological classificationdata miningreal time computingmachine learningcomputer science
Exact Primitives for Time Series Data Mining,Sbdullah Al Mueen (Microsoft),2231034229,"Data mining and knowledge discovery algorithms for time series data use primitives such as bursts, periods, motifs, outliers and shapelets as building blocks. For example a model of global temperature considers both bursts (i.e. solar fare) and periods (i.e. sunspot cycle) of the sun. Algorithms for finding these primitives are required to be fast to process large datasets. Because exact algorithms that guarantee the optimum solutions are very slow for their immense computational requirements, existing algorithms find primitives approximately. This thesis presents efficient exact algorithms for two primitives, time series motif and time series shapelet. A time series motif is any repeating segment whose appearances in the time series are too similar to happen at random and thus expected to bear important information about the structure of the data. A time series shapelet is any subsequence that describes a class of time series differentiating from other classes and thus can be used to classify unknown instances.We extend the primitives for different environments. We show exact methods to find motifs in three different types of time series data. They are the in-memory datasets suitable for batched processing, the massive archives of time series stored in hard drives and finally, the streaming time series with limited storage. We also describe an exact algorithm for logical-shapelet discovery that combines multiple shapelets to better describe complex concepts.We use efficient bounds to the goodness measures to increase the efficiency of the exact algorithms. The algorithms are orders of magnitude faster than the trivial solutions and successfully discover motifs/shapelets of real time series from diverse sensors such as EEG, ECG, EPG, EOG, Accelerometers and Motion captures. We show applicability of these algorithms as subroutines in high-level data mining tasks such as summarization, classification and compression.",2012,Knowledge Discovery and Data Mining,Fields of study: motifstream processingsearch engine indexingtime seriestheoretical computer sciencedata miningmachine learningstatisticscomputer sciencemathematics
Experiments in social computation: (and the data they generate),Michael Kearns (University of Pennsylvania),2118586410,"For a number of years we have been conducting controlled human-subject experiments in distributed social computation in networks with only limited and local communication. These experiments cast a number of traditional computational, economic and sociological problems (including graph coloring, consensus, independent set, networked bargaining, biased voting and network formation) as games of strategic interaction in which subjects have financial incentives to collectively ""compute"" global solutions. I will overview and summarize the many behavioral findings from this line of experimentation. I will give particular emphasis to the novel data the experiments have generated, and the analyses this data has permitted, including quantitative studies of subject ""personality"" traits such as stubbornness, altruism, and patience, and whether those traits seem helpful or harmful to individual and collective performance.",2012,Knowledge Discovery and Data Mining,Fields of study: network formationgraph coloringindependent setsocial networksocial computingmanagement sciencedata miningmachine learningsimulationstatisticscomputer sciencemathematics
Extracting data records from web using suffix tree,Xiaoqin Xie (Harbin Engineering University)Yixiang Fang (Harbin Institute of Technology)Zhiqiang Zhang (Harbin Engineering University)Li Li (Harbin Engineering University),"2706943170,2277390951,2654216231,2636731856","There are many automatic methods that can extract lists of objects from the Web, but they often fail to handle multi-type pages automatically. This paper introduces a new method for record extraction using suffix tree which can find the repeated sub-string. Our method transfers a distinct group of tag paths appearing repeatedly in the DOM tree of the Web document to a sequence of integers firstly, and then builds a suffix tree by using this sequence. Four refining filter rules are defined. After the refining processes we can capture the useful data region patterns which can be used to extract data records. Experiments on real data show that this method is applicable for various web pages and can achieve higher accuracy and better robustness than previous methods.",2012,Knowledge Discovery and Data Mining,Fields of study: compressed suffix arrayweb pageworld wide webdata miningdatabasecomputer science
Information processing in social networks,Ming-Syan Chen (National Taiwan University),2122365371,"In the current social network, a user may have hundreds of friends and find it very time consuming to categorize and tag every friend manually. When a user is going to initiate an activity by issuing a corresponding query, he/she needs to consider the relationship among candidate attendees to find a group of mutually close friends. Meanwhile, he/she also needs to consider the schedule of candidate attendees to find an activity period available for all attendees. It would certainly be desirable if the efficiency of such process is improved. In this talk, information processing in social networks will first be reviewed in three phrases, namely (i) from content to social relationship, (ii) mining on social relationship, and (iii) from social relationship to content organization. In addition, we shall present an effective procedure which helps a user to organize an event with proper attendees with minimum total social distance and commonly available time. Moreover, it is noted that the information retrieved from the social networks is also able to facilitate those user-dependent and human-centric services. In light of this, we shall explore the quality of recommendation through incorporating the notion of social filtering and collaborative filtering. Finally, it is recognized that the cloud computing has offered many new capabilities of storing and processing huge amounts of heterogeneous data in social networks. In view of this, we shall also examine how this paradigm shift will affect the information processing in social networks.",2012,Knowledge Discovery and Data Mining,Fields of study: social heuristicssocial distanceparadigm shiftcloud computingcollaborative filteringsocial networksocial computingrecommender systeminternet privacyworld wide webdata miningmachine learningcomputer science
There is more than complex contagion: an indirect influence analysis on Twitter,Xin Shuai (Indiana University Bloomington)Shanshan Chen (Indiana University Bloomington)Ying Ding (Indiana University Bloomington)Yuyin Sun (Indiana University Bloomington)Jerome Busemeyer (Indiana University Bloomington)Jie Tang (Tsinghua University),"2321235923,2237048876,2161065209,2106070283,1225778819,2158012360","Social influence in social networks has been extensively researched. Most studies have focused on direct influence, while another interesting question can be raised as whether indirect influence exists between two users who're not directly connected in the network and what affects such influence . In addition, the theory of complex contagion tells us that more spreaders will enhance the indirect influence between two users. Our observation of intensity of indirect influence, propagated by n parallel spreaders and quantified by retweeting probability in two Twitter social networks, shows that complex contagion is validated globally but is violated locally. In other words, the retweeting probability increases non-monotonically with some local drops. A quantum cognition based probabilistic model is proposed to account for these local drops.",2012,Knowledge Discovery and Data Mining,Fields of study: complex contagionsocial influencesocial networkstatistical modelquantum cognition
Towards understanding dominant processes in complex dynamical systems: case of precipitation extremes,Debasish Das (Northeastern University)Auroop Ganguly (Northeastern University)Arindam Banerjee (University of Minnesota)Zoran Obradovic (Temple University),"2296325559,2146888960,2037585042,2029694244","Complex dynamical systems like precipitation extremes under climate variability or change are typically governed by multiple processes at multiple scales. The processes themselves may be manifested at multiple scales and would need to be captured through key indicator variables, which in turn may be better projected by physical models than the variables of interest. We posit that hybrid approaches based on physically-motivated approaches and data-driven methods, which in turn are conditioned on both observations and simulations from large-scale physics-based models, may offer novel and quantifiable insights. The data-driven approaches may need to extend and adapt methods developed and tested in statistics, data mining and machine learning to the concept of dominant processes. In this paper, we performed some exploratory data analysis to characterize the effect of dominant processes on precipitation extremes, annually and seasonally, and from global and century scale to regional and decadal scale, and found some interesting insights that pointed towards need of improved understanding. We identified the gaps in understanding the regional drivers where data-driven methods can make useful improvements and eventually lead to a predictive model for precipitation extremes. Although we do not propose any specific method for solving the problem, we realize that any successful data mining solution should include all or a subset of tools like dimensionality reduction, grouped variable selection, non-linear regression and graphical models. The concepts of dominant processes proposed here would likely generalize broadly to climate extremes while the solution frameworks themselves may generalize beyond climate.",2012,Knowledge Discovery and Data Mining,Fields of study: graphical modelpredictive modellingfeature selectioneconometricsdata miningmachine learningstatisticscomputer science
Developing data mining applications,Geoffrey Holmes (University of Waikato),2189262995,"In this talk I will review several real-world applications developed at the University of Waikato over the past 15 years. These include the use of near infrared spectroscopy coupled with data mining as an alternate laboratory technique for predicting compound concentrations in soil and plant samples, and the analysis of gas chromatography mass spectrometry (GCMS) data, a technique used to determine in environmental applications, for example, the petroleum content in soil and water samples. I will then briefly discuss how experience with these applications has led to the development of an open-source framework for application development.",2012,Knowledge Discovery and Data Mining,Fields of study: rapid application developmentnear infrared spectroscopygas chromatography mass spectrometrydata sciencedata miningcomputer science
Tamper-proof image watermarking using self-embedding,Anamitra Makur (Nanyang Technological University)S. Nikhil Narayan (Nanyang Technological University),"2641444793,2225055092","We propose a fragile watermarking with self-embedding for recovery of tampered image that does not use authentication bits. We use a robust spread spectrum based watermarking scheme using block based embedding, DCT based compression, and other improvements. Simulation results showing recovery performance are presented.",2012,Knowledge Discovery and Data Mining,Fields of study: digital watermarking alliancespread spectrumtheoretical computer scienceinternet privacycomputer visioncomputer science
Opinion interaction network: opinion dynamics in social networks with heterogeneous relationships,Pengyi Fan (National University of Defense Technology)Pei Li (National University of Defense Technology)Hui Wang (National University of Defense Technology)Zhihong Jiang (National University of Defense Technology)Wei Li (National University of Defense Technology),"2143810635,2666510496,2720114341,2121152791,2703898669","Recent empirical studies have discovered that many social networks have heterogeneous relationships, which are signed and weighted relationships between individual nodes. To explore the pattern of opinion dynamics in diverse social networks with heterogeneous relationships, we set up a general agent-based simulation framework named opinion interaction network (OIN), and propose a novel model of opinion dynamics, in which the influence of agents depends on their heterogeneous relationships. Then, by conducting a series of simulations based on OIN, we find that the opinions at steady state depend on the degree of social harmoniousness and average connectivity, and the similar pattern can be observed in the network of Erdos_Renyi, small world and scale free, which illustrates that the topological properties such as short path length, high clustering, and heterogeneous degrees have few effects on opinion dynamics with heterogeneous relationships.",2012,Knowledge Discovery and Data Mining,Fields of study: complex networkdata sciencedata miningartificial intelligencecomputer science
Proceedings of the First ACM International Workshop on Hot Topics on Interdisciplinary Social Networks Research,Xiaoming Fu (University of Göttingen)Peter Gloor (Massachusetts Institute of Technology)Jie Tang (Tsinghua University),"2680142310,2011912922,2158012360","With the blessing of information technology, we are living in an increasingly networked world. People, information and other entities are connected via World Wide Web, email networks, instant messaging networks, mobile communication networks, online social networks, etc. These online networks grow fast and possess huge amount of recorded information, which presents great opportunities in understanding the science of these networks, and in developing new applications from these networks and for these networks. The increasingly networked society has fundamentally changed our way of thinking, individual behaviors and social activities. It is foreseen that the public health relating to epidemic diseases is greatly impacted by this emerging connectivity as they are by nature mediated by direct or indirect human interactions and mobility. However, new challenges have to be met --- the networks are huge and information is noisy, and they demand new methodologies in accessing and analyzing these networks, and in developing theories and applications for the networks. To meet with these challenges, researchers from a wide range of academic fields, including theory and algorithms, data mining and machine learning, computer systems and networks, statistical physics and complex systems, sociology, social psychology, economics and managerial science, etc. are all actively studying various aspects concerning social and information networks. However, we lack the proper opportunities for people from these diverse backgrounds to directly interact with each other. The diversity of approaches and methodologies to study various social networks has raised the need for an interdisciplinary effort to create the required expertise to address the fundamental open questions in this field. This workshop is intended to present such an opportunity and serve as a forum to bring together people from various fields to exchange their latest research results and to sparkle new ideas and directions to properly understand these networks. It will be held on August 12, 2012, in conjunction with ACM KDD 2012, August 12-16, Beijing, China.",2012,Knowledge Discovery and Data Mining,Fields of study: management sciencedata miningmachine learningsimulationcomputer science
Intention oriented itinerary recommendation by bridging physical trajectories and online social networks,Xiangxu Meng (National University of Defense Technology)Xinye Lin (National University of Defense Technology)Xiaodong Wang (National University of Defense Technology),"2308997741,2114849733,2643267474","Compared with traditional itinerary planning, intention oriented itinerary recommendation can provide more flexible activity planning without the user pre-determined destinations and is specially helpful for those strangers in unfamiliar environment. Rank and classification of points of interest (POI) from location based social networks (LBSN) are used to indicate different user intentions. Mining on physical trajectories of vehicles can provide exact civil traffic information for path planning. In this paper, a POI category-based itinerary recommendation framework combining physical trajectories with LBSN is proposed. Specifically, a Voronoi graph based GPS trajectory analysis method is proposed to build traffic information networks, and an ant colony algorithm for multi-object optimization is also implemented to find the most appropriate itineraries. We conduct experiments on datasets from FourSquare and Geo-Life project. A test on satisfaction of recommended items is also performed. Results show that the satisfaction reaches 80% in average.",2012,Knowledge Discovery and Data Mining,Fields of study: point of interestant colony optimization algorithmsmotion planningmulti objective optimizationsocial networkdata miningartificial intelligencesimulationcomputer science
Where do I start?: algorithmic strategies to guide intelligence analysts,Hao Wu (Virginia Tech)Michael Mampaey (Utrecht University)Nikolaj Tatti (University of Antwerp)Jilles Vreeken (University of Antwerp)M. Shahriar Hossain (Virginia Tech)Naren Ramakrishnan (Virginia Tech),"2305209292,2644773209,1367500519,1971070670,2120864034,2199255697","The ""where do I start?"" problem is a veritable one in intelligence analysis. We identify several classes of algorithmic strategies that can supply starting points to analysts in their exploration of a document collection. We present nine methods with origins in association analysis, graph metrics, and probabilistic modeling, and systematically evaluate them over multiple document collections. One of these methods, a novel approach to modeling ""surprise"", is our specific contribution and, further, supports the iterative refinement of suggestions based on user feedback. We demonstrate how these methods guide the analysts to start their investigation on intelligence document collections. Our results reveal selective superiorities of the algorithmic strategies and lead to several design recommendations for creating document exploration capabilities.",2012,Knowledge Discovery and Data Mining,Fields of study: intelligence analysisgenetic associationprinciple of maximum entropystatistical modeltext miningdata sciencedata miningmachine learningstatisticscomputer science
Visual exploration of collaboration networks based on graph degeneracy,Christos Giatsidis (École Polytechnique)Klaus Berberich (École Polytechnique)Dimitrios M. Thilikos (National and Kapodistrian University of Athens)Michalis Vazirgiannis (Athens State University),"1958188848,2064029816,1926143158,1914497179","We demonstrate a system that supports the visual exploration of collaboration networks. The system leverages the notion of fractional cores introduced in earlier work to rank vertices in a collaboration network and filter vertices' neighborhoods. Fractional cores build on the idea of graph degeneracy as captured by the notion of k -cores in graph theory and extend it to undirected edge-weighted graphs. In a co-authorship network, for instance, the fractional core index of an author intuitively reflects the degree of collaboration with equally or higher-ranked authors. Our system has been deployed on a real-world co-authorship network derived from DBLP, demonstrating that the idea of fractional cores can be applied even to large-scale networks. The system provides an easy-to-use interface to query for the fractional core index of an author, to see who the closest equally or higher-ranked co-authors are, and explore the entire co-authorship network in an incremental manner.",2012,Knowledge Discovery and Data Mining,Fields of study: degeneracywindmill graphsimplex graphstrength of a graphquartic graphvoltage graphfriendship graphcomplement graphgraph powerbutterfly graphlevel structurenull graphgraph labelingrandom geometric graphline graphloopgraphdirected graphgraph theorytheoretical computer sciencecombinatoricsdistributed computingmachine learningmathematics
Efficient evaluation of large sequence kernels,Pavel P. Kuksa (NEC)Vladimir Pavlovic (Rutgers University),"371500171,1969768610","Classification of sequences drawn from a finite alphabet using a family of string kernels with inexact matching (e.g., spectrum or mismatch) has shown great success in machine learning. However, selection of optimal mismatch kernels for a particular task is severely limited by inability to compute such kernels for long substrings ( k -mers) with potentially many mismatches ( m ). In this work we introduce a new method that allows us to exactly evaluate kernels for large k , m and arbitrary alphabet size. The task can be accomplished by first solving the more tractable problem for small alphabets, and then trivially generalizing to any alphabet using a small linear system of equations. This makes it possible to explore a larger set of kernels with a wide range of kernel parameters, opening a possibility to better model selection and improved performance of the string kernels. To investigate the utility of large ( k , m ) string kernels, we consider several sequence classification problems, including protein remote homology detection, fold prediction, and music classification. Our results show that increased k -mer lengths with larger substitutions can improve classification performance.",2012,Knowledge Discovery and Data Mining,Fields of study: string kernelsystem of linear equationsmodel selectionspectrumcombinatoricspattern recognitionmachine learningstatisticscomputer sciencemathematics
VOXSUP: a social engagement framework,Yusheng Xie (Northwestern University)Daniel Honbo (Northwestern University)Alok N. Choudhary (Northwestern University)Kunpeng Zhang (Northwestern University)Yu Cheng (Northwestern University)Ankit Agrawal (Northwestern University),"2123854923,2063259713,2147783234,2144435392,2544119651,2160807299","Social media websites are currently central hubs on the Internet. Major online social media platforms are not only places for individual users to socialize but are increasingly more important as channels for companies to advertise, public figures to engage, etc. In order to optimize such advertising and engaging efforts, there is an emerging challenge for knowledge discovery on today's Internet. The goal of knowledge discovery is to understand the entire online social landscape instead of merely summarizing the statistics. To answer this challenge, we have created VOXSUP as a unified social engagement framework. Unlike most existing tools, VOXSUP not only aggregates and filters social data from the Internet, but also provides what we call Voxsupian Knowledge Discovery (VKD). VKD consists of an almost human-level understanding of social conversations at any level of granularity from a single comment sentiment to multi-lingual inter-platform user demographics. Here we describe the technologies that are crucial to VKD, and subsequently go beyond experimental verification and present case studies from our live VOXSUP system.",2012,Knowledge Discovery and Data Mining,Fields of study: topic modelsocial mediasentiment analysisknowledge extractionknowledge managementworld wide webdata miningmachine learningcomputer science
D-INDEX: a web environment for analyzing dependences among scientific collaborators,Claudio Schifanella (University of Turin)Luigi Di Caro (University of Turin)Mario Cataldi (École Centrale Paris)Marie-Aude Aufaure (École Centrale Paris),"17144103,2153458393,2101211219,743058936","In this work, we demonstrate a web application, available at http://d-index.di.unito.it , that permits to analyze the scientific profiles of all the researchers indexed by DBLP by focusing on the collaborations that contributed to define their curricula. The presented application allows the user to analyze the profile of a researcher, her dependence degrees on all the co-authors (along her entire scientific publication history) and to make comparisons among them in terms of dependence patterns. In particular, it is possible to estimate and visualize how much a researcher has benefited from collaboration with another researcher as well as the communities in which she has been involved. Moreover, the application permits to compare, in a single chart, each researcher with all the scientists indexed in DBLP by focusing on their dependences with respect to many other parameters like the total number of papers, the number of collaborations and the length of the scientific careers.",2012,Knowledge Discovery and Data Mining,Fields of study: scientometricsdata scienceworld wide webdata miningcomputer science
Proceedings of the ACM SIGKDD Workshop on Intelligence and Security Informatics,Christopher C. Yang (Drexel University)Hsinchun Chen (University of Arizona)Wenji Mao (Chinese Academy of Sciences)Fei-Yue Wang (Chinese Academy of Sciences),"2128034517,2157743775,2164388523,2157465989",-,2012,Knowledge Discovery and Data Mining,Fields of study: data sciencecomputer science
Modeling Content and Users: Structured Probabilistic Representation and Scalable Inference Algorithms,Amr Ahmed (Yahoo!),2259645355,"Online content have become an important medium to disseminate information and express opinions. With the proliferation of online document collections, users are faced with the problem of missing the big picture in a sea of irrelevant and/or diverse content. In this thesis, we addresses the problem of information organization of online document collections, and provide algorithms that create a structured representation of the otherwise unstructured content. We leverage the expressiveness of latent probabilistic models (e.g. topic models) and non-parametric Bayes techniques (e.g. Dirichlet processes), and give online and distributed inference algorithms that scale to terabyte datasets and adapt the inferred representation with the arrival of new documents. Throughout the thesis, we consider two different domains: research publications and social media (news articles and blog posts); and focus on modeling two facets of content: temporal dynamics and structural correspondence. To model the temporal dynamics of document collections, we introduce a non-parametric Bayes model that we call the recurrent Chinese restaurant process (RCRP). RCRP is a framework for modeling complex longitudinal data, in which the number of mixture components at each time point is unbounded. On top of this process, we develop a hierarchical extension and use it to build an infinite dynamic topic model that recovers the timeline of ideas in research publications. Despite the expressiveness of the aforementioned model, it fails to capture the essential element of dynamics in social media: stories. To remedy this, we developed a multi-resolution model that treats stories as a first-citizen object and combines long-term, high-level topics with short-lived, tightly-focused storylines. Inference in the new model is carried out via a sequential Monte Carlo algorithm that processes new documents on real time. We then consider the problem of structural correspondence in document collections both across modalities and communities. In research publications, this problem arises due to the multi-modalities of research papers and the pressing need for developing systems that can retrieve relevant documents based on any of these modalities (e.g. figures, text, named entities, to name a few). In social media this problem arises due to ideological bias of the document's author that mixes facts with opinions. For both problems we develop a series of factored models. In research publications, the developed model represents ideas across modalities and as such can solve the aforementioned retrieval problem. In social media, the model contrasts the same idea across different ideologies, and as such can explain the bias of a given document on a topical-level and help the user staying informed by providing documents that express alternative views. Finally, we address the problem of inferring users' intent when they interact with document collections, and how this intent changes over time. The induced user model can then be used in matching users with relevant content.",2012,Knowledge Discovery and Data Mining,Fields of study: automatic meter readingdata scienceworld wide webdata miningmachine learningstatisticscomputer sciencemathematics
ACM SIGSOFT Impact Paper Award 2012: Systematic Software Testing: The Korat Approach,Chandrasekhar Boyapati (Google)Sarfraz Khurshid (University of Texas at Austin)Darko Marinov (University of Illinois at Urbana–Champaign),"1561335754,2103685797,1702668839",-,2012,Knowledge Discovery and Data Mining,Fields of study: human computer interactioncomputer science
Cross-media knowledge discovery,Zhongzhi Shi (Chinese Academy of Sciences),2112067332,"In this talk I introduce cloud computing based cross-media knowledge discovery. We propose a framework for cross-media semantic understanding which contains discriminative modeling, generative modeling and cognitive modeling. In cognitive modeling a new model entitled CAM is proposed which is suitable for cross-media semantic understanding. We develop an agent-aid model for load balance in cloud computing environment. For quality of service we present a utility function to evaluate the cloud performance. A Cross-Media Intelligent Retrieval System (CMIRS), which is managed by ontology-based knowledge system KMSphere, will be illustrated. Finally, the directions for further researches on cloud computing based cross-media knowledge discovery will be pointed out and discussed.",2012,Knowledge Discovery and Data Mining,Fields of study: discriminative modelcognitive modelcloud computingload balancingquality of serviceknowledge extractionknowledge based systemsdata scienceknowledge managementdata miningmachine learningcomputer science
An expected utility-based approach for mining action rules,Peng Su (Shandong jianzhu university 山東建築大學)Dunfeng Li (Shandong University)Kun Su (Shandong University),"2689070303,2231536835,2697376654","One of the central issues in data mining community is to make the mined patterns actionable. Action rules are those actionable patterns, which provide hints to a user what actions (i.e., changes within some values of flexible attributes) should be taken to reclassify some objects from an undesired decision class to a desired one. Both changing the value of a flexible attribute and the corresponding change of the value of a decision attribute may incur cost (negative utility) or bring benefit (positive utility) for the user. Obviously, the user is more interested in the rules which are expected to bring higher utility. In this paper, we formally define the expected utility of an action rule for measuring its interestingness. Our definitions explicitly state the problem of mining action rules as a search problem in a framework of support and expected utility. We also propose an effective algorithm for mining action rules with higher expected utilities. Our experiment shows the usefulness of the proposed approach.",2012,Knowledge Discovery and Data Mining,Fields of study: expected utility hypothesisdata scienceknowledge managementdata miningmathematics
Watermark based recovery of tampered documents,Anamitra Makur (Nanyang Technological University)Govindarajan Sridharan (Nanyang Technological University),"2641444793,2232037438","In this work recovery of text documents is addressed. Text documents are watermarked using pixel flipping based watermarking and self-embedding for not only authentication but eventual recovery in presence of tampering. A scheme is proposed that recovers against all possible editing attacks such as substitution, insertion and deletion of characters. Algorithms for tamper detection and recovery are suggested and their functioning explained with detailed examples.",2012,Knowledge Discovery and Data Mining,Fields of study: recoveryauthenticationinternet privacyworld wide webcomputer securitycomputer science
Proceedings of the 11th International Workshop on Data Mining in Bioinformatics,Jake Chen (Indiana University – Purdue University Indianapolis)Mohammed J. Zaki (Rensselaer Polytechnic Institute)Tamer Kahveci (University of Florida)Saeed Salem (North Dakota State University)Mehmet Koyutürk (Case Western Reserve University),"2662611124,2165917828,2633491540,2164451553,128763826","The past two decades have witnessed rapid technological advances in biological data collection and acquisition. These advances in biotechnology enabled interrogation of cellular systems at various levels, leading to generation and collection of large-scale biological data (mostly in public databases) at an exponential rate. The explosion of biological data is leading to a paradigm shift in research methods in life sciences; from hypothesis-driven research to data driven research. In the last decade, sophisticated algorithms for knowledge discovery and data mining have demonstrated great promise in extracting novel biological information from complex, heterogeneous, and very high-dimensional biological datasets. The International Workshop on Data Mining in Bioinformatics (BIOKDD) has successfully established a tradition in providing a platform for the presentation and discussion of advances in data mining techniques that primarily target biological data in the last ten years. This year's BIOKDD continued the tradition of bringing together data mining researchers and life scientists, emphasizing novel problems with various types of biological data, emerging types of biological data (e.g., next generation sequencing, RNA-seq, electronic medical records), integration of heterogenous biological datasets, and emerging applications in the new era of life and medical sciences (e.g., systems biology, genome-wide association studies, translational science, personalized genomics).",2012,Knowledge Discovery and Data Mining,Fields of study: data sciencebioinformaticsdata miningcomputer science
Maximizing return and minimizing cost with the right decision management systems,Rich Holada (IBM),2222981935,"The ability to achieve operational efficiency, product leadership, and customer intimacy still eludes many organizations due, in large part, to the chaos of business. Inconsistent prioritization and decision making; poor visibility between systems; processes that are not well controlled; and individual front-line decisions that seem small but, in totality, have a huge impact make it difficult for organizations to link strategy to execution and back. During this presentation, we will demonstrate how automating and optimizing decisions (operational efficiency) with business rules and predictive models enables better data driven results across the enterprise, and how this is implemented at the point of impact (customer intimacy) to transform an organization and support market leadership.",2012,Knowledge Discovery and Data Mining,Fields of study: business rule management systembusiness decision mappingbusiness rulepredictive analyticsreturn on investmentmanagement systempredictive modellingcustomer relationship managementmanagement scienceknowledge managementdata miningmachine learningcomputer science
Personalized resource categorisation in folksonomies,Muzaffer Ege Alper (Istanbul Technical University)Şule Gündüz Öğüdücü (Istanbul Technical University),"2032931501,2163128979","Folksonomies constitute an important type of Web 2.0 services, where users collectively annotate (or ""tag"") resources to create custom categories. Semantic relation of these categories hint at the possibility of another categorization at a higher level. Discovering these more general categories, called ""topics"", is an important task. One problem is to discover these semantically coherent topics and the accompanying small sets of tags that cover these topics in order to facilitate more detailed item search. Another important problem is to find words/phrases that describe these topics, i.e. labels or ""meta-tag""s. These labeled topics can immensely increase the item search efficiency of users in a folksonomy service. However, this possibility has not been sufficiently exploited to date. In this paper, a probabilistic model is used to identify topics in a folksonomy, which are then associated with relevant, descriptive meta-tags. In addition, a small set of diverse and relevant tags are found which cover the semantics of the topic well. The resulting topics form a personalized categorization of folksonomy data due to the personalized nature of the model employed. The results show that the proposed method is successful at discovering important topics and the corresponding identifying meta-tags.",2012,Knowledge Discovery and Data Mining,Fields of study: topic modelstatistical modelworld wide webinformation retrievaldata miningcomputer science
Building an engine for big data,Masaru Kitsuregawa (University of Tokyo),390930505,IT program in Japan to build powerful engine for big data was launched. Quite recently the initial version is commercialized. This presentation will give a brief overview of the project. Also some of the potential applications will be introduced.,2012,Knowledge Discovery and Data Mining,Fields of study: search enginedata scienceworld wide webdata miningcomputer science
Proceedings of the 1st International Workshop on Cross Domain Knowledge Discovery in Web and Social Network Mining,Bo Long (Yahoo!)Yi Chang (Yahoo!)Hang Li (Microsoft),"2512786853,2168000538,2128739099","In the field of Web and social network mining, more and more learning tasks can easily acquire multiple data sets from various domains. For example, a modern search engine system often conducts ranking learning tasks in various domains with different languages (e.g., English text search, Spanish text search, etc.), or different verticals/topics (e.g., news search, product search, etc.); and recently recommendation systems start to leverage multiple types of user data from different domains, such as user browsing history data, user shopping record data, and user social network data. At the same time, the need for knowledge transfer is increasingly evident as many new datasets, or parts of data, are only very sparsely annotated. Different from traditional single-domain learning problems based on the assumption that training and test data are drawn from identical distribution, cross domain learning problems are built on multiple domain data that may have different degrees of relatedness to target tasks, offering an opportunity to help one another. To better leverage multiple domain data, mining and transferring of shared knowledge across multiple domains is likely to become a crucial step in Web and social network mining in the future.",2012,Knowledge Discovery and Data Mining,Fields of study: data stream miningdata scienceworld wide webdata miningmachine learningcomputer science
Proceedings of the Thirteenth International Workshop on Multimedia Data Mining,Aaron Baughman (IBM)Jiang (John) Gao (Google)Jia-Yu Pan (Google)Fang Chu (University of Pittsburgh)Yizhou Wang (New Jersey Institute of Technology),"2005987481,2488008490,2159823517,2616599985,2615259783","The theme of the MDMKDD'13 workshop is\mining large scale rich content in a networked society."" The workshop brought together experts in computational analysis on digital media content and multimedia databases, as well as knowledge engineers and domain experts from different applied disciplines with potential in multimedia data mining. The new theme of this year's workshop highlights the integration of multimedia in people's daily lives. With the rapid growth of mobile devices and personalized data, we wanted the workshop to focus on mining large scale rich content which is abundant in today's networked societies. Vast amount of multimedia data are produced, shared, and accessed everyday in various social platforms. These multimedia objects (images, videos, texts, tags, etc.) represent rich, multifaceted recordings of human behavior in the networked society, which lead to a range of social applications such as consumer behavior forecasting, social driven advertising / business, local knowledge discovery (e.g., for tourism or shopping), detection of emergent news events and trends, and so on. In addition to techniques for mining single media items, all of these applications require new methods for extracting robust features and discovering stable relationships among different media modalities and the users, in a dynamic, social context rich, and likely noisy environment. Multimedia is designed to stimulate the human senses beyond text. Using the rich content in multiple media types, we can capture the full experience of an event. Mining the data within media rich platforms and immersive environments (such as online communities, blogs, social networks, and virtual worlds) integrates the digital and physical experiences and information. The aim of the MDMKDD'13 workshop is to explore the role that multimedia data mining can play to enhance virtual society experiences.",2012,Knowledge Discovery and Data Mining,Fields of study: data sciencemultimediaworld wide webdata miningcomputer science
China's national personal credit scoring system: a real-life intelligent knowledge application,Yong Shi (Chinese Academy of Sciences),1986476467,"Credit Reference Centre (CRC) of People's Bank of China (PBC) has built a big data: the largest personal credit database in the world with 800 million people's accounts collected from all commercial banks in China since 2003. From June 2006 to Sept 2009, Research Centre on Fictitious Economy and Data Science, Chinese Academy of Sciences (CASFEDS) and CRC jointly developed China's National Personal Credit Scoring System, known as ""China Score"", which is a unique and advanced KDD application under intelligent knowledge management on this big data. The system will be eventually serving all 1.3 billion population of China for their daily financial activities, such as bank accounts, credit card application, mortgage, personal loans, etc. It can become one of the most influential events of KDD techniques to human kind. This talk will introduce the key components of China Score project that includes objectives, modeling process, KDD techniques used in the projects, intelligent knowledge management and experience of the project development. In addition, the talk will also outline a number of policy recommendations based on China Score project which has been potentially impacting Chinese Government on its strategic decision making for China's economic developments.",2012,Knowledge Discovery and Data Mining,Fields of study: credit referencecredit historyobject modelactuarial sciencedata miningcomputer science
Experience with discovering knowledge by acquiring it,Paul Compton (University of New South Wales),2130286701,"Machines and people have complementary skills in Knowledge Discovery. Automated techniques can process enormous amounts of data to find new relationships, but generally these are represented by fairly simple models. On the other hand people are endlessly inventive in creating models to explain data at hand, but have problems developing consistent overall models to explain all the data that might occur in a domain; and the larger the model, the more difficult it becomes to maintain consistency. Ripple-Down Rules is a technique that has been developed to allow people to make real-time updates to a model whenever they notice some data that the model does not yet explain, while at the same time maintaining consistency. This allows an entire knowledge base to be built while it is already in use by making updates. There are now 100s of Ripple-Down-Rule knowledge bases in use and this paper presents some observations from log files tracking how people build these systems, and also outlines some recent research on how such techniques can be used to add greater specificity to the simpler models developed by automated techniques.",2012,Knowledge Discovery and Data Mining,Fields of study: knowledge extractionknowledge basedata sciencedata miningartificial intelligencemachine learningcomputer science
Contraction network for solving maximum flow problem,Shu Zhao (Anhui University)Xiansheng Xu (Anhui University)Bo Hua (Anhui University)Yanping Zhang (Anhui University),"2681126617,2134694850,2668970275,2296583798","In this paper, we present a novel idea to solve maximum flow problem in the directed networks. Given a directed flow network which we call original network here, we propose a method of Contracting Neighbor-node-set Approach (CNA) based on Quotient Space Theory. It selects some nodes from fine-grained original network as equivalence class to form a new node in the new network. The coarse-grained new network is called quotient network. Then we apply classic algorithms on quotient network to solve the maximum flow of original network. The performance in practice of our algorithm is better than that of classic algorithms and the method of simplifying network. Experimental results show that CNA can approximate accurately solve the maximum flow of original network. The correctness of maximum flow is over 95%. At the same time, the node scale reduces to 55.45% of node scale of original network and edge scale reduces to 32.10% of edge scale of original network, averagely.",2012,Knowledge Discovery and Data Mining,Fields of study: circulation problemminimum cost flow problemmulti commodity flow problemmaximum flow problemflow networksocial networkcombinatoricsartificial intelligencemachine learningalgorithmmathematics
EvaPlanner: an evacuation planner with social-based flocking kinetics,Cheng-Te Li (National Taiwan University)Shou-De Lin (National Taiwan University),"2139086518,2114357324","This paper demonstrates a system that exploits graph mining, social network analysis, and agent-based crowd simulation techniques to investigate the evacuation dynamics during fire emergency. We create a novel evacuation planning system, EvaPlanner , to deal with three tasks. First, the system identifies the preferable locations to establish the exits to facilitate efficient evacuation from the dangerous areas. Second, it determines the most effective positions to place the emergency signs such that panic crowd can quickly find the exits. Third, it faithfully simulates the evacuation dynamics of crowd considering not only the individual movement kinetics but also the social connections between people. EvaPlanner provides a flexible experimental platform for investigating the evacuation dynamics under a variety of settings, and can further be utilized for animation and movie production. In addition, it can serve as a tool to assist architects address the safety concern during the planning phase. The demo system can be found in the link: http://mslab.csie.ntu.edu.tw/evaplanner/",2012,Knowledge Discovery and Data Mining,Fields of study: crowd simulationsocial network analysissocial networkkineticscomputer securityartificial intelligencesimulation
Bayesian relational data analysis,Naonori Ueda (Nippon Telegraph and Telephone),2147735823,"Recently there have been many collections of relational data in diverse areas such as the internet, social networks, customer shopping records, bioinformatics, etc. The main goal of the relational data analysis is to discover latent structure from the data. The conventional data mining algorithms based on exhaustive enumeration have an inherent limitation for this purpose because of the combinatorial nature of the methods. In contrast, in machine learning a lot of statistical models have been proposed for the relational data analysis. In this talk, first I will review the statistical approach, especially Bayesian approach, for the relational data analysis with recent advancements in machine learning literature. Then, as a future research I will also talk about a statistical approach for combining multiple relational data.",2012,Knowledge Discovery and Data Mining,Fields of study: change data captureentity relationship modeldata modelbayesian probabilitystatistical relational learningdata sciencedata miningmachine learningstatisticscomputer science
A new challenge of information processing under the 21st century,Bo Zhang (Tsinghua University),2438699337,"In web era, we are confronted with a huge amount of raw data and a tremendous change of man-machine interaction modes. We have to deal with the content (semantics) of data rather than their form alone. Traditional information processing approaches face a new challenge since they cannot deal with the semantic meaning or content of information. But humans can handle such a problem easily. So it's needed a new information processing strategy that correlated with the content of information by learning some mechanisms from human beings. Therefore, we need (1) a set of robust detectors for detecting semantically meaningful features such as boundaries, shapes, etc. in images, words, sentences, etc. in text, and (2) a set of methods that can effectively analyze and exploit the information structures that encode the content of information. During the past 40 years the probability theory has made a great progress. It has provided a set of mathematical tools for representing and analyzing information structures. In the talk we will discuss what difficulty we face, what we can do, and how we should do in the content-based information processing.",2012,Knowledge Discovery and Data Mining,Fields of study: information algebrainformation processorinformation filtering systeminteraction informationinformation mappinginformation qualitysemantic webinformation integrationinformation needsprobability theoryinformation processingnatural language processinginformation retrievaldata miningmachine learningstatisticscomputer science
NASA: achieving lower regrets and faster rates via adaptive stepsizes,Hua Ouyang (Georgia Institute of Technology)Alexander G. Gray (Georgia Institute of Technology),"2135169950,2112810595","The classic Stochastic Approximation (SA) method achieves optimal rates under the black-box model. This optimality does not rule out better algorithms when more information about functions and data is available. We present a family of Noise Adaptive Stochastic Approximation (NASA) algorithms for online convex optimization and stochastic convex optimization. NASA is an adaptive variant of Mirror Descent Stochastic Approximation. It is novel in its practical variation-dependent stepsizes and better theoretical guarantees. We show that comparing with state-of-the-art adaptive and non-adaptive SA methods, lower regrets and faster rates can be achieved under low-variation assumptions.",2012,Knowledge Discovery and Data Mining,Fields of study: stochastic gradient descentadaptive learningstochastic approximationstochastic optimizationconvex optimizationtheoretical computer sciencemachine learningmathematical optimizationmathematics
Algorithms for mining uncertain graph data,Jianzhong Li (Harbin Institute of Technology),2130201582,"With the rapid development of advanced data acquisition techniques such as high-throughput biological experiments and wireless sensor networks, large amount of graph-structured data, graph data for short, have been collected in a wide range of applications. Discovering knowledge from graph data has witnessed a number of applications and received a lot of research attentions. Recently, it is observed that uncertainties are inherent in the structures of some graph data. For example, protein-protein interaction (PPI) data can be represented as a graph, where vertices represent proteins, and edges represent PPI's. Due to the limits of PPI detection methods, it is uncertain that a detected PPI exist in practice. Other examples of uncertain graph data include topologies of wireless sensor networks, social networks and so on. Managing and mining such large-scale uncertain graph data is of both theoretical and practical significance. Many solid works have been conducted on uncertain graph mining from the aspects of models, semantics, methodology and algorithms in last few years. A number of research papers on managing and mining uncertain graph data have been published in the database and data mining conferences such as VLDB, ICDE, KDD, CIKM and EDBT. This talk focuses on the data model, semantics, computational complexity and algorithms of uncertain graph mining. In the talk, some typical research work in the field of uncertain graph mining will also be introduced, including frequent subgraph pattern mining, dense subgraph detection, reliable subgraph discovery, and clustering on uncertain graph data.",2012,Knowledge Discovery and Data Mining,Fields of study: molecule miningwait for graphgraph databasepower graph analysisdata modeldata stream miningdata sciencedata miningmachine learningcomputer science
Keyword-propagation-based information enriching and noise removal for web news videos,Jun Zhang (Tsinghua University)Xiaoming Fan (Tsinghua University)Jianyong Wang (Tsinghua University)Lizhu Zhou (Tsinghua University),"2641497478,2222601610,2105625159,2286125848","The volume of Web videos have increased sharply through the past several years because of the evolvement of Web video sites.Enhanced algorithms on retrieval, classification and TDT (abbreviation of Topic Detection and Tracking ) can bring lots of convenience to Web users as well as release tedious work from the administrators. Nevertheless, due to the the insufficiency of annotation keywords and the gap between video features and semantic concepts, it is still far away from satisfactory to implement them based on initial keywords and visual features. In this paper we utilize a keyword propagation algorithm based on manifold structure to enrich the keyword information and remove the noise for videos. Both text similarity and temporal similarity are employed to explore the relationship between any pair of videos and to construct the propagation model. We explore three applications, i.e., TDT, Retrieval and Classification based on a Web news video dataset obtained from a famous online video-distributing website, YouKu , and evaluate our approach. Experimental results demonstrate that they achieve satisfactory performance and always outperform the baseline methods.",2012,Knowledge Discovery and Data Mining,Fields of study: multimediaworld wide webinformation retrievaldata miningmachine learningcomputer science
Leveraging predictive modeling to reduce signal theft in a multi-service organization environment,Seymour Douglas (Deloitte),2673709183,"Signal theft can be defined as the interdiction, consumption or usage of carrier signal from a provider's network without payment or payment of an amount less than the level of service consumed. High levels of signal theft can potentially reflect open technical network issues, failure of electronic countermeasures or operational gaps that are estimated to cost the cable industry providers more than $5 billion annually. This session will discuss the business challenges associated with the quantification of signal theft-related losses, outline some of the countermeasures taken by MSOs, and then provide views on the development of predictive models to help identify the potential likelihood of signal theft in a given environment. We will examine the performance of certain machine learning algorithms as well as data challenges associated with both the architecture construction and analytical efforts, and conclude with a lessons-learned discussion and views on future approaches.",2012,Knowledge Discovery and Data Mining,Fields of study: analytic signallevel of servicepredictive modellingdata analysiscomputer securitydata miningmachine learningsimulationcomputer science
Efficient distributed computation of human mobility aggregates through user mobility profiles,Mirco Nanni (Istituto di Scienza e Tecnologie dell'Informazione)Roberto Trasarti (Istituto di Scienza e Tecnologie dell'Informazione)Giulio Rossetti (Istituto di Scienza e Tecnologie dell'Informazione)Dino Pedreschi (Istituto di Scienza e Tecnologie dell'Informazione),"2110486381,2193623800,2120671011,2429417668","A basic task of urban mobility management is the real-time monitoring of traffic within key areas of the territory, such as main entrances to the city, important attractors and possible bottlenecks. Some of them are well known areas, while while others can appear, disappear or simply change during the year, or even during the week, due for instance to roadworks, accidents and special events (strikes, demonstrations, concerts, new toll road fares). Especially in the latter cases, it would be useful to have a traffic monitoring system able to dynamically adapt to reference areas specified by the user. In this paper we propose and study a solution exploiting on-board location devices in private cars mobility, that continuously trace the position of the vehicle and periodically communicate it to a central station. Such vehicles provide a statistical sample of the whole population, and therefore can be used to compute a summary of the traffic conditions for the mobility manager. However, the large mass of information to be transmitted and processed to achieve that might be too much for a real-time monitoring system, the main problem being the systematic communication from each vehicle to a unique, centralized station. In this work we tackle the problem by adopting the general view of distributed systems for the computation of a global function, consisting in minimizing the amount of information communicated through a careful coordination of the single nodes (vehicles) of the system. Our approach involves the use of predictive models that allow the central station to guess (in most cases and within some given error threshold) the location of the monitored vehicles and then to estimate the density of key areas without communications with the nodes.",2012,Knowledge Discovery and Data Mining,Fields of study: error thresholdmobility modelpredictive modellingcomputer securitydata miningsimulationcomputer science
Efficient Algorithms for Detecting Genetic Interactions in Genome-Wide Association Study,Xiang Zhang (Case Western Reserve University),2553248206,-,2012,Knowledge Discovery and Data Mining,Fields of study: demographycomputer science
Interaction and collective intelligence in internet computing,Deyi Li (Chinese Academy of Engineering),2584309846,"Network interconnection, information interoperability, and crowds interaction on the Internet could inspire better computation models than Turing machine, since that human plays an important factor in Internet computing, so that the human-machine and machine-machine interactions have evolved to be the kernel of Internet computing. Internet has not been simply equivalent to a virtual huge computer, or a set of computers. On the Internet, human's behaviors are uncertain, the interactions and influence among people are also uncertain. These uncertainties cannot be described by Turing machine and traditional interaction machine. As a new computation platform, Internet computing requires new theories and methods. By combining topology in mathematics with the field theory in physics, we propose the topological potential approach, which set up a virtual field by the topological space to reflect individual activities, local effects and preferential attachment. This approach can be used to research the emergence of collective intelligence. Here, we introduce three case studies to illustrate the analysis on the collective intelligence on the Internet and discuss some potential applications of the topological potential approach.",2012,Knowledge Discovery and Data Mining,Fields of study: topological spacecollective intelligenceturing machinefield theorycomputer simulationtheoretical computer sciencedata miningartificial intelligencemachine learningsimulationcomputer sciencemathematics
Ensembles and model delivery for tax compliance,Graham Williams (Australian Taxation Office),2106893266,"Revenue authorities characteristically have large stores of historic audit data, with outcomes, ready for analysis. The Australian Taxation Office established one of the largest data mining teams in Australia in 2004 as a foundation to becoming a knowledge-based organization. Today, every tax return lodged in Australia is risk assessed by one or more models developed through data mining, generally based on historic data. We observe that any of the traditional modeling approaches, particularly including random forests, generally deliver similar models in terms of accuracy. We take advantage of combining different model types and modeling approaches for risk scoring, and in particular report on recent research that increases the diversity of trees that make up a random forest. We also review, in a practical context, how such models are evaluated and delivered.",2012,Knowledge Discovery and Data Mining,Fields of study: random forestknowledge basedata miningmachine learningcomputer science
Social tie mining in company networks,Xun Zheng (Tsinghua University)Yang Yang (Tsinghua University)Yanting Zhao (Tsinghua University),"2296583651,2673426936,2231433081","Detecting competitive relationship plays a key role in a company's strategy formulation. Despite the importance, however, there are few automatic approaches to this problem, except several methods based on web data, which inevitably involve data noise. The challenging points of this problem are (1) to look for a set of data that can represent general information of the companies appropriately and (2) to figure out how underlying factors influence the competitive relationships between companies. In this paper, we choose patent records as data source and propose a novel approach to competitor detection problem. Specifically, we construct a patent network and employ an algorithm based on Random walk with restarts (RWR). Experimental results validate the of the algorithm. We further discuss to what extent the underlying factors influence the competitive relationship and finally demonstrate several representative results through case study.",2012,Knowledge Discovery and Data Mining,Fields of study: random walkweb miningdata miningmachine learningstatisticscomputer science
Similarity search in real world networks,Cuiping Li (Renmin University of China),2129719679,"Recently there has been a lot of interest in graph-based analysis. One of the most important aspects of graph-based analysis is to measure similarity between nodes and to do similarity search in a graph. For example, in social networks such as Facebook, system may want to recommend potential friends to a particular user based on connections between users. In custom-product networks such as eBay, one may wish to recommend products to others based on purchases history. In this talk, I will introduce some methods on vertex similarities computations and their applications on similarity search in real world networks.",2012,Knowledge Discovery and Data Mining,Fields of study: similarity heuristicnetwork sciencesemantic similaritysocial networkworld wide webinformation retrievaldata miningmathematics
Proceedings of the First International Workshop on Software Mining,Ming Li (Nanjing University)Hongyu Zhang (Tsinghua University)David Lo (Singapore Management University),"2252453783,2619848081,2132927693","Software systems have been playing important roles in business, scientific research, and our everyday lives. It is critical to improve both software productivity and quality, which are major challenges to software engineering researchers and practitioners. In recent years, software mining has emerged as a promising means to address these challenges. It has been successfully applied to discover knowledge from software artifacts (e.g., specifications, source code, documentations, execution logs, and bug reports) to improve software quality and development process (e.g., to obtain the insights for the causes leading to poor software quality, to help software engineers locate and identify problems quickly, and to help the managers optimize the resources for better productivity). Software mining has attracted much attention in both software engineering and data mining communities. The first International Workshop on Software Mining (SoftwareMining-2012) aims to bridge research in the data mining community and software engineering community by providing an open and interactive forum for researchers who are interested in software mining to discuss the methodologies and technical foundations of software mining, approaches and techniques for mining various types of software-related data, applications of data mining to facilitate specialized tasks in software engineering. The participants of diverse background in either data mining or software engineering can benefit from this workshop by sharing their expertise, exchanging ideas and discussing new research results.",2012,Knowledge Discovery and Data Mining,Fields of study: software engineering process grouppackage development processcrowdsourcing software developmentsoftware peer reviewsocial software engineeringlong term supportsoftware analyticssoftware miningsoftware walkthroughsoftware constructionteam software processpersonal software processsoftware requirementssoftware deploymentsoftware qualitysoftware developmentsoftware systemdata sciencedata miningcomputer science
Proceedings of the 1st International Workshop on Context Discovery and Data Mining,Jilei Tian (BMW)Alvin Chin (BMW)Enhong Chen (University of Science and Technology of China),"2622187931,2275715398,2136372366","Welcome to the 1st International Workshop on Context Discovery and Data Mining (ContextDD 2012). The workshop was held in Beijing, China, on August 2012, in conjunction with the 18th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD 2012). ContextDD 2012 is a forum to discuss the research issues and challenges within rich context mobile computing from both an academic and industrial research perspective. Some of these research issues and challenges include: context data collection, rich context data definition, context sensing, context recognition, context aggregation, context segmentation, context management and inference, context fusion, context awareness methods, context frameworks, context datasets and algorithms for mining user behavior and other intelligence. Applications of this includes context recommendation, context-aware personalized smart applications and services, location based services, etc. The workshop aims to attract and bring together researchers working in data mining and retrieval, mobile computing and pervasive computing to attend, as well as students, professors, and industry researchers from leading universities and research labs internationally that are doing work in the context, sensing, mobile, social and pervasive computing areas.",2012,Knowledge Discovery and Data Mining,Fields of study: data scienceworld wide webdata miningcomputer science
Back-buy prediction based on TriFG,Xiaoqin Xie (Harbin Engineering University)Li Li (Harbin Engineering University)Zhiqiang Zhang (Harbin Engineering University)Yixiang Fang (Harbin Engineering University),"2706943170,2636731856,2654216231,2277390951","Reciprocal Relationship in twitter can be predicted by TriFG model. Based on this model, we study the extent to which the formation of a two-way relationship can be predicted in a dynamic e-commerce web site which is composed of products and customers, especially the back-buy behavior. Back-buy behavior represents a more stable interest direction of customers. Understanding the formation of back-buy behavior can provide us insights into the potential e-commerce trends and lead to more efficient advertisement. In this paper, we propose a learning framework to formulate the problem of back-buy prediction into a graphical model---Back-buy model (BBModel). Employing such e-shopping sites as Amazon as a source for our experimental data, BBModel can predict the probability of the customers back and buy the products. Finally, the experiments show that the BBModel is feasible and effective for prediction. The two-way relationship prediction can fit to the e-commerce web site.",2012,Knowledge Discovery and Data Mining,Fields of study: graphical modelpredictive modellinge commercedata miningmachine learningsimulationcomputer science
Diversity in ranking using negative reinforcement,Rama Badrinath (Indian Institute of Science)C. E. Veni Madhavan (Indian Institute of Science),"2073134400,502999431","In this paper, we consider the problem of diversity in ranking of the nodes in a graph. The task is to pick the top-k nodes in the graph which are both 'central' and 'diverse'. Many graph-based models of NLP like text summarization, opinion summarization involve the concept of diversity in generating the summaries. We develop a novel method which works in an iterative fashion based on random walks to achieve diversity. Specifically, we use negative reinforcement as a main tool to introduce diversity in the Personalized PageRank framework. Experiments on two benchmark datasets show that our algorithm is competitive to the existing methods.",2012,Knowledge Discovery and Data Mining,Fields of study: rankingreinforcementrandom walkautomatic summarizationworld wide webdata miningmachine learningstatisticscomputer science
Nine real hard problems we'd like you to solve,Robin Li (Baidu),2688531413,"Since 2000, Baidu has set its mission as providing the best way for people to find what they're looking for. Today, the company has become the world's largest Chinese search engine. Everyday, we process billions of search queries and serve hundreds of millions of internet users. The huge volume of online text and multimedia content, as well as user log data, provide us unprecedented opportunities and challenges for further accomplishing our mission. In addition, we have seen several megatrends, cloud computing is becoming a pervasive service infrastructure, mobile internet will surpass traditional internet in user time spend, and social platform has demonstrated its great power. In response to all these opportunities, challenges, and megatrends, we must think ahead on the major technology focuses that may help Baidu to serve our users better. In this talk, I would like to share with the audience the nine areas that are the most important and interesting in my mind. For each of the areas, I will describe the challenges and explain why it is important and interesting. I hope the research community can get excited, and help us provide better services for users.",2012,Knowledge Discovery and Data Mining,Fields of study: cloud computingsearch engineinternet privacyworld wide webdata miningsimulationcomputer science
Understanding users' satisfaction for search engine evaluation,Gordon Sun (Tencent),2308389364,"To fulfill users' search needs, the search engine must have good performance, easy-to-use functionalities, and good search result quality. Search quality evaluation becomes challenging when users' satisfaction may not be able to judge by a single search and even within a single search judgments from various sources are not consistent. In this talk, I will discuss how user's satisfaction is decomposed into different components in general, and how we measure them with various means - human judgment, automatic computation with query log, and outsourcing, and their pros and cons with operational implications. For an outlook, I will postulate potential evaluation approaches for a better user's satisfaction.",2012,Knowledge Discovery and Data Mining,Fields of study: search analyticssearch engineworld wide webinformation retrievaldata miningcomputer science
A Linear Ensemble of Individual and Blended Modelsfor Music Rating Prediction,P.-L. Chen al,2609216077,-,2012,Knowledge Discovery and Data Mining,Fields of study: data sciencespeech recognitionmachine learning
Towards linear time overlapping community detection in social networks,Jierui Xie (Rensselaer Polytechnic Institute)Boleslaw K. Szymanski (Rensselaer Polytechnic Institute),"2162938309,1974741261","Membership diversity is a characteristic aspect of social networks in which a person may belong to more than one social group. For this reason, discovering overlapping structures is necessary for realistic social analysis. In this paper, we present a fast algorithm, called SLPA, for overlapping community detection in large-scale networks. SLPA spreads labels according to dynamic interaction rules. It can be applied to both unipartite and bipartite networks. It is also able to uncover overlapping nested hierarchy . The time complexity of SLPA scales linearly with the number of edges in the network. Experiments in both synthetic and real-world networks show that SLPA has an excellent performance in identifying both node and community level overlapping structures.",2012,Knowledge Discovery and Data Mining,Fields of study: social psychologymachine learningmathematics
Instant social graph search,Sen Wu (Tsinghua University)Jie Tang (Tsinghua University)Bo Gao (Tsinghua University),"2443423181,2158012360,2649313175","In this paper, we study a new problem of instant social graph search, which aims to find a sub graph that closely connects two and more persons in a social network. This is a natural requirement in our real daily life, such as ""Who can be my referrals for applying for a job position?"". In this paper, we formally define the problem and present a series of approximate algorithms to solve this problem: Path, Influence, and Diversity. To evaluate the social graph search results, we have developed two prototype systems, which are online available and have attracted thousands of users. In terms of both user's viewing time and the number of user clicks, we demonstrate that the three algorithms can significantly outperform (+34.56%-+131.37%) the baseline algorithm.",2012,Knowledge Discovery and Data Mining,Fields of study: critical graphconnectivityworld wide webdata miningmachine learningsimulationcomputer sciencemathematics
Towards personalized context-aware recommendation by mining context logs through topic models,Kuifei Yu (Chinese Academy of Sciences)Baoxian Zhang (Chinese Academy of Sciences)Hengshu Zhu (University of Science and Technology of China)Huanhuan Cao (University of Science and Technology of China)Jilei Tian (BMW),"2104257765,2697206278,2098414524,2101313578,2098156231","The increasing popularity of smart mobile devices and their more and more powerful sensing ability make it possible to capture rich contextual information and personal context-aware preferences of mobile users by user context logs in devices. By leveraging such information, many context-aware services can be provided for mobile users such as personalized context-aware recommendation. However, to the best knowledge of ours, how to mine user context logs for personalized context-aware recommendation is still under-explored. A critical challenge of this problem is that individual user's historical context logs may be too few to mine their context-aware preferences. To this end, in this paper we propose to mine common context-aware preferences from many users' context logs through topic models and represent each user's personal context-aware preferences as a distribution of the mined common context-aware preferences. The experiments on a real-world data set contains 443 mobile users' historical context data and activity records clearly show the approach is effective and outperform baselines in terms of personalized context-aware recommendation.",2012,Knowledge Discovery and Data Mining,Fields of study: personalizationrecommender systemworld wide webinformation retrievaldata miningmachine learningcomputer science
Finding collections of k -clique percolated components in attributed graphs,Pierre-Nicolas Mougel (University of Lyon)Christophe Rigotti (University of Lyon)Olivier Gandrillon (University of Lyon),"1988545203,2124275409,131434759","In this paper, we consider graphs where a set of Boolean attributes is associated to each vertex, and we are interested in k -clique percolated components (components made of overlapping cliques) in such graphs. We propose the task of finding the collections of homogeneous k -clique percolated components, where homogeneity means sharing a common set of attributes having value true. A sound and complete algorithm based on subgraph enumeration is proposed. We report experiments on two real databases (a social network of scientific collaborations and a network of gene interactions), showing that the extracted patterns capture meaningful structures.",2012,Knowledge Discovery and Data Mining,Fields of study: clique sumblock graphclique problemsplit graphclique graphtreewidthchordal graphcliquenetwork analysisdiscrete mathematicscombinatoricsmachine learningcomputer sciencemathematics
Hybrid- ε -greedy for mobile context-aware recommender system,Djallel Bouneffouf (Telecom SudParis)Amel Bouzeghoub (Telecom SudParis)Alda Lopes Gançarski (Telecom SudParis),"2515860825,193626364,2238896797","The wide development of mobile applications provides a considerable amount of data of all types. In this sense, Mobile Context-aware Recommender Systems (MCRS) suggest the user suitable information depending on her/his situation and interests. Our work consists in applying machine learning techniques and reasoning process in order to adapt dynamically the MCRS to the evolution of the user's interest. To achieve this goal, we propose to combine bandit algorithm and case-based reasoning in order to define a contextual recommendation process based on different context dimensions (social, temporal and location). This paper describes our ongoing work on the implementation of a MCRS based on a hybrid- e -greedy algorithm. It also presents preliminary results by comparing the hybrid- e -greedy and the standard e -greedy algorithm.",2012,Knowledge Discovery and Data Mining,Fields of study: personalizationrecommender systemknowledge managementdata miningmachine learningcomputer science
Quantifying reciprocity in large weighted communication networks,Leman Akoglu (Carnegie Mellon University)Pedro O. S. Vaz de Melo (Universidade Federal de Minas Gerais)Christos Faloutsos (Carnegie Mellon University),"2288278917,2162838672,2198983026","If a friend called you 50 times last month, how many times did you call him back? Does the answer change if we ask about SMS, or e-mails? We want to quantify reciprocity between individuals in weighted networks, and we want to discover whether it depends on their topological features (like degree, or number of common neighbors). Here we answer these questions, by studying the call- and SMS records of millions of mobile phone users from a large city, with more than 0.5 billion phone calls and 60 million SMSs, exchanged over a period of six months. Our main contributions are: (1) We propose a novel distribution, the Triple Power Law (3PL), that fits the reciprocity behavior of all 3 datasets we study, with a better fit than older competitors, (2) 3PL is parsimonious; it has only three parameters and thus avoids over-fitting, (3) 3PL can spot anomalies, and we report the most surprising ones, in our real networks, (4) We observe that the degree of reciprocity between users is correlated with their local topological features; reciprocity is higher among mutual users with larger local network overlap and greater degree similarity.",2012,Knowledge Discovery and Data Mining,Fields of study: data miningartificial intelligencemathematics
Building decision trees for the multi-class imbalance problem,T. Ryan Hoens (University of Notre Dame)Qi Qian (Nanjing University)Nitesh V. Chawla (University of Notre Dame)Zhi-Hua Zhou (Nanjing University),"679846916,2486661495,1979796846,2286237009","Learning in imbalanced datasets is a pervasive problem prevalent in a wide variety of real-world applications. In imbalanced datasets, the class of interest is generally a small fraction of the total instances, but misclassification of such instances is often expensive. While there is a significant body of research on the class imbalance problem for binary class datasets, multi-class datasets have received considerably less attention. This is partially due to the fact that the multi-class imbalance problem is often much harder than its related binary class problem, as the relative frequency and cost of each of the classes can vary widely from dataset to dataset. In this paper we study the multi-class imbalance problem as it relates to decision trees (specifically C4.4 and HDDT), and develop a new multi-class splitting criterion. From our experiments we show that multi-class Hellinger distance decision trees, when combined with decomposition techniques, outperform C4.4.",2012,Knowledge Discovery and Data Mining,Fields of study: data miningpattern recognitionmachine learningstatisticsmathematics
Discovering coverage patterns for banner advertisement placement,"P. Gowtham Srinivas (International Institute of Information Technology, Hyderabad)P. Krishna Reddy (International Institute of Information Technology, Hyderabad)S. Bhargav (International Institute of Information Technology, Hyderabad)R. Uday Kiran (International Institute of Information Technology, Hyderabad)D. Satheesh Kumar (International Institute of Information Technology, Hyderabad)","2109152216,2143169954,2551233407,2139065608,2113910735","We propose a model of coverage patterns and a methodology to extract coverage patterns from transactional databases. We have discussed how the coverage patterns are useful by considering the problem of banner advertisements placement in e-commerce web sites. Normally, advertiser expects that the banner advertisement should be displayed to a certain percentage of web site visitors. On the other hand, to generate more revenue for a given web site, the publisher has to meet the coverage demands of several advertisers by providing appropriate sets of web pages. Given web pages of a web site, a coverage pattern is a set of pages visited by a certain percentage of visitors. The coverage patterns discovered from click-stream data could help the publisher in meeting the demands of several advertisers. The efficiency and advantages of the proposed approach is shown by conducting experiments on real world click-stream data sets.",2012,Knowledge Discovery and Data Mining,Fields of study: link exchangeonline advertisinginternet privacyworld wide webdata miningcomputer science
Scalable mining of frequent tri-concepts from folksonomies,Chiraz Trabelsi (Tunis University)Nader Jelassi (Tunis University)Sadok Ben Yahia (Tunis University),"2136373646,1543602933,2163739133","Mining frequent tri-concepts from folksonomies is an interesting problem with broad applications. Most of the previous tri-concepts mining based algorithms avoided a straightforward handling of the triadic contexts and paid attention to an unfruitful projection of the induced search space into dyadic contexts. As a such projection is very computationally expensive since several tri-concepts are computed redundantly, scalable mining of folksonomies remains a challenging problem. In this paper, we introduce a new algorithm, called Tricons, that directly tackles the triadic form of folksonomies towards a scalable extraction of tri-concepts. The main thrust of the introduced algorithm stands in the application of an appropriate closure operator that splits the search space into equivalence classes for the the localization of tri-minimal generators. These tri-minimal generators make the computation of the tri-concepts less arduous than do the pioneering approches of the literature.The experimental results show that the Tricons enables the scalable frequent tri-concepts mining over two real-life folksonomies .",2012,Knowledge Discovery and Data Mining,Fields of study: closure operatorequivalence classdata miningartificial intelligencemachine learningmathematics
Multiple instance learning for group record linkage,Zhichun Fu (Science College)Jun Zhou (Science College)Peter Christen (Science College)Mac Boot (Australian National University),"2135087989,2492838068,2023765750,2099029329","Record linkage is the process of identifying records that refer to the same entities from different data sources. While most research efforts are concerned with linking individual records, new approaches have recently been proposed to link groups of records across databases. Group record linkage aims to determine if two groups of records in two databases refer to the same entity or not. One application where group record linkage is of high importance is the linking of census data that contain household information across time. In this paper we propose a novel method to group record linkage based on multiple instance learning. Our method treats group links as bags and individual record links as instances. We extend multiple instance learning from bag to instance classification to reconstruct bags from candidate instances. The classified bag and instance samples lead to a significant reduction in multiple group links, thereby improving the overall quality of linked data. We evaluate our method with both synthetic data and real historical census data.",2012,Knowledge Discovery and Data Mining,Fields of study: name resolutionrecord linkagepopulation statisticssynthetic datadata sciencedata miningmachine learningcomputer science
Heterogeneous ensemble for feature drifts in data streams,Hai-Long Nguyen (Nanyang Technological University)Yew-Kwong Woon (Nanyang Technological University)Wee Keong Ng (Nanyang Technological University)Li Wan (New York University),"2120452001,2579311635,2120738522,2222847076","The nature of data streams requires classification algorithms to be real-time, efficient, and able to cope with high-dimensional data that are continuously arriving. It is a known fact that in high-dimensional datasets, not all features are critical for training a classifier. To improve the performance of data stream classification, we propose an algorithm called HEFT-Stream ( H eterogeneous E nsemble with F eature drif T for Data Streams ) that incorporates feature selection into a heterogeneous ensemble to adapt to different types of concept drifts. As an example of the proposed framework, we first modify the FCBF [13] algorithm so that it dynamically update the relevant feature subsets for data streams. Next, a heterogeneous ensemble is constructed based on different online classifiers, including Online Naive Bayes and CVFDT [5]. Empirical results show that our ensemble classifier outperforms state-of-the-art ensemble classifiers (AWE [15] and OnlineBagging [21]) in terms of accuracy, speed, and scalability. The success of HEFT-Stream opens new research directions in understanding the relationship between feature selection techniques and ensemble learning to achieve better classification performance.",2012,Knowledge Discovery and Data Mining,Fields of study: ensemble learningdata miningpattern recognitionmachine learningcomputer science
Reciprocal and heterogeneous link prediction in social networks,Xiongcai Cai (University of New South Wales)Michael Bain (University of New South Wales)Alfred Krzywicki (University of New South Wales)Wayne Wobcke (University of New South Wales)Yang Sok Kim (University of New South Wales)Paul Compton (University of New South Wales)Ashesh Mahidadia (University of New South Wales),"2149099612,1948433388,2040505259,1269867290,2105835376,2130286701,2007104460","Link prediction is a key technique in many applications in social networks, where potential links between entities need to be predicted. Conventional link prediction techniques deal with either homogeneous entities, e.g., people to people, item to item links, or non-reciprocal relationships, e.g., people to item links. However, a challenging problem in link prediction is that of heterogeneous and reciprocal link prediction, such as accurate prediction of matches on an online dating site, jobs or workers on employment websites, where the links are reciprocally determined by both entities that heterogeneously belong to disjoint groups. The nature and causes of interactions in these domains makes heterogeneous and reciprocal link prediction significantly different from the conventional version of the problem. In this work, we address these issues by proposing a novel learnable framework called ReHeLP , which learns heterogeneous and reciprocal knowledge from collaborative information and demonstrate its impact on link prediction. Evaluation on a large commercial online dating dataset shows the success of the proposed method and its promise for link prediction.",2012,Knowledge Discovery and Data Mining,Fields of study: social network analysisrecommender systemdata miningartificial intelligencemachine learningcomputer science
Shape-Based clustering for time series data,Warissara Meesrikamolkul (Chulalongkorn University)Vit Niennattrakul (Chulalongkorn University)Chotirat Ann Ratanamahatana (Chulalongkorn University),"324287996,233889838,2091934636","One of the most famous algorithms for time series data clustering is k -means clustering with Euclidean distance as a similarity measure. However, many recent works have shown that Dynamic Time Warping (DTW) distance measure is more suitable for most time series data mining tasks due to its much improved alignment based on shape. Unfortunately, k -means clustering with DTW distance is still not practical since the current averaging functions fail to preserve characteristics of time series data within the cluster. Recently, Shape-based Template Matching Framework (STMF) has been proposed to discover a cluster representative of time series data. However, STMF is very computationally expensive. In this paper, we propose a Shape-based Clustering for Time Series (SCTS) using a novel averaging method called Ranking Shape-based Template Matching Framework (RSTMF), which can average a group of time series effectively but take as much as 400 times less computational time than that of STMF. In addition, our method outperforms other well-known clustering techniques in terms of accuracy and criterion based on known ground truth.",2012,Knowledge Discovery and Data Mining,Fields of study: k medians clusteringcorrelation clusteringconstrained clusteringdata stream clusteringcure data clustering algorithmfuzzy clusteringclustering high dimensional datacluster analysistime seriesdata miningpattern recognitionmachine learningstatisticscomputer sciencemathematics
Prioritizing disease genes by bi-random walk,Maoqiang Xie (Nankai University)Taehyun Hwang (University of Minnesota)Rui Kuang (University of Minnesota),"2164751544,2132795199,2037866308","Random walk methods have been successfully applied to prioritizing disease causal genes. In this paper, we propose a bi-random walk algorithm (BiRW) based on a regularization framework for graph matching to globally prioritize disease genes for all phenotypes simultaneously. While previous methods perform random walk either on the protein-protein interaction network or the complete phenome-genome heterogenous network, BiRW performs random walk on the Kronecker product graph between the protein-protein interaction network and the phenotype similarity network. Three variations of BiRW that perform balanced or unbalanced bi-directional random walks are analyzed and compared with other random walk methods. Experiments on analyzing the disease phenotype-gene associations in Online Mendelian Inheritance in Man (OMIM) demonstrate that BiRW effectively improved disease gene prioritization over existing methods by ranking more known associations in the top 100 out of nearly 10,000 candidate genes.",2012,Knowledge Discovery and Data Mining,Fields of study: bioinformaticsmachine learningstatisticsmathematics
The pattern next door: towards spatio-sequential pattern discovery,Hugo Alatrista SalasSandra Bringay (Centre national de la recherche scientifique)Frédéric Flouvat (University of New Caledonia)Nazha Selmaoui-Folcher (University of New Caledonia)Maguelonne Teisseire (Centre national de la recherche scientifique),"2155199951,150302470,2093516567,1967950925,2141863019","Health risks management such as epidemics study produces large quantity of spatio-temporal data. The development of new methods able to manage such specific characteristics becomes crucial. To tackle this problem, we define a theoretical framework for extracting spatio-temporal patterns (sequences representing evolution of locations and their neighborhoods over time). Classical frequency support doesn't consider the pattern neighbor neither its evolution over time. We thus propose a new interestingness measure taking into account both spatial and temporal aspects. An algorithm based on pattern-growth approach with efficient successive projections over the database is proposed. Experiments conducted on real datasets highlight the relevance of our method.",2012,Knowledge Discovery and Data Mining,Fields of study: data sciencedata miningmachine learning
Scalable similarity matching in streaming time series,Alice Marascu (University of Trento)Suleiman Ali Khan (Aalto University)Themis Palpanas (University of Trento),"1948455341,2097550979,2010554420","Nowadays online monitoring of data streams is essential in many real life applications, like sensor network monitoring, manufacturing process control, and video surveillance. One major problem in this area is the online identification of streaming sequences similar to a predefined set of pattern-sequences. In this paper, we present a novel solution that extends the state of the art both in terms of effectiveness and efficiency. We propose the first online similarity matching algorithm based on Longest Common SubSequence that is specifically designed to operate in a streaming context, and that can effectively handle time scaling, as well as noisy data. In order to deal with high stream rates and multiple streams, we extend the algorithm to operate on multilevel approximations of the streaming data, therefore quickly pruning the search space. Finally, we incorporate in our approach error estimation mechanisms in order to reduce the number of false negatives. We perform an extensive experimental evaluation using forty real datasets, diverse in nature and characteristics, and we also compare our approach to previous techniques. The experiments demonstrate the validity of our approach.",2012,Knowledge Discovery and Data Mining,Fields of study: time seriestheoretical computer sciencedata miningmachine learningstatisticscomputer science
RadialViz: an orientation-free frequent pattern visualizer,Carson Kai-Sang Leung (University of Manitoba)Fan Jiang (University of Manitoba),"2128845036,2164473926","Frequent pattern mining algorithms aim to find sets of frequently co-occurring items. Visual representation of the mining results is more comprehensible to users than the traditional long textual list of frequent patterns. Existing visualizers mostly show frequent patterns as graphs in a two-dimensional space with ( x , y )-coordinates. Nowadays, in a collaborative environment, it is not uncommon for users to have face-to-face meetings when they show the graphs visualizing frequent patterns. In these situations, the viewing orientation of the graphs plays an important role as different orientations positively or negatively impact the graph legibility. A legible right-side-up graph to one user may become an illegible upside-down graph towards another user. In this paper, we propose a visualizer that uses a radial layout--which is orientation free--to show frequent patterns. Having such a visualizer is beneficial in the collaborative environment.",2012,Knowledge Discovery and Data Mining,Fields of study: genetic associationdata sciencedata miningmachine learningcomputer science
Clustering-Based k -anonymity,Xianmang He (Ningbo University)HuaHui Chen (Ningbo University)Yefang Chen (Ningbo University)Yihong Dong (Ningbo University)Peng Wang (Fudan University)Zhenhua Huang (Tongji University),"2100627051,2122075554,2143214494,2665375727,2642335288,2702272805","Privacy is one of major concerns when data containing sensitive information needs to be released for ad hoc analysis, which has attracted wide research interest on privacy-preserving data publishing in the past few years. One approach of strategy to anonymize data is generalization. In a typical generalization approach, tuples in a table was first divided into many QI (quasi-identifier)-groups such that the size of each QI-group is no less than k . Clustering is to partition the tuples into many clusters such that the points within a cluster are more similar to each other than points in different clusters. The two methods share a common feature: distribute the tuples into many small groups. Motivated by this observation, we propose a clustering-based k -anonymity algorithm, which achieves k -anonymity through clustering. Extensive experiments on real data sets are also conducted, showing that the utility has been improved by our approach.",2012,Knowledge Discovery and Data Mining,Fields of study: internet privacyworld wide webdata miningcomputer sciencemathematics
Neighborhood-Based smoothing of external cluster validity measures,Ken-ichi Fukui (International Society for Intelligence Research)Masayuki Numao (International Society for Intelligence Research),"2585058503,2584096547","This paper proposes a methodology for introducing a neighborhood relation of clusters to the conventional cluster validity measures using external criteria, that is, class information. The extended measure evaluates the cluster validity together with connectivity of class distribution based on a neighborhood relation of clusters. A weighting function is introduced for smoothing the basic statistics to set-based measures and to pairwise-based measures. Our method can extend any cluster validity measure based on a set or pairwise of data points. In the experiment, we examined the neighbor component of the extended measure and revealed an appropriate neighborhood radius and some properties using synthetic and real-world data.",2012,Knowledge Discovery and Data Mining,Fields of study: weight functiondata miningpattern recognitionstatisticsmathematics
Exploiting label dependency for hierarchical multi-label classification,Noor Alaydie (Wayne State University)Chandan K. Reddy (Wayne State University)Farshad Fotouhi (Wayne State University),"334427614,2100435683,2039005146","Hierarchical multi-label classification is a variant of traditional classification in which the instances can belong to several labels, that are in turn organized in a hierarchy. Existing hierarchical multi-label classification algorithms ignore possible correlations between the labels. Moreover, most of the current methods predict instance labels in a ""flat"" fashion without employing the ontological structures among the classes. In this paper, we propose HiBLADE (Hierarchical multi-label Boosting with LAbel DEpendency), a novel algorithm that takes advantage of not only the pre-established hierarchical taxonomy of the classes, but also effectively exploits the hidden correlation among the classes that is not shown through the class hierarchy, thereby improving the quality of the predictions. According to our approach, first, the pre-defined hierarchical taxonomy of the labels is used to decide upon the training set for each classifier. Second, the dependencies of the children for each label in the hierarchy are captured and analyzed using Bayes method and instance-based similarity. Our experimental results on several real-world biomolecular datasets show that the proposed method can improve the performance of hierarchical multi-label classification.",2012,Knowledge Discovery and Data Mining,Fields of study: boostingcorrelationdata miningpattern recognitionmachine learningcomputer sciencemathematics
A vertex similarity probability model for finding network community structure,Kan Li (Beijing Institute of Technology)Yin Pang (Beijing Institute of Technology),"2641966119,2647471172","Most methods for finding community structure are based on the prior knowledge of network structure type. These methods grouped the communities only when known network is unipartite or bipartite. This paper presents a vertex similarity probability (VSP) model which can find community structure without priori knowledge of network structure type. Vertex similarity, which assumes that, for any type of network structures, vertices in the same community have similar properties. In the VSP model, ""Common neighbor index"" is used to measure the vertex similarity probability, as it has been proved to be an effective index for vertex similarity. We apply the algorithm to real-world network data. The results show that the VSP model is uniform for both unipartite networks and bipartite networks, and it is able to find the community structure successfully without the use of the network structure type.",2012,Knowledge Discovery and Data Mining,Fields of study: vertexcommunity structurediscrete mathematicscombinatoricsmachine learningstatisticsmathematics
Time-Evolving relational classification and ensemble methods,Ryan A. Rossi (Purdue University)Jennifer Neville (Purdue University),"2060818872,2124572662","Relational networks often evolve over time by the addition, deletion, and changing of links, nodes, and attributes. However, accurately incorporating the full range of temporal dependencies into relational learning algorithms remains a challenge. We propose a novel framework for discovering temporal-relational representations for classification. The framework considers transformations over all the evolving relational components (attributes, edges, and nodes) in order to accurately incorporate temporal dependencies into relational models. Additionally, we propose temporal ensemble methods and demonstrate their effectiveness against traditional and relational ensembles on two real-world datasets. In all cases, the proposed temporal-relational models outperform competing models that ignore temporal information.",2012,Knowledge Discovery and Data Mining,Fields of study: statistical relational learningdata miningpattern recognitionmachine learningcomputer science
Applying NLP techniques for query reformulation to information retrieval with geographical references,José M. Perea-Ortega (University of Seville)Miguel A. García-Cumbreras (University of Jaén)L. Alfonso Ureña-López (University of Jaén),"2311155486,1825494946,1967272513","Geographic Information Retrieval (GIR) is an active and growing research area that focuses on the retrieval of textual documents according to a geographical criteria of relevance. However, since a GIR system can be treated as a traditional Information Retrieval (IR) system, it is important to pay attention to finding effective methods for query reformulation. In this way, the search results will improve their quality and recall. In this paper, we propose different Natural Language Processing (NLP) techniques of query reformulation related to the modification and/or expansion of both parts thematic and geospatial that are usually recognized in a geographical query. We have evaluated each of the reformulations proposed using GeoCLEF as an evaluation framework for GIR systems. The results obtained show that all proposed query reformulations retrieved relevant documents that were not retrieved using the original query.",2012,Knowledge Discovery and Data Mining,Fields of study: sargablerankingrdf query languageconcept searchweb search queryweb query classificationquery expansionquery optimizationquery languageinformation retrievaldata miningdatabasecomputer science
TeamSkill evolved: mixed classification schemes for team-based multi-player games,Colin DeLong (University of Minnesota)Jaideep Srivastava (University of Minnesota),"2130915846,2192802387","In this paper, we introduce several approaches for maintaining weights over the aggregate skill ratings of subgroups of teams during the skill assessment process and extend our earlier work in this area to include game-specific performance measures as features alongside aggregate skill ratings as part of the online prediction task. We find that the inclusion of these game-specific measures do not improve prediction accuracy in the general case, but do when competing teams are considered evenly matched. As such, we develop a ""mixed"" classification method called TeamSkill-EVMixed which selects a classifier based on a threshold determined by the prior probability of one team defeating another. This mixed classification method outperforms all previous approaches in most evaluation settings and particularly so in tournament environments. We also find that TeamSkill-EVMixed's ability to perform well in close games is especially useful early on in the rating process where little game history is available.",2012,Knowledge Discovery and Data Mining,Fields of study: perceptrondata miningartificial intelligencemachine learningsimulationcomputer science
Discovering unknown but interesting items on personal social network,Juang-Lin Duan (Yuan Ze University)Shashi Prasad (Yuan Ze University)Jen-Wei Huang (Yuan Ze University),"2416036257,2233649237,2104987549","Social networking service has become very popular recently. Many recommendation systems have been proposed to integrate with social networking websites. Traditional recommendation systems focus on providing popular items or items posted by close friends. This strategy causes some problems. Popular items always occupy the recommendation list and they are usually already known by the user. In addition, items recommended by familiar users, who frequently communicate with the target user, may not be interesting. Moreover, interesting items from similar users with lower popularity are ignored. In this paper, we propose an algorithm, UBI, to discover unknown but interesting items. We propose three scores, i.e., Quartile-aided Popularity Score, Social Behavior Score, and User Similarity Score, to model the popularity of items, the familiarity of friends, and the similarity of users respectively in the target user's personal social network. Combining these three scores, the recommendation list containing unknown but interesting items can be generated. Experimental results show that UBI outperforms traditional methods in terms of the percentages of unknown and interesting items in the recommendation list.",2012,Knowledge Discovery and Data Mining,Fields of study: social networkinternet privacyworld wide webdata miningcomputer science
Scalable random forests for massive data,Bingguo Li (Chinese Academy of Sciences)Xiaojun Chen (Chinese Academy of Sciences)Mark Junjie Li (Chinese Academy of Sciences)Joshua Zhexue Huang (Chinese Academy of Sciences)Shengzhong Feng (Chinese Academy of Sciences),"2544204708,2135711298,2146476422,2111159692,2658284317","This paper proposes a scalable random forest algorithm SRF with MapReduce implementation. A breadth-first approach is used to grow decision trees for a random forest model. At each level of the trees, a pair of map and reduce functions split the nodes. A mapper is dispatched to a local machine to compute the local histograms of subspace features of the nodes from a data block. The local histograms are submitted to reducers to compute the global histograms from which the best split conditions of the nodes are calculated and sent to the controller on the master machine to update the random forest model. A random forest model is built with a sequence of map and reduce functions. Experiments on large synthetic data have shown that SRF is scalable to the number of trees and the number of examples. The SRF algorithm is able to build a random forest of 100 trees in a little more than 1 hour from 110 Gigabyte data with 1000 features and 10 million records.",2012,Knowledge Discovery and Data Mining,Fields of study: random foresthistogramtheoretical computer sciencedistributed computingdata miningmachine learningstatisticscomputer science
Mining mobile users' activities based on search query text and context,Bingyue Peng (Beihang University)Yujing Wang (Peking University)Jian-Tao Sun (Microsoft),"2642397819,2636256297,2131116857","Mobile search market is growing very fast. Mining mobile search activities is helpful for understanding user preference, interest and even regular patterns. In previous works, text information contained by either search queries or web pages visited by users is well studied to mine search activities. Since rich context information (e.g., time, location and other sensor inputs) is contained in the mobile search data, it has also been leveraged by researchers for mining user activities. However, the two types of information were used separately. In this paper, we propose a graphical model approach, namely the Text and Context-based User Activity Model (TCUAM), which mines user activity patterns by utilizing query text and context simultaneously. The model is developed based on Latent Dirichlet Allocation (LDA) by regarding users' activities as latent topics. In order to guide the activity mining process, we borrow some external knowledge of topic-word relationship to build a constrained TCUAM model. The experimental results indicate that the TCUAM model yields better results compared with text-only and context-only approaches. We also find that the constrained TCUAM model is more effective than the unconstrained TCUAM model.",2012,Knowledge Discovery and Data Mining,Fields of study: mobile searchlatent dirichlet allocationgraphical modelweb pageworld wide webinformation retrievaldata miningmachine learningcomputer science
Accelerating outlier detection with uncertain data using graphics processors,Takazumi Matsumoto (Hong Kong Polytechnic University)Edward Hung (Hong Kong Polytechnic University),"2098691659,2278353976","Outlier detection (also known as anomaly detection) is a common data mining task in which data points that lie outside expected patterns in a given dataset are identified. This is useful in areas such as fault detection, intrusion detection and in pre-processing before further analysis. There are many approaches already in use for outlier detection, typically adapting other existing data mining techniques such as cluster analysis, neural networks and classification methods such as Support Vector Machines. However, in many cases data from sources such as sensor networks can be better represented with an uncertain model. Detecting outliers with uncertain data involves far more computation as each data object is usually represented by a number of probability density functions ( pdf s). In this paper, we demonstrate an implementation of outlier detection with uncertain objects based on an existing density sampling method that we have parallelized using the cross-platform OpenCL framework. While the density sampling method is a well understood and relatively straightforward outlier detection technique, its application to uncertain data results in a much higher computational workload. Our optimized implementation uses an inexpensive GPU (Graphics Processing Unit) to greatly reduce the running time. This improvement in performance may be leveraged when attempting to detect outliers with uncertain data in time sensitive situations such as when responding to sensor failure or network intrusion.",2012,Knowledge Discovery and Data Mining,Fields of study: anomaly detectiondata miningreal time computingmachine learningcomputer science
Topological comparisons of proximity measures,Djamel Abdelkader Zighed (Ericsson)Rafik Abdesselam (Ericsson)Asmelash Hadgu (Ericsson),"2001241048,113817547,2476999799","In many fields of application, the choice of proximity measure directly affects the results of data mining methods, whatever the task might be: clustering, comparing or structuring of a set of objects. Generally, in such fields of application, the user is obliged to choose one proximity measure from many possible alternatives. According to the notion of equivalence, such as the one based on pre-ordering, certain proximity measures are more or less equivalent, which means that they should produce almost the same results. This information on equivalence might be helpful for choosing one such measure. However, the complexity O ( n 4 ) of this approach makes it intractable when the size n of the sample exceeds a few hundred. To cope with this limitation, we propose a new approach with less complexity O ( n 2 ). This is based on topological equivalence and it exploits the concept of local neighbors. It defines equivalence between two proximity measures as having the same neighborhood structure on the objects. We illustrate our approach by considering 13 proximity measures used on datasets with continuous attributes.",2012,Knowledge Discovery and Data Mining,Fields of study: discrete mathematicscombinatoricsmathematics
Life activity modeling of news event on twitter using energy function,Rong Lu (Chinese Academy of Sciences)Zhiheng Xu (Chinese Academy of Sciences)Yang Zhang (Chinese Academy of Sciences)Qing Yang (Chinese Academy of Sciences),"2718684869,2436186820,2668645726,2696202282","This research is the first exploration on modeling life activity of news event on Twitter. We consider a news event as a natural life form, and use an energy function to evaluate its activity. A news event on Twitter becomes more active with a burst of tweets discussing it, and it fades away with time. These changes of the activity are well captured by the energy function. Then, we incorporate this energy function into the traditional single-pass clustering algorithm, and propose a more adaptive on-line news event detection method. A corpus of tweets which discuss news events was analyzed using our method. Experimental results show that our method not only compares favorably to those of other methods in official TDT measures like precision, recall etc., but also has better time and memory performance, which makes it more suitable for a real system.",2012,Knowledge Discovery and Data Mining,Fields of study: internet privacymultimediaworld wide web
An associative classifier for uncertain datasets,Metanat Hooshsadat (University of Alberta)Osmar R. Zaïane (University of Alberta),"223267653,2308328903","The classification of uncertain datasets is an emerging research problem that has recently attracted significant attention. Some attempts to devise a classification model with uncertain training data have been proposed using decision trees, neural networks, or other approaches. Among those, the associative classifiers have inspired some of the uncertain classification algorithms given their promising results on standard datasets. We propose a novel associative classifier for uncertain data. Our method, Uncertain Associative Classifier (UAC) is efficient and has an effective rule pruning strategy. Our experimental results on real datasets show that in most cases, UAC reaches better accuracies than the state of the art algorithms.",2012,Knowledge Discovery and Data Mining,Fields of study: data miningpattern recognitionmachine learningcomputer science
WeightTransmitter: weighted association rule mining using landmark weights,Yun Sing Koh (University of Auckland)Russel Pears (Auckland University of Technology)Gillian Dobbie (University of Auckland),"1987110578,2022249107,2002900660","Weighted Association Rule Mining (WARM) is a technique that is commonly used to overcome the well-known limitations of the classical Association Rule Mining approach. The assignment of high weights to important items enables rules that express relationships between high weight items to be ranked ahead of rules that only feature less important items. Most previous research to weight assignment has used subjective measures to assign weights and are reliant on domain specific information. Whilst there have been a few approaches that automatically deduce weights from patterns of interaction between items, none of them take advantage of the situation where weights of only a subset of items are known in advance. We propose a model, WeightTransmitter, that interpolates the unknown weights from a known subset of weights.",2012,Knowledge Discovery and Data Mining,Fields of study: association rule learningdata miningpattern recognitionmachine learningcomputer sciencemathematics
Automatic identification of protagonist in fairy tales using verb,Hui-Ngo Goh (Multimedia University)Lay-Ki Soon (Multimedia University)Su-Cheng Haw (Multimedia University),"2033956374,2083957521,2182557365","Named entity recognition (NER) has been a well-studied problem in the area of text mining for locating atomic element into predefined categories, where ""name of people"" is one of the most commonly studied categories. Numerous new NER techniques have been unfolded to accommodate the needs of the application developed. However, most research works carried out focused on non-fiction domain. Fiction domain exhibits complexity and uncertainty in locating protagonist as it represents name of person in a diverse spectrums, ranging from living things (animals, plants, person) to non-living things (vehicle, furniture). This paper proposes automated protagonist identification in fiction domain, particularly in fairy tales. Verb has been used as a determinant in substantiating the existence of protagonist with the assistance of WordNet. The experimental results show that it is viable to use verb in identifying named entity, particularly ""people"" category and it can be applied in a small text size environment.",2012,Knowledge Discovery and Data Mining,Fields of study: entity linkingtext miningnatural language processingdata miningartificial intelligencecomputer science
Neighborhood random classification,Djamel Abdelkader Zighed (University of Lyon)Diala Ezzeddine (Ericsson)Fabien Rico (Ericsson),"2001241048,2282065763,2095800850","Ensemble methods (EMs) have become increasingly popular in data mining because of their efficiency. These methods(EMs) generate a set of classifiers using one or several machine learning algorithms (MLAs) and aggregate them into a single classifier (Meta-Classifier, MC). Of the MLAs, k-Nearest Neighbors (kNN) is one of the most well-known used in the context of EMs. However, handling the parameter k can be difficult. This drawback is the same for all MLA that are instance based. Here, we propose an approach based on neighborhood graphs as an alternative. Thanks to these related graphs, like relative neighborhood graphs (RNGs) or Gabriel graphs (GGs), we provide a generalized approach with less arbitrary parameters. Neighborhood graphs have never been introduced into EM approaches before. The results of our algorithm : Neighborhood Random Classification are very promising as they are equal to the best EM approaches such as Random Forest or those based on SVMs. In this exploratory and experimental work, we provide the methodological approach and many comparative results.",2012,Knowledge Discovery and Data Mining,Fields of study: k nearest neighbors algorithmensemble learningdata miningpattern recognitionmachine learningcomputer sciencemathematics
Relevant feature selection from EEG signal for mental task classification,Akshansh Gupta (Jawaharlal Nehru University)R. K. Agrawal (Jawaharlal Nehru University),"2299471914,2309964180","In last few years, the research community has shown interest in the development of Brain Computer Interface which may assists physically challenged people to communicate with the help of brain signal. The two important components of such BCI system are to determine appropriate features and classification method to achieve better performance. In literature, Empirical Mode Decomposition is suggested for feature extraction from EEG which is suitable for the analysis of non-linear and non-stationary time series. However, the features obtained from EEG may contain irrelevant and redundant features which make them inefficient for machine learning. Relevant features not only decrease the processing time to train a classifier but also provide better generalization. Hence, relevant features which provide maximum classification accuracy are selected using ratio of scatter matrices, Chernoff distance measure and linear regression. The performance of different mental task using different measures used for feature selection is compared and evaluated in terms of classification accuracy. Experimental results show that there is significant improvement in classification accuracy with features selected using all feature selection methods and in particular with ratio of scatter matrices.",2012,Knowledge Discovery and Data Mining,Fields of study: brain computer interfacehilbert huang transformfeaturelinear regressionfeature selectionspeech recognitionpattern recognitionmachine learningstatisticscomputer science
Co-occurring cluster mining for damage patterns analysis of a fuel cell,Daiki Inaba (Osaka University)Ken-ichi Fukui (Osaka University)Kazuhisa Sato (Tohoku University)Junichirou Mizusaki (Tohoku University)Masayuki Numao (Osaka University),"2124519150,2114405250,2723436421,2143893348,2268008111","In this study, we research the mechanical correlations among components of solid oxide fuel cell (SOFC) by analyzing the co-occurrence of acoustic emission (AE) events which are caused by damage. Then we propose a novel method for mining patterns from the numerical data such as AE. The proposed method extracts patterns of two clusters considering co-occurrence between clusters and similarity within each cluster at the same time. In addition, we utilize the dendrogram obtained from hierarchical clustering for reduction of the search space. We applied the proposed method to AE data, and the damage patterns which represent the main mechanical correlations were extracted. We can acquire novel knowledge about damage mechanism of SOFC from the results.",2012,Knowledge Discovery and Data Mining,Fields of study: cluster analysisdata miningmachine learningcomputer science
New exact concise representation of rare correlated patterns: application to intrusion detection,Souad Bouasker (Tunis University)Tarek Hamrouni (Centre national de la recherche scientifique)Sadok Ben Yahia (Telecom SudParis),"1983334297,2308133698,2637901927","During the last years, many works focused on the exploitation of rare patterns. In fact, these patterns allow conveying knowledge on unexpected events. Nevertheless, a main problem is related to their very high number and to the low quality of several mined rare patterns. In order to overcome these limits, we propose to integrate the correlation measure bond aiming at only mining the set of rare correlated patterns. A characterization of the resulting set is then detailed, based on the study of constraints of different natures induced by the rarity and the correlation. In addition, based on the equivalence classes associated to a closure operator dedicated to the bond measure, we propose a new exact concise representation of rare correlated patterns. We then design the new RcprMiner algorithm allowing an efficient extraction of the proposed representation. The carried out experimental studies prove the compactness rate offered by our approach. We also design an association rules based classifier and we prove its effectiveness in the context of intrusion detection.",2012,Knowledge Discovery and Data Mining,Fields of study: closure operatorequivalence classconstraintcorrelationdiscrete mathematicscombinatoricsdata miningmachine learningstatisticsalgorithmmathematics
Generating balanced classifier-independent training samples from unlabeled data,"Youngja Park (IBM)Zijie Qi (University of California, Davis)Suresh N. Chari (IBM)Ian M. Molloy (IBM)","2634435588,2162338994,2319998151,1991166589","We consider the problem of generating balanced training samples from an unlabeled data set with an unknown class distribution. While random sampling works well when the data is balanced, it is very ineffective for unbalanced data. Other approaches, such as active learning and cost-sensitive learning, are also suboptimal as they are classifier-dependent, and require misclassification costs and labeled samples. We propose a new strategy for generating training samples which is independent of the underlying class distribution of the data and the classifier that will be trained using the labeled data. Our methods are iterative and can be seen as variants of active learning, where we use semi-supervised clustering at each iteration to perform biased sampling from the clusters. Several strategies are provided to estimate the underlying class distributions in the clusters and increase the balancedness in the training samples. Experiments with both highly skewed and balanced data from the UCI repository and a private data show that our algorithm produces much more balanced samples than random sampling or uncertainty sampling. Further, our sampling strategy is substantially more efficient than active learning methods. The experiments also validate that, with more balanced training data, classifiers trained with our samples outperform classifiers trained with random sampling or active learning.",2012,Knowledge Discovery and Data Mining,Fields of study: semi supervised learningdata miningpattern recognitionmachine learningstatisticscomputer science
Co-embedding of structurally missing data by locally linear alignment,Takehisa Yairi (University of Tokyo),226036427,"This paper proposes a ""co-embedding"" method to embed the row and column vectors of an observation matrix data whose large portion is structurally missing into low-dimensional latent spaces simultaneously. A remarkable characteristic of this method is that the co-embedding is efficiently obtained via eigendecomposition of a matrix, unlike the conventional methods which require iterative estimation of missing values and suffer from local optima. Besides, we extend the unsupervised co-embedding method to a semi-supervised version, which is reduced to a system of linear equations.In an experimental study, we apply the proposed method to two kinds of tasks --- (1) Structure from Motion (SFM) and (2) Simultaneous Localization and Mapping (SLAM).",2012,Knowledge Discovery and Data Mining,Fields of study: data miningmachine learningmathematical optimizationmathematics
Active learning for hierarchical text classification,Xiao Li (University of Western Ontario)Da Kuang (University of Western Ontario)Charles X. Ling (University of Western Ontario),"2428882175,2142576100,2664974409","Hierarchical text classification plays an important role in many real-world applications, such as webpage topic classification, product categorization and user feedback classification. Usually a large number of training examples are needed to build an accurate hierarchical classification system. Active learning has been shown to reduce the training examples significantly, but it has not been applied to hierarchical text classification due to several technical challenges. In this paper, we study active learning for hierarchical text classification. We propose a realistic multi-oracle setting as well as a novel active learning framework, and devise several novel leveraging strategies under this new framework. Hierarchical relation between different categories has been explored and leveraged to improve active learning further. Experiments show that our methods are quite effective in reducing the number of oracle queries (by 74% to 90%) in building accurate hierarchical classification systems. As far as we know, this is the first work that studies active learning in hierarchical text classification with promising results.",2012,Knowledge Discovery and Data Mining,Fields of study: one class classificationdata miningpattern recognitionmachine learningcomputer science
Getting a grasp on clinical pathway data: an approach based on process mining,Jochen De Weerdt (Katholieke Universiteit Leuven)Filip Caron (Katholieke Universiteit Leuven)Jan Vanthienen (Katholieke Universiteit Leuven)Bart Baesens (Katholieke Universiteit Leuven),"2044761747,2131095577,153925120,2061851337","Since healthcare processes are pre-eminently heterogeneous and multi-disciplinary, information systems supporting these processes face important challenges in terms of design, implementation and diagnosis. Nonetheless, streamlining clinical pathways with the purpose of delivering high quality care while at the same time reducing costs is a promising goal. In this paper, we propose a methodology founded on process mining for intelligent analysis of clinical pathway data. Process mining can be considered a valuable approach to obtain a better understanding about the actual way of working in human-centric processes such as clinical pathways by investigating the event data as recorded in healthcare information systems. However, capturing tangible knowledge from clinical processes with their ad hoc and complex nature proves difficult. Accordingly, this paper proposes a data analysis methodology focussing on the extraction of tangible insights from clinical pathway data by adopting both a drill up and a drill down perspective.",2012,Knowledge Discovery and Data Mining,Fields of study: process miningdata sciencedata miningdatabasecomputer science
Sequential entity group topic model for getting topic flows of entity groups within one document,Young-Seob Jeong (KAIST)Ho-Jin Choi (KAIST),"2150755158,2109838815","Topic mining is regarded as a powerful method to analyze documents, and topic models are used to annotate relationships or to get a topic flow. The research aim in this paper is to get topic flows of entities and entity groups within one document. We propose two topic models: Entity Group Topic Model (EGTM) and Sequential Entity Group Topic Model (S-EGTM). These models provide two contributions. First, topic distributions of entities and entity groups can be analyzed. Second, the topic flow of each entity or each entity group can be captured, through segments in one document. We develop collapsed gibbs sampling methods for performing approximate inference of the models. By experiments, we demonstrate the models by showing the analysis of topics, prediction performance, and the topic flows over segments in one document.",2012,Knowledge Discovery and Data Mining,Fields of study: information retrievaldata miningpattern recognitioncomputer science
Active learning with c-certainty,Eileen A. Ni (University of Western Ontario)Charles X. Ling (University of Western Ontario),"2001420301,2664974409","It is well known that the noise in labels deteriorates the performance of active learning. To reduce the noise, works on multiple oracles have been proposed. However, there is still no way to guarantee the label quality. In addition, most previous works assume that the noise level of oracles is evenly distributed or example-independent which may not be realistic. In this paper, we propose a novel active learning paradigm in which oracles can return both labels and confidences. Under this paradigm, we then propose a new and effective active learning strategy that can guarantee the quality of labels by querying multiple oracles. Furthermore, we remove the assumptions of the previous works mentioned above, and design a novel algorithm that is able to select the best oracles to query. Our empirical study shows that the new algorithm is robust, and it performs well with given different types of oracles. As far as we know, this is the first work that proposes this new active learning paradigm and an active learning algorithm in which label quality is guaranteed.",2012,Knowledge Discovery and Data Mining,Fields of study: active learningactive learningdistributed computingdata miningmachine learningcomputer science
Active learning for cross language text categorization,Yue Liu (Beijing Institute of Technology)Lin Dai (Beijing Institute of Technology)Weitao Zhou (Beijing Institute of Technology)Heyan Huang (Beijing Institute of Technology),"2682449561,2649851849,1996871186,2659662649","Cross Language Text Categorization (CLTC) is the task of assigning class labels to documents written in a target language (e.g. Chinese) while the system is trained using labeled examples in a source language (e.g. English). With the technique of CLTC, we can build classifiers for multiple languages employing the existing training data in only one language, therefore avoid the cost of preparing training data for each individual language. One challenge for CLTC is the culture differences between languages, which causes the classifier trained on the source language doesn't perform well on the target language. In this paper, we propose an active learning algorithm for CLTC, which takes full advantage of both labeled data in the source language and unlabeled data in the target language. The classifier first learns the classification knowledge from the source language, and then learns the cultural dependent knowledge from the target language. In addition, we extend our algorithm to double viewed form by considering the source and target language as two views of the classification problem. Experiments show that our algorithm can effectively improve the cross language classification performance.",2012,Knowledge Discovery and Data Mining,Fields of study: cache language modeldata control languageuniversal networking languagelanguage identificationactive learningnatural language processingspeech recognitionpattern recognitionmachine learningcomputer science
Learning tree structure of label dependency for multi-label learning,"Bin Fu (Beijing Jiaotong University)Zhihai Wang (Beijing Jiaotong University)Rong Pan (Aalborg University)Guandong Xu (Victoria University, Australia)Peter Dolog (Aalborg University)","2435288915,2166803213,2141611174,2482500164,1122187853","There always exists some kind of label dependency in multi-label data. Learning and utilizing those dependencies could improve the learning performance further. Therefore, an approach for multi-label learning is proposed in this paper, which quantifies the dependencies of pairwise labels firstly, and then builds a tree structure of the labels to describe them. Thus the approach could find out potential strong label dependencies and produce more generalized dependent relationships. The experimental results have validated that compared with other state-of-the-art algorithms, the method is not only a competitive alternative, but also has shown better performance after ensemble learning especially.",2012,Knowledge Discovery and Data Mining,Fields of study: biological classificationdata miningpattern recognitionmachine learningcomputer science
Visualizing clusters in parallel coordinates for visual knowledge discovery,Yang Xiang (Ohio State University)David Fuhry (Ohio State University)Ruoming Jin (Kent State University)Ye Zhao (Kent State University)Kun Huang (Ohio State University),"2572284403,2398106800,2119237514,2174886271,2108564265","Parallel coordinates is frequently used to visualize multi-dimensional data. In this paper, we are interested in how to effectively visualize clusters of multi-dimensional data in parallel coordinates for the purpose of facilitating knowledge discovery. In particular, we would like to efficiently find a good order of coordinates for different emphases on visual knowledge discovery. To solve this problem, we link it to the metric-space Hamiltonian path problem by defining the cost between every pair of coordinates as the number of inter-cluster or intra-cluster crossings. This definition connects to various efficient solutions and leads to very fast algorithms. In addition, to better observe cluster interactions, we also propose to shape clusters smoothly by an energy reduction model which provides both macro and micro view of clusters.",2012,Knowledge Discovery and Data Mining,Fields of study: parallel coordinatesmetric spaceclusterknowledge extractiongraph theorytheoretical computer sciencecombinatoricsdata miningmachine learningcomputer sciencemathematics
Incremental set recommendation based on class differences,Yasuyuki Shirai (Hokkaido University)Koji Tsuruma (Hokkaido University)Yuko Sakurai (Kyushu University)Satoshi Oyama (Hokkaido University)Shin-ichi Minato (Hokkaido University),"2125120641,2091132481,2199993651,2126393252,2095881753","In this paper, we present a set recommendation framework that proposes sets of items, whereas conventional recommendation methods recommend each item independently. Our new approach to the set recommendation framework can propose sets of items on the basis on the user's initially chosen set. In this approach, items are added to or deleted from the initial set so that the modified set matches the target classification. Since the data sets created by the latest applications can be quite large, we use ZDD (Zero-suppressed Binary Decision Diagram) to make the searching more efficient. This framework is applicable to a wide range of applications such as advertising on the Internet and healthy life advice based on personal lifelog data.",2012,Knowledge Discovery and Data Mining,Fields of study: collaborative filteringbiological classificationinformation retrievaldata miningdatabasemachine learningcomputer science
Mining of temporal coherent subspace clusters in multivariate time series databases,Hardy Kremer (RWTH Aachen University)Stephan Günnemann (RWTH Aachen University)Arne Held (RWTH Aachen University)Thomas Seidl (RWTH Aachen University),"2130568417,316694267,2160783003,2140301036","Mining temporal multivariate data by clustering techniques is recently gaining importance. However, the temporal data obtained in many of today's applications is often complex in the sense that interesting patterns are neither bound to the whole dimensional nor temporal extent of the data domain. Under these conditions, patterns mined by existing multivariate time series clustering and temporal subspace clustering techniques cannot correctly reflect the true patterns in the data. In this paper, we propose a novel clustering method that mines temporal coherent subspace clusters. In our model, these clusters are reflected by sets of objects and relevant intervals. Relevant intervals indicate those points in time in which the clustered time series show a high similarity. In our model, each dimension has an individual set of relevant intervals, which together ensure temporal coherence. In the experimental evaluation we demonstrate the effectiveness of our method in comparison to related approaches.",2012,Knowledge Discovery and Data Mining,Fields of study: correlation clusteringcluster analysisdata sciencedata miningstatisticscomputer science
Unsupervised ensemble learning for mining top-n outliers,Jun Gao (Chinese Academy of Sciences)Weiming Hu (Chinese Academy of Sciences)Zhongfei(Mark) Zhang (Binghamton University)Ou Wu (Chinese Academy of Sciences),"2709295430,2124189993,2177285464,2131587498","Outlier detection is an important and attractive problem in knowledge discovery in large datasets. Instead of detecting an object as an outlier, we study detecting the n most outstanding outliers, i.e. the top-n outlier detection. Further, we consider the problem of combining the top-n outlier lists from various individual detection methods. A general framework of ensemble learning in the top-n outlier detection is proposed based on the rank aggregation techniques. A score-based aggregation approach with the normalization method of outlier scores and an order-based aggregation approach based on the distance-based Mallows model are proposed to accommodate various scales and characteristics of outlier scores from different detection methods. Extensive experiments on several real datasets demonstrate that the proposed approaches always deliver a stable and effective performance independent of different datasets in a good scalability in comparison with the state-of-the-art literature.",2012,Knowledge Discovery and Data Mining,Fields of study: data miningpattern recognitionmachine learningmathematics
OMC-IDS: at the cross-roads of OLAP mining and intrusion detection,Hanen BrahmiImen BrahmiSadok Ben Yahia (Telecom SudParis),"1967246901,1943149586,2637901927","Due to the growing threat of network attacks, the efficient detection as well as the network abuse assessment are of paramount importance. In this respect, the Intrusion Detection Systems (IDS) are intended to protect information systems against intrusions. However, IDS are plugged with several problems that slow down their development, such as low detection accuracy and high false alarm rate. In this paper, we introduce a new IDS, called OMC-IDS, which integrates data mining techniques and On Line Analytical Processing (OLAP) tools. The association of the two fields can be a powerful solution to deal with the defects of IDS. Our experiment results show the effectiveness of our approach in comparison with those fitting in the same trend.",2012,Knowledge Discovery and Data Mining,Fields of study: anomaly based intrusion detection systemintrusion prevention systemonline analytical processingintrusion detection systemassociation rule learningbiological classificationdata warehousecomputer securitydata miningdatabasecomputer science
Quad-tuple PLSA: incorporating entity and its rating in aspect identification,Wenjuan Luo (Chinese Academy of Sciences)Fuzhen Zhuang (Chinese Academy of Sciences)Qing He (Chinese Academy of Sciences)Zhongzhi Shi (Chinese Academy of Sciences),"2122642649,2050314250,2167314737,2112067332","With the opinion explosion on Web, there are growing research interests in opinion mining. In this study we focus on an important problem in opinion mining -- Aspect Identification (AI), which aims to extract aspect terms in entity reviews. Previous PLSA based AI methods exploit the 2-tuples (e.g. the co-occurrence of head and modifier), where each latent topic corresponds to an aspect. Here, we notice that each review is also accompanied by an entity and its overall rating, resulting in quad-tuples joined with the previously mentioned 2-tuples. Believing that the quad-tuples contain more co-occurrence information and thus provide more ability in differentiating topics, we propose a model of Quad-tuple PLSA, which incorporates two more items -- entity and its rating, into topic modeling for more accurate aspect identification. The experiments on different numbers of hotel and restaurant reviews show the consistent and significant improvements of the proposed model compared to the 2-tuple PLSA based methods.",2012,Knowledge Discovery and Data Mining,Fields of study: co occurrencesentiment analysisdata sciencespeech recognitiondata miningcomputer science
Detecting multiple stochastic network motifs in network data,Kai Liu (Hong Kong Baptist University)William K. Cheung (Hong Kong Baptist University)Jiming Liu (Hong Kong Baptist University),"2508384906,2099585548,2121827670","Network motif detection methods are known to be important for studying the structural properties embedded in network data. Extending them to stochastic ones help capture the interaction uncertainties in stochastic networks. In this paper, we propose a finite mixture model to detect multiple stochastic motifs in network data with the conjecture that interactions to be modeled in the motifs are of stochastic nature. Component-wise Expectation Maximization algorithm is employed so that both the optimal number of motifs and the parameters of their corresponding probabilistic models can be estimated. For evaluating the effectiveness of the algorithm, we applied the stochastic motif detection algorithm to both synthetic and benchmark datasets. Also, we discuss how the obtained stochastic motifs could help the domain experts to gain better insights on the over-represented patterns in the network data.",2012,Knowledge Discovery and Data Mining,Fields of study: stochastic neural networksocial networkexpectation maximization algorithmstochastic optimizationdata miningpattern recognitionmachine learningcomputer sciencemathematics
Privacy-Preserving EM algorithm for clustering on social network,Bin Yang (University of Tokyo)Issei Sato (University of Tokyo)Hiroshi Nakagawa (University of Tokyo),"2432059432,2148390776,2094248864","We consider the clustering problem in a private social network, in which all vertices are independent and private, and each of them knows nothing about vertices other than itself and its neighbors. Many clustering methods for networks have recently been proposed. Some of these works have dealt with a mixed network of assortative and disassortative models. These methods have been based on the fact that the entire structure of the network is observable. However, entities in real social network may be private and thus cannot be observed. We propose a privacy-preserving EM algorithm for clustering on distributed networks that not only deals with the mixture of assortative and disassortative models but also protects the privacy of each vertex in the network. In our solution, each vertex is treated as an independent private party, and the problem becomes an n -party privacy-preserving clustering, where n is the number of vertices in the network. Our algorithm does not reveal any intermediate information through its execution. The total running time is only related to the number of clusters and the maximum degree of the network but this is nearly independent of the total vertex number.",2012,Knowledge Discovery and Data Mining,Fields of study: dynamic network analysiscombinatoricsdata miningmachine learningmathematics
Clustering and understanding documents via discrimination information maximization,Malik Tahir Hassan (Lahore University of Management Sciences)Asim Karim (Lahore University of Management Sciences),"2112325462,2206695298","Text document clustering is a popular task for understanding and summarizing large document collections. Besides the need for efficiency, document clustering methods should produce clusters that are readily understandable as collections of documents relating to particular contexts or topics. Existing clustering methods often ignore term-document semantics while relying upon geometric similarity measures. In this paper, we present an efficient iterative partitional clustering method, CDIM, that maximizes the sum of discrimination information provided by documents. The discrimination information of a document is computed from the discrimination information provided by the terms in it, and term discrimination information is estimated from the currently labeled document collection. A key advantage of CDIM is that its clusters are describable by their highly discriminating terms --- terms with high semantic relatedness to their clusters' contexts. We evaluate CDIM both qualitatively and quantitatively on ten text data sets. In clustering quality evaluation, we find that CDIM produces high-quality clusters superior to those generated by the best methods. We also demonstrate the understandability provided by CDIM, suggesting its suitability for practical document clustering.",2012,Knowledge Discovery and Data Mining,Fields of study: fuzzy clusteringdocument clusteringinformation retrievaldata miningpattern recognitionmachine learningcomputer science
Peer matrix alignment: a new algorithm,Mohammed Kayed (Beni-Suef University),2478289502,"Web data extraction has been one of the keys for web content mining that tries to understand Web pages and discover valuable information from them. Most of the developed Web data extraction systems have used data (string/tree) alignment techniques. In this paper, we suggest a new algorithm for multiple string (peer matrix) alignment. Each row in the matrix represents one string of characters, where every character (symbol) corresponds to a subtree in the DOM tree of a web page. Two subtrees take the same symbol in the peer matrix if they are similar, where similarity can be measured using either structural, content, or visual information. Our algorithm is not a generalization of 2-strings alignment; it looks at multiple strings at the same time. Also, our algorithm considers the common problems in the field of Web data extraction: missing, multi-valued, multi-ordering, and disjunctive attributes. The experiments show a perfect alignment result with the matrices constructed from the nodes closed to the top (root) and an encourage result for the nodes closed to the leaves of the DOM trees of the test web pages.",2012,Knowledge Discovery and Data Mining,Fields of study: information extractiontheoretical computer scienceworld wide webdata miningmachine learningcomputer science
Learning to diversify expert finding with subtopics,Hang Su (Beihang University)Jie Tang (Tsinghua University)Wanling Hong (Beihang University),"2627558300,2158012360,2679414619","Expert finding is concerned about finding persons who are knowledgeable on a given topic. It has many applications in enterprise search, social networks, and collaborative management. In this paper, we study the problem of diversification for expert finding. Specifically, employing an academic social network as the basis for our experiments, we aim to answer the following question: Given a query and an academic social network, how to diversify the ranking list, so that it captures the whole spectrum of relevant authors' expertise? We precisely define the problem and propose a new objective function by incorporating topic-based diversity into the relevance ranking measurement. A learning-based model is presented to solve the objective function. Our empirical study in a real system validates the effectiveness of the proposed method, which can achieve significant improvements (+15.3%-+94.6% by MAP) over alternative methods.",2012,Knowledge Discovery and Data Mining,Fields of study: data sciencedata miningmachine learningcomputer science
Feature enriched nonparametric bayesian co-clustering,Pu Wang (George Mason University)Carlotta Domeniconi (George Mason University)Huzefa Rangwala (George Mason University)Kathryn B. Laskey (George Mason University),"2296294083,45678088,2096698710,2009108647","Co-clustering has emerged as an important technique for mining relational data, especially when data are sparse and high-dimensional. Co-clustering simultaneously groups the different kinds of objects involved in a relation. Most co-clustering techniques typically only leverage the entries of the given contingency matrix to perform the two-way clustering. As a consequence, they cannot predict the interaction values for new objects. In many applications, though, additional features associated to the objects of interest are available. The Infinite Hidden Relational Model (IHRM) has been proposed to make use of these features. As such, IHRM has the capability to forecast relationships among previously unseen data. The work on IHRM lacks an evaluation of the improvement that can be achieved when leveraging features to make predictions for unseen objects. In this work, we fill this gap and re-interpret IHRM from a co-clustering point of view. We focus on the empirical evaluation of forecasting relationships between previously unseen objects by leveraging object features. The empirical evaluation demonstrates the effectiveness of the feature-enriched approach and identifies the conditions under which the use of features is most useful, i.e., with sparse data.",2012,Knowledge Discovery and Data Mining,Fields of study: biclusteringdata miningpattern recognitionmachine learningstatisticscomputer science
ALIVE: a multi-relational link prediction environment for the healthcare domain,Reid A. Johnson (University of Notre Dame)Yang Yang 0008 (University of Notre Dame)Everaldo Aguiar (University of Notre Dame)Andrew K. Rider (University of Notre Dame)Nitesh V. Chawla (University of Notre Dame),"2127251370,2673426936,2125151847,1969557230,1979796846","An underlying assumption of biomedical informatics is that decisions can be more informed when professionals are assisted by analytical systems. For this purpose, we propose ALIVE, a multi-relational link prediction and visualization environment for the healthcare domain. ALIVE combines novel link prediction methods with a simple user interface and intuitive visualization of data to enhance the decision-making process for healthcare professionals. It also includes a novel link prediction algorithm, MRPF, which outperforms many comparable algorithms on multiple networks in the biomedical domain. ALIVE is one of the first attempts to provide an analytical and visual framework for healthcare analytics, promoting collaboration and sharing of data through ease of use and potential extensibility. We encourage the development of similar tools, which can assist in facilitating successful sharing, collaboration, and a vibrant online community.",2012,Knowledge Discovery and Data Mining,Fields of study: data scienceknowledge managementdata miningmachine learningcomputer science
Towards more efficient multi-label classification using dependent and independent dual space reduction,Eakasit Pacharawongsakda (Sirindhorn International Institute of Technology)Thanaruk Theeramunkong (Sirindhorn International Institute of Technology),"87579199,202049940","While multi-label classification can be widely applied for problems where multiple classes can be assigned to an object, its effectiveness may be sacrificed due to curse of dimensionality in the feature space and sparseness of dimensionality in the label space. Moreover, it suffers with high computational cost when there exist a high number of dimensions, as well as with lower accuracy when there are a number of noisy examples. As a solution, this paper presents two alternative methods, namely Dependent Dual Space Reduction and Independent Dual Space Reduction, to reduce dimensions in the dual spaces, i.e., the feature and label spaces, using Singular Value Decomposition (SVD). The first approach constructs the covariance matrix to represent dependency between the features and labels, project both of them into a single reduced space, and then perform prediction on the reduced space. On the other hand, the second approach handles the feature space and the label space separately by constructing a covariance matrix for each space to represent feature dependency and label dependency before performing SVD on dependency profile of each space to reduce dimension and for noise elimination and then predicting using their reduced dimensions. A number of experiments evidence that prediction on the reduced spaces for both dependent and independent reduction approaches can obtain better classification performance as well as faster computation, compared to the prediction using the original spaces. The dependent approach helps saving computational time while the independent approach tends to obtain better classification performance.",2012,Knowledge Discovery and Data Mining,Fields of study: singular value decompositiondiscrete mathematicsmachine learningmathematical optimizationcomputer sciencemathematics
EWNI: efficient anonymization of vulnerable individuals in social networks,Frank Nagle (Georgetown University)Lisa Singh (Georgetown University)Aris Gkoulalas-Divanis (IBM),"2632017286,2095714368,57288862","Social networks, patient networks, and email networks are all examples of graphs that can be studied to learn about information diffusion, community structure and different system processes; however, they are also all examples of graphs containing potentially sensitive information. While several anonymization techniques have been proposed for social network data publishing, they all apply the anonymization procedure on the entire graph. Instead, we propose a local anonymization algorithm that focuses on obscuring structurally important nodes that are not well anonymized, thereby reducing the cost of the overall anonymization procedure. Based on our experiments, we observe that we reduce the cost of anonymization by an order of magnitude while maintaining, and even improving, the accuracy of different graph centrality measures, e.g. degree and betweenness, when compared to another well known data publishing approach.",2012,Knowledge Discovery and Data Mining,Fields of study: data scienceworld wide webdata miningmachine learningcomputer science
Hybrid random forests: advantages of mixed trees in classifying text data,Baoxun Xu (Harbin Institute of Technology)Joshua Zhexue Huang (Chinese Academy of Sciences)Graham J. Williams (Chinese Academy of Sciences)Mark Junjie Li (Chinese Academy of Sciences)Yunming Ye (Harbin Institute of Technology),"2143509956,2111159692,2106893266,2146476422,2167640884","Random forests are a popular classification method based on an ensemble of a single type of decision tree. In the literature, there are many different types of decision tree algorithms, including C4.5, CART and CHAID. Each type of decision tree algorithms may capture different information and structures. In this paper, we propose a novel random forest algorithm, called a hybrid random forest. We ensemble multiple types of decision trees into a random forest, and exploit diversity of the trees to enhance the resulting model. We conducted a series of experiments on six text classification datasets to compare our method with traditional random forest methods and some other text categorization methods. The results show that our method consistently outperforms these compared methods.",2012,Knowledge Discovery and Data Mining,Fields of study: incremental decision treealternating decision treerandom forestdecision tree learningdecision treebiological classificationdata miningpattern recognitionmachine learningcomputer science
Nyström approximate model selection for LSSVM,Lizhong Ding (Tianjin University)Shizhong Liao (Tianjin University),"2635585107,2686442972","Model selection is critical to least squares support vector machine (LSSVM). A major problem of existing model selection approaches is that a standard LSSVM needs to be solved with O ( n 3) complexity for each iteration, where n is the number of training examples. In this paper, we propose an approximate approach to model selection of LSSVM. We use Nystrom method to approximate a given kernel matrix by a low rank representation of it. With such approximation, we first design an efficient LSSVM algorithm and theoretically analyze the effect of kernel matrix approximation on the decision function of LSSVM. Based on the matrix approximation error bound of Nystrom method, we derive a model approximation error bound, which is a theoretical guarantee of approximate model selection. We finally present an approximate model selection scheme, whose complexity is lower than the previous approaches. Experimental results on benchmark datasets demonstrate the effectiveness of approximate model selection.",2012,Knowledge Discovery and Data Mining,Fields of study: nystrom methodleast squares support vector machinemodel selectionpattern recognitionmachine learningmathematical optimizationstatisticscomputer sciencemathematics
CD: a coupled discretization algorithm,"Can Wang (University of Technology, Sydney)Mingchun Wang (Tianjin University of Technology and Education)Zhong She (University of Technology, Sydney)Longbing Cao (University of Technology, Sydney)","2293456513,2146615514,2118659272,2115085568","Discretization technique plays an important role in data mining and machine learning. While numeric data is predominant in the real world, many algorithms in supervised learning are restricted to discrete variables. Thus, a variety of research has been conducted on discretization, which is a process of converting the continuous attribute values into limited intervals. Recent work derived from entropy-based discretization methods, which has produced impressive results, introduces information attribute dependency to reduce the uncertainty level of a decision table; but no attention is given to the increment of certainty degree from the aspect of positive domain ratio. This paper proposes a discretization algorithm based on both positive domain and its coupling with information entropy, which not only considers information attribute dependency but also concerns deterministic feature relationship. Substantial experiments on extensive UCI data sets provide evidence that our proposed coupled discretization algorithm generally outperforms other seven existing methods and the positive domain based algorithm proposed in this paper, in terms of simplicity, stability, consistency, and accuracy.",2012,Knowledge Discovery and Data Mining,Fields of study: discretization of continuous featuresdiscretization errordiscretizationdata miningartificial intelligencemachine learningstatisticscomputer sciencemathematics
Feature weighting by RELIEF based on local hyperplane approximation,Hongmin Cai (South China University of Technology)Michael Ng (Hong Kong Baptist University),"2664280227,2110705915","In this paper, we propose a new feature weighting algorithm through the classical RELIEF framework. The key idea is to estimate the feature weights through local approximation rather than global measurement, as used in previous methods. The weights obtained by our method are more robust to degradation of noisy features, even when the number of dimensions is huge. To demonstrate the performance of our method, we conduct experiments on classification by combining hyperplane KNN model (HKNN) and the proposed feature weight scheme. Empirical study on both synthetic and real-world data sets demonstrate the superior performance of the feature selection for supervised learning, and the effectiveness of our algorithm.",2012,Knowledge Discovery and Data Mining,Fields of study: featurek nearest neighbors algorithmbiological classificationpattern recognitionmachine learningmathematical optimizationcomputer sciencemathematics
Mining web data for epidemiological surveillance,Didier BretonSandra Bringay (Centre national de la recherche scientifique)François MarquesPascal Poncelet (Centre national de la recherche scientifique)Mathieu Roche (Centre national de la recherche scientifique),"2709458222,150302470,2100791901,733374064,2115057967","Epidemiological surveillance is an important issue of public health policy. In this paper, we describe a method based on knowledge extraction from news and news classification to understand the epidemic evolution. Descriptive studies are useful for gathering information on the incidence and characteristics of an epidemic. New approaches, based on new modes of mass publication through the web, are developed: based on the analysis of user queries or on the echo that an epidemic may have in the media. In this study, we focus on a particular media: web news. We propose the Epimining approach, which allows the extraction of information from web news (based on pattern research) and a fine classification of these news into various classes (new cases, deaths...). The experiments conducted on a real corpora (AFP news) showed a precision greater than 94% and an F-measure above 85%. We also investigate the interest of tacking into account the data collected through social networks such as Twitter to trigger alarms.",2012,Knowledge Discovery and Data Mining,Fields of study: data scienceworld wide webdata miningcomputer science
Expectation-Maximization collaborative filtering with explicit and implicit feedback,Bin Wang (University of Calgary)Mohammadreza Rahimi (University of Calgary)Dequan Zhou (University of Calgary)Xin Wang (University of Calgary),"2311883549,2289119426,2134379510,2646086397","Collaborative Filtering (CF) is a popular strategy for recommender systems, which infers users' preferences typically using either explicit feedback (e.g., ratings) or implicit feedback (e.g., clicks). Explicit feedback is more accurate, but the quantity is not sufficient; whereas implicit feedback has an abundant quantity, but can be fairly inaccurate. In this paper, we propose a novel method, Expectation-Maximization Collaborative Filtering (EMCF), based on matrix factorization. The contributions of this paper include: first, we combine explicit and implicit feedback together in EMCF to infer users' preferences by learning latent factor vectors from matrix factorization; second, we observe four different cases of implicit feedback in terms of the distribution of latent factor vectors, and then propose different methods to estimate implicit feedback for different cases in EMCF; third, we develop an algorithm for EMCF to iteratively propagate the estimations of implicit feedback and update the latent factor vectors in order to fully utilize implicit feedback. We designed experiments to compare EMCF with other CF methods. The experimental results show that EMCF outperforms other methods by combining explicit and implicit feedback.",2012,Knowledge Discovery and Data Mining,Fields of study: theoretical computer sciencecontrol theorysimulationcomputer science
Evasion attack of multi-class linear classifiers,Han Xiao (Technische Universität München)Thomas Stibor (Technische Universität München)Claudia Eckert (Technische Universität München),"2701259695,304242683,2296566421","Machine learning has yield significant advances in decision-making for complex systems, but are they robust against adversarial attacks? We generalize the evasion attack problem to the multi-class linear classifiers, and present an efficient algorithm for approximating the optimal disguised instance. Experiments on real-world data demonstrate the effectiveness of our method.",2012,Knowledge Discovery and Data Mining,Fields of study: computer securitydata miningmachine learningcomputer science
Modality classification for medical images using sparse coded affine-invariant descriptors,Viktor Gál (Ghent University)Illés Solt (Indian Department of Telecommunications)Etienne E. Kerre (Ghent University)Mike Nachtegael (Ghent University),"2305238011,2675457609,1190317667,283735180","Modality is a key facet in medical image retrieval, as a user is likely interested in only one of e.g. radiology images, flowcharts, and pathology photos. While assessing image modality is trivial for humans, reliable automatic methods are required to deal with large un-annotated image bases, such as figures taken from the millions of scientific publications. We present a multi-disciplinary approach to tackle the classification problem by combining image features, meta-data, textual and referential information. We test our system's accuracy on the Image- CLEF 2011 medical modality classification data set. We show that using a fully affine-invariant feature descriptor and sparse coding on these descriptors in the Bag-of-Words image representation significantly increases the classification accuracy. Our best method achieves 87.89 and outperforms the state of the art.",2012,Knowledge Discovery and Data Mining,Fields of study: feature detectionneural codingcontextual image classificationautomatic image annotationtext mininginformation retrievalcomputer visiondata miningpattern recognitionmachine learningcomputer science
A term association translation model for naive bayes text classification,Meng-Sung Wu (Academia Sinica)Hsin-Min Wang (Academia Sinica),"2122701712,2147214055","Text classification (TC) has long been an important research topic in information retrieval (IR) related areas. In the literature, the bag-of-words (BoW) model has been widely used to represent a document in text classification and many other applications. However, BoW, which ignores the relationships between terms, offers a rather poor document representation. Some previous research has shown that incorporating language models into the naive Bayes classifier (NBC) can improve the performance of text classification. Although the widely used N -gram language models (LM) can exploit the relationships between words to some extent, they cannot model the long-distance dependencies of words. In this paper, we study the term association modeling approach within the translation LM framework for TC. The new model is called the term association translation model (TATM). The innovation is to incorporate term associations into the document model. We employ the term translation model to model such associative terms in the documents. The term association translation model can be learned based on either the joint probability (JP) of the associative terms through the Bayes rule or the mutual information (MI) of the associative terms. The results of TC experiments evaluated on the Reuters-21578 and 20newsgroups corpora demonstrate that the new model implemented in both ways outperforms the standard NBC method and the NBC with a unigram LM.",2012,Knowledge Discovery and Data Mining,Fields of study: mutual informationbayes theoremnatural language processingspeech recognitiondata miningpattern recognitionmachine learningstatisticscomputer science
A semi-supervised incremental clustering algorithm for streaming data,Maria Halkidi (University of Piraeus)Myra Spiliopoulou (Humboldt University of Berlin)Aikaterini Pavlou (Athens University of Economics and Business),"339332626,2672394148,2283911091","Nowadays many applications need to deal with evolving data streams . In this work, we propose an incremental clustering approach for the exploitation of user constraints on data streams. Conventional constraints do not make sense on streaming data, so we extend the classic notion of constraint set into a constraint stream . We propose methods for using the constraint stream as data items are forgotten or new items arrive. Also we present an on-line clustering approach for the cost-based enforcement of the constraints during cluster adaptation on evolving data streams. Our method introduces the concept of multi-clusters (m-clusters) to capture arbitrarily shaped clusters. An m-cluster consists of multiple dense overlapping regions, named s-clusters, each of which can be efficiently represented by a single point. Also it proposes the definition of outliers clusters in order to handle outliers while it provides methods to observe changes in structure of clusters as data evolves.",2012,Knowledge Discovery and Data Mining,Fields of study: correlation clusteringconstrained clusteringdata stream clusteringcure data clustering algorithmfuzzy clusteringcluster analysissemi supervised learningdata miningdatabasemachine learningcomputer science
Unsupervised sparse matrix co-clustering for marketing and sales intelligence,Anastasios Zouzias (University of Toronto)Michail Vlachos (IBM)Nikolaos M. Freris (IBM),"1221811721,2146138755,1548368623","Business intelligence focuses on the discovery of useful retail patterns by combining both historical and prognostic data. Ultimate goal is the orchestration of more targeted sales and marketing efforts. A frequent analytic task includes the discovery of associations between customers and products. Matrix co-clustering techniques represent a common abstraction for solving this problem. We identify shortcomings of previous approaches, such as the explicit input for the number of co-clusters and the common assumption for existence of a block-diagonal matrix form. We address both of these issues and present techniques for automated matrix co-clustering. We formulate the problem as a recursive bisection on Fiedler vectors in conjunction with an eigengap-driven termination criterion. Our technique does not assume perfect block-diagonal matrix structure after reordering. We explore and identify off-diagonal cluster structures by devising a Gaussian-based density estimator. Finally, we show how to explicitly couple co-clustering with product recommendations, using real-world business intelligence data. The final outcome is a robust co-clustering algorithm that can discover in an automatic manner both disjoint and overlapping cluster structures, even in the preserve of noisy observations.",2012,Knowledge Discovery and Data Mining,Fields of study: data miningartificial intelligencemachine learningstatisticscomputer science +1 other
SRF: a framework for the study of classifier behavior under training set mislabeling noise,Katsiaryna Mirylenka (University of Trento)George Giannakopoulos (University of Trento)Themis Palpanas (University of Trento),"250322829,2106620877,2010554420","Machine learning algorithms perform differently in settings with varying levels of training set mislabeling noise. Therefore, the choice of a good algorithm for a particular learning problem is crucial. In this paper, we introduce the ""Sigmoid Rule"" Framework focusing on the description of classifier behavior in noisy settings. The framework uses an existing model of the expected performance of learning algorithms as a sigmoid function of the signal-to-noise ratio in the training instances. We study the parameters of the above sigmoid function using five different classifiers, namely, Naive Bayes, kNN, SVM, a decision tree classifier, and a rule-based classifier. Our study leads to the definition of intuitive criteria based on the sigmoid parameters that can be used to compare the behavior of learning algorithms in the presence of varying levels of noise. Furthermore, we show that there exists a connection between these parameters and the characteristics of the underlying dataset, hinting at how the inherent properties of a dataset affect learning. The framework is applicable to concept drift scenaria, including modeling user behavior over time, and mining of noisy data series, as in sensor networks.",2012,Knowledge Discovery and Data Mining,Fields of study: concept driftbiological classificationlearning classifier systemdata miningpattern recognition +3 others
Hierarchical graph summarization: leveraging hybrid information through visible and invisible linkage,Rui Yan (Peking University)Zi Yuan (Beihang University)Xiaojun Wan (Peking University)Yan Zhang (Peking University)Xiaoming Li (Peking University),"2109109241,2722432310,2146508076,2686606814,2633873560","Graph-based ranking algorithm has been recently exploited for summarization by using sentence-to-sentence relationships. Given a document set with linkage information to summarize, different sentences belong to different documents or clusters (either visible cluster via anchor texts or invisible cluster by semantics), which enables a hierarchical structure. It is challenging and interesting to investigate the impacts and weights of source documents/clusters: sentence from important ones are deemed more salient than the others. This paper aims to integrate three types of hierarchical linkage into traditional graph-based methods by proposing Hierarchical Graph Summarization (HGS). We utilize a hierarchical language model to measure the sentence relationships in HGS. We develop experimental systems to compare 5 rival algorithms on 4 instinctively different datasets which amount to 5197 documents. Performance comparisons between different system-generated summaries and manually created ones by human editors demonstrate the effectiveness of our approach in ROUGE metrics.",2012,Knowledge Discovery and Data Mining,Fields of study: automatic summarizationdata miningpattern recognitionmachine learningcomputer science
Foundation of mining class-imbalanced data,Da Kuang (University of Western Ontario)Charles X. Ling (University of Western Ontario)Jun Du (University of Western Ontario),"2142576100,2664974409,2619459523","Mining class-imbalanced data is a common yet challenging problem in data mining and machine learning. When the class is imbalanced, the error rate of the rare class is usually much higher than that of the majority class. How many samples do we need in order to bound the error of the rare class (and the majority class)? If the misclassification cost of the class is known, can the cost-weighted error be bounded as well? In this paper, we attempt to answer those questions with PAC-learning. We derive several upper bounds on the sample size that guarantee the error on a particular class (the rare and majority class) and the cost-weighted error, with the consistent and agnostic learners. Similar to the upper bounds in traditional PAC learning, our upper bounds are quite loose. In order to make them more practical, we empirically study the pattern observed in our upper bounds. From the empirical results we obtain some interesting implications for data mining in real-world applications. As far as we know, this is the first work providing theoretical bounds and the corresponding practical implications for mining class-imbalanced data with unequal cost.",2012,Knowledge Discovery and Data Mining,Fields of study: data miningmachine learningstatisticsmathematics
Named entity recognition and identification for finding the owner of a home page,Vassilis Plachouras (École Polytechnique)Matthieu Rivière (École Polytechnique)Michalis Vazirgiannis (École Polytechnique),"2310191992,2337859905,1914497179","Entity-based applications, such as expert search or online social networks where users search for persons, require high-quality datasets of named entity references. Obtaining such high-quality datasets can be achieved by automatically extracting metadata from Web pages. In this work, we focus on the identification of the named entity that corresponds to the owner of a particular Web page, for example, a home page or an organizational staff Web page. More specifically, from a set of named entities that have already been extracted from a Web page, we identify the one which corresponds to the owner of the home page. First, we develop a set of features which are combined in a scoring function to select the named entity of the Web page owner. Second, we formulate the problem as a classification problem in which a pair of a Web page and named entity is classified as being associated or not. We evaluate the proposed approaches on a set of Web pages in which we have previously identified named entities. Our experimental results show that we can identify the named entity corresponding to the owner of a home page with accuracy over 90%.",2012,Knowledge Discovery and Data Mining,Fields of study: page viewentity linkingworld wide webinformation retrievaldata mining +1 other
Unsupervised multi-label text classification using a world knowledge ontology,Xiaohui Tao (University of Southern Queensland)Yuefeng Li (Queensland University of Technology)Raymond Y. K. Lau (City University of Hong Kong)Hua Wang (University of Southern Queensland),"2144301289,2122434521,2099970733,2306205450","The development of text classification techniques has been largely promoted in the past decade due to the increasing availability and widespread use of digital documents. Usually, the performance of text classification relies on the quality of categories and the accuracy of classifiers learned from samples. When training samples are unavailable or categories are unqualified, text classification performance would be degraded. In this paper, we propose an unsupervised multi-label text classification method to classify documents using a large set of categories stored in a world ontology. The approach has been promisingly evaluated by compared with typical text classification methods, using a real-world document collection and based on the ground truth encoded by human experts.",2012,Knowledge Discovery and Data Mining,Fields of study: library classificationinformation retrievaldata miningpattern recognitioncomputer science
Techniques for efficient learning without search,"Houssam Salem (Faculty of Information Technology, University Džemal Bijedić of Mostar)Pramuditha Suraweera (Faculty of Information Technology, University Džemal Bijedić of Mostar)Geoffrey I. Webb (Faculty of Information Technology, University Džemal Bijedić of Mostar)Janice R. Boughton (Faculty of Information Technology, University Džemal Bijedić of Mostar)","2169046289,2250940354,2126304162,2150007396","Averaged n -Dependence Estimators (A n DE) is a family of learning algorithms that range from low variance coupled with high bias through to high variance coupled with low bias. The asymptotic error of the lowest bias variant is the Bayes optimal. The A n DE family of algorithms have a training time that is linear with respect to the training examples, learn in a single pass through the data, support incremental learning, handle missing values directly and are robust in the face of noise. These characteristics make the algorithms particularly well suited to learning from large data. However, for higher orders of n they are very computationally demanding. This paper presents data structures and algorithms developed to reduce both memory and time for training and classification. These enhancements have enabled the evaluation and comparison of A3DE's effectiveness. The results provide further support for the hypothesis that as the number of training examples increases, decreasing error will be attained by members of the A n DE family with increasing levels of n .",2012,Knowledge Discovery and Data Mining,Fields of study: naive bayes classifiersupervised learningdata miningpattern recognitionmachine learning +2 others
A pruning-based approach for searching precise and generalized region for synthetic minority over-sampling,Kamthorn Puntumapon (Kasetsart University)Kitsana Waiyamai (Kasetsart University),"1992914049,343859126","One solution to deal with class imbalance is to modify its class distribution. Synthetic over-sampling is a well-known method to modify class distribution by generating new synthetic minority data. Synthetic Minority Over-sampling TEchnique (SMOTE) is a state-of-the-art synthetic over-sampling algorithm that generates new synthetic data along the line between the minority data and their selected nearest neighbors. Advantages of SMOTE is to have decision regions larger and less specific to original data. However, its drawback is the over-generalization problem where synthetic data is generated into majority class region. Over-generalization leads to misclassify non-minority class region into minority class. To overcome the over-generalization problem, we propose an algorithm, called TRIM, to search for precise minority region while maintaining its generalization. TRIM iteratively filters out irrelevant majority data from the precise minority region. Output of the algorithm is the multiple set of seed minority data, and each individual set will be used for generating new synthetic data. Compared with state-of-the-art over-sampling algorithms, experimental results show significant performance improvement in terms of F-measure and AUC. This suggests over-generalization has a significant impact on the performance of the synthetic over-sampling method.",2012,Knowledge Discovery and Data Mining,Fields of study: data miningartificial intelligencemachine learningmathematics
A generic classifier-ensemble approach for biomedical named entity recognition,Zhihua Liao (Hunan Normal University)Zili Zhang (Southwest University),"2718330000,2100814928","In named entity recognition (NER) for biomedical literature, approaches based on combined classifiers have demonstrated great performance improvement compared to a single (best) classifier. This is mainly owed to sufficient level of diversity exhibited among classifiers, which is a selective property of classifier set. Given a large number of classifiers, how to select different classifiers to put into a classifier-ensemble is a crucial issue of multiple classifier-ensemble design. With this observation in mind, we proposed a generic genetic classifier-ensemble method for the classifier selection in biomedical NER. Various diversity measures and majority voting are considered, and disjoint feature subsets are selected to construct individual classifiers. A basic type of individual classifier --- Support Vector Machine (SVM) classifier is adopted as SVM-classifier committee. A multi-objective Genetic algorithm (GA) is employed as the classifier selector to facilitate the ensemble classifier to improve the overall sample classification accuracy. The proposed approach is tested on the benchmark dataset --- GENIA version 3.02 corpus, and compared with both individual best SVM classifier and SVM-classifier ensemble algorithm as well as other machine learning methods such as CRF, HMM and MEMM. The results show that the proposed approach outperforms other classification algorithms and can be a useful method for the biomedical NER problem.",2012,Knowledge Discovery and Data Mining,Fields of study: margin classifiercascading classifiersmarginrandom subspace methodlinear classifier +5 others
Two-View online learning,Tam T. Nguyen (Nanyang Technological University)Kuiyu Chang (Nanyang Technological University)Siu Cheung Hui (Nanyang Technological University),"2119014062,2147346151,2166513862","We propose a two-view online learning algorithm that utilizes two different views of the same data to achieve something that is greater than the sum of its parts. Our algorithm is an extension of the single-view Passive Aggressive (PA) algorithm, where we minimize the changes in the two view weights and disagreements between the two classifiers. The final classifier is an equally weighted sum of the individual classifiers. As a result, disagreements between the two views are tolerated as long as the final combined classifier output is not compromised. Our approach thus allows the stronger voice (view) to dominate whenever the two views disagree. This additional allowance of diversity between the two views is what gives our approach the edge, as espoused by classical ensemble learning theory. Our algorithm is evaluated and compared to the original PA algorithm on three datasets. The experimental results show that it consistently outperforms the PA algorithm on individual views and concatenated view by up to 3%.",2012,Knowledge Discovery and Data Mining,Fields of study: data miningartificial intelligencemachine learningcomputer science
Instance-Ranking: a new perspective to consider the instance dependency for classification,Xin Xia (Zhejiang University)Xiaohu Yang (Zhejiang University)Shanping Li (Zhejiang University)Chao Wu (Zhejiang University),"2167586149,2689573558,2651143720,2149177375","Single-label classification refers to the task to predict an instance to be one unique label in a set of labels. Different from single-label classification, for multi-label classification, one instance is associated with one or more labels in a set of labels simultaneously. Various works have focused on the algorithms for those two types of classification. Since the ranking problem is always coexisting with the classification problem, and traditional researches mainly assume the uniform distribution for the instances, in this paper, we propose a new perspective for the ranking problem. With the assumption that the distribution for the instance is not uniform, different instances have different influences for the distribution, the Instance-Ranking algorithm is presented. With the Instance- Ranking algorithm, the famous K-nearest-neighbors (KNN) algorithm is modified to confirm the validity of our algorithm. Lastly, the Instance-Ranking algorithm is combined with the ML.KNN algorithm for multi-label classification. Experiment with different datasets show that our Instance-Ranking algorithm achieves better performance than the original state-of-art algorithm such as KNN and ML.KNN.",2012,Knowledge Discovery and Data Mining,Fields of study: k nearest neighbors algorithmdata miningpattern recognitionmachine learningcomputer science +1 other
Selecting feature subset via constraint association rules,Guangtao Wang (Xi'an Jiaotong University)Qinbao Song (Xi'an Jiaotong University),"2130410677,2084840102","In this paper, a novel feature selection algorithm FEAST is proposed based on association rule mining. The proposed algorithm first mines association rules from a data set; then, it identifies the relevant and interactive feature values with the constraint association rules whose consequent is the target concept, and detects the redundant feature values with constraint association rules whose consequent and antecedent are both single feature value. After that, it eliminates the redundant feature values, and obtains the feature subset by mapping the relevant feature values to corresponding features. The efficiency and effectiveness of FEAST are tested upon both synthetic and real world data sets, and the classification results of the three different types of classifiers (including Naive Bayes, C4.5 and PART) with the other four representative feature subset selection algorithms (including CFS, FCBF, INTERACT and associative-based FSBAR) were compared. The results on synthetic data sets show that FEAST can effectively identify irrelevant and redundant features while reserving interactive ones. The results on the real world data sets show that FEAST outperformed other feature subset selection algorithms in terms of average classification accuracy and Win/Draw/Loss record.",2012,Knowledge Discovery and Data Mining,Fields of study: association rule learningfeaturedata miningpattern recognitionmachine learning +2 others
Triangular kernel nearest-neighbor-based clustering algorithm for discovering true clusters,Aina Musdholifah (Universiti Teknologi Malaysia)Siti Zaiton Mohd Hashim (Universiti Teknologi Malaysia),"2339081377,2141920092","Clustering is a powerful exploratory technique for extracting the knowledge of given data. Several clustering techniques that have been proposed require predetermined number of clusters. However, the triangular kernel-nearest neighbor-based clustering (TKNN) has been proven able to determine the number and member of clusters automatically. TKNN provides good solutions for clustering non-spherical and high-dimensional data without prior knowledge of data labels. On the other hand, there is no definite measure to evaluate the accuracy of the clustering result. In order to evaluate the performance of the proposed TKNN clustering algorithm, we utilized various benchmark classification datasets. Thus, TKNN is proposed for discovering true clusters with arbitrary shape, size and density contained in the datasets. The experimental results on benched-mark datasets showed the effectiveness of our technique. Our proposed TKNN achieved more accurate clustering results and required less time processing compared with k-means, ILGC, DBSCAN and KFCM.",2012,Knowledge Discovery and Data Mining,Fields of study: k medians clusteringcanopy clustering algorithmcorrelation clusteringconstrained clusteringdata stream clustering +12 others
"SHARD: a framework for sequential, hierarchical anomaly ranking and detection",Jason Robinson (Georgetown University)Margaret Lonergan (Georgetown University)Lisa Singh (Georgetown University)Allison Candido (Georgetown University)Mehmet Sayal (Hewlett-Packard),"2537855141,2535007275,2095714368,2535824596,2131501168","This work explores unsupervised anomaly detection within sequential, hierarchical data. We present a flexible framework for detecting, ranking and analyzing anomalies. The framework 1) allows users to incorporate complex, multidimensional, hierarchical data into the anomaly detection process; 2) uses an ensemble method that can incorporate multiple unsupervised anomaly detection algorithms and configurations; 3) identifies anomalies from combinations of categorical, numeric and temporal data at different conceptual resolutions of hierarchical data; 4) supports a set of anomaly ranking schemes; and 5) uses an interactive tree hierarchy visualization to highlight anomalous regions and relationships. Using both synthetic and real world data, we show that standard anomaly detection algorithms, when plugged into our framework, maintain a high anomaly detection accuracy and identify both micro-level, detailed anomalies and macro-level global anomalies in the data.",2012,Knowledge Discovery and Data Mining,Fields of study: data miningpattern recognitionmachine learningmathematics
Diversity analysis on boosting nominal concepts,Nida Meddouri (Tunis El Manar University)Héla Khoufi (Tunis El Manar University)Mondher Sadok Maddouri (Taibah University),"41786152,2283372709,2440760796","In this paper, we investigate how the diversity of nominal classifier ensembles affects the AdaBoost performance [13]. Using 5 real data sets from the UCI Machine Learning Repository and 3 different diversity measures, we show that $\mathcal{Q}$ Statistic measure is mostly correlated with AdaBoost performance for 2-class problems. The experimental results suggest that the performance of AdaBoost depend on the nominal classifier diversity that can be used as a stopping criteria in ensemble learning.",2012,Knowledge Discovery and Data Mining,Fields of study: boostingadaboostdata miningpattern recognitionmachine learning +1 other
Extreme value prediction for zero-inflated data,Fan Xin (Michigan State University)Zubin Abraham (Michigan State University),"2286959756,1994574529","Depending on the domain, there may be significant ramifications associated with the occurrence of an extreme event (for e.g., the occurrence of a flood from a climatological perspective). However, due to the relative low occurrence rate of extreme events, the accurate prediction of extreme values is a challenging endeavor. When it comes to zero-inflated time series, standard regression methods such as multiple linear regression and generalized linear models, which emphasize estimating the conditional expected value, are not best suited for inferring extreme values. And so is the case when the the conditional distribution of the data does not conform to the parametric distribution assumed by the regression model. This paper presents a coupled classification and regression framework that focuses on reliable prediction of extreme value events in a zero-inflated time series. The framework was evaluated by applying it on a real-world problem of statistical downscaling of precipitation for the purpose of climate impact assessment studies. The results suggest that the proposed framework is capable of detecting the timing and magnitude of extreme precipitation events effectively compared with several baseline methods.",2012,Knowledge Discovery and Data Mining,Fields of study: extreme value theoryeconometricsdata miningstatistics
A double-ensemble approach for classifying skewed data streams,Chongsheng Zhang (Henan University)Paolo Soda (Università Campus Bio-Medico),"2684556227,2038168928","Nowadays, many applications need to handle large amounts of streaming data, which often presents a skewed distribution, i.e. one or more classes are largely under-represented in comparison to the others. Unfortunately, little effort has been directed towards the classification of skewed data streams, although class-imbalance learning has already been studied in the area of pattern recognition on static data. Furthermore, while existing class-imbalance learning methods increase the recognition accuracy on minority class, they often harm the global classification accuracy. Motivated by these observations, we develop an approach suited for classifying skewed data streams, which integrates two ensembles of classifiers, each one suited for non-skewed and skewed data. This approach substantially increases the global accuracy compared to existing classification methods for skewed data. Experimental tests have been carried out on three public datasets showing interesting results. As a further contribution, we will study metrics to evaluate the performance of skewed data streams classification. We will also review the literature on class-imbalance learning, and skewed data streams classification.",2012,Knowledge Discovery and Data Mining,Fields of study: data miningpattern recognitionmachine learningcomputer science
An improved genetic clustering algorithm for categorical data,Hongwu Qin (Universiti Malaysia Pahang)Xiuqin Ma (Universiti Malaysia Pahang)Tutut Herawan (Universiti Malaysia Pahang)Jasni Mohamad Zain (Universiti Malaysia Pahang),"2144708694,2133330932,2680092893,2147701941","Deng et al. [Deng, S., He, Z., Xu, X.: G-ANMI: A mutual information based genetic clustering algorithm for categorical data, Knowledge-Based Systems 23, 144---149(2010)] proposed a mutual information based genetic clustering algorithm named G-ANMI for categorical data. While G-ANMI is superior or comparable to existing algorithms for clustering categorical data in terms of clustering accuracy, it is very time-consuming due to the low efficiency of genetic algorithm (GA). In this paper, we propose a new initialization method for G-ANMI to improve its efficiency. Experimental results show that the new method greatly improves the efficiency of G-ANMI as well as produces higher clustering accuracy.",2012,Knowledge Discovery and Data Mining,Fields of study: flame clusteringhierarchical clustering of networksk medians clusteringbrown clusteringcanopy clustering algorithmdetermining the number of clusters in a data setdbscancorrelation clusteringdata stream clusteringcure data clustering algorithmaffinity propagationfuzzy clusteringclustering high dimensional datacategorical variablecluster analysisgenetic algorithmconsensus clusteringbiclusteringconceptual clusteringdata miningpattern recognitionmachine learningcomputer science
Spread of information in a social network using influential nodes,Arpan Chaudhury (Heritage Institute of Technology)Partha Basuchowdhuri (Heritage Institute of Technology)Subhashis Majumder (Heritage Institute of Technology),"2426708923,1151627567,2115615678","Viral marketing works with a social network as its backbone, where social interactions help spreading a message from one person to another. In social networks, a node with a higher degree can reach larger number of nodes in a single hop, and hence can be considered to be more influential than a node with lesser degree. For viral marketing with limited resources, initially the seller can focus on marketing the product to a certain influential group of individuals, here mentioned as core . If k persons are targeted for initial marketing, then the objective is to find the initial set of k active nodes, which will facilitate the spread most efficiently. We did a degree based scaling in graphs for making the edge weights suitable for degree based spreading. Then we detect the core from the maximum spanning tree (MST) of the graph by finding the top k influential nodes and the paths in MST that joins them. The paths within the core depict the key interaction sequences that will trigger the spread within the network. Experimental results show that the set of k influential nodes found by our core finding method spreads information faster than the greedy k -center method for the same k value.",2012,Knowledge Discovery and Data Mining,Fields of study: social network analysistheoretical computer sciencedistributed computingmachine learningmathematics
An explicit description of the extended gaussian kernel,Yong Liu (Tianjin University)Shizhong Liao (Tianjin University),"2686106898,2686442972","Kernel methods play an important role in machine learning, pattern recognition and data mining. Although the kernel functions are the central part of the kernel methods, little is known about the structure of its reproducing kernel Hilbert spaces (RKHS) and the eigenvalues of the integral operator. In this paper, we first give the definition of the extended Gaussian kernel which includes the Gaussian kernel as its special case. Then, through a generalization form of the Weyl inner product, we present an explicit description of the RKHS of the extended Gaussian kernel. Furthermore, using the Funk-Hecke formula, we get the eigenvalues and eigenfunctions of the integral operator on the unit sphere.",2012,Knowledge Discovery and Data Mining,Fields of study: kernel embedding of distributionskernelszegő kernelvariable kernel density estimationrepresenter theorempolynomial kernelbergman kernelpoisson kernelstring kernelkernelkernel smootherreproducing kernel hilbert spaceradial basis function kernelkernel principal component analysiskernel methodgaussian functiondot productkerneleigenvalues and eigenvectorsdiscrete mathematicsmathematical analysismachine learningmathematical optimizationcomputer sciencemathematics
Adaptive evidence accumulation clustering using the confidence of the objects' assignments,João M. M. Duarte (International Student Exchange Programs)Ana L. N. Fred (Instituto Superior Técnico)F. Jorge F. Duarte (International Student Exchange Programs),"2430008008,2297453548,2304948219","Ensemble methods are known to increase the performance of learning algorithms, both on supervised and unsupervised learning. Boosting algorithms are quite successful in supervised ensemble methods. These algorithms build incrementally an ensemble of classifiers by focusing on objects previously misclassified while training the current classifier. In this paper we propose an extension to the Evidence Accumulation Clustering method inspired by the Boosting algorithms. While on supervised learning the identification of misclassified objects is a trivial task because the labels for each object are known, on unsupervised learning these are unknown, making it difficult to identify the objects on which the clustering algorithm should focus. The proposed approach uses the information contained in the co-association matrix to identify degrees of confidence of the assignments of each object to its cluster. The degree of confidence is then used to select which objects should be emphasized in the learning process of the clustering algorithm. New consensus partition validity measures, based on the notion of degree of confidence, are also proposed. In order to evaluate the performance of our approaches, experiments on several artificial and real data sets were performed and shown the adaptive clustering ensemble method and the consensus partition validity measure help to improve the quality of data clustering.",2012,Knowledge Discovery and Data Mining,Fields of study: flame clusteringcanopy clustering algorithmcorrelation clusteringconstrained clusteringfuzzy clusteringcluster analysisconsensus clusteringconceptual clusteringensemble learningdata miningpattern recognitionmachine learningcomputer science
An aggressive margin-based algorithm for incremental learning,JuiHsi Fu (National Chung Cheng University)SingLing Lee (National Chung Cheng University),"2147391244,2147161411","In incremental learning, the classification model is incrementally updated using the small datasets. Different with existing methods, our approach updates the current classifier according to each sample in the dataset, respectively. The classifier is updated by adjusting more than the margin of each sample. Then the new classifier is generated by carefully analyzing classifier adjustments caused for labeled samples. Additionally the new classifier shall correct prediction mistakes of the previous classifier as many as possible. In details, we formulate simple constrained optimization problems and then the updated classifier is the solution derived using Lagrange multipliers. In our experiments, 13 real-world dataset are used to present the effectiveness of the proposed approach. The experimental results are shown that our update strategy is able to adjust the classifier properly. And it is also shown that the proposed incremental learning approach is suitable to be applied for the requirement of frequently adjusting the existing classifiers.",2012,Knowledge Discovery and Data Mining,Fields of study: margin classifiermarginquadratic classifierlearning classifier systemdata miningpattern recognitionmachine learningcomputer science
Domain transfer dimensionality reduction via discriminant kernel learning,Ming Zeng (Sun Yat-sen University)Jiangtao Ren (Sun Yat-sen University),"2500670830,2170114694","Kernel discriminant analysis (KDA) is a popular technique for discriminative dimensionality reduction in data analysis. But, when a limited number of labeled data is available, it is often hard to extract the required low dimensional representation from a high dimensional feature space. Thus, one expects to improve the performance with the labeled data in other domains. In this paper, we propose a method, referred to as the domain transfer discriminant kernel learning (DTDKL), to find the optimal kernel by using the other labeled data from out-of-domain distribution to carry out discriminant dimensionality reduction. Our method learns a kernel function and discriminative projection by maximizing the Fisher discriminant distance and minimizing the mismatch between the in-domain and out-of-domain distributions simultaneously, by which we may get a better feature space for discriminative dimensionality reduction with cross-domain.",2012,Knowledge Discovery and Data Mining,Fields of study: kernel embedding of distributionsoptimal discriminant analysistree kernelvariable kernel density estimationpolynomial kernelmultiple discriminant analysisstring kerneltransfer of learningkernel fisher discriminant analysisradial basis function kernelkernel principal component analysisdimensionality reductionlinear discriminant analysispattern recognitionmachine learningstatisticscomputer sciencemathematics
DisClose : discovering colossal closed itemsets via a memory efficient compact row-tree,"Nurul F. Zulkurnain (International Islamic University Malaysia)David J. Haglin (Pacific Northwest National Laboratory)John A. Keane (School of Computer Science, University of Manchester)","1967718217,2634845762,2136990782","A recent focus in itemset mining has been the discovery of frequent itemsets from high-dimensional datasets. With exponentially increasing running time as average row length increases, mining such datasets renders most conventional algorithms impractical. Unfortunately, large cardinality itemsets are likely to be more informative than small cardinality itemsets in this type of dataset. This paper proposes an approach, termed DisClose, to extract large cardinality (colossal) closed itemsets from high-dimensional datasets. The approach relies on a Compact Row-Tree data structure to represent itemsets during the search process. Large cardinality itemsets are enumerated first followed by smaller ones. In addition, we utilize a minimum cardinality threshold to further reduce the search space. Experimental results show that DisClose can achieve extraction of colossal closed itemsets in the discovered datasets, even for low support thresholds. The algorithm immediately discovers closed itemsets without needing to check if each new closed itemset has previously been found.",2012,Knowledge Discovery and Data Mining,Fields of study: data miningdatabasealgorithmmathematics
Towards identity disclosure control in private hypergraph publishing,Yidong Li (Beijing Jiaotong University)Hong Shen (Beijing Jiaotong University),"2161846278,2194483685","Identity disclosure control (IDC) on complex data has attracted increasing interest in security and database communities. Most existing work focuses on preventing identity disclosure in graphs that describes pairwise relations between data entities. Many data analysis applications need information about multi-relations among entities, which can be well represented with hypergraphs. However, the IDC problem has been little studied in publishing hypergraphs due to the diversity of hypergraph information which may expose to many types of background knowledge attacks. In this paper, we introduce a novel attack model with the properties of hyperedge rank as background knowledge, and formalize the rank-based hypergraph anonymization (RHA) problem. We propose an algorithm running in near-quadratic time on hypergraph size for rank anonymization which we show to be NP-hard, and in the meanwhile, maintaining data utility for community detection. We also show how to construct the hypergraph under the anonymized properties to protect a hypergraph from rank-based attacks. The performances of the methods have been validated by extensive experiments on real-world datasets. Our rank-based attack model and algorithms for rank anonymization and hypergraph construction are, to our best knowledge, the first systematic study for private hypergraph publishing.",2012,Knowledge Discovery and Data Mining,Fields of study: theoretical computer scienceworld wide webdata miningmathematics
The relevance of spatial relation terms and geographical feature types,Chunju Zhang (Nanjing Normal University)Xueying Zhang (Nanjing Normal University)Chaoli Du (Nanjing Normal University),"2096540977,2287131203,2110828357","Spatial relation terms can generally indicate spatial relations described in natural language context. Their semantic representation is closely related to geographical entities and their characteristics e.g. geometry, scale and geographical feature types. This paper proposes a quantitative approach to explore the semantic relevance of spatial relation terms and geographical feature types in text. Firstly, a classification of spatial relation terms is performed. Secondly, the ""Overlap"" similarity measure is introduced to define the relevance of spatial relation terms and geographical feature types based on a large scale annotation corpus. Thirdly, the relevance is expanded with the semantic distance and hierarchical relationship of the classification system of geographical feature types. Finally, a knowledge base based on protege is developed to formally represent and visualize geographical feature types, spatial relation classifications, and the relevance of spatial relation terms and geographical feature types. This study indicates that spatial relation terms are strongly relevant to geographical feature types. The semantic representation of topological relation terms is diverse and their relevance with geographical feature types is much stronger than directional relation and distance relation terms, but the annotation quality and the classification granularity of geographical entities in the corpus have a great effect on the performance.",2012,Knowledge Discovery and Data Mining,Fields of study: spatial relationrelevanceinformation retrievaldata miningpattern recognition
Semantic social network analysis with text corpora,Dong-mei YangHui ZhengJi-kun YanYe Jin,"2121266600,2637088408,2158350010,2250366037","We present the Document-Entity-Topic (DET) model for semantic social network analysis which tries to find out the interested entities through the topics we aim at, detect groups according to the entities which concern the similar topics, and rank the plentiful entities in a document to figure out the most valuable ones. DET model learns the topic distributions by the literal descriptions of entities. The model is similar to Author-Topic (AT) model, adding the key attribute that the distribution of entities in a document is not uniform but Dirichlet allocation. We experiment on the ""Libya Event"" data set which is collected from the Internet. DET model increases the precision on tasks of social network analysis and gives much lower perplexity than AT model.",2012,Knowledge Discovery and Data Mining,Fields of study: topic modelnatural language processinginformation retrievaldata miningmachine learning +1 other
Survey on web spam detection: principles and algorithms,Nikita Spirin (University of Illinois at Urbana–Champaign)Jiawei Han (University of Illinois at Urbana–Champaign),"2145605135,2121939561","Search engines became a de facto place to start information acquisition on the Web. Though due to web spam phenomenon, search results are not always as good as desired. Moreover, spam evolves that makes the problem of providing high quality search even more challenging. Over the last decade research on adversarial information retrieval has gained a lot of interest both from academia and industry. In this paper we present a systematic review of web spam detection techniques with the focus on algorithms and underlying principles. We categorize all existing algorithms into three categories based on the type of information they use: content-based methods, link-based methods, and methods based on non-traditional data such as user behaviour, clicks, HTTP sessions. In turn, we perform a subcategorization of link-based category into five groups based on ideas and principles used: labels propagation, link pruning and reweighting, labels refinement, graph regularization, and featurebased. We also define the concept of web spam numerically and provide a brief survey on various spam forms. Finally, we summarize the observations and underlying principles applied for web spam detection.",2012,Knowledge Discovery and Data Mining,Fields of study: content farmspambotadversarial information retrievallink farmspamdexing +11 others
Introduction to the special section on educational data mining,T Toon Calders (Eindhoven University of Technology)M Mykola Pechenizkiy (Eindhoven University of Technology),"2064105222,2019713363","Educational Data Mining (EDM) is an emerging multidisciplinary research area, in which methods and techniques for exploring data originating from various educational information systems have been developed. EDM is both a learning science, as well as a rich application area for data mining, due to the growing availability of educational data. EDM contributes to the study of how students learn, and the settings in which they learn. It enables data-driven decision making for improving the current educational practice and learning material. We present a brief overview of EDM and introduce four selected EDM papers representing a crosscut of different application areas for data mining in education.",2012,Knowledge Discovery and Data Mining,Fields of study: information systemdata scienceknowledge managementdata miningcomputer science
Data mining for improving textbooks,Rakesh Agrawal (Microsoft)Sreenivas Gollapudi (Microsoft)Anitha Kannan (Microsoft)Krishnaram Kenthapadi (Microsoft),"2537924216,2023254819,2146153601,2088122068","We present our early explorations into developing a data mining based approach for enhancing the quality of textbooks. We describe a diagnostic tool to algorithmically identify deficient sections in textbooks. We also discuss techniques for algorithmically augmenting textbook sections with links to selective content mined from the Web. Our evaluation, employing widely-used textbooks from India, indicates that developing technological approaches to help improve textbooks holds promise.",2012,Knowledge Discovery and Data Mining,Fields of study: data scienceinformation retrievaldata miningmachine learningcomputer science
The sum is greater than the parts: ensembling models of student knowledge in educational software,Pardos Zachary A (Worcester Polytechnic Institute)Sujith M. Gowda (Worcester Polytechnic Institute)Ryan S.J.d. Baker (Worcester Polytechnic Institute)Neil T. Heffernan (Worcester Polytechnic Institute),"2332217518,2166930069,2097900229,542498001","Many competing models have been proposed in the past decade for predicting student knowledge within educational software. Recent research attempted to combine these models in an effort to improve performance but have yielded inconsistent results. While work in the 2010 KDD Cup data set showed the benefits of ensemble methods, work in the Genetics Tutor failed to show similar benefits. We hypothesize that the key factor has been data set size. We explore the potential for improving student performance prediction with ensemble methods in a data set drawn from a different tutoring system, the ASSISTments Platform, which contains 15 times the number of responses of the Genetics Tutor data set. We evaluated the predictive performance of eight student models and eight methods of ensembling predictions. Within this data set, ensemble approaches were more effective than any single method with the best ensemble approach producing predictions of student performance 10% better than the best individual student knowledge model.",2012,Knowledge Discovery and Data Mining,Fields of study: educational softwaredata sciencedata miningmachine learningcomputer science
Mapping question items to skills with non-negative matrix factorization,Michel C. Desmarais (École Polytechnique de Montréal),848380144,"Intelligent learning environments need to assess the student skills to tailor course material, provide helpful hints, and in general provide some kind of personalized interaction. To perform this assessment, question items, exercises, and tasks are presented to the student. This assessment relies on a mapping of tasks to skills. However, the process of deciding which skills are involved in a given task is tedious and challenging. Means to automate it are highly desirable, even if only partial automation that provides supportive tools can be achieved. A recent technique based on Non-negative Matrix Factorization (NMF) was shown to offer valuable results, especially due to the fact that the resulting factorization allows a straightforward interpretation in terms of a Q-matrix. We investigate the factors and assumptions under which NMF can effectively derive the underlying high level skills behind assessment results. We demonstrate the use of different techniques to analyze and interpret the output of NMF. We propose a simple model to generate simulated data and to provide lower and upper bounds for quantifying skill effect. Using the simulated data, we show that, under the assumption of independent skills, the NMF technique is highly effective in deriving the Q-matrix. However, the NMF performance degrades under different ratios of variance between subject performance, item difficulty, and skill mastery. The results corroborates conclusions from previous work in that high level skills, corresponding to general topics like World History and Biology, seem to have no substantial effect on test performance, whereas other topics like Mathematics and French do. The analysis and visualization techniques of the NMF output, along with the simulation approach presented in this paper, should be useful for future investigations using NMF for Q-matrix induction from data.",2012,Knowledge Discovery and Data Mining,Fields of study: non negative matrix factorizationdata miningartificial intelligencemachine learningsimulation +2 others
