Model Accuracy and Runtime Tradeoff in Distributed Deep Learning: A Systematic Study,Suyog Gupta (IBM)Wei Zhang (University of Wisconsin-Madison)Fei Wang,"2483773042,2714421985,2630066771","Deep learning with a large number of parametersrequires distributed training, where model accuracy and runtimeare two important factors to be considered. However, there hasbeen no systematic study of the tradeoff between these two factorsduring the model training process. This paper presents Rudra, aparameter server based distributed computing framework tunedfor training large-scale deep neural networks. Using variants ofthe asynchronous stochastic gradient descent algorithm we studythe impact of synchronization protocol, stale gradient updates, minibatch size, learning rates, and number of learners on runtimeperformance and model accuracy. We introduce a new learningrate modulation strategy to counter the effect of stale gradientsand propose a new synchronization protocol that can effectivelybound the staleness in gradients, improve runtime performanceand achieve good model accuracy. Our empirical investigationreveals a principled approach for distributed training of neuralnetworks: the mini-batch size per learner should be reducedas more learners are added to the system to preserve the modelaccuracy. We validate this approach using commonly-used imageclassification benchmarks: CIFAR10 and ImageNet.",2016,International Conference on Data Mining,Fields of study: serversynchronizationcommunications protocolcomputational modelmathematical modelartificial neural networktheoretical computer sciencedistributed computingdata miningmachine learningsimulationcomputer science
Large-Scale Embedding Learning in Heterogeneous Event Data,Huan Gui (University of Illinois at Urbana–Champaign)Jialu Liu (Google)Fangbo Tao (University of Illinois at Urbana–Champaign)Meng Jiang (University of Illinois at Urbana–Champaign)Brandon Norick (University of Illinois at Urbana–Champaign)Jiawei Han (Beckman Institute for Advanced Science and Technology),"2706458373,2578119270,2130643217,2115305989,2222819066,2121939561","Heterogeneous events, which are defined as events connecting strongly-typed objects, are ubiquitous in the real world. We propose a HyperEdge-Based Embedding (Hebe) framework for heterogeneous event data, where a hyperedge represents the interaction among a set of involving objects in an event. The Hebe framework models the proximity among objects in an event by predicting a target object given the other participating objects in the event (hyperedge). Since each hyperedge encapsulates more information on a given event, Hebe is robust to data sparseness. In addition, Hebe is scalable when the data size spirals. Extensive experiments on large-scale real-world datasets demonstrate the efficacy and robustness of Hebe.",2016,International Conference on Data Mining,Fields of study: data modelingcontext modelpredictive modellingrobustnessdata miningreal time computingmachine learningcomputer science
Asynchronous Multi-task Learning,Inci M. Baytas (Michigan State University)Ming Yan (Michigan State University)Anil K. Jain (Michigan State University)Jiayu Zhou (Michigan State University),"2520407735,2691241414,2162010601,2691468835","Many real-world machine learning applications involveseveral learning tasks which are inter-related. For example, in healthcare domain, we need to learn a predictive model of a certain disease for many hospitals. The models for each hospital may be different because of the inherent differences in the distributions of the patient populations. However, the models are also closely related because of the nature of the learning tasks modeling the same disease. By simultaneously learning all the tasks, multi-task learning (MTL) paradigm performs inductive knowledge transfer among tasks to improve the generalization performance. When datasets for the learning tasks are stored at different locations, it may not always be feasible to transfer the data to provide a data centralized computing environment due to various practical issues such as high data volume and privacy. In this paper, we propose a principled MTL framework for distributed and asynchronous optimization to address the aforementioned challenges. In our framework, gradient update does not wait for collecting the gradient information from all the tasks. Therefore, the proposed method is very efficient when the communication delay is too high for some task nodes. We show that many regularized MTL formulations can benefit from this framework, including the low-rank MTL for sharedsubspace learning. Empirical studies on both synthetic and realworld datasets demonstrate the efficiency and effectiveness of the proposed framework.",2016,International Conference on Data Mining,Fields of study: multi task learninggeneralization errordata modelingserverpredictive modellingsemi supervised learningdata miningartificial intelligencemachine learningstatisticscomputer science
Robust Convex Clustering Analysis,Qi Wang (Michigan State University)Pinghua Gong (University of Michigan)Shiyu Chang (University of Illinois at Urbana–Champaign)Thomas S. Huang (University of Illinois at Urbana–Champaign)Jiayu Zhou (Arizona State University),"2431888963,2686114302,2098291119,2149631809,2097879502","Clustering is an unsupervised learning approach that explores data and seeks groups of similar objects. Many classical clustering models such as k-means and DBSCAN are based on heuristics algorithms and suffer from local optimal solutions and numerical instability. Recently convex clustering has received increasing attentions, which leverages the sparsity inducing norms and enjoys many attractive theoretical properties. However, convex clustering is based on Euclidean distance and is thus not robust against outlier features. Since the outlier features are very common especially when dimensionality is high, the vulnerability has greatly limited the applicability of convex clustering to analyze many real-world datasets. In this paper, we address the challenge by proposing a novel robust convex clustering method that simultaneously performs convex clustering and identifies outlier features. Specifically, the proposed method learns to decompose the data matrix into a clustering structure component and a group sparse component that captures feature outliers. We develop a block coordinate descent algorithm which iteratively performs convex clustering after outliers features are identified and eliminated. We also propose an efficient algorithm for solving the convex clustering by exploiting the structures on its dual problem. Moreover, to further illustrate the statistical stability, we present the theoretical performance bound of the proposed clustering method. Empirical studies on synthetic data and real-world data demonstrate that the proposed robust convex clustering can detect feature outliers as well as improve cluster quality.",2016,International Conference on Data Mining,Fields of study: flame clusteringk medians clusteringsubclubrown clusteringcanopy clustering algorithmdetermining the number of clusters in a data setdbscancorrelation clusteringconstrained clusteringdata stream clusteringcure data clustering algorithmaffinity propagationfuzzy clusteringconvex functionclustering high dimensional datasparse matrixrobustnesscluster analysisconsensus clusteringbiclusteringunsupervised learningdata miningpattern recognitionmachine learningstatisticscomputer sciencemathematics
Classification of Normal and Pathological Brain Networks Based on Similarity in Graph Partitions,Anvar KurmukovYulia DodonovaLeonid Zhukov (California Institute of Technology),"2670588071,2717433498,2128415670","We consider a task of classifying normal and pathological brain networks. These networks (called connectomes) represent macroscale connections between predefined brain regions, hence, the nodes of connectomes are uniquely labeled and the set of labels (brain regions) is the same across different brains. We make use of this property and hypothesize that connectomes obtained from normal and pathological brains differ in how brain regions cluster into communities. We develop an algorithm that computes distances between brain networks based on similarity in their partitions and uses these distances to produce a kernel for a support vector machine (SVM) classifier. We demonstrate how the proposed model classifies brain networks of carriers and non-carriers of an allele associated with an increased risk of Alzheimer's disease. The obtained classification quality is ROC AUC 0.7 which is higher than that of the baseline.",2016,International Conference on Data Mining,Fields of study: kernelsupport vector machinecluster analysisdata miningpattern recognitionmachine learningcomputer sciencemathematics
EdgeCentric: Anomaly Detection in Edge-Attributed Networks,Neil Shah (North Carolina State University)Alex Beutel (Carnegie Mellon University)Bryan Hooi (Carnegie Mellon University)Leman Akoglu (Stony Brook University)Stephan Gunnemann (Carnegie Mellon University)Disha MakhijaMohit KumarChristos Faloutsos (Carnegie Mellon University),"2397954333,2045447989,1755863881,2288278917,316694267,2553135857,2552649077,2198983026","Given a network with attributed edges, how can we identify anomalous behavior? Networks with edge attributes are ubiquitous, and capture rich information about interactions between nodes. In this paper, we aim to utilize exactly this information to discern suspicious from typical behavior in an unsupervised fashion, lending well to the traditional scarcity of ground-truth labels in practical anomaly detection scenarios. Our work has a number of notable contributions, including (a) formulation: while most other graph-based anomaly detection works use structural graph connectivity or node information, we focus on the new problem of leveraging edge information, (b) methodology: we introduce EdgeCentric, an intuitive and scalable compression-based approach for detecting edge-attributed graph anomalies, and (c) practicality: we show that EdgeCentric successfully spots numerous such anomalies in several large, edge-attributed real-world graphs, including the Flipkart e-commerce graph with over 3 million product reviews between 1.1 million users and 545 thousand products, where it achieved 0.87 precision over the top 100 results.",2016,International Conference on Data Mining,Fields of study: data modelingchannel codehidden markov modeltheoretical computer scienceworld wide webdata miningmachine learningcomputer science
A Rotation Invariant Latent Factor Model for Moveme Discovery from Static Poses,Matteo Ruggero Ronchi (California Institute of Technology)Joon Sik KimYisong Yue,"2564173573,2696738432,2711699559","We tackle the problem of learning a rotation invariant latent factor model when the training data is comprised of lower-dimensional projections of the original feature space. The main goal is the discovery of a set of 3-D bases poses that can characterize the manifold of primitive human motions, or movemes, from a training set of 2-D projected poses obtained from still images taken at various camera angles. The proposed technique for basis discovery is data-driven rather than hand-designed. The learned representation is rotation invariant, and can reconstruct any training instance from multiple viewing angles. We apply our method to modeling human poses in sports (via the Leeds Sports Dataset), and demonstrate the effectiveness of the learned bases in a range of applications such as activity classification, inference of dynamics from a single frame, and synthetic representation of movements.",2016,International Conference on Data Mining,Fields of study: data modelingtraining setsolid modelingmatrix decompositioniterative reconstructionactivity recognitioncomputer visionpattern recognitionmachine learningcomputer science
Efficient and Distributed Algorithms for Large-Scale Generalized Canonical Correlations Analysis,"Xiao Fu (University of Minnesota)Kejun Huang (University of Minnesota)Evangelos E. Papalexakis (University of California, Riverside)Hyun Ah Song (Carnegie Mellon University)Partha Pratim Talukdar (Indian Institute of Science)Nicholas D. Sidiropoulos (University of Minnesota)Christos Faloutsos (Carnegie Mellon University)Tom M. Mitchell (Carnegie Mellon University)","2277345172,2123583482,1418764031,2158947101,2407474466,2268379917,2198983026,2151014374","Generalized canonical correlation analysis (GCCA) aims at extracting common structure from multiple 'views', i.e., high-dimensional matrices representing the same objects in different feature domains – an extension of classical two-view CCA. Existing (G)CCA algorithms have serious scalability issues, since they involve square root factorization of the correlation matrices of the views. The memory and computational complexity associated with this step grow as a quadratic and cubic function of the problem dimension (the number of samples / features), respectively. To circumvent such difficulties, we propose a GCCA algorithm whose memory and computational costs scale linearly in the problem dimension and the number of nonzero data elements, respectively. Consequently, the proposed algorithm can easily handle very large sparse views whose sample and feature dimensions both exceed 100,000 – while the current approaches can only handle thousands of features / samples. Our second contribution is a distributed algorithm for GCCA, which computes the canonical components of different views in parallel and thus can further reduce the runtime significantly (by ≥ 30% in experiments) if multiple cores are available. Judiciously designed synthetic and real-data experiments using a multilingual dataset are employed to showcase the effectiveness of the proposed algorithms.",2016,International Conference on Data Mining,Fields of study: sparse matrixcorrelationfeature extractionalgorithm designdistributed algorithmapproximation algorithmtheoretical computer sciencecombinatoricsdata miningmachine learningstatisticscomputer sciencemathematics
