Subjective interestingness of subgraph patterns,Matthijs van Leeuwen (Katholieke Universiteit Leuven);Tijl De Bie (University of Bristol);Eirini Spyropoulou (University of Bristol);Cédric Mesnage (University of Bristol);,"2143928993,2080198120,1981173521,1988021698","The utility of a dense subgraph in gaining a better understanding of a graph has been formalised in numerous ways, each striking a different balance between approximating actual interestingness and computational efficiency. A difficulty in making this trade-off is that, while computational cost of an algorithm is relatively well-defined, a pattern's interestingness is fundamentally subjective. This means that this latter aspect is often treated only informally or neglected, and instead some form of density is used as a proxy. We resolve this difficulty by formalising what makes a dense subgraph pattern interesting to a given user. Unsurprisingly, the resulting measure is dependent on the prior beliefs of the user about the graph. For concreteness, in this paper we consider two cases: one case where the user only has a belief about the overall density of the graph, and another case where the user has prior beliefs about the degrees of the vertices. Furthermore, we illustrate how the resulting interestingness measure is different from previous proposals. We also propose effective exact and approximate algorithms for mining the most interesting dense subgraph according to the proposed measure. Usefully, the proposed interestingness measure and approach lend themselves well to iterative dense subgraph discovery. Contrary to most existing approaches, our method naturally allows subsequently found patterns to be overlapping. The empirical evaluation highlights the properties of the new interestingness measure given different prior belief sets, and our approach's ability to find interesting subgraphs that other methods are unable to find.",2016,Knowledge Discovery and Data Mining,principle of maximum entropy;knowledge extraction;data science;data mining;machine learning;statistics;computer science;mathematics;
A Closed-Loop Approach in Data-Driven Resource Allocation to Improve Network User Experience,"Yanan Bao (University of California, Davis);Huasen Wu (University of California, Davis);Xin Liu (University of California, Davis);","2103719961,2114102195,2616096743",-,2016,Knowledge Discovery and Data Mining,resource allocation;resource allocation;knowledge management;
Transferring Knowledge between Cities: A Perspective of Multimodal Data and A Case Study in Air Qual,Ying Wei (Hong Kong University of Science and Technology);Yu Zheng (Microsoft);Qiang Yang (Hong Kong University of Science and Technology);,"2235263654,2145115012,2109031554",-,2016,Knowledge Discovery and Data Mining,management science;knowledge management;data mining;
Stochastic Optimization Techniques for Quantification Performance Measures,Harikrishna Narasimhan (Harvard University);Shuai Li (University of Insubria);Purushottam Kar (Indian Institute of Technology Kanpur);Sanjay Chawla (Qatar Airways);Fabrizio Sebastiani (Qatar Airways);,"2637028757,2708734412,2136781283,2596304911,2596389720","The estimation of class prevalence, i.e., the fraction of the population that belongs to a certain class, is a very useful tool in data analytics and learning, and finds applications in many domains, such as sentiment analysis, epidemiology, etc. For example, in sentiment analysis, the objective is often not to estimate whether a specific text conveys positive or negative sentiment, but rather estimate the overall distribution of positive and negative sentiment during an event window. A popular way of performing the above task, often dubbed quantification, is to use supervised learning to train a prevalence estimator from labelled data. In this paper we propose the first online stochastic algorithms for directly optimizing (i) performance measures for quantification, and (ii) hybrid performance measures that seek to balance quantification and classification performance. We prove rigorous bounds for our algorithms which demonstrate that they exhibit optimal convergence. Our algorithms present a significant advancement in the theory of multivariate optimization. We also report extensive experiments on benchmark and real data sets which demonstrate that our methods significantly outperform existing optimization techniques used for the quantification problem.",2016,Knowledge Discovery and Data Mining,econometrics;data mining;machine learning;statistics;computer science;
Fast Unsupervised Online Drift Detection,Denis Dos Reis (University of São Paulo);Gustavo Batista (University of São Paulo);Peter Flach (University of Bristol);Stan Matwin (Dalhousie University);,"2366579606,2165222361,1814273096,2631100416",-,2016,Knowledge Discovery and Data Mining,-
Pseudo-Document-based Topic Modeling of Short Texts without Auxiliary Information,Yuan Zuo (Beihang University);Junjie Wu (Beihang University);Hao Lin (Beihang University);Hui Xiong (Rutgers University);,"2271787629,2149366604,2293401935,2153710278",-,2016,Knowledge Discovery and Data Mining,data science;natural language processing;information retrieval;computer science;
Recurrent Temporal Point Process,Nan Du (Georgia Institute of Technology);Hanjun Dai (Georgia Institute of Technology);Rakshit Trivedi (Georgia Institute of Technology);Utkarsh Upadhyay (Max Planck Society);Manuel Gomez-Rodriguez (Max Planck Society);Le Song (Georgia Institute of Technology);,"2681008474,2646439162,2150403271,2353884032,2279633593,2113868374",-,2016,Knowledge Discovery and Data Mining,-
Generalized Hierarchical Sparse Model for Arbitrary-Order Interactive Antigenic Sites Identification,Lei Han (Rutgers University);Yu Zhang (Hong Kong University of Science and Technology);Xiu-Feng Wan (Mississippi State University);Tong Zhang (Rutgers University);,"2531318857,2648094648,2718011926,2510858842",-,2016,Knowledge Discovery and Data Mining,bioinformatics;pattern recognition;machine learning;
Optimal Linear Aggregate Query Processing under Approximate Differential Privacy,Ganzhao Yuan (South China University of Technology);Yin Yang (Khalifa University);Zhenjie Zhang (National University of Singapore);Zhifeng Hao (South China University of Technology);,"2118188494,2280962535,2161417483,2684555521",-,2016,Knowledge Discovery and Data Mining,theoretical computer science;data mining;database;computer science;
Burstiness Scale: a highly parsimonious model forcharacterizing random series of events,Rodrigo A S Alves (Centro Federal de Educação Tecnológica de Minas Gerais);Renato Assunção (Universidade Federal de Minas Gerais);Pedro O S Vaz de Melo (Universidade Federal de Minas Gerais);,"2683066055,2001700347,2162838672","The problem to accurately and parsimoniously characterize random series of events (RSEs) present in the Web, such as e-mail conversations or Twitter hashtags, is not trivial. Reports found in the literature reveal two apparent conflicting visions of how RSEs should be modeled. From one side, the Poissonian processes, of which consecutive events follow each other at a relatively regular time and should not be correlated. On the other side, the self-exciting processes, which are able to generate bursts of correlated events and periods of inactivities. The existence of many and sometimes conflicting approaches to model RSEs is a consequence of the unpredictability of the aggregated dynamics of our individual and routine activities, which sometimes show simple patterns, but sometimes results in irregular rising and falling trends. In this paper we propose a highly parsimonious way to characterize general RSEs, namely the Burstiness Scale (BuSca) model. BuSca views each RSE as a mix of two independent process: a Poissonian and a self-exciting one. Here we describe a fast method to extract the two parameters of BuSca that, together, gives the burstyness scale, which represents how much of the RSE is due to bursty and viral effects. We validated our method in eight diverse and large datasets containing real random series of events seen in Twitter, Yelp, e-mail conversations, Digg, and online forums. Results showed that, even using only two parameters, BuSca is able to accurately describe RSEs seen in these diverse systems, what can leverage many applications.",2016,Knowledge Discovery and Data Mining,data mining;simulation;statistics;
XGBoost: A Scalable Tree Boosting System,Tianqi Chen (University of Washington);Carlos Guestrin (University of Washington);,"2126135973,1988556028","Tree boosting is a highly effective and widely used machine learning method. In this paper, we describe a scalable end-to-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems.",2016,Knowledge Discovery and Data Mining,incremental decision tree;id3 algorithm;data mining;pattern recognition;machine learning;computer science;
"""Why Should I Trust You?"": Explaining the Predictions of Any Classifier",Marco Tulio Ribeiro (University of Washington);Sameer Singh (University of Washington);Carlos Guestrin (University of Washington);,"2141784236,2279876130,1988556028","Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally varound the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.",2016,Knowledge Discovery and Data Mining,data mining;artificial intelligence;machine learning;computer science;
node2vec: Scalable Feature Learning for Networks,Aditya Grover (Stanford University);Jure Leskovec (Stanford University);,"2008277149,1878631932","Prediction tasks over nodes and edges in networks require careful effort in engineering features used by learning algorithms. Recent research in the broader field of representation learning has led to significant progress in automating prediction by learning the features themselves. However, present feature learning approaches are not expressive enough to capture the diversity of connectivity patterns observed in networks. Here we propose node2vec, an algorithmic framework for learning continuous feature representations for nodes in networks. In node2vec, we learn a mapping of nodes to a low-dimensional space of features that maximizes the likelihood of preserving network neighborhoods of nodes. We define a flexible notion of a node's network neighborhood and design a biased random walk procedure, which efficiently explores diverse neighborhoods. Our algorithm generalizes prior work which is based on rigid notions of network neighborhoods, and we argue that the added flexibility in exploring neighborhoods is the key to learning richer representations. We demonstrate the efficacy of node2vec over existing state-of-the-art techniques on multi-label classification and link prediction in several real-world networks from diverse domains. Taken together, our work represents a new way for efficiently learning state-of-the-art task-independent representations in complex networks.",2016,Knowledge Discovery and Data Mining,evolving networks;competitive learning;feature learning;semi supervised learning;theoretical computer science;combinatorics;artificial intelligence;machine learning;computer science;mathematics;
Joint Community and Structural Hole Spanner Detection via Harmonic Modularity,Lifang He (Shenzhen University);Chun Ta Lu (University of Illinois at Chicago);Jiaqi Ma (Tsinghua University);Jianping Cao (National University of Defense Technology);Linlin Shen (Shenzhen University);Philip S. Yu (University of Illinois at Chicago);,"2555188826,2224372854,2510249200,2144024520,2665312263,2125104194","Detecting communities (or modular structures) and structural hole spanners, the nodes bridging different communities in a network, are two essential tasks in the realm of network analytics. Due to the topological nature of communities and structural hole spanners, these two tasks are naturally tangled with each other, while there has been little synergy between them. In this paper, we propose a novel harmonic modularity method to tackle both tasks simultaneously. Specifically, we apply a harmonic function to measure the smoothness of community structure and to obtain the community indicator. We then investigate the sparsity level of the interactions between communities, with particular emphasis on the nodes connecting to multiple communities, to discriminate the indicator of SH spanners and assist the community guidance. Extensive experiments on real-world networks demonstrate that our proposed method outperforms several state-of-the-art methods in the community detection task and also in the SH spanner identification task (even the methods that require the supervised community information). Furthermore, by removing the SH spanners spotted by our method, we show that the quality of other community detection methods can be further improved.",2016,Knowledge Discovery and Data Mining,modularity;harmonic function;modularity;social network;machine learning;simulation;
Smart Reply: Automated Response Suggestion for Email,Anjuli Kannan (Google);Karol Kurach (Google);Sujith Ravi (Google);Tobias Kaufmann (Google);Andrew Tomkins (Google);Balint Miklos (Google);Greg Corrado (Google);Laszlo Lukacs (Google);Marina Ganea (Google);Peter Young (Google);Vivek Ramavajjala (Google);,"2128062926,2421278887,2590734359,2515993056,2535415812,2342928363,1994222016,2435589108,2514249824,2511757444,2507702640","In this paper we propose and investigate a novel end-to-end method for automatically generating short email responses, called Smart Reply. It generates semantically diverse suggestions that can be used as complete email responses with just one tap on mobile. The system is currently used in Inbox by Gmail and is responsible for assisting with 10% of all mobile responses. It is designed to work at very high throughput and process hundreds of millions of messages daily. The system exploits state-of-the-art, large-scale deep learning. We describe the architecture of the system as well as the challenges that we faced while building it, like response diversity and scalability. We also introduce a new method for semantic clustering of user-generated content that requires only a modest amount of explicitly labeled data.",2016,Knowledge Discovery and Data Mining,html email;long short term memory;cluster analysis;deep learning;semantics;internet privacy;world wide web;data mining;artificial intelligence;machine learning;computer science;
The Evolving Meaning of Information Security,Whitfield Diffie (Stanford University);,2307656538,"When you are developing security systems, new penetration techniques seem to appear as responses to new security measures but in general the flow is the other way around: security exists and evolves because of the evolution of threats. Beginning with the rise of radio in the 20th Century attacks on communication networks have shown two forms: those that go for the big kill --- such as the breaking of Enigma --- and those that assemble small seemingly innocuous leaks of information into a comprehensive understanding of the target's behavior. We will analyze the way in which these trends interact with others to create a situation in which what is possible in security and even the meaning of security in communication networks needs reexamination.",2016,Knowledge Discovery and Data Mining,security through obscurity;information security standards;asset;cloud computing security;security testing;covert channel;communications security;security engineering;information security;internet privacy;computer security;data mining;computer science;
Assessing Human Error Against a Benchmark of Perfection,Ashton Anderson (Microsoft);Jon M. Kleinberg (Cornell University);Sendhil Mullainathan (Harvard University);,"2155788867,2261367123,109652375","An increasing number of domains are providing us with detailed trace data on human decisions in settings where we can evaluate the quality of these decisions via an algorithm. Motivated by this development, an emerging line of work has begun to consider whether we can characterize and predict the kinds of decisions where people are likely to make errors. To investigate what a general framework for human error prediction might look like, we focus on a model system with a rich history in the behavioral sciences: the decisions made by chess players as they select moves in a game. We carry out our analysis at a large scale, employing datasets with several million recorded games, and using chess tablebases to acquire a form of ground truth for a subset of chess positions that have been completely solved by computers but remain challenging even for the best players in the world. We organize our analysis around three categories of features that we argue are present in most settings where the analysis of human error is applicable: the skill of the decision-maker, the time available to make the decision, and the inherent difficulty of the decision. We identify rich structure in all three of these categories of features, and find strong evidence that in our domain, features describing the inherent difficulty of an instance are significantly more powerful than features based on skill or time.",2016,Knowledge Discovery and Data Mining,human error;artificial intelligence;machine learning;simulation;statistics;computer science;
Label Noise Reduction in Entity Typing by Heterogeneous Partial-Label Embedding,Xiang Ren (University of Illinois at Urbana–Champaign);Wenqi He (University of Illinois at Urbana–Champaign);Meng Qu (University of Illinois at Urbana–Champaign);Clare R. Voss (United States Army Research Laboratory);Heng Ji (Rensselaer Polytechnic Institute);Jiawei Han (University of Illinois at Urbana–Champaign);,"2129405715,2518651166,2508960786,2118725505,2282310307,2121939561","Current systems of fine-grained entity typing use distant supervision in conjunction with existing knowledge bases to assign categories (type labels) to entity mentions. However, the type labels so obtained from knowledge bases are often noisy ( i.e. , incorrect for the entity mention's local context). We define a new task, Label Noise Reduction in Entity Typing (LNR), to be the automatic identification of correct type labels (type-paths) for training examples , given the set of candidate type labels obtained by distant supervision with a given type hierarchy. The unknown type labels for individual entity mentions and the semantic similarity between entity types pose unique challenges for solving the LNR task. We propose a general framework, called PLE , to jointly embed entity mentions, text features and entity types into the same low-dimensional space where, in that space, objects whose types are semantically close have similar representations . Then we estimate the type-path for each training example in a top-down manner using the learned embeddings. We formulate a global objective for learning the embeddings from text corpora and knowledge bases, which adopts a novel margin-based loss that is robust to noisy labels and faithfully models type correlation derived from knowledge bases. Our experiments on three public typing datasets demonstrate the effectiveness and robustness of PLE, with an average of 25% improvement in accuracy compared to next best method.",2016,Knowledge Discovery and Data Mining,entity linking;knowledge base;data mining;pattern recognition;artificial intelligence;machine learning;computer science;
Compressing Convolutional Neural Networks in the Frequency Domain,Wenlin Chen (Washington University in St. Louis);James T. Wilson (University of Edinburgh);Stephen Tyree (Nvidia);Kilian Q. Weinberger (Cornell University);Yixin Chen (Washington University in St. Louis);,"2231133452,2515705533,2655516276,2003907699,2295009377","Convolutional neural networks (CNN) are increasingly used in many areas of computer vision. They are particularly attractive because of their ability to ""absorb"" great quantities of labeled data through millions of parameters. However, as model sizes increase, so do the storage and memory requirements of the classifiers, hindering many applications such as image and speech recognition on mobile phones and other devices. In this paper, we present a novel net- work architecture, Frequency-Sensitive Hashed Nets (FreshNets), which exploits inherent redundancy in both convolutional layers and fully-connected layers of a deep learning model, leading to dramatic savings in memory and storage consumption. Based on the key observation that the weights of learned convolutional filters are typically smooth and low-frequency, we first convert filter weights to the frequency domain with a discrete cosine transform (DCT) and use a low-cost hash function to randomly group frequency parameters into hash buckets. All parameters assigned the same hash bucket share a single value learned with standard back-propagation. To further reduce model size, we allocate fewer hash buckets to high-frequency components, which are generally less important. We evaluate FreshNets on eight data sets, and show that it leads to better compressed performance than several relevant baselines.",2016,Knowledge Discovery and Data Mining,hash function;convolutional neural network;theoretical computer science;speech recognition;data mining;machine learning;computer science;
CatchTartan: Representing and Summarizing Dynamic Multicontextual Behaviors,Meng Jiang (University of Illinois at Urbana–Champaign);Christos Faloutsos (Carnegie Mellon University);Jiawei Han (University of Illinois at Urbana–Champaign);,"2115305989,2198983026,2121939561","Representing and summarizing human behaviors with rich contexts facilitates behavioral sciences and user-oriented services. Traditional behavioral modeling represents a behavior as a tuple in which each element is one contextual factor of one type, and the tensor-based summaries look for high-order dense blocks by clustering the values (including timestamps) in each dimension. However, the human behaviors are multicontextual and dynamic: (1) each behavior takes place within multiple contexts in a few dimensions, which requires the representation to enable non-value and set-values for each dimension; (2) many behavior collections, such as tweets or papers, evolve over time. In this paper, we represent the behavioral data as a two-level matrix (temporal-behaviors by dimensional-values) and propose a novel representation for behavioral summary called Tartan that includes a set of dimensions, the values in each dimension, a list of consecutive time slices and the behaviors in each slice. We further develop a propagation method CatchTartan to catch the dynamic multicontextual patterns from the temporal multidimensional data in a principled and scalable way: it determines the meaningfulness of updating every element in the Tartan by minimizing the encoding cost in a compression manner. CatchTartan outperforms the baselines on both the accuracy and speed. We apply CatchTartan to four Twitter datasets up to 10 million tweets and the DBLP data, providing comprehensive summaries for the events, human life and scientific development.",2016,Knowledge Discovery and Data Mining,minimum description length;data science;theoretical computer science;data mining;machine learning;statistics;computer science;
Matrix Computations and Optimization in Apache Spark,"Reza Bosagh Zadeh (Stanford University);Xiangrui Meng;Alexander Ulanov (HP Labs);Burak Yavuz;Li Pu (Twitter);Shivaram Venkataraman (University of California, Berkeley);Evan R. Sparks (University of California, Berkeley);Aaron Staple;Matei Zaharia (Massachusetts Institute of Technology);","2021036281,2522147838,2110026627,2484040650,2553136135,2232375382,2095953017,2481868097,2009645378","We describe matrix computations available in the cluster programming framework, Apache Spark. Out of the box, Spark provides abstractions and implementations for distributed matrices and optimization routines using these matrices. When translating single-node algorithms to run on a distributed cluster, we observe that often a simple idea is enough: separating matrix operations from vector operations and shipping the matrix operations to be ran on the cluster, while keeping vector operations local to the driver. In the case of the Singular Value Decomposition, by taking this idea to an extreme, we are able to exploit the computational power of a cluster, while running code written decades ago for a single core. Another example is our Spark port of the popular TFOCS optimization package, originally built for MATLAB, which allows for solving Linear programs as well as a variety of other convex programs. We conclude with a comprehensive set of benchmarks for hardware accelerated matrix computations from the JVM, which is interesting in its own right, as many cluster programming frameworks use the JVM. The contributions described in this paper are already merged into Apache Spark and available on Spark installations by default, and commercially supported by a slew of companies which provide further services.",2016,Knowledge Discovery and Data Mining,scala;spark;package;linear algebra;theoretical computer science;parallel computing;operating system;distributed computing;real time computing;machine learning;programming language;algorithm;computer science;
GMove: Group-Level Mobility Modeling Using Geo-Tagged Social Media,Chao Zhang (University of Illinois at Urbana–Champaign);Keyang Zhang (University of Illinois at Urbana–Champaign);Quan Yuan (University of Illinois at Urbana–Champaign);Luming Zhang (Hefei University of Technology);Tim Hanratty (United States Army Research Laboratory);Jiawei Han (University of Illinois at Urbana–Champaign);,"2618902948,2510995246,2683532865,2138764651,2311313130,2121939561","Understanding human mobility is of great importance to various applications, such as urban planning, traffic scheduling, and location prediction. While there has been fruitful research on modeling human mobility using tracking data (e.g., GPS traces), the recent growth of geo-tagged social media (GeoSM) brings new opportunities to this task because of its sheer size and multi-dimensional nature. Nevertheless, how to obtain quality mobility models from the highly sparse and complex GeoSM data remains a challenge that cannot be readily addressed by existing techniques. We propose GMove, a group-level mobility modeling method using GeoSM data. Our insight is that the GeoSM data usually contains multiple user groups, where the users within the same group share significant movement regularity. Meanwhile, user grouping and mobility modeling are two intertwined tasks: (1) better user grouping offers better within-group data consistency and thus leads to more reliable mobility models; and (2) better mobility models serve as useful guidance that helps infer the group a user belongs to. GMove thus alternates between user grouping and mobility modeling, and generates an ensemble of Hidden Markov Models (HMMs) to characterize group-level movement regularity. Furthermore, to reduce text sparsity of GeoSM data, GMove also features a text augmenter. The augmenter computes keyword correlations by examining their spatiotemporal distributions. With such correlations as auxiliary knowledge, it performs sampling-based augmentation to alleviate text sparsity and produce high-quality HMMs. Our extensive experiments on two real-life data sets demonstrate that GMove can effectively generate meaningful group-level mobility models. Moreover, with context-aware location prediction as an example application, we find that GMove significantly outperforms baseline mobility models in terms of prediction accuracy.",2016,Knowledge Discovery and Data Mining,social media;mobility model;world wide web;data mining;machine learning;simulation;statistics;computer science;
Dynamic and Robust Wildfire Risk Prediction System: An Unsupervised Approach,Mahsa Salehi (IBM);Laura Irina Rusu (IBM);Timothy M. Lynar (IBM);Anna Phan (IBM);,"2408564922,2096641372,2593345984,2485260314","Ability to predict the risk of damaging events (e.g. wildfires) is crucial in helping emergency services in their decision making processes, to mitigate and reduce the impact of such events. Today, wildfire rating systems have been in operation extensively in many countries around the world to estimate the danger of wildfires. In this paper we propose a data-driven approach to predict wildfire risk using weather data. We show how we address the inherent challenge arising due to the temporal dynamicity of weather data. Weather observations naturally change in time, with finer-scale variation (e.g. stationary day or night) or large variations (nonstationary day or night), and this determines a temporal variation of the predicted wildfire danger. We show how our dynamic wildfire danger prediction model addresses the aforementioned challenge using context-based anomaly detection techniques. We call our predictive model a Context-Based Fire Risk (CBFR) model. The advantage of our model is that it maintains multiple historical models for different temporal variations (e.g. day versus night), and uses ensemble learning techniques to predict wildfire risk with high accuracy. In addition, it is completely unsupervised and does not rely on expert knowledge, which makes it flexible and easily applied to any region of interest. Our CBFR model is also scalable and can potentially be parallelised to speed up computation. We have considered multiple wildfire locations in the Blue Mountains, Australia as a case study, and compared the results of our system with the existing well-established Australian wildfire rating system. The experimental results show that our predictive model has a substantially higher accuracy in predicting wildfire risk, which makes it an effective model to supplement the operational Australian wildfire rating system.",2016,Knowledge Discovery and Data Mining,data stream mining;unsupervised learning;machine learning;simulation;computer science;
From Truth Discovery to Trustworthy Opinion Discovery: An Uncertainty-Aware Quantitative Modeling Approach,"Mengting Wan (University of California, San Diego);Xiangyu Chen (University of Illinois at Urbana–Champaign);Lance M. Kaplan (United States Army Research Laboratory);Jiawei Han 0001 (University of Illinois at Urbana–Champaign);Jing Gao (University at Buffalo);Bo Zhao (LinkedIn);","2665663483,2630097597,2119398415,2121939561,2096731881,2674375462","In this era of information explosion, conflicts are often encountered when information is provided by multiple sources. Traditional truth discovery task aims to identify the truth the most trustworthy information, from conflicting sources in different scenarios. In this kind of tasks, truth is regarded as a fixed value or a set of fixed values. However, in a number of real-world cases, objective truth existence cannot be ensured and we can only identify single or multiple reliable facts from opinions. Different from traditional truth discovery task, we address this uncertainty and introduce the concept of trustworthy opinion of an entity, treat it as a random variable , and use its distribution to describe consistency or controversy, which is particularly difficult for data which can be numerically measured, i.e. quantitative information. In this study, we focus on the quantitative opinion, propose an uncertainty-aware approach called Kernel Density Estimation from Multiple Sources ( KDEm ) to estimate its probability distribution, and summarize trustworthy information based on this distribution. Experiments indicate that KDEm not only has outstanding performance on the classical numeric truth discovery task, but also shows good performance on multi-modality detection and anomaly detection in the uncertain-opinion setting.",2016,Knowledge Discovery and Data Mining,kernel density estimation;data science;data mining;artificial intelligence;statistics;computer science;mathematics;
DopeLearning: A Computational Approach to Rap Lyrics Generation,Eric Malmi (Aalto University);Pyry Takala (Aalto University);Hannu Toivonen (University of Helsinki);Tapani Raiko (Aalto University);Aristides Gionis (Aalto University);,"2289295535,2078188445,2250270171,344142627,737311942","Writing rap lyrics requires both creativity to construct a meaningful, interesting story and lyrical skills to produce complex rhyme patterns, which form the cornerstone of good flow. We present a rap lyrics generation method that captures both of these aspects. First, we develop a prediction model to identify the next line of existing lyrics from a set of candidate next lines. This model is based on two machine-learning techniques: the RankSVM algorithm and a deep neural network model with a novel structure. Results show that the prediction model can identify the true next line among 299 randomly selected lines with an accuracy of 17%, i.e., over 50 times more likely than by random. Second, we employ the prediction model to combine lines from existing songs, producing lyrics with rhyme and a meaning. An evaluation of the produced lyrics shows that in terms of quantitative rhyme density, the method outperforms the best human rappers by 21%. The rap lyrics generator has been deployed as an online tool called DeepBeat, and the performance of the tool has been assessed by analyzing its usage logs. This analysis shows that machine-learned rankings correlate with user preferences.",2016,Knowledge Discovery and Data Mining,modigliani risk adjusted performance;deep learning;speech recognition;machine learning;computer science;
Mining Reliable Information from Passively and Actively Crowdsourced Data,Jing Gao (University at Buffalo);Qi Li (University at Buffalo);Bo Zhao (LinkedIn);Wei Fan (Baidu);Jiawei Han (University of Illinois at Urbana–Champaign);,"2096731881,2261907930,2674375462,2422054197,2121939561","Recent years have witnessed an astonishing growth of crowd-contributed data, which has become a powerful information source that covers almost every aspect of our lives. This big treasure trove of information has fundamentally changed the ways in which we learn about our world. Crowdsourcing has attracted considerable attentions with various approaches developed to utilize these enormous crowdsourced data from different perspectives. From the data collection perspective, crowdsourced data can be divided into two types: ""passively"" crowdsourced data and ""actively"" crowdsourced data; from task perspective, crowdsourcing research includes information aggregation, budget allocation, worker incentive mechanism, etc. To answer the need of a systematic introduction of the field and comparison of the techniques, we will present an organized picture on crowdsourcing methods in this tutorial. The covered topics will be interested for both advanced researchers and beginners in this field.",2016,Knowledge Discovery and Data Mining,crowdsourcing;data science;world wide web;data mining;computer science;
Positive-Unlabeled Learning in Streaming Networks,Shiyu Chang (University of Illinois at Urbana–Champaign);Yang Zhang (University of Illinois at Urbana–Champaign);Jiliang Tang (Michigan State University);Dawei Yin (Yahoo!);Yi Chang (Yahoo!);Mark A. Hasegawa-Johnson (University of Illinois at Urbana–Champaign);Thomas S. Huang (University of Illinois at Urbana–Champaign);,"2098291119,2677264140,2147392410,2170531144,2168000538,2234370517,2149631809","Data of many problems in real-world systems such as link prediction and one-class recommendation share common characteristics. First, data are in the form of positive unlabeled (PU) measurements (e.g. Twitter ""following"", Facebook ""like"", etc.) that do not provide negative information, which can be naturally represented as networks. Second, in the era of big data, such data are generated temporally-ordered, continuously and rapidly, which determines its streaming nature. These common characteristics allow us to unify many problems into a novel framework -- PU learning in streaming networks. In this paper, a principled probabilistic approach SPU is proposed to leverage the characteristics of the streaming PU inputs. In particular, SPU captures temporal dynamics and provides real-time adaptations and predictions by identifying the potential negative signals concealed in unlabeled data. Our empirical results on various real-world datasets demonstrate the effectiveness of the proposed framework over other state-of-the-art methods in both link prediction and recommendation.",2016,Knowledge Discovery and Data Mining,pu learning;dynamic network analysis;world wide web;data mining;machine learning;computer science;
Structural Deep Network Embedding,Daixin Wang (Tsinghua University);Peng Cui (Tsinghua University);Wenwu Zhu (Tsinghua University);,"2099175083,2113115369,2111511002","Network embedding is an important method to learn low-dimensional representations of vertexes in networks, aiming to capture and preserve the network structure. Almost all the existing network embedding methods adopt shallow models. However, since the underlying network structure is complex, shallow models cannot capture the highly non-linear network structure, resulting in sub-optimal network representations. Therefore, how to find a method that is able to effectively capture the highly non-linear network structure and preserve the global and local structure is an open yet important problem. To solve this problem, in this paper we propose a Structural Deep Network Embedding method, namely SDNE . More specifically, we first propose a semi-supervised deep model, which has multiple layers of non-linear functions, thereby being able to capture the highly non-linear network structure. Then we propose to exploit the first-order and second-order proximity jointly to preserve the network structure. The second-order proximity is used by the unsupervised component to capture the global network structure. While the first-order proximity is used as the supervised information in the supervised component to preserve the local network structure. By jointly optimizing them in the semi-supervised deep model, our method can preserve both the local and global network structure and is robust to sparse networks. Empirically, we conduct the experiments on five real-world networks, including a language network, a citation network and three social networks. The results show that compared to the baselines, our method can reconstruct the original network significantly better and achieves substantial gains in three applications, i.e. multi-label classification, link prediction and visualization.",2016,Knowledge Discovery and Data Mining,dynamic network analysis;network formation;network simulation;network analysis;deep belief network;deep learning;theoretical computer science;data mining;machine learning;computer science;
Interpretable Decision Sets: A Joint Framework for Description and Prediction,Himabindu Lakkaraju (Stanford University);Stephen H. Bach (Stanford University);Jure Leskovec (Stanford University);,"2237122046,2712713707,1878631932","One of the most important obstacles to deploying predictive models is the fact that humans do not understand and trust them. Knowing which variables are important in a model's prediction and how they are combined can be very powerful in helping people understand and trust automatic decision making systems. Here we propose interpretable decision sets, a framework for building predictive models that are highly accurate, yet also highly interpretable. Decision sets are sets of independent if-then rules. Because each rule can be applied independently, decision sets are simple, concise, and easily interpretable. We formalize decision set learning through an objective function that simultaneously optimizes accuracy and interpretability of the rules. In particular, our approach learns short, accurate, and non-overlapping rules that cover the whole feature space and pay attention to small but important classes. Moreover, we prove that our objective is a non-monotone submodular function, which we efficiently optimize to find a near-optimal set of rules. Experiments show that interpretable decision sets are as accurate at classification as state-of-the-art machine learning techniques. They are also three times smaller on average than rule-based models learned by other methods. Finally, results of a user study show that people are able to answer multiple-choice questions about the decision boundaries of interpretable decision sets and write descriptions of classes based on them faster and more accurately than with other rule-based models that were designed for interpretability. Overall, our framework provides a new approach to interpretable machine learning that balances accuracy, interpretability, and computational efficiency.",2016,Knowledge Discovery and Data Mining,decision rule;biological classification;data mining;pattern recognition;machine learning;mathematics;
"Beyond Sigmoids: The NetTide Model for Social Network Growth, and Its Applications",Chengxi Zang (Tsinghua University);Peng Cui (Tsinghua University);Christos Faloutsos (Carnegie Mellon University);,"2514402881,2113115369,2198983026","What is the growth pattern of social networks, like Facebook and WeChat? Does it truly exhibit exponential early growth, as predicted by textbook models like the Bass model, SI, or the Branching Process? How about the count of links, over time, for which there are few published models? We examine the growth of several real networks, including one of the world's largest online social network, ``WeChat'', with 300 million nodes and 4.75 billion links by 2013; and we observe power law growth for both nodes and links, a fact that completely breaks the sigmoid models (like SI, and Bass). In its place, we propose NETTIDE, along with differential equations for the growth of the count of nodes, as well as links. Our model accurately fits the growth patterns of real graphs; it is general , encompassing as special cases all the known, traditional models (including Bass, SI, log-logistic growth); while still remaining parsimonious , requiring only a handful of parameters. Moreover, our NETTIDE for link growth is the first one of its kind, accurately fitting real data, and naturally leading to the densification phenomenon. We validate our model with four real, time-evolving social networks, where NETTIDE gives good fitting accuracy, and, more importantly, applied on the WeChat data, our NETTIDE forecasted more than 730 days into the future, with 3% error.",2016,Knowledge Discovery and Data Mining,social network;econometrics;data mining;artificial intelligence;machine learning;simulation;statistics;computer science;mathematics;
FRAUDAR: Bounding Graph Fraud in the Face of Camouflage,Bryan Hooi (Carnegie Mellon University);Hyun Ah Song (Carnegie Mellon University);Alex Beutel (Carnegie Mellon University);Neil Shah (Carnegie Mellon University);Kijung Shin (Carnegie Mellon University);Christos Faloutsos (Carnegie Mellon University);,"1755863881,2158947101,2045447989,2397954333,2226806500,2198983026","Given a bipartite graph of users and the products that they review, or followers and followees, how can we detect fake reviews or follows? Existing fraud detection methods (spectral, etc.) try to identify dense subgraphs of nodes that are sparsely connected to the remaining graph. Fraudsters can evade these methods using camouflage, by adding reviews or follows with honest targets so that they look ""normal"". Even worse, some fraudsters use hijacked accounts from honest users, and then the camouflage is indeed organic. Our focus is to spot fraudsters in the presence of camouflage or hijacked accounts. We propose FRAUDAR, an algorithm that (a) is camouflage-resistant, (b) provides upper bounds on the effectiveness of fraudsters, and (c) is effective in real-world data. Experimental results under various attacks show that FRAUDAR outperforms the top competitor in accuracy of detecting both camouflaged and non-camouflaged fraud. Additionally, in real-world experiments with a Twitter follower-followee graph of 1.47 billion edges, FRAUDAR successfully detected a subgraph of more than 4000 detected accounts, of which a majority had tweets showing that they used follower-buying services.",2016,Knowledge Discovery and Data Mining,internet privacy;computer security;data mining;computer science;
Predicting Matchups and Preferences in Context,Shuo Chen (Cornell University);Thorsten Joachims (Cornell University);,"2295814016,245171893","We present a general probabilistic framework for predicting the outcome of pairwise matchups (e.g. two-player sport matches) and pairwise preferences (e.g. product preferences), both of which have widespread applications ranging from matchmaking in computer games to recommendation in e-commerce. Unlike existing models for these tasks, our model not only learns representations of the items in a more expressive latent vector space, but also models how context modifies matchup and preference outcomes. For example, the context ""weather"" may alter the winning probability in a tennis match, or the fact that the user is on a mobile device may alter his preferences among restaurants. More generally, the model is capable of handling any symmetric game/comparison problem that can be described by vectorized player/item and game/context features. We provide a comprehensive evaluation of its predictive performance with real datasets from both domains to show its ability to predict preference and game outcomes more accurately than existing models. Furthermore, we demonstrate on synthetic datasets the expressiveness of the model when compared against theoretical limits.",2016,Knowledge Discovery and Data Mining,ranking;pairwise comparison;games;feature learning;data mining;machine learning;simulation;statistics;computer science;
ABRA: Approximating Betweenness Centrality in Static and Dynamic Graphs with Rademacher Averages,Matteo Riondato (Brown University);Eli Upfal (Brown University);,"1555209364,2685185700","We present ABRA, a suite of algorithms to compute and maintain probabilistically-guaranteed, high-quality, approximations of the betweenness centrality of all nodes (or edges) on both static and fully dynamic graphs. Our algorithms use progressive random sampling and their analysis rely on Rademacher averages and pseudodimension, fundamental concepts from statistical learning theory. To our knowledge, this is the first application of these concepts to the field of graph analysis. Our experimental results show that ABRA is much faster than exact methods, and vastly outperforms, in both runtime and number of samples, state-of-the-art algorithms with the same quality guarantees.",2016,Knowledge Discovery and Data Mining,random walk closeness centrality;rademacher complexity;centrality;betweenness centrality;sampling;combinatorics;machine learning;statistics;mathematics;
Finding Gangs in War from Signed Networks,Lingyang Chu (Simon Fraser University);Zhefeng Wang (University of Science and Technology of China);Jian Pei (Simon Fraser University);Jiannan Wang (Simon Fraser University);Zijin Zhao (Simon Fraser University);Enhong Chen (University of Science and Technology of China);,"2157117012,2227256930,2126330539,2103786759,2509178450,2136372366","Given a signed network where edges are weighted in real number, and positive weights indicate cohesion between vertices and negative weights indicate opposition, we are interested in finding k -Oppositive Cohesive Groups ( k -OCG). Each k -OCG is a group of k subgraphs such that (1) the edges within each subgraph are dense and cohesive; and (2) the edges crossing different subgraphs are dense and oppositive. Finding k -OCGs is challenging since the subgraphs are often small, there are multiple k -OCGs in a large signed network, and many existing dense subgraph extraction methods cannot handle edges of two signs. We model k -OCG finding task as a quadratic optimization problem. However, the classical Proximal Gradient method is very costly since it has to use the entire adjacency matrix, which is huge on large networks. Thus, we develop FOCG, an algorithm that is two orders of magnitudes faster than the Proximal Gradient method. The main idea is to only search in small subgraphs and thus avoids using a major portion of the adjacency matrix. Our experimental results on synthetic and real data sets as well as a case study clearly demonstrate the effectiveness and efficiency of our method.",2016,Knowledge Discovery and Data Mining,cluster analysis;theoretical computer science;discrete mathematics;combinatorics;machine learning;computer science;mathematics;
Asymmetric Transitivity Preserving Graph Embedding,Mingdong Ou (Tsinghua University);Peng Cui (Tsinghua University);Jian Pei (Simon Fraser University);Ziwei Zhang (Tsinghua University);Wenwu Zhu (Tsinghua University);,"2157830877,2113115369,2126330539,2510900624,2111511002","Graph embedding algorithms embed a graph into a vector space where the structure and the inherent properties of the graph are preserved. The existing graph embedding methods cannot preserve the asymmetric transitivity well, which is a critical property of directed graphs. Asymmetric transitivity depicts the correlation among directed edges, that is, if there is a directed path from u to v, then there is likely a directed edge from u to v. Asymmetric transitivity can help in capturing structures of graphs and recovering from partially observed graphs. To tackle this challenge, we propose the idea of preserving asymmetric transitivity by approximating high-order proximity which are based on asymmetric transitivity. In particular, we develop a novel graph embedding algorithm, High-Order Proximity preserved Embedding ( HOPE for short), which is scalable to preserve high-order proximities of large scale graphs and capable of capturing the asymmetric transitivity. More specifically, we first derive a general formulation that cover multiple popular high-order proximity measurements, then propose a scalable embedding algorithm to approximate the high-order proximity measurements based on their general formulation. Moreover, we provide a theoretical upper bound on the RMSE (Root Mean Squared Error) of the approximation. Our empirical experiments on a synthetic dataset and three real-world datasets demonstrate that HOPE can approximate the high-order proximities significantly better than the state-of-art algorithms and outperform the state-of-art algorithms in tasks of reconstruction, link prediction and vertex recommendation.",2016,Knowledge Discovery and Data Mining,planar straight line graph;voltage graph;complement graph;asymmetric graph;comparability graph;book embedding;feedback arc set;null graph;topological graph theory;graph property;line graph;graph embedding;graph;planar graph;directed graph;discrete mathematics;combinatorics;topology;mathematics;
Computational Social Science: Exciting Progress and Future Challenges,Duncan Watts (Microsoft);,2138805444,"The past 15 years have witnessed a remarkable increase in both the scale and scope of social and behavioral data available to researchers, leading some to herald the emergence of a new field: ""computational social science."" Against these exciting developments stands a stubborn fact: that in spite of many thousands of published papers, there has been surprisingly little progress on the ""big"" questions that motivated the field in the first place?questions concerning systemic risk in financial systems, problem solving in complex organizations, and the dynamics of epidemics or social movements, among others. In this talk I highlight some examples of research that would not have been possible just a handful of years ago and that illustrate the promise of CSS. At the same time, they illustrate its limitations. I then conclude with some thoughts on how CSS can bridge the gap between its current state and its potential.",2016,Knowledge Discovery and Data Mining,management science;computer science;
Robust Influence Maximization,Wei Chen (Microsoft);Tian Lin (Tsinghua University);Zihan Tan (Tsinghua University);Mingfei Zhao (Tsinghua University);Xuren Zhou (Hong Kong University of Science and Technology);,"2527738285,2122568511,2113456608,2512255978,2511313199","In this paper, we address the important issue of uncertainty in the edge influence probability estimates for the well studied influence maximization problem --- the task of finding k seed nodes in a social network to maximize the influence spread. We propose the problem of robust influence maximization, which maximizes the worst-case ratio between the influence spread of the chosen seed set and the optimal seed set, given the uncertainty of the parameter input. We design an algorithm that solves this problem with a solution-dependent bound. We further study uniform sampling and adaptive sampling methods to effectively reduce the uncertainty on parameters and improve the robustness of the influence maximization task. Our empirical results show that parameter uncertainty may greatly affect influence maximization performance and prior studies that learned influence probabilities could lead to poor performance in robust influence maximization due to relatively large uncertainty in parameter estimates, and information cascade based adaptive sampling method may be an effective way to improve the robustness of influence maximization.",2016,Knowledge Discovery and Data Mining,social network;robust optimization;social science;machine learning;mathematical optimization;statistics;computer science;mathematics;
Deep Visual-Semantic Hashing for Cross-Modal Retrieval,Yue Cao (Tsinghua University);Mingsheng Long (Tsinghua University);Jianmin Wang (Tsinghua University);Qiang Yang (Hong Kong University of Science and Technology);Philip S. Yu (University of Illinois at Chicago);,"2503224341,2152250954,2310637432,2109031554,2125104194","Due to the storage and retrieval efficiency, hashing has been widely applied to approximate nearest neighbor search for large-scale multimedia retrieval. Cross-modal hashing, which enables efficient retrieval of images in response to text queries or vice versa, has received increasing attention recently. Most existing work on cross-modal hashing does not capture the spatial dependency of images and temporal dynamics of text sentences for learning powerful feature representations and cross-modal embeddings that mitigate the heterogeneity of different modalities. This paper presents a new Deep Visual-Semantic Hashing (DVSH) model that generates compact hash codes of images and sentences in an end-to-end deep learning architecture, which capture the intrinsic cross-modal correspondences between visual data and natural language. DVSH is a hybrid deep architecture that constitutes a visual-semantic fusion network for learning joint embedding space of images and text sentences, and two modality-specific hashing networks for learning hash functions to generate compact binary codes. Our architecture effectively unifies joint multimodal embedding and cross-modal hashing, which is based on a novel combination of Convolutional Neural Networks over images, Recurrent Neural Networks over sentences, and a structured max-margin objective that integrates all things together to enable learning of similarity-preserving and high-quality hash codes. Extensive empirical evidence shows that our DVSH approach yields state of the art results in cross-modal retrieval experiments on image-sentences datasets, i.e. standard IAPR TC-12 and large-scale Microsoft COCO.",2016,Knowledge Discovery and Data Mining,feature hashing;locality preserving hashing;dynamic perfect hashing;universal hashing;extendible hashing;locality sensitive hashing;hash function;hash table;theoretical computer science;data mining;pattern recognition;machine learning;computer science;
Multi-layer Representation Learning for Medical Concepts,Edward Choi (Georgia Institute of Technology);Mohammad Taha Bahadori (Georgia Institute of Technology);Elizabeth Searles;Catherine Coffey;Michael Thompson;James Bost;Javier Tejedor-Sojo;Jimeng Sun (Georgia Institute of Technology);,"2524090778,2032867848,2510366178,2554429167,2682301811,2590039928,2477983982,2110385854","Proper representations of medical concepts such as diagnosis, medication, procedure codes and visits from Electronic Health Records (EHR) has broad applications in healthcare analytics. Patient EHR data consists of a sequence of visits over time, where each visit includes multiple medical concepts, e.g., diagnosis, procedure, and medication codes. This hierarchical structure provides two types of relational information, namely sequential order of visits and co-occurrence of the codes within a visit. In this work, we propose Med2Vec , which not only learns the representations for both medical codes and visits from large EHR datasets with over million visits, but also allows us to interpret the learned representations confirmed positively by clinical experts. In the experiments, Med2Vec shows significant improvement in prediction accuracy in clinical applications compared to baselines such as Skip-gram, GloVe, and stacked autoencoder, while providing clinically meaningful interpretation.",2016,Knowledge Discovery and Data Mining,artificial neural network;data science;knowledge management;data mining;machine learning;computer science;
Unbounded Human Learning: Optimal Scheduling for Spaced Repetition,Siddharth Reddy (Cornell University);Igor Labutov (Cornell University);Siddhartha Banerjee (Cornell University);Thorsten Joachims (Cornell University);,"2286135830,1028054495,2146436969,245171893","In the study of human learning, there is broad evidence that our ability to retain information improves with repeated exposure and decays with delay since last exposure. This plays a crucial role in the design of educational software, leading to a trade-off between teaching new material and reviewing what has already been taught. A common way to balance this trade-off is spaced repetition , which uses periodic review of content to improve long-term retention. Though spaced repetition is widely used in practice, e.g., in electronic flashcard software, there is little formal understanding of the design of these systems. Our paper addresses this gap in three ways. First, we mine log data from spaced repetition software to establish the functional dependence of retention on reinforcement and delay. Second, we use this memory model to develop a stochastic model for spaced repetition systems. We propose a queueing network model of the Leitner system for reviewing flashcards, along with a heuristic approximation that admits a tractable optimization problem for review scheduling. Finally, we empirically evaluate our queueing model through a Mechanical Turk experiment, verifying a key qualitative prediction of our model: the existence of a sharp phase transition in learning outcomes upon increasing the rate of new item introductions.",2016,Knowledge Discovery and Data Mining,memory;real time computing;artificial intelligence;machine learning;simulation;statistics;computer science;
"People, Computers, and The Hot Mess of Real Data","Joseph M. Hellerstein (University of California, Berkeley);",2063640528,"In practice, end-to-end data analysis is rarely a cleanly engineered process. Acquiring data can be tricky. Data assessment, wrangling and feature extraction are time-consuming and subjective. Models and algorithms used to derive data products are highly contextualized by time-varying properties of data sources, code and application needs. All of these issues would ideally benefit from an organizational view, but are often driven by individual users. Viewed holistically, both agile analytics and the establishment of analytic pipelines involve interactions between people, computation and infrastructure. In this talk I'll share some anecdotes from our research, user studies, and field experience with companies (Trifacta, Captricity), as well as an emerging open-source project (Ground).",2016,Knowledge Discovery and Data Mining,data science;data mining;database;artificial intelligence;machine learning;computer science;
Squish: Near-Optimal Compression for Archival of Relational Datasets,Yihan Gao (University of Illinois at Urbana–Champaign);Aditya G. Parameswaran (University of Illinois at Urbana–Champaign);,"2171035091,2077695977","Relational datasets are being generated at an alarmingly rapid rate across organizations and industries. Compressing these datasets could significantly reduce storage and archival costs. Traditional compression algorithms, e.g., gzip, are suboptimal for compressing relational datasets since they ignore the table structure and relationships between attributes. We study compression algorithms that leverage the relational structure to compress datasets to a much greater extent. We develop Squish, a system that uses a combination of Bayesian Networks and Arithmetic Coding to capture multiple kinds of dependencies among attributes and achieve near-entropy compression rate. Squish also supports user-defined attributes: users can instantiate new data types by simply implementing five functions for a new class interface. We prove the asymptotic optimality of our compression algorithm and conduct experiments to show the effectiveness of our system: Squish achieves a reduction of over 50% in storage size relative to systems developed in prior work on a variety of real datasets.",2016,Knowledge Discovery and Data Mining,lossless compression;bayesian network;arithmetic coding;lossy compression;data compression;theoretical computer science;data mining;database;machine learning;statistics;computer science;
Recurrent Marked Temporal Point Processes: Embedding Event History to Vector,Nan Du (Georgia Institute of Technology);Hanjun Dai (Georgia Institute of Technology);Rakshit Trivedi (Georgia Institute of Technology);Utkarsh Upadhyay (Max Planck Society);Manuel Gomez-Rodriguez (Max Planck Society);Le Song (Georgia Institute of Technology);,"2681008474,2646439162,2150403271,2353884032,2279633593,2113868374","Large volumes of event data are becoming increasingly available in a wide variety of applications, such as healthcare analytics, smart cities and social network analysis. The precise time interval or the exact distance between two events carries a great deal of information about the dynamics of the underlying systems. These characteristics make such data fundamentally different from independently and identically distributed data and time-series data where time and space are treated as indexes rather than random variables. Marked temporal point processes are the mathematical framework for modeling event data with covariates. However, typical point process models often make strong assumptions about the generative processes of the event data, which may or may not reflect the reality, and the specifically fixed parametric assumptions also have restricted the expressive power of the respective processes. Can we obtain a more expressive model of marked temporal point processes? How can we learn such a model from massive data? In this paper, we propose the Recurrent Marked Temporal Point Process (RMTPP) to simultaneously model the event timings and the markers. The key idea of our approach is to view the intensity function of a temporal point process as a nonlinear function of the history, and use a recurrent neural network to automatically learn a representation of influences from the event history. We develop an efficient stochastic gradient algorithm for learning the model parameters which can readily scale up to millions of events. Using both synthetic and real world datasets, we show that, in the case where the true models have parametric specifications, RMTPP can learn the dynamics of such models without the need to know the actual parametric forms; and in the case where the true models are unknown, RMTPP can also learn the dynamics and achieve better predictive performance than other parametric alternatives based on particular prior assumptions.",2016,Knowledge Discovery and Data Mining,recurrent neural network;stochastic process;econometrics;data mining;machine learning;statistics;computer science;mathematics;
Come-and-Go Patterns of Group Evolution: A Dynamic Model,Tianyang Zhang (Tsinghua University);Peng Cui (Tsinghua University);Christos Faloutsos (Carnegie Mellon University);Yunfei Lu (Tsinghua University);Hao Ye (Tencent);Wenwu Zhu (Tsinghua University);Shiqiang Yang (Tsinghua University);,"2294458475,2113115369,2198983026,2511457056,2710084553,2111511002,2127183023","How do social groups, such as Facebook groups and Wechat groups, dynamically evolve over time? How do people join the social groups, uniformly or with burst? What is the pattern of people quitting from groups? Is there a simple universal model to depict the come-and-go patterns of various groups? In this paper, we examine temporal evolution patterns of more than 100 thousands social groups with more than 10 million users. We surprisingly find that the evolution patterns of real social groups goes far beyond the classic dynamic models like SI and SIR. For example, we observe both diffusion and non-diffusion mechanism in the group joining process, and power-law decay in group quitting process, rather than exponential decay as expected in SIR model. Therefore we propose a new model comeNgo , a concise yet flexible dynamic model for group evolution. Our model has the following advantages: (a) unification power: it generalizes earlier theoretical models and different joining and quitting mechanisms we find from observation. (b) succinctness and interpretability: it contains only six parameters with clear physical meanings. (c) accuracy: it can capture various kinds of group evolution patterns preciously and the goodness of fit increase by 58% over baseline. (d) usefulness: it can be used in multiple application scenarios such as forecasting and pattern discovery.",2016,Knowledge Discovery and Data Mining,artificial intelligence;simulation;statistics;
Detecting Devastating Diseases in Search Logs,John Paparrizos (Columbia University);Ryen W. White (Microsoft);Eric Horvitz (Microsoft);,"2412068506,2096583854,1970391018","Web search queries can offer a unique population-scale window onto streams of evidence that are useful for detecting the emergence of health conditions. We explore the promise of harnessing behavioral signals in search logs to provide advance warning about the presence of devastating diseases such as pancreatic cancer. Pancreatic cancer is often diagnosed too late to be treated effectively as the cancer has usually metastasized by the time of diagnosis. Symptoms of the early stages of the illness are often subtle and nonspecific. We identify searchers who issue credible, first-person diagnostic queries for pancreatic cancer and we learn models from prior search histories that predict which searchers will later input such queries. We show that we can infer the likelihood of seeing the rise of diagnostic queries months before they appear and characterize the tradeoff between predictivity and false positive rate. The findings highlight the potential of harnessing search logs for the early detection of pancreatic cancer and more generally for harnessing search systems to reduce health risks for individuals.",2016,Knowledge Discovery and Data Mining,data science;world wide web;data mining;
Aircraft Trajectory Prediction Made Easy with Predictive Analytics,"Samet Ayhan (University of Maryland, College Park);Hanan Samet (University of Maryland, College Park);","2479110344,89934235","At the heart of Air Traffic Management (ATM) lies the Decision Support Systems (DST) that rely upon accurate trajectory prediction to determine how the airspace will look like in the future to make better decisions and advisories. Dealing with airspace that is prone to congestion due to environmental factors still remains the challenge especially when a deterministic approach is used in the trajectory prediction process. In this paper, we describe a novel stochastic trajectory prediction approach for ATM that can be used for more efficient and realistic flight planning and to assist airspace flow management, potentially resulting in higher safety, capacity, and efficiency commensurate with fuel savings thereby reducing emissions for a better environment. Our approach considers airspace as a 3D grid network, where each grid point is a location of a weather observation. We hypothetically build cubes around these grid points, so the entire airspace can be considered as a set of cubes. Each cube is defined by its centroid, the original grid point, and associated weather parameters that remain homogeneous within the cube during a period of time. Then, we align raw trajectories to a set of cube centroids which are basically fixed 3D positions independent of trajectory data. This creates a new form of trajectories which are 4D joint cubes, where each cube is a segment that is associated with not only spatio-temporal attributes but also with weather parameters. Next, we exploit machine learning techniques to train inference models from historical data and apply a stochastic model, a Hidden Markov Model (HMM), to predict trajectories taking environmental uncertainties into account. During the process, we apply time series clustering to generate input observations from an excessive set of weather parameters to feed into the Viterbi algorithm. Our experiments use a real trajectory dataset with pertaining weather observations and demonstrate the effectiveness of our approach to the trajectory prediction process for ATM.",2016,Knowledge Discovery and Data Mining,predictive analytics;time series;hidden markov model;data mining;machine learning;simulation;computer science;
Healthcare Data Mining with Matrix Models,Fei Wang (Cornell University);Ping Zhang (IBM);Joel Dudley (Icahn School of Medicine at Mount Sinai);,"2614650233,2621458975,1987535789","In the last decade, advances in high-throughput technologies, growth of clinical data warehouses, and rapid accumulation of biomedical knowledge provided unprecedented opportunities and challenges to researchers in biomedical informatics. One distinct solution, to efficiently conduct big data analytics for biomedical problems, is the application of matrix computation and factorization methods such as non-negative matrix factorization, joint matrix factorization, tensor factorization. Compared to probabilistic and information theoretic approaches, matrix-based methods are fast, easy to understand and implement. In this tutorial, we provide a review of recent advances in algorithms and methods using matrix and their potential applications in biomedical informatics. We survey various related articles from data mining venues as well as from biomedical informatics venues to share with the audience key problems and trends in matrix computation research, with different novel applications such as drug repositioning, personalized medicine, and electronic phenotyping.",2016,Knowledge Discovery and Data Mining,data science;theoretical computer science;data mining;computer science;
Point-of-Interest Recommendations: Learning Potential Check-ins from Friends,Huayu Li (University of North Carolina System);Yong Ge (University of Arizona);Richang Hong (Hefei University of Technology);Hengshu Zhu (Baidu);,"2304865086,2695934969,2632078846,2098414524","The emergence of Location-based Social Network (LBSN) services provides a wonderful opportunity to build personalized Point-of-Interest (POI) recommender systems. Although a personalized POI recommender system can significantly facilitate users' outdoor activities, it faces many challenging problems, such as the hardness to model user's POI decision making process and the difficulty to address data sparsity and user/location cold-start problem. To cope with these challenges, we define three types of friends (i.e., social friends, location friends, and neighboring friends) in LBSN, and develop a two-step framework to leverage the information of friends to improve POI recommendation accuracy and address cold-start problem. Specifically, we first propose to learn a set of potential locations that each individual's friends have checked-in before and this individual is most interested in. Then we incorporate three types of check-ins (i.e., observed check-ins, potential check-ins and other unobserved check-ins) into matrix factorization model using two different loss functions (i.e., the square error based loss and the ranking error based loss). To evaluate the proposed model, we conduct extensive experiments with many state-of-the-art baseline methods and evaluation metrics on two real-world data sets. The experimental results demonstrate the effectiveness of our methods.",2016,Knowledge Discovery and Data Mining,point of interest;matrix decomposition;world wide web;data mining;machine learning;
Parallel Dual Coordinate Descent Method for Large-scale Linear Classification in Multi-core Environments,Wei-Lin Chiang (National Taiwan University);Mu-Chu Lee (National Taiwan University);Chih-Jen Lin (National Taiwan University);,"2250130882,2246527660,2168176072","Dual coordinate descent method is one of the most effective approaches for large-scale linear classification. However, its sequential design makes the parallelization difficult. In this work, we target at the parallelization in a multi-core environment. After pointing out difficulties faced in some existing approaches, we propose a new framework to parallelize the dual coordinate descent method. The key idea is to make the majority of all operations (gradient calculation here) parallelizable. The proposed framework is shown to be theoretically sound. Further, we demonstrate through experiments that the new framework is robust and efficient in a multi-core environment.",2016,Knowledge Discovery and Data Mining,coordinate descent;linear classifier;theoretical computer science;distributed computing;machine learning;computer science;
A Multi-Task Learning Formulation for Survival Analysis,Yan Li (Wayne State University);Jie Wang (University of Michigan);Jieping Ye (University of Michigan);Chandan K. Reddy (Wayne State University);,"2607418379,2471893859,2305258894,2100435683","Predicting the occurrence of a particular event of interest at future time points is the primary goal of survival analysis. The presence of incomplete observations due to time limitations or loss of data traces is known as censoring which brings unique challenges in this domain and differentiates survival analysis from other standard regression methods. The popularly used survival analysis methods such as Cox proportional hazard model and parametric survival regression suffer from some strict assumptions and hypotheses that are not realistic in most of the real-world applications. To overcome the weaknesses of these two types of methods, in this paper, we reformulate the survival analysis problem as a multi-task learning problem and propose a new multi-task learning based formulation to predict the survival time by estimating the survival status at each time interval during the study duration. We propose an indicator matrix to enable the multi-task learning algorithm to handle censored instances and incorporate some of the important characteristics of survival problems such as non-negative non-increasing list structure into our model through max-heap projection. We employ the L2,1-norm penalty which enables the model to learn a shared representation across related tasks and hence select important features and alleviate over-fitting in high-dimensional feature spaces; thus, reducing the prediction error of each task. To efficiently handle the two non-smooth constraints, in this paper, we propose an optimization method which employs Alternating Direction Method of Multipliers (ADMM) algorithm to solve the proposed multi-task learning problem. We demonstrate the performance of the proposed method using real-world microarray gene expression high-dimensional benchmark datasets and show that our method outperforms state-of-the-art methods.",2016,Knowledge Discovery and Data Mining,multi task learning;clustering high dimensional data;regularization;survival analysis;econometrics;data mining;machine learning;statistics;computer science;
City-Scale Map Creation and Updating using GPS Collections,Chen Chen 0018 (Stanford University);Cewu Lu (Stanford University);Qixing Huang (Toyota Technological Institute at Chicago);Qiang Yang 0001 (Hong Kong University of Science and Technology);Dimitrios Gunopulos (National and Kapodistrian University of Athens);Leonidas J. Guibas (Stanford University);,"2646844496,2120761920,2166170891,2109031554,2712250546,356043702","Applications such as autonomous driving or real-time route recommendations require up-to-date and accurate digital maps. However, manually creating and updating such maps is too costly to meet the rising demands. As large collections of GPS trajectories become widely available, constructing and updating maps using such trajectory collections can greatly reduce the cost of such maps. Unfortunately, due to GPS noise and varying trajectory sampling rates, inferring maps from GPS trajectories can be very challenging. In this paper, we present a framework to create up-to-date maps with rich knowledge from GPS trajectory collections. Starting from an unstructured GPS point cloud, we discover road segments using novel graph-based clustering techniques with prior knowledge on road design. Based on road segments, we develop a scale- and orientation-invariant traj-SIFT feature to localize and recognize junctions using a supervised learning framework. Maps with rich knowledge are created based on discovered road segments and junctions. Compared to state-of-the-art methods, our approach can efficiently construct high-quality maps at city scales from large collections of GPS trajectories.",2016,Knowledge Discovery and Data Mining,computer vision;data mining;
A Multiple Test Correction for Streams and Cascades of Statistical Hypothesis Tests,Geoffrey I. Webb (Monash University);François Petitjean (Monash University);,"2126304162,2709762332","Statistical hypothesis testing is a popular and powerful tool for inferring knowledge from data. For every such test performed, there is always a non-zero probability of making a false discovery, i.e.~rejecting a null hypothesis in error. Familywise error rate (FWER) is the probability of making at least one false discovery during an inference process. The expected FWER grows exponentially with the number of hypothesis tests that are performed, almost guaranteeing that an error will be committed if the number of tests is big enough and the risk is not managed; a problem known as the multiple testing problem. State-of-the-art methods for controlling FWER in multiple comparison settings require that the set of hypotheses be predetermined. This greatly hinders statistical testing for many modern applications of statistical inference, such as model selection, because neither the set of hypotheses that will be tested, nor even the number of hypotheses, can be known in advance. This paper introduces Subfamilywise Multiple Testing, a multiple-testing correction that can be used in applications for which there are repeated pools of null hypotheses from each of which a single null hypothesis is to be rejected and neither the specific hypotheses nor their number are known until the final rejection decision is completed. To demonstrate the importance and relevance of this work to current machine learning problems, we further refine the theory to the problem of model selection and show how to use Subfamilywise Multiple Testing for learning graphical models. We assess its ability to discover graphical models on more than 7,000 datasets, studying the ability of Subfamilywise Multiple Testing to outperform the state of the art on data with varying size and dimensionality, as well as with varying density and power of the present correlations. Subfamilywise Multiple Testing provides a significant improvement in statistical efficiency, often requiring only half as much data to discover the same model, while strictly controlling FWER.",2016,Knowledge Discovery and Data Mining,testing hypotheses suggested by the data;p rep;per comparison error rate;false discovery rate;model selection;multiple comparisons problem;statistical hypothesis testing;statistical power;econometrics;machine learning;statistics;mathematics;
When Social Influence Meets Item Inference,Hui-Ju Hung (Pennsylvania State University);Hong-Han Shuai (Academia Sinica);De-Nian Yang (Academia Sinica);Liang-Hao Huang (Academia Sinica);Wang-Chien Lee (Pennsylvania State University);Jian Pei (Simon Fraser University);Ming-Syan Chen (National Taiwan University);,"2145006012,2184003779,2096343151,2146630906,2143778659,2126330539,2122365371","Research issues and data mining techniques for product recommendation and viral marketing have been widely studied. Existing works on seed selection in social networks do not take into account the effect of product recommendations in e-commerce stores. In this paper, we investigate the seed selection problem for viral marketing that considers both effects of social influence and item inference (for product recommendation). We develop a new model, Social Item Graph (SIG) , that captures both effects in the form of hyperedges. Accordingly, we formulate a seed selection problem, called Social Item Maximization Problem (SIMP) , and prove the hardness of SIMP. We design an efficient algorithm with performance guarantee, called Hyperedge-Aware Greedy (HAG), for SIMP and develop a new index structure, called SIG-index, to accelerate the computation of diffusion process in HAG. Moreover, to construct realistic SIG models for SIMP, we develop a statistical inference based framework to learn the weights of hyperedges from data. Finally, we perform a comprehensive evaluation on our proposals with various baselines. Experimental result validates our ideas and demonstrates the effectiveness and efficiency of the proposed model and algorithms over baselines.",2016,Knowledge Discovery and Data Mining,viral marketing;world wide web;data mining;artificial intelligence;machine learning;statistics;computer science;mathematics;
Collaborative Knowledge Base Embedding for Recommender Systems,Fuzheng Zhang (Microsoft);Nicholas Jing Yuan (Microsoft);Defu Lian (University of Electronic Science and Technology of China);Xing Xie (Microsoft);Wei-Ying Ma (Microsoft);,"2110384818,2096490164,2110195189,2125800575,2134693834","Among different recommendation techniques, collaborative filtering usually suffer from limited performance due to the sparsity of user-item interactions. To address the issues, auxiliary information is usually used to boost the performance. Due to the rapid collection of information on the web, the knowledge base provides heterogeneous information including both structured and unstructured data with different semantics, which can be consumed by various applications. In this paper, we investigate how to leverage the heterogeneous information in a knowledge base to improve the quality of recommender systems. First, by exploiting the knowledge base, we design three components to extract items' semantic representations from structural content, textual content and visual content, respectively. To be specific, we adopt a heterogeneous network embedding method, termed as TransR, to extract items' structural representations by considering the heterogeneity of both nodes and relationships. We apply stacked denoising auto-encoders and stacked convolutional auto-encoders, which are two types of deep learning based embedding techniques, to extract items' textual representations and visual representations, respectively. Finally, we propose our final integrated framework, which is termed as Collaborative Knowledge Base Embedding (CKE), to jointly learn the latent representations in collaborative filtering as well as items' semantic representations from the knowledge base. To evaluate the performance of each embedding component as well as the whole system, we conduct extensive experiments with two real-world datasets from different scenarios. The results reveal that our approaches outperform several widely adopted state-of-the-art recommendation methods.",2016,Knowledge Discovery and Data Mining,collaborative filtering;recommender system;knowledge base;world wide web;data mining;machine learning;computer science;
Developing a Data-Driven Player Ranking in Soccer Using Predictive Model Weights,Joel Brooks (Massachusetts Institute of Technology);Matthew Kerr (Massachusetts Institute of Technology);John V. Guttag (Massachusetts Institute of Technology);,"2518096622,2431617671,120651226","Quantitative evaluation of the ability of soccer players to contribute to team offensive performance is typically based on goals scored, assists made, and shots taken. In this paper, we describe a novel player ranking system based entirely on the value of passes completed. This value is derived based on the relationship of pass locations in a possession and shot opportunities generated. This relationship is learned by applying a supervised machine learning model to pass locations in event data from the 2012-2013 La Liga season. Interestingly, though this metric is based entirely on passes, the derived player rankings are largely consistent with general perceptions of offensive ability, e.g., Messi and Ronaldo are near the top. Additionally, when used to rank midfielders, it separates the more offensively-minded players from others.",2016,Knowledge Discovery and Data Mining,multimedia;machine learning;simulation;computer science;
"Extreme Multi-label Loss Functions for Recommendation, Tagging, Ranking & Other Missing Label Applications",Himanshu Jain (Indian Institute of Technology Delhi);Yashoteja Prabhu (Indian Institute of Technology Delhi);Manik Varma (Microsoft);,"2423330096,1988058231,2130043413","The choice of the loss function is critical in extreme multi-label learning where the objective is to annotate each data point with the most relevant subset of labels from an extremely large label set. Unfortunately, existing loss functions, such as the Hamming loss, are unsuitable for learning, model selection, hyperparameter tuning and performance evaluation. This paper addresses the issue by developing propensity scored losses which: (a) prioritize predicting the few relevant labels over the large number of irrelevant ones; (b) do not erroneously treat missing labels as irrelevant but instead provide unbiased estimates of the true loss function even when ground truth labels go missing under arbitrary probabilistic label noise models; and (c) promote the accurate prediction of infrequently occurring, hard to predict, but rewarding tail labels. Another contribution is the development of algorithms which efficiently scale to extremely large datasets with up to 9 million labels, 70 million points and 2 million dimensions and which give significant improvements over the state-of-the-art. This paper's results also apply to tagging, recommendation and ranking which are the motivating applications for extreme multi-label learning. They generalize previous attempts at deriving unbiased losses under the restrictive assumption that labels go missing uniformly at random from the ground truth. Furthermore, they provide a sound theoretical justification for popular label weighting heuristics used to recommend rare items. Finally, they demonstrate that the proposed contributions align with real world applications by achieving superior clickthrough rates on sponsored search advertising in Bing.",2016,Knowledge Discovery and Data Mining,ranking;information retrieval;data mining;machine learning;statistics;computer science;
Bid-aware Gradient Descent for Unbiased Learning with Censored Data in Display Advertising,Weinan Zhang (Shanghai Jiao Tong University);Tianxiong Zhou;Jun Wang (University College London);Jian Xu;,"2527611484,2510556814,2557836567,2705616049","In real-time display advertising, ad slots are sold per impression via an auction mechanism. For an advertiser, the campaign information is incomplete --- the user responses (e.g, clicks or conversions) and the market price of each ad impression are observed only if the advertiser's bid had won the corresponding ad auction. The predictions, such as bid landscape forecasting, click-through rate (CTR) estimation, and bid optimisation, are all operated in the pre-bid stage with full-volume bid request data. However, the training data is gathered in the post-bid stage with a strong bias towards the winning impressions. A common solution for learning over such censored data is to reweight data instances to correct the discrepancy between training and prediction. However, little study has been done on how to obtain the weights independent of previous bidding strategies and consequently integrate them into the final CTR prediction and bid generation steps. In this paper, we formulate CTR estimation and bid optimisation under such censored auction data. Derived from a survival model, we show that historic bid information is naturally incorporated to produce Bid-aware Gradient Descents (BGD) which controls both the importance and the direction of the gradient to achieve unbiased learning. The empirical study based on two large-scale real-world datasets demonstrates remarkable performance gains from our solution. The learning framework has been deployed on Yahoo!'s real-time bidding platform and provided 2.97% AUC lift for CTR estimation and 9.30% eCPC drop for bid optimisation in an online A/B test.",2016,Knowledge Discovery and Data Mining,unique bid auction;real time bidding;censoring;data mining;machine learning;statistics;
Transfer Knowledge between Cities,Ying Wei (Hong Kong University of Science and Technology);Yu Zheng (Microsoft);Qiang Yang (Hong Kong University of Science and Technology);,"2235263654,2145115012,2109031554","The rapid urbanization has motivated extensive research on urban computing. It is critical for urban computing tasks to unlock the power of the diversity of data modalities generated by different sources in urban spaces, such as vehicles and humans. However, we are more likely to encounter the label scarcity problem and the data insufficiency problem when solving an urban computing task in a city where services and infrastructures are not ready or just built. In this paper, we propose a FLexible multimOdal tRAnsfer Learning (FLORAL) method to transfer knowledge from a city where there exist sufficient multimodal data and labels, to this kind of cities to fully alleviate the two problems. FLORAL learns semantically related dictionaries for multiple modalities from a source domain, and simultaneously transfers the dictionaries and labelled instances from the source into a target domain. We evaluate the proposed method with a case study of air quality prediction.",2016,Knowledge Discovery and Data Mining,transfer of learning;data mining;artificial intelligence;machine learning;simulation;computer science;
Images Don't Lie: Transferring Deep Visual Semantic Features to Large-Scale Multimodal Learning to Rank,Corey Lynch;Kamelia Aryafar (Drexel University);Josh Attenberg (Polytechnic Institute of New York University);,"2158118044,286824370,2065581641","Search is at the heart of modern e-commerce. As a result, the task of ranking search results automatically (learning to rank) is a multibillion dollar machine learning problem. Traditional models optimize over a few hand-constructed features based on the item's text. In this paper, we introduce a multimodal learning to rank model that combines these traditional features with visual semantic features transferred from a deep convolutional neural network. In a large scale experiment using data from the online marketplace Etsy, we verify that moving to a multimodal representation significantly improves ranking quality. We show how image features can capture fine-grained style information not available in a text-only representation. In addition, we show concrete examples of how image information can successfully disentangle pairs of highly different items that are ranked similarly by a text-only model.",2016,Knowledge Discovery and Data Mining,deep learning;learning to rank;computer vision;pattern recognition;machine learning;computer science;
Data-Driven Metric Development for Online Controlled Experiments: Seven Lessons Learned,Alex Deng (Microsoft);Xiaolin Shi (Yahoo!);,"2172042952,2529829703","Online controlled experiments, also called A/B testing, have been established as the mantra for data-driven decision making in many web-facing companies. In recent years, there are emerging research works focusing on building the platform and scaling it up, best practices and lessons learned to obtain trustworthy results, and experiment design techniques and various issues related to statistical inference and testing. However, despite playing a central role in online controlled experiments, there is little published work on treating metric development itself as a data-driven process. In this paper, we focus on the topic of how to develop meaningful and useful metrics for online services in their online experiments, and show how data-driven techniques and criteria can be applied in metric development process. In particular, we emphasize two fundamental qualities for the goal metrics (or Overall Evaluation Criteria) of any online service: directionality and sensitivity. We share lessons on why these two qualities are critical, how to measure these two qualities of metrics of interest, how to develop metrics with clear directionality and high sensitivity by using approaches based on user behavior models and data-driven calibration, and how to choose the right goal metrics for the entire online services.",2016,Knowledge Discovery and Data Mining,management science;data mining;simulation;computer science;
Understanding Behaviors that Lead to Purchasing: A Case Study of Pinterest,Caroline Lo (Stanford University);Dan Frankowski;Jure Leskovec (Stanford University);,"2511782389,2639739921,1878631932","Online e-commerce applications are becoming a primary vehicle for people to find, compare, and ultimately purchase products. One of the fundamental questions that arises in e-commerce is to characterize, understand, and model user long-term purchasing intent, which is important as it allows for personalized and context relevant e-commerce services. In this paper we study user activity and purchasing behavior with the goal of building models of time-varying user purchasing intent. We analyze the purchasing behavior of nearly three million Pinterest users to determine short-term and long-term signals in user behavior that indicate higher purchase intent. We find that users with long-term purchasing intent tend to save and clickthrough on more content. However, as users approach the time of purchase their activity becomes more topically focused and actions shift from saves to searches. We further find that purchase signals in online behavior can exist weeks before a purchase is made and can also be traced across different purchase categories. Finally, we synthesize these insights in predictive models of user purchasing intent. Taken together, our work identifies a set of general principles and signals that can be used to model user purchasing intent across many content discovery applications.",2016,Knowledge Discovery and Data Mining,purchase order;
Efficient Shift-Invariant Dictionary Learning,Guoqing Zheng (Carnegie Mellon University);Yiming Yang (Carnegie Mellon University);Jaime G. Carbonell (Carnegie Mellon University);,"2619846774,2159253281,2100444261","Shift-invariant dictionary learning (SIDL) refers to the problem of discovering a set of latent basis vectors (the dictionary) that captures informative local patterns at different locations of the input sequences, and a sparse coding for each sequence as a linear combination of the latent basis elements. It differs from conventional dictionary learning and sparse coding where the latent basis has the same dimension as the input vectors, where the focus is on global patterns instead of shift-invariant local patterns. Unsupervised discovery of shift-invariant dictionary and the corresponding sparse coding has been an open challenge as the number of candidate local patterns is extremely large, and the number of possible linear combinations of such local patterns is even more so. In this paper we propose a new framework for unsupervised discovery of both the shift-invariant basis and the sparse coding of input data, with efficient algorithms for tractable optimization. Empirical evaluations on multiple time series data sets demonstrate the effectiveness and efficiency of the proposed method.",2016,Knowledge Discovery and Data Mining,k svd;neural coding;sparse approximation;time series;speech recognition;pattern recognition;machine learning;statistics;computer science;
Computational Drug Repositioning Using Continuous Self-Controlled Case Series,Zhaobin Kuang (University of Wisconsin-Madison);James A. Thomson (Morgridge Institute for Research);Michael Caldwell (Marshfield Clinic);Peggy L. Peissig (Marshfield Clinic);Ron M. Stewart (Morgridge Institute for Research);David Page (University of Wisconsin-Madison);,"2513022391,2141025565,2098265439,2028114619,2173824337,1975712082",-,2016,Knowledge Discovery and Data Mining,-
MAP: Frequency-Based Maximization of Airline Profits based on an Ensemble Forecasting Approach,"Bo An (Nanyang Technological University);Haipeng Chen (Nanyang Technological University);Noseong Park (University of Maryland, College Park);V.S. Subrahmanian (University of Maryland, College Park);","2169236860,2710629891,2304434517,2261167843","Though there are numerous traditional models to predict market share and demand along airline routes, the prediction of existing models is not precise enough and, to the best of our knowledge, there is no use of data-mining based forecasting techniques to improve airline profitability. We propose the MAP (Maximizing Airline Profits) architecture designed to help airlines and make two key contributions in airline market share and route demand prediction and prediction-based airline profit optimization. Compared with past methods to forecast market share and demand along airline routes, we introduce a novel Ensemble Forecasting (MAP-EF) approach considering two new classes of features: (i) features derived from clusters of similar routes, and (ii) features based on equilibrium pricing. We show that MAP-EF achieves much better Pearson Correlation Coefficients (over 0.95 vs. 0.82 for market share, 0.98 vs. 0.77 for demand) and R2-values compared with three state-of-the-art works for forecasting market share and demand, while showing much lower variance. Using the results of MAP-EF, we develop MAP-Bilevel Branch and Bound (MAP-BBB) and MAP-Greedy (MAP-G) algorithms to optimally allocate flight frequencies over multiple routes, to maximize an airline's profit. Experimental results show that airlines can increase profits by a significant margin. All experiments were conducted with data aggregated from four sources: US Bureau of Transportation Statistics (BTS), US Bureau of Economic Analysis (BEA), the National Transportation Safety Board (NTSB), and the US Census Bureau (CB).",2016,Knowledge Discovery and Data Mining,regression;
Scalable Pattern Matching over Compressed Graphs via Dedensification,Antonio Maccioni (Roma Tre University);Daniel J. Abadi (Yale University);,"2096345549,2170206103","One of the most common operations on graph databases is graph pattern matching ( e.g. , graph isomorphism and more general types of ""subgraph pattern matching""). In fact, in some graph query languages every single query is expressed as a graph matching operation. Consequently, there has been a significant amount of research effort in optimizing graph matching operations in graph database systems. As graph databases have scaled in recent years, so too has recent work on scaling graph matching operations. However, the performance of recent proposals for scaling graph pattern matching is limited by the presence of high-degree nodes. These high-degree nodes result in an explosion of intermediate result sizes during query execution, and therefore significant performance bottlenecks. In this paper we present a dedensification technique that losslessly compresses the neighborhood around high-degree nodes. Furthermore, we introduce a query processing technique that enables direct operation of graph query processing operations over the compressed data, without ever having to decompress the data. For pattern matching operations, we show how this technique can be implemented as a layer above existing graph database systems, so that the end-user can benefit from this technique without requiring modifications to the core graph database engine code. Our technique reduces the size of the intermediate result sets during query processing, and thereby improves query performance.",2016,Knowledge Discovery and Data Mining,folded cube graph;factor critical graph;distance hereditary graph;simplex graph;strength of a graph;voltage graph;complement graph;graph bandwidth;graph power;graph operations;null graph;clique width;graph property;sparql;graph database;graph;rdf;power law;theoretical computer science;data mining;database;computer science;
Robust Extreme Multi-label Learning,"Chang Xu (Peking University);Dacheng Tao (University of Technology, Sydney);Chao Xu (Peking University);","2115321199,2104129307,2210879162","Tail labels in the multi-label learning problem undermine the low-rank assumption. Nevertheless, this problem has rarely been investigated. In addition to using the low-rank structure to depict label correlations, this paper explores and exploits an additional sparse component to handle tail labels behaving as outliers, in order to make the classical low-rank principle in multi-label learning valid. The divide-and-conquer optimization technique is employed to increase the scalability of the proposed algorithm while theoretically guaranteeing its performance. A theoretical analysis of the generalizability of the proposed algorithm suggests that it can be improved by the low-rank and sparse decomposition given tail labels. Experimental results on real-world data demonstrate the significance of investigating tail labels and the effectiveness of the proposed algorithm.",2016,Knowledge Discovery and Data Mining,data mining;artificial intelligence;machine learning;statistics;computer science;
Online Optimization Methods for the Quantification Problem,Purushottam Kar (Indian Institute of Technology Kanpur);Shuai Li (University of Insubria);Harikrishna Narasimhan (Harvard University);Sanjay Chawla (Qatar Computing Research Institute);Fabrizio Sebastiani (Qatar Computing Research Institute);,"2136781283,2708734412,2637028757,2201421368,2130902957","The estimation of class prevalence, i.e., of the fraction of a population that belongs to a certain class, is an important task in data analytics, and finds applications in many domains such as the social sciences, market research, epidemiology, and others. For example, in sentiment analysis the goal is often not to estimate whether a specific text conveys a positive or a negative sentiment, but rather to estimate the overall distribution of positive and negative sentiments, e.g., in a certain time frame. A popular way of performing the above task, often dubbed quantification , is to use supervised learning in order to train a prevalence estimator from labeled data. In the literature there are several performance metrics for measuring the success of such prevalence estimators. In this paper we propose the first online stochastic algorithms for directly optimizing these quantification-specific performance measures. We also provide algorithms that optimize hybrid performance measures that seek to balance quantification and classification performance. Our algorithms present a significant advancement in the theory of multivariate optimization; we show, via a rigorous theoretical analysis, that they exhibit optimal convergence. We also report extensive experiments on benchmark and real data sets which demonstrate that our methods significantly outperform existing optimization techniques used for these performance measures.",2016,Knowledge Discovery and Data Mining,biological classification;stochastic optimization;econometrics;data mining;machine learning;statistics;computer science;mathematics;
Smart Broadcasting: Do You Want to be Seen?,Mohammad Reza Karimi (Sharif University of Technology);Erfan Tavakoli (Sharif University of Technology);Mehrdad Farajtabar (Georgia Institute of Technology);Le Song (Georgia Institute of Technology);Manuel Gomez Rodriguez (Max Planck Society);,"2515129431,2518046445,1198110563,2113868374,2421855118","Many users in online social networks are constantly trying to gain attention from their followers by broadcasting posts to them. These broadcasters are likely to gain greater attention if their posts can remain visible for a longer period of time among their followers' most recent feeds. Then when to post? In this paper, we study the problem of smart broadcasting using the framework of temporal point processes, where we model users feeds and posts as discrete events occurring in continuous time. Based on such continuous-time model, then choosing a broadcasting strategy for a user becomes a problem of designing the conditional intensity of her posting events. We derive a novel formula which links this conditional intensity with the visibility of the user in her followers' feeds. Furthermore, by exploiting this formula, we develop an efficient convex optimization framework for the when-to-post problem. Our method can find broadcasting strategies that reach a desired visibility level with provable guarantees. We experimented with data gathered from Twitter, and show that our framework can consistently make broadcasters' post more visible than alternatives.",2016,Knowledge Discovery and Data Mining,visibility;social network;world wide web;telecommunications;social science;distributed computing;machine learning;simulation;statistics;computer science;
Gemello: Creating a Detailed Energy Breakdown from Just the Monthly Electricity Bill,Nipun Batra (Indraprastha Institute of Information Technology);Amarjeet Singh (Indraprastha Institute of Information Technology);Kamin Whitehouse (University of Virginia);,"2116968475,2146959451,2001774235","The first step to saving energy in the home is often to create an energy breakdown: the amount of energy used by each individual appliance in the home. Unfortunately, current techniques that produce an energy breakdown are not scalable: they require hardware to be installed in each and every home. In this paper, we propose a more scalable solution called Gemello that estimates the energy breakdown for one home by matching it with similar homes for which the breakdown is already known. This matching requires only the monthly energy bill and household characteristics such as square footage of the home and the size of the household. We evaluate this approach using 57 homes and results indicate that the accuracy of Gemello is comparable to or better than existing techniques that use sensing infrastructure in each home. The information required by Gemello is often publicly available and, as such, it can be immediately applied to many homes around the world.",2016,Knowledge Discovery and Data Mining,feedback;telecommunications;simulation;computer science;
Learning Cumulatively to Become More Knowledgeable,Geli Fei (University of Illinois at Chicago);Shuai Wang (University of Illinois at Chicago);Bing Liu (University of Illinois at Chicago);,"2133167487,2441217776,2244698799","In classic supervised learning, a learning algorithm takes a fixed training data of several classes to build a classifier. In this paper, we propose to study a new problem, i.e., building a learning system that learns cumulatively. As time goes by, the system sees and learns more and more classes of data and becomes more and more knowledgeable. We believe that this is similar to human learning. We humans learn continuously, retaining the learned knowledge, identifying and learning new things, and updating the existing knowledge with new experiences. Over time, we cumulate more and more knowledge. A learning system should be able to do the same. As algorithmic learning matures, it is time to tackle this cumulative machine learning (or simply cumulative learning ) problem, which is a kind of lifelong machine learning problem. It presents two major challenges. First, the system must be able to detect data from unseen classes in the test set. Classic supervised learning, however, assumes all classes in testing are known or seen at the training time. Second, the system needs to be able to selectively update its models whenever a new class of data arrives without re-training the whole system using the entire past and present training data. This paper proposes a novel approach and system to tackle these challenges. Experimental results on two datasets with learning from 2 classes to up to 100 classes show that the proposed approach is highly promising in terms of both classification accuracy and computational efficiency.",2016,Knowledge Discovery and Data Mining,online machine learning;stability;preference learning;inductive transfer;multi task learning;robot learning;synchronous learning;generalization error;competitive learning;active learning;error driven learning;algorithmic learning theory;learning classifier system;semi supervised learning;computational learning theory;instance based learning;unsupervised learning;data mining;artificial intelligence;machine learning;computer science;
Scalable Fast Rank-1 Dictionary Learning for fMRI Big Data Analysis,"Xiang Li (University of Georgia);Milad Makkie (University of Georgia);Binbin Lin (University of Michigan);Mojtaba Sedigh Fazli (University of Georgia);Ian Davidson (University of California, Davis);Jieping Ye (University of Michigan);Tianming Liu (University of Georgia);Shannon Quinn (University of Georgia);","2068388382,2480698404,2643646471,2630584540,2560595684,2305258894,2161381501,2517111332","It has been shown from various functional neuroimaging studies that sparsity-regularized dictionary learning could achieve superior performance in decomposing comprehensive and neuroscientifically meaningful functional networks from massive fMRI signals. However, the computational cost for solving the dictionary learning problem has been known to be very demanding, especially when dealing with large-scale data sets. Thus in this work, we propose a novel distributed rank-1 dictionary learning (D-r1DL) model and apply it for fMRI big data analysis. The model estimates one rank-1 basis vector with sparsity constraint on its loading coefficient from the input data at each learning step through alternating least squares updates. By iteratively learning the rank-1 basis and deflating the input data at each step, the model is then capable of decomposing the whole set of functional networks. We implement and parallelize the rank-1 dictionary learning algorithm using Spark engine and deployed the resilient distributed dataset (RDDs) abstracts for the data distribution and operations. Experimental results from applying the model on the Human Connectome Project (HCP) data show that the proposed D-r1DL model is efficient and scalable towards fMRI big data analytics, thus enabling data-driven neuroscientific discovery from massive fMRI big data in the future.",2016,Knowledge Discovery and Data Mining,k svd;neural coding;theoretical computer science;data mining;machine learning;computer science;
Contextual Intent Tracking for Personal Assistants,Yu Sun (University of Melbourne);Nicholas Jing Yuan (Microsoft);Yingzi Wang (University of Science and Technology of China);Xing Xie (Microsoft);Kieran McDonald (Microsoft);Rui Zhang (University of Melbourne);,"2617049789,2096490164,2274817761,2125800575,2508691199,2690388134","A new paradigm of recommendation is emerging in intelligent personal assistants such as Apple's Siri, Google Now, and Microsoft Cortana, which recommends ""the right information at the right time"" and proactively helps you ""get things done"". This type of recommendation requires precisely tracking users' contemporaneous intent, i.e., what type of information (e.g., weather, stock prices) users currently intend to know, and what tasks (e.g., playing music, getting taxis) they intend to do. Users' intent is closely related to context, which includes both external environments such as time and location, and users' internal activities that can be sensed by personal assistants. The relationship between context and intent exhibits complicated co-occurring and sequential correlation, and contextual signals are also heterogeneous and sparse, which makes modeling the context intent relationship a challenging task. To solve the intent tracking problem, we propose the Kalman filter regularized PARAFAC2 (KP2) nowcasting model, which compactly represents the structure and co-movement of context and intent. The KP2 model utilizes collaborative capabilities among users, and learns for each user a personalized dynamic system that enables efficient nowcasting of users' intent. Extensive experiments using real-world data sets from a commercial personal assistant show that the KP2 model significantly outperforms various methods, and provides inspiring implications for deploying large-scale proactive recommendation systems in personal assistants.",2016,Knowledge Discovery and Data Mining,multi task learning;multimedia;world wide web;data mining;machine learning;simulation;computer science;
Learning to Learn and Compositionality with Deep Recurrent Neural Networks: Learning to Learn and Compositionality,Nando de Freitas (Google);,2112824674,"Deep neural network representations play an important role in computer vision, speech, computational linguistics, robotics, reinforcement learning and many other data-rich domains. In this talk I will show that learning-to-learn and compositionality are key ingredients for dealing with knowledge transfer so as to solve a wide range of tasks, for dealing with small-data regimes, and for continual learning. I will demonstrate this with several examples from my research team: learning to learn by gradient descent by gradient descent, neural programmers and interpreters, and learning communication.",2016,Knowledge Discovery and Data Mining,principle of compositionality;transfer of learning;robot learning;synchronous learning;recurrent neural network;competitive learning;active learning;error driven learning;deep learning;instance based learning;artificial neural network;natural language processing;artificial intelligence;machine learning;computer science;
DeepIntent: Learning Attentions for Online Advertising with Recurrent Neural Networks,Shuangfei Zhai (Binghamton University);Keng-hao Chang (Microsoft);Ruofei Zhang (Microsoft);Zhongfei Mark Zhang (Microsoft);,"2178509154,2603165909,2681549092,2427460458","In this paper, we investigate the use of recurrent neural networks (RNNs) in the context of search-based online advertising. We use RNNs to map both queries and ads to real valued vectors, with which the relevance of a given (query, ad) pair can be easily computed. On top of the RNN, we propose a novel attention network, which learns to assign attention scores to different word locations according to their intent importance (hence the name DeepIntent). The vector output of a sequence is thus computed by a weighted sum of the hidden states of the RNN at each word according their attention scores. We perform end-to-end training of both the RNN and attention network under the guidance of user click logs, which are sampled from a commercial search engine. We show that in most cases the attention network improves the quality of learned vector representations, evaluated by AUC on a manually labeled dataset. Moreover, we highlight the effectiveness of the learned attention scores from two aspects: query rewriting and a modified BM25 metric. We show that using the learned attention scores, one is able to produce sub-queries that are of better qualities than those of the state-of-the-art methods. Also, by modifying the term frequency with the attention scores in a standard BM25 formula, one is able to improve its performance evaluated by AUC.",2016,Knowledge Discovery and Data Mining,okapi bm25;deep learning;online advertising;speech recognition;data mining;machine learning;computer science;
Towards Confidence in the Truth: A Bootstrapping based Truth Discovery Approach,Houping Xiao (State University of New York System);Jing Gao (State University of New York System);Qi Li (State University of New York System);Fenglong Ma (State University of New York System);Lu Su (State University of New York System);Yunlong Feng (Katholieke Universiteit Leuven);Aidong Zhang (State University of New York System);,"2303863390,2096731881,2261907930,2227076362,2148733542,2705548786,2228514421","The demand for automatic extraction of true information (i.e., truths) from conflicting multi-source data has soared recently. A variety of truth discovery methods have witnessed great successes via jointly estimating source reliability and truths. All existing truth discovery methods focus on providing a point estimator for each object's truth, but in many real-world applications, confidence interval estimation of truths is more desirable, since confidence interval contains richer information. To address this challenge, in this paper, we propose a novel truth discovery method ( ETCIBoot ) to construct confidence interval estimates as well as identify truths, where the bootstrapping techniques are nicely integrated into the truth discovery procedure. Due to the properties of bootstrapping, the estimators obtained by ETCIBoot are more accurate and robust compared with the state-of-the-art truth discovery approaches. Theoretically, we prove the asymptotical consistency of the confidence interval obtained by ETCIBoot . Experimentally, we demonstrate that ETCIBoot is not only effective in constructing confidence intervals but also able to obtain better truth estimates.",2016,Knowledge Discovery and Data Mining,bootstrapping;confidence interval;econometrics;data mining;statistics;computer science;mathematics;
Repeat Buyer Prediction for E-Commerce,"Guimei Liu (Agency for Science, Technology and Research);Tam T. Nguyen (Agency for Science, Technology and Research);Gang Zhao;Wei Zha (Agency for Science, Technology and Research);Jianbo Yang (General Electric);Jianneng Cao (Agency for Science, Technology and Research);Min Wu (Agency for Science, Technology and Research);Peilin Zhao (Agency for Science, Technology and Research);Wei Chen;","2641724321,2537889826,2667920047,2532738178,2538785804,2630008670,2723782748,2096910461,2675036247","A large number of new buyers are often acquired by merchants during promotions. However, many of the attracted buyers are one-time deal hunters, and the promotions may have little long-lasting impact on sales. It is important for merchants to identify who can be converted to regular loyal buyers and then target them to reduce promotion cost and increase the return on investment (ROI). At International Joint Conferences on Artificial Intelligence (IJCAI) 2015, Alibaba hosted an international competition for repeat buyer prediction based on the sales data of the ``Double 11"" shopping event in 2014 at Tmall.com. We won the first place at stage 1 of the competition out of 753 teams. In this paper, we present our winning solution, which consists of comprehensive feature engineering and model training. We created profiles for users, merchants, brands, categories, items and their interactions via extensive feature engineering. These profiles are not only useful for this particular prediction task, but can also be used for other important tasks in e-commerce, such as customer segmentation, product recommendation, and customer base augmentation for brands. Feature engineering is often the most important factor for the success of a prediction task, but not much work can be found in the literature on feature engineering for prediction tasks in e-commerce. Our work provides some useful hints and insights for data science practitioners in e-commerce.",2016,Knowledge Discovery and Data Mining,feature;e commerce;data mining;machine learning;computer science;
Extracting Optimal Performance from Dynamic Time Warping,"Abdullah Mueen (University of New Mexico);Eamonn J. Keogh (University of California, Riverside);","2083987245,2170070822","Dynamic Time Warping (DTW) is a distance measure that compares two time series after optimally aligning them. DTW is being used for decades in thousands of academic and industrial projects despite the very expensive computational complexity, O(n 2 ). These applications include data mining, image processing, signal processing, robotics and computer graphics among many others. In spite of all this research effort, there are many myths and misunderstanding about DTW in the literature, for example ""it is too slow to be useful"" or ""the warping window size does not matter much."" In this tutorial, we correct these misunderstandings and we summarize the research efforts in optimizing both the efficiency and effectiveness of both the basic DTW algorithm, and of the higher-level algorithms that exploit DTW such as similarity search, clustering and classification. We will discuss variants of DTW such as constrained DTW, multidimensional DTW and asynchronous DTW, and optimization techniques such as lower bounding, early abandoning, run-length encoding, bounded approximation and hardware optimization. We will discuss a multitude of application areas including physiological monitoring, social media mining, activity recognition and animal sound processing. The optimization techniques are generalizable to other domains on various data types and problems.",2016,Knowledge Discovery and Data Mining,dynamic time warping;pruning;approximation;time series;theoretical computer science;speech recognition;data mining;machine learning;statistics;computer science;
Deploying Analytics with the Portable Format for Analytics (PFA),Jim Pivarski (University of Chicago);Collin Bennett;Robert L. Grossman (University of Chicago);,"2549702503,2131176913,2095279874","We introduce a new language for deploying analytic models into products, services and operational systems called the Portable Format for Analytics (PFA). PFA is an example of what is sometimes called a model interchange format, a language for describing analytic models that is independent of specific tools, applications or systems. Model interchange formats allow one application (the model producer) to export models and another application (the model consumer or scoring engine) to import models. The core idea behind PFA is to support the safe execution of statistical functions, mathematical functions, and machine learning algorithms and their compositions within a safe execution environment. With this approach, the common analytic models used in data science can be implemented, as well as the data transformations and data aggregations required for pre- and post-processing data. PFA compliant scoring engines can be extended by adding new user defined functions described in PFA. We describe the design of PFA. A Data Mining Group (DMG) Working Group is developing the PFA standard. The current version is 0.8.1 and contains many of the commonly used statistical and machine learning models, including regression, clustering, support vector machines, neural networks, etc. We also describe two implementations of Hadrian, one in Scala and one in Python. We discuss four case studies that use PFA and Hadrian to specify analytic models, including two that are deployed in operations at client sites.",2016,Knowledge Discovery and Data Mining,predictive model markup language;world wide web;data mining;database;machine learning;computer science;
Identifying Earmarks in Congressional Bills,Ellery Wulczyn (Stanford University);Madian Khabsa (Microsoft);Vrushank Vora (University of Chicago);Matthew Heston (Northwestern University);Joe Walsh (University of Chicago);Christopher Berry (University of Chicago);Rayid Ghani (University of Chicago);,"2701281806,296516693,2614720474,2647221767,2680879499,2169003226,2655769349","Earmarks are legislative provisions that direct federal funds to specific projects, circumventing the competitive grant-making process of federal agencies. Identifying and cataloging earmarks is a tedious, time-consuming process carried out by experts from public interest groups. In this paper, we present a machine learning system for automatically extracting earmarks from congressional bills and reports. We first describe a table-parsing algorithm for extracting budget allocations from appropriations tables in congressional bills. We then use machine learning classifiers to identify budget allocations as earmarked objects with an out of sample ROC AUC score of 0.89. Using this system, we construct the first publicly available database of earmarks dating back to 1995. Our machine learning approach adds transparency, accuracy, and speed to the congressional appropriations process.",2016,Knowledge Discovery and Data Mining,information extraction;data science;data mining;machine learning;computer science;
Robust Influence Maximization,Xinran He (University of Southern California);David Kempe (University of Southern California);,"2096901250,2137221145","Uncertainty about models and data is ubiquitous in the computational social sciences, and it creates a need for robust social network algorithms, which can simultaneously provide guarantees across a spectrum of models and parameter settings. We begin an investigation into this broad domain by studying robust algorithms for the Influence Maximization problem, in which the goal is to identify a set of k nodes in a social network whose joint influence on the network is maximized. We define a Robust Influence Maximization framework wherein an algorithm is presented with a set of influence functions, typically derived from different influence models or different parameter settings for the same model. The different parameter settings could be derived from observed cascades on different topics, under different conditions, or at different times. The algorithm's goal is to identify a set of k nodes who are simultaneously influential for all influence functions, compared to the (function-specific) optimum solutions. We show strong approximation hardness results for this problem unless the algorithm gets to select at least a logarithmic factor more seeds than the optimum solution. However, when enough extra seeds may be selected, we show that techniques of Krause et al. can be used to approximate the optimum robust influence to within a factor of 1-1/e. We evaluate this bicriteria approximation algorithm against natural heuristics on several real-world data sets. Our experiments indicate that the worst-case hardness does not necessarily translate into bad performance on real-world data sets; all algorithms perform fairly well.",2016,Knowledge Discovery and Data Mining,noise;uncertainty;robust optimization;econometrics;machine learning;mathematical optimization;statistics;mathematics;
Designing Policy Recommendations to Reduce Home Abandonment in Mexico,"Klaus Ackermann (Monash University);Eduardo Blancas Reyes (University of Chicago);Sue He (University of Virginia);Thomas Anderson Keller (University of California, San Diego);Paul van der Boor (University of Chicago);Romana Khan (Northwestern University);Rayid Ghani (University of Chicago);José Carlos González;","2504984702,2507525880,2618421523,2514371254,2581893250,2107509328,2655769349,2689512287","Infonavit, the largest provider of mortgages in Mexico, assists working families to obtain low-interest rate housing solutions. An increasingly prevalent problem is home abandonment: when a homeowner decides to leave their property and forego their investment. A major causal factor of this outcome is a mismatch between the homeowner's needs, in terms of access to services and employment, and the location characteristics of the home. This paper describes our collaboration with Infonavit to reduce home abandonment at two levels: develop policy recommendations for targeted improvements in location characteristics, and develop a decision-support tool to assist the homeowner in the home location decision. Using 20 years of mortgage history data combined with surveys, census, and location information, we develop a model to predict the probability of home abandonment based on both individual and location characteristics. The model is used to develop a tool that provides Infonavit the ability to give advice to Mexican workers when they apply for a loan, evaluate and improve the locations of new housing developments, and provide data-driven recommendations to the federal government to influence local development initiatives and infrastructure investments. The result is improving economic outcomes for the citizens of Mexico by pre-emptively identifying at-risk home mortgages, thereby allowing them to be altered or remedied before they result in abandonment.",2016,Knowledge Discovery and Data Mining,common good;actuarial science;machine learning;computer science;
Singapore in Motion: Insights on Public Transport Service Level Through Farecard and Mobile Data Analytics,"Hasan Poonawala (IBM);Vinay Kolar (Cisco Systems, Inc.);Sebastien Blandin (IBM);Laura Wynter (IBM);Sambit Sahu (IBM);","2521892632,2720556456,2117376187,1990593394,2169578810","Given the changing dynamics of mobility patterns and rapid growth of cities, transport agencies seek to respond more rapidly to needs of the public with the goal of offering an effective and competitive public transport system. A more data-centric approach for transport planning is part of the evolution of this process. In particular, the vast penetration of mobile phones provides an opportunity to monitor and derive insights on transport usage. Real time and historical analyses of such data can give a detailed understanding of mobility patterns of people and also suggest improvements to current transit systems. On its own, however, mobile geolocation data has a number of limitations. We thus propose a joint telco-and-farecard-based learning approach to understanding urban mobility. The approach enhances telecommunications data by leveraging it jointly with other sources of real-time data. The approach is illustrated on the First- and last-mile problem as well as route choice estimation within a densely-connected train network.",2016,Knowledge Discovery and Data Mining,big data;data mining;simulation;computer science;
Ranking Causal Anomalies via Temporal and Dynamical Analysis on Vanishing Correlations,"Wei Cheng (NEC);Kai Zhang (NEC);Haifeng Chen (NEC);Guofei Jiang (NEC);Zhengzhang Chen (NEC);Wei Wang (University of California, Los Angeles);","2620045292,2600447970,2571838709,2168090285,2132666618,2315689540","Modern world has witnessed a dramatic increase in our ability to collect, transmit and distribute real-time monitoring and surveillance data from large-scale information systems and cyber-physical systems. Detecting system anomalies thus attracts significant amount of interest in many fields such as security, fault management, and industrial optimization. Recently, invariant network has shown to be a powerful way in characterizing complex system behaviours. In the invariant network, a node represents a system component and an edge indicates a stable, significant interaction between two components. Structures and evolutions of the invariance network, in particular the vanishing correlations, can shed important light on locating causal anomalies and performing diagnosis. However, existing approaches to detect causal anomalies with the invariant network often use the percentage of vanishing correlations to rank possible casual components, which have several limitations: 1) fault propagation in the network is ignored; 2) the root casual anomalies may not always be the nodes with a high-percentage of vanishing correlations; 3) temporal patterns of vanishing correlations are not exploited for robust detection. To address these limitations, in this paper we propose a network diffusion based framework to identify significant causal anomalies and rank them. Our approach can effectively model fault propagation over the entire invariant network, and can perform joint inference on both the structural, and the time-evolving broken invariance patterns. As a result, it can locate high-confidence anomalies that are truly responsible for the vanishing correlations, and can compensate for unstructured measurement noise in the system. Extensive experiments on synthetic datasets, bank information system datasets, and coal plant cyber-physical system datasets demonstrate the effectiveness of our approach.",2016,Knowledge Discovery and Data Mining,non negative matrix factorization;data mining;machine learning;statistics;computer science;mathematics;
From Prediction to Action: A Closed-Loop Approach for Data-Guided Network Resource Allocation,"Yanan Bao (University of California, Davis);Huasen Wu (University of California, Davis);Xin Liu (University of California, Davis);","2103719961,2114102195,2616096743","Machine learning methods have been widely used in modeling and predicting network user experience. In this paper, moving beyond user experience prediction, we propose a closed-loop approach that uses data-generated prediction models to explicitly guide resource allocation for user experience improvement. The closed-loop approach leverages and verifies the causal relation that often exists between certain feature values (e.g., bandwidth) and user experience in computer networks. The approach consists of three components: we train a neural network classifier to predict user experience, utilize the trained neural network classifier as the objective function to allocate network resource, and then evaluate user experience with allocated resource to (in)validate and adjust the original model. Specifically, we propose a dual decomposition algorithm to solve the neural network-based resource optimization problem, which is complex and non-convex. We further develop an iterative mechanism for classifier optimization. Numerical results show that the dual algorithm reduces the expected number of unsatisfied users by up to 2x compared with the baseline, and the optimized classifier further improves the performance by 50%.",2016,Knowledge Discovery and Data Mining,biological classification;data mining;machine learning;simulation;computer science;
Accelerated Stochastic Block Coordinate Descent with Optimal Sampling,Aston Zhang (University of Illinois at Urbana–Champaign);Quanquan Gu (University of Virginia);,"2102121030,2167348148","We study the composite minimization problem where the objective function is the sum of two convex functions: one is the sum of a finite number of strongly convex and smooth functions, and the other is a general convex function that is non-differentiable. Specifically, we consider the case where the non-differentiable function is block separable and admits a simple proximal mapping for each block. This type of composite optimization is common in many data mining and machine learning problems, and can be solved by block coordinate descent algorithms. We propose an accelerated stochastic block coordinate descent (ASBCD) algorithm, which incorporates the incrementally averaged partial derivative into the stochastic partial derivative and exploits optimal sampling. We prove that ASBCD attains a linear rate of convergence. In contrast to uniform sampling, we reveal that the optimal non-uniform sampling can be employed to achieve a lower iteration complexity. Experimental results on different large-scale real data sets support our theory.",2016,Knowledge Discovery and Data Mining,random coordinate descent;coordinate descent;stochastic gradient descent;sampling;discrete mathematics;combinatorics;mathematical optimization;statistics;computer science;mathematics;
Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,Balaji Krishnapuram (IBM);Mohak Shah (Bosch);Alex Smola (Amazon.com);Charu Aggarwal (IBM);Dou Shen (Baidu);Rajeev Rastogi (Amazon.com);,"2582962261,2277080105,2593672690,2146335907,2136428695,2163156181","It is our great pleasure to welcome you to the 2016 ACM Conference on Knowledge Discovery and Data Mining -- KDD'16. We hope that the content and the professional network at KDD'16 will help you succeed professionally by enabling you to: identify technology trends early; make new/creative contributions; increase your productivity by using newer/better tools, processes or ways of organizing teams; identify new job opportunities; and hire new team members. We are living in an exciting time for our profession. On the one hand, we are witnessing the industrialization of data science, and the emergence of the industrial assembly line processes characterized by the division of labor, integrated processes/pipelines of work, standards, automation, and repeatability. Data science practitioners are organizing themselves in more sophisticated ways, embedding themselves in larger teams in many industry verticals, improving their productivity substantially, and achieving a much larger scale of social impact. On the other hand we are also witnessing astonishing progress from research in algorithms and systems -- for example the field of deep neural networks has revolutionized speech recognition, NLP, computer vision, image recognition, etc. By facilitating interaction between practitioners at large companies & startups on the one hand, and the algorithm development researchers including leading academics on the other, KDD'16 fosters technological and entrepreneurial innovation in the area of data science. This year's conference continues its tradition of being the premier forum for presentation of results in the field of data mining, both in the form of cutting edge research, and in the form of insights from the development and deployment of real world applications. Further, the conference continues with its tradition of a strong tutorial and workshop program on leading edge issues of data mining. The mission of this conference has broadened in recent years even as we placed a significant amount of focus on both the research and applied aspects of data mining. As an example of this broadened focus, this year we have introduced a strong hands-on tutorial program nduring the conference in which participants will learn how to use practical tools for data mining. KDD'16 also gives researchers and practitioners a unique opportunity to form professional networks, and to share their perspectives with others interested in the various aspects of data mining. For example, we have introduced office hours for budding entrepreneurs from our community to meet leading Venture Capitalists investing in this area. We hope that KDD 2016 conference will serve as a meeting ground for researchers, practitioners, funding agencies, and investors to help create new algorithms and commercial products. The call for papers attracted a significant number of submissions from countries all over the world. In particular, the research track attracted 784 submissions and the applied data science track attracted 331 submissions. Papers were accepted either as full papers or as posters. The overall acceptance rate either as full papers or posters was less than 20%. For full papers in the research track, the acceptance rate was lower than 10%. This is consistent with the fact that the KDD Conference is a premier conference in data mining and the acceptance rates historically tend to be low. It is noteworthy that the applied data science track received a larger number of submissions compared to previous years. We view this as an encouraging sign that research in data mining is increasingly becoming relevant to industrial applications. All papers were reviewed by at least three program committee members and then discussed by the PC members in a discussion moderated by a meta-reviewer. Borderline papers were thoroughly reviewed by the program chairs before final decisions were made.",2016,Knowledge Discovery and Data Mining,operations research;data mining;computer science;
Deep Crossing: Web-Scale Modeling without Manually Crafted Combinatorial Features,Ying Shan (Microsoft);T. Ryan Hoens (Microsoft);Jian Jiao (Microsoft);Haijing Wang (Microsoft);Dong Yu (Microsoft);Jc Mao (Microsoft);,"2634058586,2525304567,2514994191,2517320129,2685486628,2508991298","Manually crafted combinatorial features have been the ""secret sauce"" behind many successful models. For web-scale applications, however, the variety and volume of features make these manually crafted features expensive to create, maintain, and deploy. This paper proposes the Deep Crossing model which is a deep neural network that automatically combines features to produce superior models. The input of Deep Crossing is a set of individual features that can be either dense or sparse. The important crossing features are discovered implicitly by the networks, which are comprised of an embedding and stacking layer, as well as a cascade of Residual Units. Deep Crossing is implemented with a modeling tool called the Computational Network Tool Kit (CNTK), powered by a multi-GPU platform. It was able to build, from scratch, two web-scale models for a major paid search engine, and achieve superior results with only a sub-set of the features used in the production models. This demonstrates the potential of using Deep Crossing as a general modeling paradigm to improve existing products, as well as to speed up the development of new models with a fraction of the investment in feature engineering and acquisition of deep domain knowledge.",2016,Knowledge Discovery and Data Mining,deep learning;data mining;machine learning;simulation;computer science;
Goal-Directed Inductive Matrix Completion,"Si Si (University of Texas at Austin);Kai-Yang Chiang (University of Texas at Austin);Cho-Jui Hsieh (University of California, Davis);Nikhil Rao (Technicolor);Inderjit S. Dhillon (University of Texas at Austin);","2099379656,2129170864,2148022289,2128988510,2033403132","Matrix completion (MC) with additional information has found wide applicability in several machine learning applications. Among algorithms for solving such problems, Inductive Matrix Completion(IMC) has drawn a considerable amount of attention, not only for its well established theoretical guarantees but also for its superior performance in various real-world applications. However, IMC based methods usually place very strong constraints on the quality of the features(side information) to ensure accurate recovery, which might not be met in practice. In this paper, we propose Goal-directed Inductive Matrix Completion(GIMC) to learn a nonlinear mapping of the features so that they satisfy the required properties. A key distinction between GIMC and IMC is that the feature mapping is learnt in a supervised manner, deviating from the traditional approach of unsupervised feature learning followed by model training. We establish the superiority of our method on several popular machine learning applications including multi-label learning, multi-class classification, and semi-supervised clustering.",2016,Knowledge Discovery and Data Mining,data mining;artificial intelligence;machine learning;algorithm;computer science;mathematics;
AnyDBC: An Efficient Anytime Density-based Clustering Algorithm for Very Large Complex Datasets,Son T. Mai (Aarhus University);Ira Assent (Aarhus University);Martin Storgaard (Aarhus University);,"2650155944,145164693,2705453835","The density-based clustering algorithm DBSCAN is a state-of-the-art data clustering technique with numerous applications in many fields. However, its O(n 2 ) time complexity still remains a severe weakness. In this paper, we propose a novel anytime approach to cope with this problem by reducing both the range query and the label propagation time of DBSCAN. Our algorithm, called AnyDBC, compresses the data into smaller density-connected subsets called primitive clusters and labels objects based on connected components of these primitive clusters for reducing the label propagation time. Moreover, instead of passively performing the range query for all objects like existing techniques, AnyDBC iteratively and actively learns the current cluster structure of the data and selects a few most promising objects for refining clusters at each iteration. Thus, in the end, it performs substantially fewer range queries compared to DBSCAN while still guaranteeing the exact final result of DBSCAN. Experiments show speedup factors of orders of magnitude compared to DBSCAN and its fastest variants on very large real and synthetic complex datasets.",2016,Knowledge Discovery and Data Mining,subclu;optics algorithm;determining the number of clusters in a data set;dbscan;active learning;clustering high dimensional data;cluster analysis;data mining;database;machine learning;computer science;
Infinite Ensemble for Image Clustering,Hongfu Liu (Northeastern University);Ming Shao (Northeastern University);Sheng Li (Northeastern University);Yun Fu (Northeastern University);,"2108071053,2106967882,2618462548,2123131494","Image clustering has been a critical preprocessing step for vision tasks, e.g., visual concept discovery, content-based image retrieval. Conventional image clustering methods use handcraft visual descriptors as basic features via K-means, or build the graph within spectral clustering. Recently, representation learning with deep structure shows appealing performance in unsupervised feature pre-treatment. However, few studies have discussed how to deploy deep representation learning to image clustering problems, especially the unified framework which integrates both representation learning and ensemble clustering for efficient image clustering still remains void. In addition, even though it is widely recognized that with the increasing number of basic partitions, ensemble clustering gets better performance and lower variances, the best number of basic partitions for a given data set is a pending problem. In light of this, we propose the Infinite Ensemble Clustering (IEC), which incorporates the power of deep representation and ensemble clustering in a one-step framework to fuse infinite basic partitions. Generally speaking, a set of basic partitions is firstly generated from the image data, then by converting the basic partitions to the 1-of- K codings, we link the marginalized auto-encoder to the infinite ensemble clustering with i.i.d. basic partitions, which can be approached by the closed-form solutions, finally we follow the layer-wise training procedure and feed the concatenated deep features to K-means for final clustering. Extensive experiments on diverse vision data sets with different levels of visual descriptors demonstrate both the time efficiency and superior performance of IEC compared to the state-of-the-art ensemble clustering and deep clustering methods.",2016,Knowledge Discovery and Data Mining,flame clustering;k medians clustering;brown clustering;canopy clustering algorithm;dbscan;correlation clustering;constrained clustering;data stream clustering;cure data clustering algorithm;fuzzy clustering;k means clustering;clustering high dimensional data;cluster analysis;consensus clustering;biclustering;conceptual clustering;ensemble learning;data mining;pattern recognition;machine learning;statistics;computer science;mathematics;
A Truth Discovery Approach with Theoretical Guarantee,Houping Xiao (State University of New York System);Jing Gao (State University of New York System);Zhaoran Wang (Princeton University);Shiyu Wang (University of Illinois at Urbana–Champaign);Lu Su (State University of New York System);Han Liu (Princeton University);,"2303863390,2096731881,2115173548,2659842982,2148733542,2614350313","In the information age, people can easily collect information about the same set of entities from multiple sources, among which conflicts are inevitable. This leads to an important task, truth discovery , i.e., to identify true facts (truths) via iteratively updating truths and source reliability. However, the convergence to the truths is never discussed in existing work, and thus there is no theoretical guarantee in the results of these truth discovery approaches. In contrast, in this paper we propose a truth discovery approach with theoretical guarantee. We propose a randomized gaussian mixture model (RGMM) to represent multi-source data, where truths are model parameters. We incorporate source bias which captures its reliability degree into RGMM formulation. The truth discovery task is then modeled as seeking the maximum likelihood estimate (MLE) of the truths. Based on expectation-maximization (EM) techniques, we propose population-based (i.e., on the limit of infinite data) and sample-based (i.e., on a finite set of samples) solutions for the MLE. Theoretically, we prove that both solutions are contractive to an e-ball around the MLE, under certain conditions. Experimentally, we evaluate our method on both simulated and real-world datasets. Experimental results show that our method achieves high accuracy in identifying truths with convergence guarantee.",2016,Knowledge Discovery and Data Mining,mixture model;econometrics;data mining;machine learning;statistics;computer science;mathematics;
The Million Domain Challenge: Broadcast Email Prioritization by Cross-domain Recommendation,Beidou Wang (Zhejiang University);Martin Ester (Simon Fraser University);Yikang Liao (Zhejiang University);Jiajun Bu (Zhejiang University);Yu Zhu (Zhejiang University);Ziyu Guan (Northwest University);Deng Cai (Zhejiang University);,"2147723110,2067196623,2507901320,2169876372,2689214791,2163043385,2141500565","With email overload becoming a billion-level drag on the economy, personalized email prioritization is of urgent need to help predict the importance level of an email. Despite lots of previous effort on the topic, broadcast email, an important type of emails with its unique challenges and intriguing opportunities, has been overlooked. The most salient opportunity lies in that effective collaborative filtering can be exploited due to thousands of receivers of a typical broadcast email. However, every broadcast email is completely cold and it is very costly to obtain users' preference feedback. Fortunately, there exist up to million-level broadcast mailing lists in a real life email system. Similar mailing lists can provide useful extra information for broadcast email prioritization in a target mailing list. How to mine such useful extra information is a challenging problem that has never been touched. In this work, we propose the first broadcast email prioritization framework considering large numbers of mailing lists by formulating this problem as a cross domain recommendation problem. An optimization framework is proposed to select the optimal set of source domains considering multiple criteria including overlap of users, feedback pattern similarity and coverage of users. Our method is thoroughly evaluated on a real world industrial dataset from Samsung Electronics and is proved highly effective and outperforms all the baselines.",2016,Knowledge Discovery and Data Mining,html email;email address harvesting;collaborative filtering;internet privacy;world wide web;data mining;machine learning;computer science;
Hierarchical Incomplete Multi-source Feature Learning for Spatiotemporal Event Forecasting,"Liang Zhao (Virginia Tech);Jieping Ye (University of Michigan);Feng Chen (University at Albany, SUNY);Chang Tien Lu (Virginia Tech);Naren Ramakrishnan (Virginia Tech);","2619584304,2305258894,2601749234,2112878203,2199255697","Forecasting significant societal events is an interesting and challenging problem as it taking into consideration multiple aspects of a society, including its economics, politics, and culture. Traditional forecasting methods based on a single data source find it hard to cover all these aspects comprehensively, thus limiting model performance. Multi source event forecasting has proven promising but still suffers from several challenges, including 1) geographical hierarchies in multi-source data features, 2) missing values, and 3) characterization of structured feature sparsity. This paper proposes a novel feature learning model that concurrently addresses all the above challenges. Specifically, given multi-source data from different geographical levels, we design a new forecasting model by characterizing the lower-level features' dependence on higher-level features. To handle the correlations amidst structured feature sets and deal with missing values among the coupled features, we propose a novel feature learning model based on an $N$th-order strong hierarchy and fused-overlapping group Lasso. An efficient algorithm is developed to optimize model parameters and ensure global optima. Extensive experiments on 10 datasets in different domains demonstrate the effectiveness and efficiency of the proposed model.",2016,Knowledge Discovery and Data Mining,feature selection;data science;data mining;machine learning;statistics;computer science;
Continuous Experience-aware Language Model,Subhabrata Mukherjee (Max Planck Society);Stephan Günnemann (Technische Universität München);Gerhard Weikum (Max Planck Society);,"2301124665,316694267,514836396","Online review communities are dynamic as users join and leave, adopt new vocabulary, and adapt to evolving trends. Recent work has shown that recommender systems benefit from explicit consideration of user experience. However, prior work assumes a fixed number of discrete experience levels, whereas in reality users gain experience and mature continuously over time. This paper presents a new model that captures the continuous evolution of user experience, and the resulting language model in reviews and other posts. Our model is unsupervised and combines principles of Geometric Brownian Motion, Brownian Motion, and Latent Dirichlet Allocation to trace a smooth temporal progression of user experience and language model respectively. We develop practical algorithms for estimating the model parameters from data and for inference with our model (e.g., to recommend items). Extensive experiments with five real-world datasets show that our model not only fits data better than discrete-model baselines, but also outperforms state-of-the-art methods for predicting item ratings.",2016,Knowledge Discovery and Data Mining,topic model;brownian motion;user experience design;data science;data mining;machine learning;simulation;statistics;computer science;
Causal Clustering for 1-Factor Measurement Models,Erich Kummerfeld (University of Pittsburgh);Joseph Ramsey (Carnegie Mellon University);,"2512442103,2113768324","Many scientific research programs aim to learn the causal structure of real world phenomena. This learning problem is made more difficult when the target of study cannot be directly observed. One strategy commonly used by social scientists is to create measurable ``indicator'' variables that covary with the latent variables of interest. Before leveraging the indicator variables to learn about the latent variables, however, one needs a measurement model of the causal relations between the indicators and their corresponding latents. These measurement models are a special class of Bayesian networks. This paper addresses the problem of reliably inferring measurement models from measured indicators, without prior knowledge of the causal relations or the number of latent variables. We present a provably correct novel algorithm, FindOneFactorClusters (FOFC), for solving this inference problem. Compared to other state of the art algorithms, FOFC is faster, scales to larger sets of indicators, and is more reliable at small sample sizes. We also present the first correctness proofs for this problem that do not assume linearity or acyclicity among the latent variables.",2016,Knowledge Discovery and Data Mining,latent class model;latent variable;factor analysis;econometrics;data mining;machine learning;statistics;computer science;mathematics;
Days on Market: Measuring Liquidity in Real Estate Markets,Hengshu Zhu (Baidu);Hui Xiong (Rutgers–Newark);Fangshuang Tang (University of Science and Technology of China);Qi Liu (University of Science and Technology of China);Yong Ge (University of Arizona);Enhong Chen (University of Science and Technology of China);Yanjie Fu (Missouri University of Science and Technology);,"2098414524,2153710278,2223294387,2420624292,2695934969,2136372366,2168873515","Days on Market (DOM) refers to the number of days a property is on the active market, which is an important measurement of market liquidity in real estate industry. Indeed, at the micro level, DOM is not only a special concern of house sellers, but also a useful indicator for potential buyers to evaluate the popularity of a house. At the macro level, DOM is an important indicator of real estate market status. However, it is very challenging to measure DOM, since there are a variety of factors which can impact on the DOM of a property. To this end, in this paper, we aim to measure real estate liquidity by examining multiple factors in a holistic manner. A special goal is to predict the DOM of a given property listing. Specifically, we first extract key features from multiple types of heterogeneous real estate-related data, such as house profiles and geo-social information of residential communities. Then, based on these features, we develop a multi-task learning based regression approach for predicting the DOM of real estates. This approach can effectively learn district-aware models for different property listings by considering multiple factors. Finally, we conduct extensive experiments on real-world real estate data collected in Beijing and develop a prototype system for practical use. The experimental results clearly validate the effectiveness of the proposed approach for measuring liquidity in real estate markets.",2016,Knowledge Discovery and Data Mining,cost approach;price on application;multi task learning;real estate;machine learning;computer science;
User Identity Linkage by Latent User Space Modelling,Xin Mu (Nanjing University);Feida Zhu (Singapore Management University);Ee-Peng Lim (Singapore Management University);Jing Xiao;Jianzong Wang;Zhi-Hua Zhou (Nanjing University);,"2512568377,2160602068,2130308643,2514138882,2600564016,2286237009","User identity linkage across social platforms is an important problem of great research challenge and practical value. In real applications, the task often assumes an extra degree of difficulty by requiring linkage across multiple platforms. While pair-wise user linkage between two platforms, which has been the focus of most existing solutions, provides reasonably convincing linkage, the result depends by nature on the order of platform pairs in execution with no theoretical guarantee on its stability. In this paper, we explore a new concept of ``Latent User Space'' to more naturally model the relationship between the underlying real users and their observed projections onto the varied social platforms, such that the more similar the real users, the closer their profiles in the latent user space. We propose two effective algorithms, a batch model(ULink) and an online model(ULink-On), based on latent user space modelling. Two simple yet effective optimization methods are used for optimizing objective function: the first one based on the constrained concave-convex procedure(CCCP) and the second on accelerated proximal gradient. To our best knowledge, this is the first work to propose a unified framework to address the following two important aspects of the multi-platform user identity linkage problem --- (I) the platform multiplicity and (II) online data generation. We present experimental evaluations on real-world data sets for not only traditional pairwise-platform linkage but also multi-platform linkage. The results demonstrate the superiority of our proposed method over the state-of-the-art ones.",2016,Knowledge Discovery and Data Mining,user modeling;social network;world wide web;data mining;machine learning;simulation;computer science;
A Subsequence Interleaving Model for Sequential Pattern Mining,Jaroslav M. Fowkes (University of Edinburgh);Charles A. Sutton (University of Edinburgh);,"2362208703,2113665458","Recent sequential pattern mining methods have used the minimum description length (MDL) principle to define an encoding scheme which describes an algorithm for mining the most compressing patterns in a database. We present a novel subsequence interleaving model based on a probabilistic model of the sequence database, which allows us to search for the most compressing set of patterns without designing a specific encoding scheme. Our proposed algorithm is able to efficiently mine the most relevant sequential patterns and rank them using an associated measure of interestingness. The efficient inference in our model is a direct result of our use of a structural expectation-maximization framework, in which the expectation-step takes the form of a submodular optimization problem subject to a coverage constraint. We show on both synthetic and real world datasets that our model mines a set of sequential patterns with low spuriousness and redundancy, high interpretability and usefulness in real-world applications. Furthermore, we demonstrate that the quality of the patterns from our approach is comparable to, if not better than, existing state of the art sequential pattern mining algorithms.",2016,Knowledge Discovery and Data Mining,generative model;sequential pattern mining;exploratory data analysis;data mining;pattern recognition;machine learning;statistics;computer science;mathematics;
EMBERS AutoGSR: Automated Coding of Civil Unrest Events,Parang Saraf (Virginia Tech);Naren Ramakrishnan (Virginia Tech);,"2160730744,2199255697","We describe the EMBERS AutoGSR system that conducts automated coding of civil unrest events from news articles published in multiple languages. The nuts and bolts of the AutoGSR system constitute an ecosystem of filtering, ranking, and recommendation models to determine if an article reports a civil unrest event and, if so, proceed to identify and encode specific characteristics of the civil unrest event such as the when, where, who, and why of the protest. AutoGSR is a deployed system for the past 6 months continually processing data 24x7 in languages such as Spanish, Portuguese, English and encoding civil unrest events in 10 countries of Latin America: Argentina, Brazil, Chile, Colombia, Ecuador, El Salvador, Mexico, Paraguay, Uruguay, and Venezuela. We demonstrate the superiority of AutoGSR over both manual approaches and other state-of-the-art encoding systems for civil unrest.",2016,Knowledge Discovery and Data Mining,text mining;computer science;
Improving Survey Aggregation with Sparsely Represented Signals,"Tianlin Shi (Stanford University);Forest Agostinelli (University of California, Irvine);Matthew Staib (Massachusetts Institute of Technology);David P. Wipf (Microsoft);Thomas Moscibroda (Microsoft);","2326692705,2592431085,2721338023,1985957493,139673579","In this paper, we develop a new aggregation technique to reduce the cost of surveying. Our method aims to jointly estimate a vector of target quantities such as public opinion or voter intent across time and maintain good estimates when using only a fraction of the data. Inspired by the James-Stein estimator, we resolve this challenge by shrinking the estimates to a global mean which is assumed to have a sparse representation in some known basis. This assumption has lead to two different methods for estimating the global mean: orthogonal matching pursuit and deep learning. Both of which significantly reduce the number of samples needed to achieve good estimates of the true means of the data and, in the case of presidential elections, can estimate the outcome of the 2012 United States elections while saving hundreds of thousands of samples and maintaining accuracy.",2016,Knowledge Discovery and Data Mining,multi task learning;compressed sensing;james stein estimator;deep learning;econometrics;data mining;machine learning;statistics;computer science;
Anomaly Detection Using Program Control Flow Graph Mining From Execution Logs,Animesh Nandi (IBM);Atri Mandal (IBM);Shubham Atreja (Indian Institute of Technology Kanpur);Gargi Banerjee Dasgupta (IBM);Subhrajit Bhattacharya (IBM);,"2232379423,2558632365,2557992055,2720295200,2195416166","We focus on the problem of detecting anomalous run-time behavior of distributed applications from their execution logs. Specifically we mine templates and template sequences from logs to form a control flow graph (cfg) spanning distributed components. This cfg represents the baseline healthy system state and is used to flag deviations from the expected behavior of runtime logs. The novelty in our work stems from the new techniques employed to: (1) overcome the instrumentation requirements or application specific assumptions made in prior log mining approaches, (2) improve the accuracy of mined templates and the cfg in the presence of long parameters and high amount of interleaving respectively, and (3) improve by orders of magnitude the scalability of the cfg mining process in terms of volume of log data that can be processed per day. We evaluate our approach using (a) synthetic log traces and (b) multiple real-world log datasets collected at different layers of application stack. Results demonstrate that our template mining, cfg mining, and anomaly detection algorithms have high accuracy. The distributed implementation of our pipeline is highly scalable and has more than 500 GB/day of log data processing capability even on a 10 low-end VM based (Spark + Hadoop) cluster. We also demonstrate the efficacy of our end-to-end system using a case study with the Openstack VM provisioning system.",2016,Knowledge Discovery and Data Mining,spark;template;data mining;database;real time computing;computer science;
Taxi Driving Behavior Analysis in Latent Vehicle-to-Vehicle Networks: A Social Influence Perspective,Tong Xu (University of Science and Technology of China);Hengshu Zhu (Baidu);Xiangyu Zhao (University of Science and Technology of China);Qi Liu (University of Science and Technology of China);Hao Zhong (Rutgers University);Enhong Chen (University of Science and Technology of China);Hui Xiong (Rutgers University);,"2291800725,2098414524,2498438471,2420624292,2523559495,2136372366,2153710278","With recent advances in mobile and sensor technologies, a large amount of efforts have been made on developing intelligent applications for taxi drivers, which provide beneficial guide and opportunity to improve the profit and work efficiency. However, limited scopes focus on the latent social interaction within cab drivers, and corresponding social propagation scheme to share driving behaviors has been largely ignored. To that end, in this paper, we propose a comprehensive study to reveal how the social propagation affects for better prediction of cab drivers' future behaviors. To be specific, we first investigate the correlation between drivers' skills and their mutual interactions in the latent vehicle-to-vehicle network, which intuitively indicates the effects of social influences. Along this line, by leveraging the classic social influence theory , we develop a two-stage framework for quantitatively revealing the latent driving pattern propagation within taxi drivers. Comprehensive experiments on a real-word data set collected from the New York City clearly validate the effectiveness of our proposed framework on predicting future taxi driving behaviors, which also support the hypothesis that social factors indeed improve the predictability of driving behaviors.",2016,Knowledge Discovery and Data Mining,social influence;simulation;
Topic Modeling of Short Texts: A Pseudo-Document View,Yuan Zuo (Beihang University);Junjie Wu (Beihang University);Hui Zhang (Beihang University);Hao Lin (Beihang University);Fei Wang (Beihang University);Ke Xu (Beihang University);Hui Xiong (Rutgers University);,"2271787629,2149366604,2634170829,2293401935,2618217994,2600070012,2153710278","Recent years have witnessed the unprecedented growth of online social media, which empower short texts as the prevalent format for information of Internet. Given the nature of sparsity, however, short text topic modeling remains a critical yet much-watched challenge in both academy and industry. Rich research efforts have been put on building different types of probabilistic topic models for short texts, among which the self aggregation methods without using auxiliary information become an emerging solution for providing informative cross-text word co-occurrences. However, models along this line are still rarely seen, and the representative one Self-Aggregation Topic Model (SATM) is prone to overfitting and computationally expensive. In light of this, in this paper, we propose a novel probabilistic model called Pseudo-document-based Topic Model (PTM) for short text topic modeling. PTM introduces the concept of pseudo document to implicitly aggregate short texts against data sparsity. By modeling the topic distributions of latent pseudo documents rather than short texts, PTM is expected to gain excellent performance in both accuracy and efficiency. A Sparsity-enhanced PTM (SPTM for short) is also proposed by applying Spike and Slab prior, with the purpose of eliminating undesired correlations between pseudo documents and latent topics. Extensive experiments on various real-world data sets with state-of-the-art baselines demonstrate the high quality of topics learned by PTM and its robustness with reduced training samples. It is also interesting to show that i ) SPTM gains a clear edge over PTM when the number of pseudo documents is relatively small, and ii ) the constraint that a short text belongs to only one pseudo document is critically important for the success of PTM. We finally take an in-depth semantic analysis to unveil directly the fabulous function of pseudo documents in finding cross-text word co-occurrences for topic modeling.",2016,Knowledge Discovery and Data Mining,topic model;latent dirichlet allocation;data science;speech recognition;data mining;machine learning;computer science;
Analyzing Volleyball Match Data from the 2014 World Championships Using Machine Learning Techniques,Jan Van Haaren (Katholieke Universiteit Leuven);Horesh Ben Shitrit (École Polytechnique Fédérale de Lausanne);Jesse Davis (Katholieke Universiteit Leuven);Pascal Fua (École Polytechnique Fédérale de Lausanne);,"2036917247,2146000585,2144802550,2461306359","This paper proposes a relational-learning based approach for discovering strategies in volleyball matches based on optical tracking data. In contrast to most existing methods, our approach permits discovering patterns that account for both spatial (that is, partial configurations of the players on the court) and temporal (that is, the order of events and positions) aspects of the game. We analyze both the men's and women's final match from the 2014 FIVB Volleyball World Championships, and are able to identify several interesting and relevant strategies from the matches.",2016,Knowledge Discovery and Data Mining,spatial analysis;multimedia;simulation;statistics;
Dynamic Clustering of Streaming Short Documents,Shangsong Liang (University College London);Emine Yilmaz (University College London);Evangelos Kanoulas (University of Amsterdam);,"2131113258,2342836604,2627690640","Clustering technology has found numerous applications in mining textual data. It was shown to enhance the performance of retrieval systems in various different ways, such as identifying different query aspects in search result diversification, improving smoothing in the context of language modeling, matching queries with documents in a latent topic space in ad-hoc retrieval, summarizing documents etc. The vast majority of clustering methods have been developed under the assumption of a static corpus of long (and hence textually rich) documents. Little attention has been given to streaming corpora of short text, which is the predominant type of data in Web 2.0 applications, such as social media, forums, and blogs. In this paper, we consider the problem of dynamically clustering a streaming corpus of short documents. The short length of documents makes the inference of the latent topic distribution challenging, while the temporal dynamics of streams allow topic distributions to change over time. To tackle these two challenges we propose a new dynamic clustering topic model - DCT - that enables tracking the time-varying distributions of topics over documents and words over topics. DCT models temporal dynamics by a short-term or long-term dependency model over sequential data, and overcomes the difficulty of handling short text by assigning a single topic to each short document and using the distributions inferred at a certain point in time as priors for the next inference, allowing the aggregation of information. At the same time, taking a Bayesian approach allows evidence obtained from new streaming documents to change the topic distribution. Our experimental results demonstrate that the proposed clustering algorithm outperforms state-of-the-art dynamic and non-dynamic clustering topic models in terms of perplexity and when integrated in a cluster-based query likelihood model it also outperforms state-of-the-art models in terms of retrieval quality.",2016,Knowledge Discovery and Data Mining,dynamic topic model;data stream clustering;topic model;clustering high dimensional data;cluster analysis;document clustering;data science;information retrieval;data mining;machine learning;computer science;
Revisiting Random Binning Features: Fast Convergence and Strong Parallelizability,Lingfei Wu (College of William & Mary);Ian En-Hsu Yen (University of Texas at Austin);Jie Chen (IBM);Rui Yan (Baidu);,"2701551955,2162095421,2718189575,2109109241","Kernel method has been developed as one of the standard approaches for nonlinear learning, which however, does not scale to large data set due to its quadratic complexity in the number of samples. A number of kernel approximation methods have thus been proposed in the recent years, among which the random features method gains much popularity due to its simplicity and direct reduction of nonlinear problem to a linear one. Different random feature functions have since been proposed to approximate a variety of kernel functions. Among them the Random Binning (RB) feature, proposed in the first random-feature paper [21], has drawn much less attention than the Random Fourier (RF) feature proposed also in [21]. In this work, we observe that the RB features, with right choice of optimization solver, could be orders-of-magnitude more efficient than other random features and kernel approximation methods under the same requirement of accuracy. We thus propose the first analysis of RB from the perspective of optimization, which by interpreting RB as a Randomized Block Coordinate Descent in the infinite-dimensional space, gives a faster convergence rate compared to that of other random features. In particular, we show that by drawing R random grids with at least κ number of non-empty bins per grid in expectation, RB method achieves a convergence rate of O (1/κ R )), which not only sharpens its O (1/√ R ) rate from Monte Carlo analysis, but also shows a κ times speedup over other random features under the same analysis framework. In addition, we demonstrate another advantage of RB in the L1-regularized setting, where unlike other random features, a RB-based Coordinate Descent solver can be parallelized with guaranteed speedup proportional to κ. Our extensive experiments demonstrate the superior performance of the RB features over other random features and kernel approximation methods.",2016,Knowledge Discovery and Data Mining,random function;machine learning;mathematical optimization;statistics;mathematics;
"Dynamics of Large Multi-View Social Networks: Synergy, Cannibalization and Cross-View Interplay",Yu Shi (University of Illinois at Urbana–Champaign);Myunghwan Kim (LinkedIn);Shaunak Chatterjee (LinkedIn);Mitul Tiwari (LinkedIn);Souvik Ghosh (LinkedIn);Rómer Rosales (LinkedIn);,"2664581530,2140237911,2198941248,2296043577,2097018069,2117656073","Most social networking services support multiple types of relationships between users, such as getting connected, sending messages, and consuming feed updates. These users and relationships can be naturally represented as a dynamic multi-view network, which is a set of weighted graphs with shared common nodes but having their own respective edges. Different network views, representing structural relationship and interaction types, could have very distinctive properties individually and these properties may change due to interplay across views. Therefore, it is of interest to study how multiple views interact and affect network dynamics and, in addition, explore possible applications to social networking. In this paper, we propose approaches to capture and analyze multi-view network dynamics from various aspects. Through our proposed descriptors, we observe the synergy and cannibalization between different user groups and network views from LinkedIn dataset. We then develop models that consider the synergy and cannibalization per new relationship, and show the outperforming predictive capability of our models compared to baseline models. Finally, the proposed models allow us to understand the interplay among different views where they dynamically change over time.",2016,Knowledge Discovery and Data Mining,network dynamics;social network;data mining;simulation;
Optimally Discriminative Choice Sets in Discrete Choice Models: Application to Data-Driven Test Design,Igor Labutov (Cornell University);Frans Schalekamp (Cornell University);Kelvin Luu (University of Washington);Hod Lipson (Columbia University);Christoph Studer (Cornell University);,"1028054495,1583385990,2395179958,2510944775,2153121925","Difficult multiple-choice (MC) questions can be made easy by providing a set of answer options of which most are obviously wrong. In the education literature, a plethora of instructional guides exist for crafting a suitable set of wrong choices (distractors) that enable the assessment of the students' understanding. The art of MC question design thus hinges on the question-maker's experience and knowledge of the potential misconceptions. In contrast, we advocate a data-driven approach, where correct and incorrect options are assembled directly from the students' own past submissions. Large-scale online classroom settings, such as massively open online courses (MOOCs), provide an opportunity to design optimal and adaptive multiple-choice questions that are maximally informative about the students' level of understanding of the material. In this work, we (i) develop a multinomial-logit discrete choice model for the setting of MC testing, (ii) derive an optimization objective for selecting optimally discriminative option sets, (iii) propose an algorithm for finding a globally-optimal solution, and (iv) demonstrate the effectiveness of our approach via synthetic experiments and a user study. We finally showcase an application of our approach to crowd-sourcing tests from technical online forums.",2016,Knowledge Discovery and Data Mining,crowdsourcing;educational assessment;adaptive learning;multimedia;data mining;machine learning;simulation;statistics;computer science;
Communication Efficient Distributed Kernel Principal Component Analysis,Maria-Florina Balcan (Carnegie Mellon University);Yingyu Liang (Princeton University);Le Song (Georgia Institute of Technology);David P. Woodruff (IBM);Bo Xie 0002 (Georgia Institute of Technology);,"2169342471,2124668569,2113868374,2142501412,2102785714","Kernel Principal Component Analysis (KPCA) is a key machine learning algorithm for extracting nonlinear features from data. In the presence of a large volume of high dimensional data collected in a distributed fashion, it becomes very costly to communicate all of this data to a single data center and then perform kernel PCA. Can we perform kernel PCA on the entire dataset in a distributed and communication efficient fashion while maintaining provable and strong guarantees in solution quality? In this paper, we give an affirmative answer to the question by developing a communication efficient algorithm to perform kernel PCA in the distributed setting. The algorithm is a clever combination of subspace embedding and adaptive sampling techniques, and we show that the algorithm can take as input an arbitrary configuration of distributed datasets, and compute a set of global kernel principal components with relative error guarantees independent of the dimension of the feature space or the total number of data points. In particular, computing k principal components with relative error e over s workers has communication cost O( spk /e+ sk 2 /e 3 ) words, where p is the average number of nonzero entries in each data point. Furthermore, we experimented the algorithm with large-scale real world datasets and showed that the algorithm produces a high quality kernel PCA solution while using significantly less communication than alternative approaches.",2016,Knowledge Discovery and Data Mining,kernel embedding of distributions;tree kernel;variable kernel density estimation;polynomial kernel;string kernel;radial basis function kernel;kernel principal component analysis;principal component regression;kernel method;principal component analysis;data mining;pattern recognition;machine learning;statistics;computer science;mathematics;
The Legislative Influence Detector: Finding Text Reuse in State Legislation,"Matthew Burgess (University of Michigan);Eugenia Giraudy (University of California, Berkeley);Julian Katz-Samuels (University of Michigan);Joe Walsh (University of Chicago);Derek Willis;Lauren Haynes (University of Chicago);Rayid Ghani (University of Chicago);","2277759540,2505760661,2715831950,2680879499,2656276355,2281046338,2655769349","State legislatures introduce at least 45,000 bills each year. However, we lack a clear understanding of who is actually writing those bills. As legislators often lack the time and staff to draft each bill, they frequently copy text written by other states or interest groups. However, existing approaches to detect text reuse are slow, biased, and incomplete. Journalists or researchers who want to know where a particular bill originated must perform a largely manual search. Watchdog organizations even hire armies of volunteers to monitor legislation for matches. Given the time-consuming nature of the analysis, journalists and researchers tend to limit their analysis to a subset of topics (e.g. abortion or gun control) or a few interest groups. This paper presents the Legislative Influence Detector (LID). LID uses the Smith-Waterman local alignment algorithm to detect sequences of text that occur in model legislation and state bills. As it is computationally too expensive to run this algorithm on a large corpus of data, we use a search engine built using Elasticsearch to limit the number of comparisons. We show how system has found 45,405 instances of bill-to-bill text reuse and 14,137 instances of model-legislation-to-bill text reuse. System reduces the time it takes to manually find text reuse from days to seconds.",2016,Knowledge Discovery and Data Mining,common good;data mining;computer science;
Catch Me If You Can: Detecting Pickpocket Suspects from Large-Scale Transit Records,Bowen Du (Beihang University);Chuanren Liu (Drexel University);Wenjun Zhou (University of Tennessee);Zhenshan Hou (Beihang University);Hui Xiong (Rutgers–Newark);,"2697129405,2169554947,2097769770,2668497643,2153710278","Massive data collected by automated fare collection (AFC) systems provide opportunities for studying both personal traveling behaviors and collective mobility patterns in the urban area. Existing studies on the AFC data have primarily focused on identifying passengers' movement patterns. In this paper, however, we creatively leveraged such data for identifying thieves in the public transit systems. Indeed, stopping pickpockets in the public transit systems has been critical for improving passenger satisfaction and public safety. However, it is challenging to tell thieves from regular passengers in practice. To this end, we developed a suspect detection and surveillance system, which can identify pick-pocket suspects based on their daily transit records. Specifically, we first extracted a number of features from each passenger's daily activities in the transit systems. Then, we took a two-step approach that exploits the strengths of unsupervised outlier detection and supervised classification models to identify thieves, who exhibit abnormal traveling behaviors. Experimental results demonstrated the effective- ness of our method. We also developed a prototype system with a user-friendly interface for the security personnel.",2016,Knowledge Discovery and Data Mining,anomaly detection;computer security;machine learning;simulation;computer science;
Identifying Police Officers at Risk of Adverse Events,Samuel Carton (University of Michigan);Jennifer Helsby (University of Chicago);Kenneth Joseph (Carnegie Mellon University);Ayesha Mahmud (Princeton University);Youngsoo Park (University of Arizona);Joe Walsh (University of Chicago);Crystal Cody;Cpt Estella Patterson;Lauren Haynes (University of Chicago);Rayid Ghani (University of Chicago);,"2224892635,2052523325,2107547116,2481417253,2443439548,2680879499,2674174511,2511718359,2281046338,2655769349","Adverse events between police and the public, such as deadly shootings or instances of racial profiling, can cause serious or deadly harm, damage police legitimacy, and result in costly litigation. Evidence suggests these events can be prevented by targeting interventions based on an Early Intervention System (EIS) that flags police officers who are at a high risk for involvement in such adverse events. Today's EIS are not data-driven and typically rely on simple thresholds based entirely on expert intuition. In this paper, we describe our work with the Charlotte-Mecklenburg Police Department (CMPD) to develop a machine learning model to predict which officers are at risk for an adverse event. Our approach significantly outperforms CMPD's existing EIS, increasing true positives by ~12% and decreasing false positives by ~32%. Our work also sheds light on features related to officer characteristics, situational factors, and neighborhood factors that are predictive of adverse events. This work provides a starting point for police departments to take a comprehensive, data-driven approach to improve policing and reduce harm to both officers and members of the public.",2016,Knowledge Discovery and Data Mining,computer security;computer science;
Lifelong Machine Learning and Computer Reading the Web,Zhiyuan Chen (Google);Estevam R. Hruschka (Federal University of São Carlos);Bing Liu (University of Illinois at Chicago);,"2143860226,2137503162,2244698799","This tutorial introduces Lifelong Machine Learning (LML) and Machine Reading. The core idea of LML is to learn continuously and accumulate the learned knowledge, and to use the knowledge to help future learning, which is perhaps the hallmark of human learning and human intelligence. By us- ing prior knowledge seamlessly and effortlessly, we humans can learn without a lot of training data, but current machine learning algorithms tend to need a huge amount of training data. LML aims to mimic this human capability. Machine Reading is a research area with the goal of building systems to read natural language text. Among different approaches employed in Machine Reading, this tutorial focuses on projects and approaches that use the idea of LML. Most current machine learning (ML) algorithms learn in isolation. They are designed to address a specific problem using a single dataset. That is, given a dataset, an ML algorithm is executed on the dataset to build a model. Although this type of isolated learning is very useful, it does not have the ability to accumulate past knowledge and to make use of the knowledge for future learning, which we believe are critical for the future of machine learning and data mining. LML aims to design and develop computational systems and algorithms with this capability, i.e., to learn as humans do in a lifelong manner. In this tutorial, we introduce this important problem and the existing LML techniques and discuss opportunities and challenges of big data for lifelong machine learning. We also want to motivate researchers and practitioners to actively explore LML as the big data provides us a golden opportunity to learn a large volume of diverse knowledge, to connect different pieces of it, and to use it to raise data mining and machine learning to a new level.",2016,Knowledge Discovery and Data Mining,inductive transfer;multi task learning;transfer of learning;robot learning;active learning;error driven learning;active learning;algorithmic learning theory;instance based learning;data science;data mining;artificial intelligence;machine learning;computer science;
Large-Scale Item Categorization in e-Commerce Using Multiple Recurrent Neural Networks,Jung-Woo Ha;Hyuna Pyo;Jeonghee Kim;,"2633226502,2417879881,2629591549","Precise item categorization is a key issue in e-commerce domains. However, it still remains a challenging problem due to data size, category skewness, and noisy metadata. Here, we demonstrate a successful report on a deep learning-based item categorization method, i.e., deep categorization network (DeepCN), in an e-commerce website. DeepCN is an end-to-end model using multiple recurrent neural networks (RNNs) dedicated to metadata attributes for generating features from text metadata and fully connected layers for classifying item categories from the generated features. The categorization errors are propagated back through the fully connected layers to the RNNs for weight update in the learning process. This deep learning-based approach allows diverse attributes to be integrated into a common representation, thus overcoming sparsity and scalability problems. We evaluate DeepCN on large-scale real-world data including more than 94 million items with approximately 4,100 leaf categories from a Korean e-commerce website. Experiment results show our method improves the categorization accuracy compared to the model using single RNN as well as a standard classification model using unigram-based bag-of-words. Furthermore, we investigate how much the model parameters and the used attributes influence categorization performances.",2016,Knowledge Discovery and Data Mining,boosting methods for object categorization;recurrent neural network;categorization;deep learning;data mining;pattern recognition;machine learning;computer science;
Targeted Topic Modeling for Focused Analysis,Shuai Wang (University of Illinois at Chicago);Zhiyuan Chen (University of Illinois at Chicago);Geli Fei (University of Illinois at Chicago);Bing Liu (University of Illinois at Chicago);Sherry Emery (University of Illinois at Chicago);,"2441217776,2143860226,2133167487,2244698799,2305915050","One of the overarching tasks of document analysis is to find what topics people talk about. One of the main techniques for this purpose is topic modeling. So far many models have been proposed. However, the existing models typically perform full analysis on the whole data to find all topics. This is certainly useful, but in practice we found that the user almost always also wants to perform more detailed analyses on some specific aspects, which we refer to as targets (or targeted aspects). Current full-analysis models are not suitable for such analyses as their generated topics are often too coarse and may not even be on target. For example, given a set of tweets about e-cigarette, one may want to find out what topics under discussion are specifically related to children. Likewise, given a collection of online reviews about a camera, a consumer or camera manufacturer may be interested in finding out all topics about the camera's screen, the targeted aspect. As we will see in our experiments, current full topic models are ineffective for such targeted analyses. This paper studies this problem and proposes a novel targeted topic model (TTM) to enable focused analyses on any specific aspect of interest. Our experimental results demonstrate the effectiveness of the TTM.",2016,Knowledge Discovery and Data Mining,topic model;data science;multimedia;bioinformatics;machine learning;computer science;
Accelerating Online CP Decompositions for Higher Order Tensors,"Shuo Zhou (University of Melbourne);Nguyen Xuan Vinh (University of Melbourne);James Bailey (University of Melbourne);Yunzhe Jia (University of Melbourne);Ian Davidson (University of California, Davis);","2478464571,1982975784,2131557737,2538254647,2560595684","Tensors are a natural representation for multidimensional data. In recent years, CANDECOMP/PARAFAC (CP) decomposition, one of the most popular tools for analyzing multi-way data, has been extensively studied and widely applied. However, today's datasets are often dynamically changing over time. Tracking the CP decomposition for such dynamic tensors is a crucial but challenging task, due to the large scale of the tensor and the velocity of new data arriving. Traditional techniques, such as Alternating Least Squares (ALS), cannot be directly applied to this problem because of their poor scalability in terms of time and memory. Additionally, existing online approaches have only partially addressed this problem and can only be deployed on third-order tensors. To fill this gap, we propose an efficient online algorithm that can incrementally track the CP decompositions of dynamic tensors with an arbitrary number of dimensions. In terms of effectiveness, our algorithm demonstrates comparable results with the most accurate algorithm, ALS, whilst being computationally much more efficient. Specifically, on small and moderate datasets, our approach is tens to hundreds of times faster than ALS, while for large-scale datasets, the speedup can be more than 3,000 times. Compared to other state-of-the-art online approaches, our method shows not only significantly better decomposition quality, but also better performance in terms of stability, efficiency and scalability.",2016,Knowledge Discovery and Data Mining,theoretical computer science;machine learning;mathematical optimization;computer science;
Collaborative Multi-View Denoising,Lei Zhang (Chinese Academy of Sciences);Shupeng Wang (Chinese Academy of Sciences);Xiaoyu Zhang (Chinese Academy of Sciences);Yong Wang (Chinese Academy of Sciences);Binbin Li (Chinese Academy of Sciences);Dinggang Shen (University of North Carolina at Chapel Hill);Shuiwang Ji (Washington State University);,"2719765012,2164944214,2147062167,2708288867,2711491591,2150708589,2149659377","In multi-view learning applications, like multimedia analysis and information retrieval, we often encounter the corrupted view problem in which the data are corrupted by two different types of noises, i.e., the intra- and inter-view noises. The noises may affect these applications that commonly acquire complementary representations from different views. Therefore, how to denoise corrupted views from multi-view data is of great importance for applications that integrate and analyze representations from different views. However, the heterogeneity among multi-view representations brings a significant challenge on denoising corrupted views. To address this challenge, we propose a general framework to jointly denoise corrupted views in this paper. Specifically, aiming at capturing the semantic complementarity and distributional similarity among different views, a novel Heterogeneous Linear Metric Learning (HLML) model with low-rank regularization, leave-one-out validation, and pseudo-metric constraints is proposed. Our method linearly maps multi-view data to a high-dimensional feature-homogeneous space that embeds the complementary information from different views. Furthermore, to remove the intra- and inter-view noises, we present a new Multi-view Semi-supervised Collaborative Denoising (MSCD) method with elementary transformation constraints and gradient energy competition to establish the complementary relationship among the heterogeneous representations. Experimental results demonstrate that our proposed methods are effective and efficient.",2016,Knowledge Discovery and Data Mining,heterogeneity;noise reduction;computer vision;data mining;machine learning;computer science;mathematics;
Compressing Graphs and Indexes with Recursive Graph Bisection,Laxman Dhulipala (Carnegie Mellon University);Igor Kabiljo (Facebook);Brian Karrer (Facebook);Giuseppe Ottaviano (Facebook);Sergey Pupyrev (Facebook);Alon Shalita (Facebook);,"2227580508,2292206520,2633680482,2153528897,2515314286,2345469155","Graph reordering is a powerful technique to increase the locality of the representations of graphs, which can be helpful in several applications. We study how the technique can be used to improve compression of graphs and inverted indexes. We extend the recent theoretical model of Chierichetti et al. (KDD 2009) for graph compression, and show how it can be employed for compression-friendly reordering of social networks and web graphs and for assigning document identifiers in inverted indexes. We design and implement a novel theoretically sound reordering algorithm that is based on recursive graph bisection. Our experiments show a significant improvement of the compression rate of graph and indexes over existing heuristics. The new method is relatively simple and allows efficient parallel and distributed implementations, which is demonstrated on graphs with billions of vertices and hundreds of billions of edges.",2016,Knowledge Discovery and Data Mining,implicit graph;voltage graph;indifference graph;complement graph;graph bandwidth;1 planar graph;graph operations;comparability graph;block graph;graph product;clique width;modular decomposition;pathwidth;chordal graph;graph;compression;social network;distributed algorithm;approximation algorithm;theoretical computer science;combinatorics;data mining;machine learning;mathematical optimization;algorithm;computer science;
Scalable Betweenness Centrality Maximization via Sampling,Ahmad Mahmoody (Brown University);Charalampos E. Tsourakakis (Harvard University);Eli Upfal (Brown University);,"2090690368,750472553,2685185700","Betweenness centrality (BWC) is a fundamental centrality measure in social network analysis. Given a large-scale network, how can we find the most central nodes? This question is of great importance to many key applications that rely on BWC, including community detection and understanding graph vulnerability. Despite the large amount of work on scalable approximation algorithm design for BWC, estimating BWC on large-scale networks remains a computational challenge. In this paper, we study the Centrality Maximization problem (CMP): given a graph G = ( V,E ) and a positive integer k , find a set S * ⊆ V that maximizes BWC subject to the cardinality constraint |S*| ≤ k . We present an efficient randomized algorithm that provides a (1 -- 1/ e -- e)-approximation with high probability, where e > 0. Our results improve the current state-of-the-art result [40]. Furthermore, we provide the first theoretical evidence for the validity of a crucial assumption in betweenness centrality estimation, namely that in real-world networks O(| V | 2 ) shortest paths pass through the top-k central nodes, where k is a constant. This also explains why our algorithm runs in near linear time on real-world networks. We also show that our algorithm and analysis can be applied to a wider range of centrality measures, by providing a general analytical framework. On the experimental side, we perform an extensive experimental analysis of our method on real-world networks, demonstrate its accuracy and scalability, and study different properties of central nodes. Then, we compare the sampling method used by the state-of-the-art algorithm with our method. Furthermore, we perform a study of BWC in time evolving networks, and see how the centrality of the central nodes in the graphs changes over time. Finally, we compare the performance of the stochastic Kronecker model [28] to real data, and observe that it generates a similar growth pattern.",2016,Knowledge Discovery and Data Mining,alpha centrality;network controllability;centrality;network science;betweenness centrality;network theory;social network;sampling;theoretical computer science;combinatorics;social science;machine learning;mathematics;
A Non-parametric Approach to Detect Epileptogenic Lesions using Restricted Boltzmann Machines,Yijun Zhao (Tufts University);Bilal Ahmed (Tufts University);Thomas Thesen (New York University);Karen E. Blackmon (New York University);Jennifer G. Dy (Northeastern University);Carla E. Brodley (Northeastern University);Ruben Kuzniekcy (New York University);Orrin Devinsky (New York University);,"2135745075,2470180175,1973210777,2641482471,2239241780,1994240001,2517002241,277843997","Visual detection of lesional areas on a cortical surface is critical in rendering a successful surgical operation for Treatment Resistant Epilepsy (TRE) patients. Unfortunately, 45% of Focal Cortical Dysplasia (FCD, the most common kind of TRE) patients have no visual abnormalities in their brains' 3D-MRI images. We collaborate with doctors from NYU Langone's Comprehensive Epilepsy Center and apply machine learning methodologies to identify the resective zones for these { MRI-negative } FCD patients. Our task is particularly challenging because MRI images can only provide a limited number of features. Furthermore, data from different patients often exhibit inter-patient variabilities due to age, gender, left/right handedness, etc. In this paper, we introduce a new approach which combines the restricted Boltzmann machines and a Bayesian non-parametric mixture model to address these issues. We demonstrate the efficacy of our model by applying it to a retrospective dataset of MRI-negative FCD patients who are seizure free after surgery.",2016,Knowledge Discovery and Data Mining,restricted boltzmann machine;mixture model;artificial intelligence;machine learning;simulation;computer science;
Engagement Capacity and Engaging Team Formation for Reach Maximization of Online Social Media Platforms,Alexander G. Nikolaev (University at Buffalo);Shounak Gore (University at Buffalo);Venu Govindaraju (University at Buffalo);,"2152231056,2510115460,2102146905","The challenges of assessing the ""health"" of online social media platforms and strategically growing them are recognized by many practitioners and researchers. For those platforms that primarily rely on user-generated content, the reach -- the degree of participation referring to the percentage and involvement of users -- is a key indicator of success. This paper lays a theoretical foundation for measuring engagement as a driver of reach that achieves growth via positive externality effects. The paper takes a game theoretic approach to quantifying engagement, viewing a platform's social capital as a cooperatively created value and finding a fair distribution of this value among the contributors. It introduces engagement capacity , a measure of the ability of users and user groups to engage peers, and formulates the Engaging Team Formation Problem (EngTFP) to identify the sets of users that ""make a platform go"". We show how engagement capacity can be useful in characterizing forum user behavior and in the reach maximization efforts. We also stress how engagement analysis differs from influence measurement. Computational investigations with Twitter and Health Forum data reveal the properties of engagement capacity and the utility of EngTFP.",2016,Knowledge Discovery and Data Mining,social network;simulation;
Online Feature Selection: A Limited-Memory Substitution Algorithm and Its Asynchronous Parallel Variation,Haichuan Yang (University of Rochester);Ryohei Fujimaki (NEC);Yukitaka Kusumura (NEC);Ji Liu (University of Rochester);,"2113438510,1124837368,2304482293,2301968909","This paper considers the feature selection scenario where only a few features are accessible at any time point. For example, features are generated sequentially and visible one by one. Therefore, one has to make an online decision to identify key features after all features are only scanned once or twice. The optimization based approach is a powerful tool for the online feature selection. However, most existing optimization based algorithms explicitly or implicitly adopt L 1 norm regularization to identify important features, and suffer two main disadvantages: 1) the penalty term for L 1 norm term is hard to choose; and 2) the memory usage is hard to control or predict. To overcome these two drawbacks, this paper proposes a limited-memory and model parameter free online feature selection algorithm, namely online substitution (OS) algorithm. To improve the selection efficiency, an asynchronous parallel extension for OS (Asy-OS) is proposed. Convergence guarantees are provided for both algorithms. Empirical study suggests that the performance of OS and Asy-OS is comparable to the benchmark algorithm Grafting, but requires much less memory cost and can be easily extended to the parallel implementation.",2016,Knowledge Discovery and Data Mining,feature selection;data mining;pattern recognition;machine learning;computer science;
Parallel Lasso Screening for Big Data Optimization,Qingyang Li (Arizona State University);Shuang Qiu (University of Michigan);Shuiwang Ji (Washington State University);Paul M. Thompson (University of Southern California);Jieping Ye (University of Michigan);Jie Wang (University of Michigan);,"2222994316,2660622005,2149659377,2527463554,2305258894,2471893859","Lasso regression is a widely used technique in data mining for model selection and feature extraction. In many applications, it remains challenging to apply the regression model to large-scale problems that have massive data samples with high-dimensional features. One popular and promising strategy is to solve the Lasso problem in parallel. Parallel solvers run multiple cores in parallel on a shared memory system to speedup the computation, while the practical usage is limited by the huge dimension in the feature space. Screening is a promising method to solve the problem of high dimensionality by discarding the inactive features and removing them from optimization. However, when integrating screening methods with parallel solvers, most of solvers cannot guarantee the convergence on the reduced feature matrix. In this paper, we propose a novel parallel framework by parallelizing screening methods and integrating it with our proposed parallel solver. We propose two parallel screening algorithms: Parallel Strong Rule (PSR) and Parallel Dual Polytope Projection (PDPP). For the parallel solver, we proposed an Asynchronous Grouped Coordinate Descent method (AGCD) to optimize the regression problem in parallel on the reduced feature matrix. AGCD is based on a grouped selection strategy to select the coordinate that has the maximum descent for the objective function in a group of candidates. Empirical studies on the real-world datasets demonstrate that the proposed parallel framework has a superior performance compared to the state-of-the-art parallel solvers.",2016,Knowledge Discovery and Data Mining,coordinate descent;theoretical computer science;machine learning;mathematical optimization;computer science;
"Collective Sensemaking via Social Sensors: Extracting, Profiling, Analyzing, and Predicting Real-world Events",Yuheng Hu (University of Illinois at Chicago);Yu-Ru Lin (University of Pittsburgh);Jiebo Luo (University of Rochester);,"2655196453,2155397203,2059910451","Social media platforms like Twitter and Facebook have emerged as some of the most important platforms for people to discover, report, share, and communicate with others about various public events, be they of global or local interest (some high profile examples include the U.S Presidential debates, the Boston bombings, the hurricane Sandy, etc). The burst of social media reaction can be seen as a valuable real-time reflection of events as they happen, and can be used for a variety of applications such as computational journalism. Until now, such analysis has been mostly done manually or through primitive tools. Scalable and automated approaches are needed given the massive amounts of both event and reaction information. These approaches must also be able to conduct in-depth analysis of complex interactions between an event and its audience. Supporting such automation and examination however poses several computational challenges. In recent years, research communities have witnessed a growing interest in tackling these challenges. Furthermore, much recent research has begun to focus on solving more complex event analytics tasks such as post-event effect quantification and event progress prediction. This tutorial aims to review and examine current state of the research progress on this emerging topic.",2016,Knowledge Discovery and Data Mining,complex event processing;social media;data science;data mining;simulation;computer science;
Convolutional Neural Networks for Steady Flow Approximation,Xiaoxiao Guo (University of Michigan);Wei Li (Autodesk);Francesco Iorio (Autodesk);,"2682870269,2634777495,2104335678","In aerodynamics related design, analysis and optimization problems, flow fields are simulated using computational fluid dynamics (CFD) solvers. However, CFD simulation is usually a computationally expensive, memory demanding and time consuming iterative process. These drawbacks of CFD limit opportunities for design space exploration and forbid interactive design. We propose a general and flexible approximation model for real-time prediction of non-uniform steady laminar flow in a 2D or 3D domain based on convolutional neural networks (CNNs). We explored alternatives for the geometry representation and the network architecture of CNNs. We show that convolutional neural networks can estimate the velocity field two orders of magnitude faster than a GPU-accelerated CFD solver and four orders of magnitude faster than a CPU-based CFD solver at a cost of a low error rate. This approach can provide immediate feedback for real-time design iterations at the early stage of design. Compared with existing approximation models in the aerodynamics domain, CNNs enable an efficient estimation for the entire velocity field. Furthermore, designers and engineers can directly apply the CNN approximation model in their design space exploration algorithms without training extra lower-dimensional surrogate models.",2016,Knowledge Discovery and Data Mining,computational fluid dynamics;convolutional neural network;theoretical computer science;machine learning;simulation;computer science;
Annealed Sparsity via Adaptive and Dynamic Shrinking,Kai Zhang (NEC);Shandian Zhe (Purdue University);Chaoran Cheng (New Jersey Institute of Technology);Zhi Wei (New Jersey Institute of Technology);Zhengzhang Chen (NEC);Haifeng Chen (NEC);Guofei Jiang (NEC);Yuan Qi (Purdue University);Jieping Ye (University of Michigan);,"2600447970,1995322055,2363752216,2126219825,2132666618,2571838709,2168090285,2115766665,2305258894","Sparse learning has received tremendous amount of interest in high-dimensional data analysis due to its model interpretability and the low-computational cost. Among the various techniques, adaptive l1-regularization is an effective framework to improve the convergence behaviour of the LASSO, by using varying strength of regularization across different features. In the meantime, the adaptive structure makes it very powerful in modelling grouped sparsity patterns as well, being particularly useful in high-dimensional multi-task problems. However, choosing an appropriate, global regularization weight is still an open problem. In this paper, inspired by the annealing technique in material science, we propose to achieve ""annealed sparsity"" by designing a dynamic shrinking scheme that simultaneously optimizes the regularization weights and model coefficients in sparse (multi-task) learning. The dynamic structures of our algorithm are twofold. Feature-wise (spatially), the regularization weights are updated interactively with model coefficients, allowing us to improve the global regularization structure. Iteration-wise (temporally), such interaction is coupled with gradually boosted l1-regularization by adjusting an equality norm-constraint, achieving an annealing effect to further improve model selection. This renders interesting shrinking behaviour in the whole solution path. Our method competes favorably with state-of-the-art methods in sparse (multi-task) learning. We also apply it in expression quantitative trait loci analysis (eQTL), which gives useful biological insights in human cancer (melanoma) study.",2016,Knowledge Discovery and Data Mining,multi task learning;lasso;compressed sensing;annealing;feature selection;pattern recognition;machine learning;mathematical optimization;statistics;computer science;mathematics;
FINAL: Fast Attributed Network Alignment,Si Zhang (Arizona State University);Hanghang Tong (Arizona State University);,"2515619255,2667261544","Multiple networks naturally appear in numerous high-impact applications. Network alignment (i.e., finding the node correspondence across different networks) is often the very first step for many data mining tasks. Most, if not all, of the existing alignment methods are solely based on the topology of the underlying networks. Nonetheless, many real networks often have rich attribute information on nodes and/or edges. In this paper, we propose a family of algorithms FINAL to align attributed networks. The key idea is to leverage the node/edge attribute information to guide (topology-based) alignment process. We formulate this problem from an optimization perspective based on the alignment consistency principle, and develop effective and scalable algorithms to solve it. Our experiments on real networks show that (1) by leveraging the attribute information, our algorithms can significantly improve the alignment accuracy (i.e., up to a 30% improvement over the existing methods); (2) compared with the exact solution, our proposed fast alignment algorithm leads to a more than 10 times speed-up, while preserving a 95% accuracy; and (3) our on-query alignment method scales linearly, with an around 90% ranking accuracy compared with our exact full alignment method and a near real-time response time.",2016,Knowledge Discovery and Data Mining,theoretical computer science;data mining;machine learning;computer science;
Semi-Markov Switching Vector Autoregressive Model-Based Anomaly Detection in Aviation Systems,Igor Melnyk (University of Minnesota);Arindam Banerjee (University of Minnesota);Bryan L. Matthews (Ames Research Center);Nikunj C. Oza (Ames Research Center);,"2132303832,2037585042,2149808489,2047114418","In this work we consider the problem of anomaly detection in heterogeneous, multivariate, variable-length time series datasets. Our focus is on the aviation safety domain, where data objects are flights and time series are sensor readings and pilot switches. In this context the goal is to detect anomalous flight segments, due to mechanical, environmental, or human factors in order to identifying operationally significant events and highlight potential safety risks. For this purpose, we propose a framework which represents each flight using a semi-Markov switching vector autoregressive (SMS-VAR) model. Detection of anomalies is then based on measuring dissimilarities between the model's prediction and data observation. The framework is scalable, due to the inherent parallel nature of most computations, and can be used to perform online anomaly detection. Extensive experimental results on simulated and real datasets illustrate that the framework can detect various types of anomalies along with the key parameters involved.",2016,Knowledge Discovery and Data Mining,graphical model;time series;anomaly detection;data mining;machine learning;simulation;computer science;
Crime Rate Inference with Big Data,Hongjian Wang (Pennsylvania State University);Daniel Kifer (Pennsylvania State University);Corina Graif (Pennsylvania State University);Zhenhui Li (Pennsylvania State University);,"2669795438,2049563562,341083243,2098136913","Crime is one of the most important social problems in the country, affecting public safety, children development, and adult socioeconomic status. Understanding what factors cause higher crime is critical for policy makers in their efforts to reduce crime and increase citizens' life quality. We tackle a fundamental problem in our paper: crime rate inference at the neighborhood level. Traditional approaches have used demographics and geographical influences to estimate crime rates in a region. With the fast development of positioning technology and prevalence of mobile devices, a large amount of modern urban data have been collected and such big data can provide new perspectives for understanding crime. In this paper, we used large-scale Point-Of-Interest data and taxi flow data in the city of Chicago, IL in the USA. We observed significantly improved performance in crime rate inference compared to using traditional features. Such an improvement is consistent over multiple years. We also show that these new features are significant in the feature importance analysis.",2016,Knowledge Discovery and Data Mining,big data;data science;computer security;data mining;computer science;
Predict Risk of Relapse for Patients with Multiple Stages of Treatment of Depression,Zhi Nie (Arizona State University);Pinghua Gong (University of Michigan);Jieping Ye (University of Michigan);,"2703094151,2323553286,2305258894","Depression is a serious mood disorder afflicting millions of people around the globe. Medications of different types and with different effects on neural activity have been developed for its treatments during the past few decades. Due to the heterogeneity of the disorder, many patients cannot achieve symptomatic remission from a single clinical trial. Instead they need multiple clinical trials to achieve remission, resulting in a multiple stage treatment pattern. Furthermore those who indeed achieve symptom remission are still faced with substantial risk of relapse. One promising approach to predicting the risk of relapse is censored regression. Traditional censored regression typically applies only to situations in which the exact time of event of interest is known. However, follow-up studies that track the patients' relapse status can only provide an interval of time during which relapse occurs. The exact time of relapse is usually unknown. In this paper, we present a censored regression approach with a truncated $l_1$ loss function that can handle the uncertainty of relapse time. Based on this general loss function, we develop a gradient boosting algorithm and a stochastic dual coordinate ascent algorithm when the hypothesis in the loss function is represented as (1) an ensemble of decision trees and (2) a linear combination of covariates, respectively. As an extension of our linear model, a multi-stage linear approach is further proposed to harness the data collected from multiple stages of treatment. We evaluate the proposed algorithms using a real-world clinical trial dataset. Results show that our methods outperform the well-known Cox proportional hazard model. In addition, the risk factors identified by our multi-stage linear model not only corroborate findings from recent research but also yield some new insights into how to develop effective measures for prevention of relapse among patients after their initial remission from the acute treatment stage.",2016,Knowledge Discovery and Data Mining,censored regression model;survival analysis;econometrics;statistics;
Portfolio Selections in P2P Lending: A Multi-Objective Perspective,Hongke Zhao (University of Science and Technology of China);Qi Liu (University of Science and Technology of China);Guifeng Wang (University of Science and Technology of China);Yong Ge (University of Arizona);Enhong Chen (University of Science and Technology of China);,"2223484430,2420624292,2508017931,2695934969,2136372366","P2P lending is an emerging wealth-management service for individuals, which allows lenders to directly bid and invest on the loans created by borrowers. In these platforms, lenders often pursue multiple objectives (e.g., non-default probability, fully-funded probability and winning-bid probability ) when they select loans to invest. How to automatically assess loans from these objectives and help lenders select loan portfolios is a very important but challenging problem. To that end, in this paper, we present a holistic study on portfolio selections in P2P lending. Specifically, we first propose to adapt gradient boosting decision tree, which combines both static features and dynamic features , to assess loans from multiple objectives. Then, we propose two strategies, i.e., weighted objective optimization strategy and multi-objective optimization strategy , to select portfolios for lenders. For each lender, the first strategy attempts to provide one optimal portfolio while the second strategy attempts to provide a Pareto-optimal portfolio set. Further, we design two algorithms, namely DPA and EVA , which can efficiently resolve the optimizations in these two strategies, respectively. Finally, extensive experiments on a large-scale real-world data set demonstrate the effectiveness of our solutions.",2016,Knowledge Discovery and Data Mining,multi objective optimization;actuarial science;
Graphons and Machine Learning: Modeling and Estimation of Sparse Massive Networks,Jennifer T. Chayes (Microsoft);,2021736417,"There are numerous examples of sparse massive networks, in particular the Internet, WWW and online social networks. How do we model and learn these networks? In contrast to conventional learning problems, where we have many independent samples, it is often the case for these networks that we can get only one independent sample. How do we use a single snapshot today to learn a model for the network, and therefore be able to predict a similar, but larger network in the future? In the case of relatively small or moderately sized networks, it's appropriate to model the network parametrically, and attempt to learn these parameters. For massive networks, a non-parametric representation is more appropriate. In this talk, we first review the theory of graphons, developed over the last decade to describe limits of dense graphs, and the more the recent theory describing sparse graphs of unbounded average degree, including power-law graphs. We then show how to use these graphons as non-parametric models for sparse networks. Finally, we show how to get consistent estimators of these non-parametric models, and moreover how to do this in a way that protects the privacy of individuals on the network.",2016,Knowledge Discovery and Data Mining,evolving networks;complex network;data science;data mining;machine learning;computer science;
Scalable Partial Least Squares Regression on Grammar-Compressed Data Matrices,Yasuo Tabei (National Presto Industries);Hiroto Saigo (Kyushu University);Yoshihiro Yamanishi (Kyushu University);Simon J. Puglisi (University of Helsinki);,"2094849311,1180009366,2029139975,324716293","With massive high-dimensional data now commonplace in research and industry, there is a strong and growing demand for more scalable computational techniques for data analysis and knowledge discovery. Key to turning these data into knowledge is the ability to learn statistical models with high interpretability. Current methods for learning statistical models either produce models that are not interpretable or have prohibitive computational costs when applied to massive data. In this paper we address this need by presenting a scalable algorithm for partial least squares regression (PLS), which we call compression-based PLS (cPLS), to learn predictive linear models with a high interpretability from massive high-dimensional data. We propose a novel grammar-compressed representation of data matrices that supports fast row and column access while the data matrix is in a compressed form. The original data matrix is grammar-compressed and then the linear model in PLS is learned on the compressed data matrix, which results in a significant reduction in working space, greatly improving scalability. We experimentally test cPLS on its ability to learn linear models for classification, regression and feature extraction with various massive high-dimensional data, and show that cPLS performs superiorly in terms of prediction accuracy, computational efficiency, and interpretability.",2016,Knowledge Discovery and Data Mining,partial least squares regression;big data;data mining;pattern recognition;machine learning;statistics;computer science;
Lexis: An Optimization Framework for Discovering the Hierarchical Structure of Sequential Data,Payam Siyari (Georgia Institute of Technology);Bistra Dilkina (Georgia Institute of Technology);Constantine Dovrolis (Georgia Institute of Technology);,"2303974943,2252452800,1945437555","Data represented as strings abounds in biology, linguistics, document mining, web search and many other fields. Such data often have a hierarchical structure, either because they were artificially designed and composed in a hierarchical manner or because there is an underlying evolutionary process that creates repeatedly more complex strings from simpler substrings. We propose a framework, referred to as Lexis, that produces an optimized hierarchical representation of a given set of ""target"" strings. The resulting hierarchy, ""Lexis-DAG"", shows how to construct each target through the concatenation of intermediate substrings, minimizing the total number of such concatenations or DAG edges. The Lexis optimization problem is related to the smallest grammar problem. After we prove its NP-hardness for two cost formulations, we propose an efficient greedy algorithm for the construction of Lexis-DAGs. We also consider the problem of identifying the set of intermediate nodes (substrings) that collectively form the ""core"" of a Lexis-DAG, which is important in the analysis of Lexis-DAGs. We show that the Lexis framework can be applied in diverse applications such as optimized synthesis of DNA fragments in genomic libraries, hierarchical structure discovery in protein sequences, dictionary-based text compression, and feature extraction from a set of documents.",2016,Knowledge Discovery and Data Mining,centrality;dna synthesis;compression;directed acyclic graph;feature extraction;data mining;pattern recognition;machine learning;computer science;mathematics;
An Engagement-Based Customer Lifetime Value System for E-commerce,Ali Vanderveld;Addhyan Pandey;Angela Han (Google);Rajesh Parekh (Facebook);,"2598640635,2507911175,2652940682,2528283674","A comprehensive understanding of individual customer value is crucial to any successful customer relationship management strategy. It is also the key to building products for long-term value returns. Modeling customer lifetime value (CLTV) can be fraught with technical difficulties, however, due to both the noisy nature of user-level behavior and the potentially large customer base. Here we describe a new CLTV system that solves these problems. This was built at Groupon, a large global e-commerce company, where confronting the unique challenges of local commerce means quickly iterating on new products and the optimal inventory to appeal to a wide and diverse audience. Given current purchaser frequency we need a faster way to determine the health of individual customers, and given finite resources we need to know where to focus our energy. Our CLTV system predicts future value on an individual user basis with a random forest model which includes features that account for nearly all aspects of each customer's relationship with our platform. This feature set includes those quantifying engagement via email and our mobile app, which give us the ability to predict changes in value far more quickly than models based solely on purchase behavior. We further model different customer types, such as one-time buyers and power users, separately so as to allow for different feature weights and to enhance the interpretability of our results. Additionally, we developed an economical scoring framework wherein we re-score a user when any trigger events occur and apply a decay function otherwise, to enable frequent scoring of a large customer base with a complex model. This system is deployed, predicting the value of hundreds of millions of users on a daily cadence, and is actively being used across our products and business initiatives.",2016,Knowledge Discovery and Data Mining,customer reference program;customer intelligence;voice of the customer;customer to customer;customer lifetime value;customer retention;random forest;e commerce;data mining;machine learning;computer science;
CNTK: Microsoft's Open-Source Deep-Learning Toolkit,Frank Seide (Microsoft);Amit Agarwal (Microsoft);,"2081329872,2488072657","This tutorial will introduce the Computational Network Toolkit, or CNTK, Microsoft's cutting-edge open-source deep-learning toolkit for Windows and Linux. CNTK is a powerful computation-graph based deep-learning toolkit for training and evaluating deep neural networks. Microsoft product groups use CNTK, for example to create the Cortana speech models and web ranking. CNTK supports feed-forward, convolutional, and recurrent networks for speech, image, and text workloads, also in combination. Popular network types are supported either natively (convolution) or can be described as a CNTK configuration (LSTM, sequence-to-sequence). CNTK scales to multiple GPU servers and is designed around efficiency. The tutorial will give an overview of CNTK's general architecture and describe the specific methods and algorithms used for automatic differentiation, recurrent-loop inference and execution, memory sharing, on-the-fly randomization of large corpora, and multi-server parallelization. We will then show how typical uses looks like for relevant tasks like image recognition, sequence-to-sequence modeling, and speech recognition.",2016,Knowledge Discovery and Data Mining,deep learning;theoretical computer science;data mining;machine learning;computer science;
Absolute Fused Lasso and Its Application to Genome-Wide Association Studies,Tao Yang (Arizona State University);Jun Liu (SAS Institute);Pinghua Gong (University of Michigan);Ruiwen Zhang (SAS Institute);Xiaotong Shen (University of Minnesota);Jieping Ye (University of Michigan);,"2688435411,2719093250,2323553286,2692486559,2125069230,2305258894","In many real-world applications, the samples/features acquired are in spatial or temporal order. In such cases, the magnitudes of adjacent samples/features are typically close to each other. Meanwhile, in the high-dimensional scenario, identifying the most relevant samples/features is also desired. In this paper, we consider a regularized model which can simultaneously identify important features and group similar features together. The model is based on a penalty called Absolute Fused Lasso (AFL). The AFL penalty encourages sparsity in the coefficients as well as their successive differences of absolute values' i.e., local constancy of the coefficient components in absolute values. Due to the non-convexity of AFL, it is challenging to develop efficient algorithms to solve the optimization problem. To this end, we employ the Difference of Convex functions (DC) programming to optimize the proposed non-convex problem. At each DC iteration, we adopt the proximal algorithm to solve a convex regularized sub-problem. One of the major contributions of this paper is to develop a highly efficient algorithm to compute the proximal operator. Empirical studies on both synthetic and real-world data sets from Genome-Wide Association Studies demonstrate the efficiency and effectiveness of the proposed approach in simultaneous identifying important features and grouping similar features.",2016,Knowledge Discovery and Data Mining,genome wide association study;machine learning;mathematical optimization;statistics;mathematics;
Fast Unsupervised Online Drift Detection Using Incremental Kolmogorov-Smirnov Test,Denis dos Reis (University of São Paulo);Peter Flach (University of Bristol);Stan Matwin (Polish Academy of Sciences);Gustavo Enrique de Almeida Prado Alves Batista (Spanish National Research Council);,"2366579606,1814273096,2195580174,2165222361","Data stream research has grown rapidly over the last decade. Two major features distinguish data stream from batch learning: stream data are generated on the fly, possibly in a fast and variable rate; and the underlying data distribution can be non-stationary, leading to a phenomenon known as concept drift. Therefore, most of the research on data stream classification focuses on proposing efficient models that can adapt to concept drifts and maintain a stable performance over time. However, specifically for the classification task, the majority of such methods rely on the instantaneous availability of true labels for all already classified instances. This is a strong assumption that is rarely fulfilled in practical applications. Hence there is a clear need for efficient methods that can detect concept drifts in an unsupervised way. One possibility is the well-known Kolmogorov-Smirnov test, a statistical hypothesis test that checks whether two samples differ. This work has two main contributions. The first one is the Incremental Kolmogorov-Smirnov algorithm that allows performing the Kolmogorov-Smirnov hypothesis test instantly using two samples that change over time, where the change is an insertion and/or removal of an observation. Our algorithm employs a randomized tree and is able to perform the insertion and removal operations in O (log N ) with high probability and calculate the Kolmogorov-Smirnov test in O (1), where N is the number of sample observations. This is a significant speed-up compared to the O ( N log N ) cost of the non-incremental implementation. The second contribution is the use of the Incremental Kolmogorov-Smirnov test to detect concept drifts without true labels. Classification algorithms adapted to use the test rely on a limited portion of those labels just to update the classification model after a concept drift is detected.",2016,Knowledge Discovery and Data Mining,treap;cartesian tree;concept drift;kolmogorov smirnov test;data mining;machine learning;statistics;computer science;
Meta Structure: Computing Relevance in Large Heterogeneous Information Networks,Zhipeng Huang (University of Hong Kong);Yudian Zheng (University of Hong Kong);Reynold Cheng (University of Hong Kong);Yizhou Sun (Northeastern University);Nikos Mamoulis (University of Hong Kong);Xiang Li (University of Hong Kong);,"2511877038,2105493843,2138267588,2131539564,18851973,2644215135","A heterogeneous information network (HIN) is a graph model in which objects and edges are annotated with types. Large and complex databases, such as YAGO and DBLP, can be modeled as HINs. A fundamental problem in HINs is the computation of closeness, or relevance, between two HIN objects. Relevance measures can be used in various applications, including entity resolution, recommendation, and information retrieval. Several studies have investigated the use of HIN information for relevance computation, however, most of them only utilize simple structure, such as path, to measure the similarity between objects. In this paper, we propose to use meta structure, which is a directed acyclic graph of object types with edge types connecting in between, to measure the proximity between objects. The strength of meta structure is that it can describe complex relationship between two HIN objects (e.g., two papers in DBLP share the same authors and topics). We develop three relevance measures based on meta structure. Due to the computational complexity of these measures, we further design an algorithm with data structures proposed to support their evaluation. Our extensive experiments on YAGO and DBLP show that meta structure-based relevance is more effective than state-of-the-art approaches, and can be efficiently computed.",2016,Knowledge Discovery and Data Mining,relevance;theoretical computer science;data mining;machine learning;computer science;
Predicting Disk Replacement towards Reliable Data Centers,Mirela Madalina Botezatu (IBM);Ioana Giurgiu (IBM);Jasmina Bogojeska (IBM);Dorothea Wiesmann (IBM);,"2229656828,2203074430,317490117,2048997920","Disks are among the most frequently failing components in today's IT environments. Despite a set of defense mechanisms such as RAID, the availability and reliability of the system are still often impacted severely. In this paper, we present a highly accurate SMART-based analysis pipeline that can correctly predict the necessity of a disk replacement even 10-15 days in advance. Our method has been built and evaluated on more than 30000 disks from two major manufacturers, monitored over 17 months. Our approach employs statistical techniques to automatically detect which SMART parameters correlate with disk replacement and uses them to predict the replacement of a disk with even 98% accuracy.",2016,Knowledge Discovery and Data Mining,time series;biological classification;embedded system;real time computing;statistics;
Towards Conversational Recommender Systems,Konstantina Christakopoulou (University of Minnesota);Filip Radlinski (Microsoft);Katja Hofmann (Microsoft);,"2223173307,2072292845,2722514380","People often ask others for restaurant recommendations as a way to discover new dining experiences. This makes restaurant recommendation an exciting scenario for recommender systems and has led to substantial research in this area. However, most such systems behave very differently from a human when asked for a recommendation. The goal of this paper is to begin to reduce this gap. In particular, humans can quickly establish preferences when asked to make a recommendation for someone they do not know. We address this cold-start recommendation problem in an online learning setting. We develop a preference elicitation framework to identify which questions to ask a new user to quickly learn their preferences. Taking advantage of latent structure in the recommendation space using a probabilistic latent factor model, our experiments with both synthetic and real world data compare different types of feedback and question selection strategies. We find that our framework can make very effective use of online user feedback, improving personalized recommendations over a static model by 25% after asking only 2 questions. Our results demonstrate dramatic benefits of starting from offline embeddings, and highlight the benefit of bandit-based explore-exploit strategies in this setting.",2016,Knowledge Discovery and Data Mining,cold start;recommender system;knowledge management;multimedia;world wide web;data mining;machine learning;computer science;
Probabilistic Robust Route Recovery with Spatio-Temporal Dynamics,Hao Wu (Fudan University);Jiangyun Mao (Fudan University);Weiwei Sun (Fudan University);Baihua Zheng (Singapore Management University);Hanyuan Zhang (Fudan University);Ziyang Chen (Fudan University);Wei Wang (Fudan University);,"2649278900,2507701226,2142659109,2200653933,2512529277,2516867379,2296847996","Vehicle trajectories are one of the most important data in location-based services. The quality of trajectories directly affects the services. However, in the real applications, trajectory data are not always sampled densely. In this paper, we study the problem of recovering the entire route between two distant consecutive locations in a trajectory. Most existing works solve the problem without using those informative historical data or solve it in an empirical way. We claim that a data-driven and probabilistic approach is actually more suitable as long as data sparsity can be well handled. We propose a novel route recovery system in a fully probabilistic way which incorporates both temporal and spatial dynamics and addresses all the data sparsity problem introduced by the probabilistic method. It outperforms the existing works with a high accuracy (over 80%) and shows a strong robustness even when the length of routes to be recovered is very long (about 30 road segments) or the data is very sparse.",2016,Knowledge Discovery and Data Mining,location based service;trajectory;data mining;machine learning;simulation;computer science;
Overcoming Key Weaknesses of Distance-based Neighbourhood Methods using a Data Dependent Dissimilarity Measure,"Kai Ming Ting (Federation University Australia);Ye Zhu (Monash University, Clayton campus);Mark James Carman (Monash University);Yue Zhu (Nanjing University);Zhi-Hua Zhou (Nanjing University);","2644558103,2597410478,2123487663,2152538905,2286237009","This paper introduces the first generic version of data dependent dissimilarity and shows that it provides a better closest match than distance measures for three existing algorithms in clustering, anomaly detection and multi-label classification. For each algorithm, we show that by simply replacing the distance measure with the data dependent dissimilarity measure, it overcomes a key weakness of the otherwise unchanged algorithm.",2016,Knowledge Discovery and Data Mining,data mining;pattern recognition;machine learning;mathematics;
Approximate Personalized PageRank on Dynamic Graphs,Hongyang Zhang (Stanford University);Peter Lofgren (Stanford University);Ashish Goel (Stanford University);,"2494713920,2503222904,2112067655","We propose and analyze two algorithms for maintaining approximate Personalized PageRank (PPR) vectors on a dynamic graph, where edges are added or deleted. Our algorithms are natural dynamic versions of two known local variations of power iteration. One, Forward Push, propagates probability mass forwards along edges from a source node, while the other, Reverse Push, propagates local changes backwards along edges from a target. In both variations, we maintain an invariant between two vectors, and when an edge is updated, our algorithm first modifies the vectors to restore the invariant, then performs any needed local push operations to restore accuracy. For Reverse Push, we prove that for an arbitrary directed graph in a random edge model, or for an arbitrary undirected graph, given a uniformly random target node t , the cost to maintain a PPR vector to t of additive error e as k edges are updated is O ( k + d/e, where d is the average degree of the graph. This is O (1) work per update, plus the cost of computing a reverse vector once on a static graph. For Forward Push, we show that on an arbitrary undirected graph, given a uniformly random start node s , the cost to maintain a PPR vector from s of degree-normalized error e as k edges are updated is O ( k + 1/e, which is again O (1) per update plus the cost of computing a PPR vector once on a static graph.",2016,Knowledge Discovery and Data Mining,simplex graph;strength of a graph;voltage graph;complement graph;multiple edges;graph power;butterfly graph;feedback arc set;mixed graph;null graph;transpose graph;random geometric graph;path;cycle graph;line graph;multigraph;degree;directed graph;random walk;theoretical computer science;combinatorics;distributed computing;mathematical optimization;statistics;mathematics;
PTE: Enumerating Trillion Triangles On Distributed Systems,Ha-Myung Park (KAIST);Sung-Hyon Myaeng (KAIST);U. Kang (Seoul National University);,"2229610034,2062572430,2708234210","How can we enumerate triangles from an enormous graph with billions of vertices and edges? Triangle enumeration is an important task for graph data analysis with many applications including identifying suspicious users in social networks, detecting web spams, finding communities, etc. However, recent networks are so large that most of the previous algorithms fail to process them. Recently, several MapReduce algorithms have been proposed to address such large networks; however, they suffer from the massive shuffled data resulting in a very long processing time. In this paper, we propose PTE (Pre-partitioned Triangle Enumeration), a new distributed algorithm for enumerating triangles in enormous graphs by resolving the structural inefficiency of the previous MapReduce algorithms. PTE enumerates trillions of triangles in a billion scale graph by decreasing three factors: the amount of shuffled data, total work, and network read. Experimental results show that PTE provides up to 47 times faster performance than recent distributed algorithms on real world graphs, and succeeds in enumerating more than 3 trillion triangles on the ClueWeb12 graph with 6.3 billion vertices and 72 billion edges, which any previous triangle computation algorithm fail to process.",2016,Knowledge Discovery and Data Mining,network analysis;big data;distributed algorithm;theoretical computer science;combinatorics;data mining;artificial intelligence;machine learning;algorithm;computer science;
TRIÈST: Counting Local and Global Triangles in Fully-Dynamic Streams with Fixed Memory Size,Lorenzo De Stefani (Brown University);Alessandro Epasto (Google);Matteo Riondato (Brown University);Eli Upfal (Brown University);,"2487262625,2303398780,1555209364,2685185700","We present TRIEST, a suite of one-pass streaming algorithms to compute unbiased, low-variance, high-quality approximations of the global and local (i.e., incident to each vertex) number of triangles in a fully-dynamic graph represented as an adversarial stream of edge insertions and deletions. Our algorithms use reservoir sampling and its variants to exploit the user-specified memory space at all times. This is in contrast with previous approaches, which require hard-to-choose parameters (e.g., a fixed sampling probability) and offer no guarantees on the amount of memory they use. We analyze the variance of the estimations and show novel concentration bounds for these quantities. Our experimental results on very large graphs demonstrate that TRIEST outperforms state-of-the-art approaches in accuracy and exhibits a small update time.",2016,Knowledge Discovery and Data Mining,graph enumeration;cycle count;reservoir sampling;probabilistic analysis of algorithms;social network;data stream mining;theoretical computer science;discrete mathematics;combinatorics;mathematical optimization;statistics;algorithm;computer science;mathematics;
Scalable Data Analytics Using R: Single Machines to Hadoop Spark Clusters,John-Mark Agosta (Microsoft);Debraj GuhaThakurta (Microsoft);Robert Horton (Microsoft);Mario Inchiosa (Microsoft);Srini Kumar (Microsoft);Mengyue Zhao (Microsoft);,"2522191117,2521559845,2516413223,2509016422,2514624990,2516893623","R is one of the most popular languages in the data science, statistical and machine learning (ML) community. However, when it comes to scalable data analysis and ML using R, many data scientists are blocked or hindered by (a) its limitations of available functions to handle large datasets efficiently, and (b) knowledge about the appropriate computing environments to scale R scripts from desktop exploratory analysis to elastic and distributed cloud services. In this tutorial we will discuss solutions that demonstrate the use of distributed compute environments and end to end solutions for R. We will present the topics through presentations and worked-out examples with sample code. In addition, we will provide a public code repository that attendees will be able to access and adapt to their own practice. We believe this tutorial will be of strong interest to a large and growing community of data scientists and developers using R for data analysis and modeling.",2016,Knowledge Discovery and Data Mining,spark;predictive analytics;sql;learning curve;scalability;visualization;statistical model;computational statistics;data science;theoretical computer science;data mining;machine learning;computer science;
Firebird: Predicting Fire Risk and Prioritizing Fire Inspections in Atlanta,"Michael Madaio (Carnegie Mellon University);Shang-Tse Chen (Georgia Institute of Technology);Oliver L. Haimson (University of California, Irvine);Wenwen Zhang (Georgia Institute of Technology);Xiang Cheng (Emory University);Matthew Hinds-Aldrich;Duen Horng Chau (Georgia Institute of Technology);Bistra Dilkina (Georgia Institute of Technology);","2504639629,2576491193,48930185,2309462951,2635027973,1498291670,2024561599,2252452800","The Atlanta Fire Rescue Department (AFRD), like many municipal fire departments, actively works to reduce fire risk by inspecting commercial properties for potential hazards and fire code violations. However, AFRD's fire inspection practices relied on tradition and intuition, with no existing data-driven process for prioritizing fire inspections or identifying new properties requiring inspection. In collaboration with AFRD, we developed the Firebird framework to help municipal fire departments identify and prioritize commercial property fire inspections, using machine learning, geocoding, and information visualization. Firebird computes fire risk scores for over 5,000 buildings in the city, with true positive rates of up to 71% in predicting fires. It has identified 6,096 new potential commercial properties to inspect, based on AFRD's criteria for inspection. Furthermore, through an interactive map, Firebird integrates and visualizes fire incidents, property information and risk scores to help AFRD make informed decisions about fire inspections. Firebird has already begun to make positive impact at both local and national levels. It is improving AFRD's inspection processes and Atlanta residents' safety, and was highlighted by National Fire Protection Association (NFPA) as a best practice for using data to inform fire inspections.",2016,Knowledge Discovery and Data Mining,flame sim;fire protection;predictive analytics;interactive visualization;computer security;simulation;computer science;
Inferring Network Effects from Observational Data,David T. Arbour (University of Massachusetts Amherst);Dan Garant (University of Massachusetts Amherst);David D. Jensen (University of Massachusetts Amherst);,"2223056767,2388112039,2124028641","We present Relational Covariate Adjustment (RCA), a general method for estimating causal effects in relational data. Relational Covariate Adjustment is implemented through two high-level operations: identification of an adjustment set and relational regression adjustment. The former is achieved through an extension of Pearl's back-door criterion to relational domains. We demonstrate how this extended definition can be used to estimate causal effects in the presence of network interference and confounding. RCA is agnostic to functional form, and it can easily model both discrete and continuous treatments as well as estimate the effects of a wider array of network interventions than existing experimental approaches. We show that RCA can yield robust estimates of causal effects using common regression models without extensive parameter tuning. Through a series of simulation experiments on a variety of synthetic and real-world network structures, we show that causal effects estimated on observational data with RCA are nearly as accurate as those estimated from well-designed network experiments",2016,Knowledge Discovery and Data Mining,causality;econometrics;data mining;machine learning;statistics;computer science;mathematics;
FUSE: Full Spectral Clustering,Wei Ye (Ludwig Maximilian University of Munich);Sebastian Goebl (Ludwig Maximilian University of Munich);Claudia Plant (University of Vienna);Christian Böhm (Ludwig Maximilian University of Munich);,"2299720385,119557107,2590664453,2486446532","Multi-scale data which contains structures at different scales of size and density is a big challenge for spectral clustering. Even given a suitable locally scaled affinity matrix, the first k eigenvectors of such a matrix still cannot separate clusters well. Thus, in this paper, we exploit the fusion of the cluster-separation information from all eigenvectors to achieve a better clustering result. Our method FU ll S pectral Clust E ring (FUSE) is based on Power Iteration (PI) and Independent Component Analysis (ICA). PI is used to fuse all eigenvectors to one pseudo-eigenvector which inherits all the cluster-separation information. To conquer the cluster-collision problem, we utilize PI to generate p ( p > k ) pseudo-eigenvectors. Since these pseudo-eigenvectors are redundant and the cluster-separation information is contaminated with noise, ICA is adopted to rotate the pseudo-eigenvectors to make them pairwise statistically independent. To let ICA overcome local optima and speed up the search process, we develop a self-adaptive and self-learning greedy search method. Finally, we select k rotated pseudo-eigenvectors (independent components) which have more cluster-separation information measured by kurtosis for clustering. Various synthetic and real-world data verifies the effectiveness and efficiency of our FUSE method.",2016,Knowledge Discovery and Data Mining,givens rotation;spectral clustering;power iteration;pattern recognition;machine learning;mathematical optimization;statistics;mathematics;
Kam1n0: MapReduce-based Assembly Clone Search for Reverse Engineering,Steven H.H. Ding (McGill University);Benjamin C.M. Fung (McGill University);Philippe Charland (Defence Research and Development Canada);,"2286376525,2015549731,2619933766","Assembly code analysis is one of the critical processes for detecting and proving software plagiarism and software patent infringements when the source code is unavailable. It is also a common practice to discover exploits and vulnerabilities in existing software. However, it is a manually intensive and time-consuming process even for experienced reverse engineers. An effective and efficient assembly code clone search engine can greatly reduce the effort of this process, since it can identify the cloned parts that have been previously analyzed. The assembly code clone search problem belongs to the field of software engineering. However, it strongly depends on practical nearest neighbor search techniques in data mining and databases. By closely collaborating with reverse engineers and Defence Research and Development Canada (DRDC), we study the concerns and challenges that make existing assembly code clone approaches not practically applicable from the perspective of data mining. We propose a new variant of LSH scheme and incorporate it with graph matching to address these challenges. We implement an integrated assembly clone search engine called Kam1n0. It is the first clone search engine that can efficiently identify the given query assembly function's subgraph clones from a large assembly code repository. Kam1n0 is built upon the Apache Spark computation framework and Cassandra-like key-value distributed storage. A deployed demo system is publicly available. Extensive experimental results suggest that Kam1n0 is accurate, efficient, and scalable for handling large volume of assembly code.",2016,Knowledge Discovery and Data Mining,world wide web;bioinformatics;data mining;database;computer science;
Mining Subgroups with Exceptional Transition Behavior,Florian Lemmerich (Leibniz Association);Martin Becker (University of Würzburg);Philipp Singer (Leibniz Association);Denis Helic (Graz University of Technology);Andreas Hotho (University of Würzburg);Markus Strohmaier (Leibniz Association);,"156522191,2343475332,2167599249,315966907,20543882,142799918","We present a new method for detecting interpretable subgroups with exceptional transition behavior in sequential data. Identifying such patterns has many potential applications, e.g., for studying human mobility or analyzing the behavior of internet users. To tackle this task, we employ exceptional model mining, which is a general approach for identifying interpretable data subsets that exhibit unusual interactions between a set of target attributes with respect to a certain model class. Although exceptional model mining provides a well-suited framework for our problem, previously investigated model classes cannot capture transition behavior. To that end, we introduce first-order Markov chains as a novel model class for exceptional model mining and present a new interestingness measure that quantifies the exceptionality of transition subgroups. The measure compares the distance between the Markov transition matrix of a subgroup and the respective matrix of the entire data with the distance of random dataset samples. In addition, our method can be adapted to find subgroups that match or contradict given transition hypotheses. We demonstrate that our method is consistently able to recover subgroups with exceptional transition models from synthetic data and illustrate its potential in two application examples. Our work is relevant for researchers and practitioners interested in detecting exceptional transition behavior in sequential data.",2016,Knowledge Discovery and Data Mining,markov chain;combinatorics;data mining;artificial intelligence;statistics;mathematics;
Robust Large-Scale Machine Learning in the Cloud,Steffen Rendle (Google);Dennis Fetterly (Google);Eugene J. Shekita (Google);Bor-yiing Su (Google);,"2516383678,2515256586,2309335715,2278618929","The convergence behavior of many distributed machine learning (ML) algorithms can be sensitive to the number of machines being used or to changes in the computing environment. As a result, scaling to a large number of machines can be challenging. In this paper, we describe a new scalable coordinate descent (SCD) algorithm for generalized linear models whose convergence behavior is always the same, regardless of how much SCD is scaled out and regardless of the computing environment. This makes SCD highly robust and enables it to scale to massive datasets on low-cost commodity servers. Experimental results on a real advertising dataset in Google are used to demonstrate SCD's cost effectiveness and scalability. Using Google's internal cloud, we show that SCD can provide near linear scaling using thousands of cores for 1 trillion training examples on a petabyte of compressed data. This represents 10,000x more training examples than the 'large-scale' Netflix prize dataset. We also show that SCD can learn a model for 20 billion training examples in two hours for about $10.",2016,Knowledge Discovery and Data Mining,coordinate descent;linear regression;data mining;machine learning;simulation;computer science;
Batch Model for Batched Timestamps Data Analysis with Application to the SSA Disability Program,Qingqi Yue (National Institutes of Health);Ao Yuan (National Institutes of Health);Xuan Che (National Institutes of Health);Minh Huynh;Chunxiao Zhou (National Institutes of Health);,"2531142739,2702211679,2647843527,2517598830,2682842892","The Office of Disability Adjudication and Review (ODAR) is responsible for holding hearings, issuing decisions, and reviewing appeals as part of the Social Security Administration's disability determining process. In order to control and process cases, the ODAR has established a Case Processing and Management System (CPMS) to record management information since December 2003. The CPMS provides a detailed case status history for each case. Due to the large number of appeal requests and limited resources, the number of pending claims at ODAR was over one million cases by March 31, 2015. Our National Institutes of Health (NIH) team collaborated with SSA and developed a Case Status Change Model (CSCM) project to meet the ODAR's urgent need of reducing backlogs and improve hearings and appeals process. One of the key issues in our CSCM project is to estimate the expected service time and its variation for each case status code. The challenge is that the system's recorded job departure times may not be the true job finished times. As the CPMS timestamps data of case status codes showed apparent batch patterns, we proposed a batch model and applied the constrained least squares method to estimate the mean service times and the variances. We also proposed a batch search algorithm to determine the optimal batch partition, as no batch partition was given in the real data. Simulation studies were conducted to evaluate the performance of the proposed methods. Finally, we applied the method to analyze a real CPMS data from ODAR/SSA.",2016,Knowledge Discovery and Data Mining,operations research;data mining;statistics;computer science;
Algorithmic Bias: From Discrimination Discovery to Fairness-aware Data Mining,Sara Hajian;Francesco Bonchi (Institute for Scientific Interchange);Carlos Castillo (Yahoo!);,"2617388898,2176652147,2125169605","Algorithms and decision making based on Big Data have become pervasive in all aspects of our daily lives lives (offline and online), as they have become essential tools in personal finance, health care, hiring, housing, education, and policies. It is therefore of societal and ethical importance to ask whether these algorithms can be discriminative on grounds such as gender, ethnicity, or health status. It turns out that the answer is positive: for instance, recent studies in the context of online advertising show that ads for high-income jobs are presented to men much more often than to women [Datta et al., 2015]; and ads for arrest records are significantly more likely to show up on searches for distinctively black names [Sweeney, 2013]. This algorithmic bias exists even when there is no discrimination intention in the developer of the algorithm. Sometimes it may be inherent to the data sources used (software making decisions based on data can reflect, or even amplify, the results of historical discrimination), but even when the sensitive attributes have been suppressed from the input, a well trained machine learning algorithm may still discriminate on the basis of such sensitive attributes because of correlations existing in the data. These considerations call for the development of data mining systems which are discrimination-conscious by-design. This is a novel and challenging research area for the data mining community. The aim of this tutorial is to survey algorithmic bias, presenting its most common variants, with an emphasis on the algorithmic techniques and key ideas developed to derive efficient solutions. The tutorial covers two main complementary approaches: algorithms for discrimination discovery and discrimination prevention by means of fairness-aware data mining. We conclude by summarizing promising paths for future research.",2016,Knowledge Discovery and Data Mining,data science;data mining;artificial intelligence;machine learning;statistics;computer science;
Bayesian Inference of Arrival Rate and Substitution Behavior from Sales Transaction Data with Stockouts,Benjamin Letham (Massachusetts Institute of Technology);Lydia M. Letham (Massachusetts Institute of Technology);Cynthia Rudin (Duke University);,"1983818389,2510404420,2600280817","When an item goes out of stock, sales transaction data no longer reflect the original customer demand, since some customers leave with no purchase while others substitute alternative products for the one that was out of stock. Here we develop a Bayesian hierarchical model for inferring the underlying customer arrival rate and choice model from sales transaction data and the corresponding stock levels. The model uses a nonhomogeneous Poisson process to allow the arrival rate to vary throughout the day, and allows for a variety of choice models. Model parameters are inferred using a stochastic gradient MCMC algorithm that can scale to large transaction databases. We fit the model to data from a local bakery and show that it is able to make accurate out-of-sample predictions, and to provide actionable insight into lost cookie sales.",2016,Knowledge Discovery and Data Mining,markov chain monte carlo;bayesian probability;econometrics;statistics;mathematics;
Scalable Time-Decaying Adaptive Prediction Algorithm,Yinyan Tan (Huawei);Zhe Fan (Huawei);Guilin Li (Huawei);Fangshan Wang (Huawei);Zhengbing Li (Huawei);Shikai Liu (Huawei);Qiuling Pan (Huawei);Eric P. Xing (Carnegie Mellon University);Qirong Ho (Carnegie Mellon University);,"2628843760,2530753879,2507703984,2646546277,2116727413,2510477028,2515210992,351197510,2716009146","Online learning is used in a wide range of real applications, e.g., predicting ad click-through rates (CTR) and personalized recommendations. Based on the analysis of users' behaviors in Video-On-Demand (VoD) recommender systems,we discover that the most recent users' actions can better reflect users' current intentions and preferences. Under this observation, we thereby propose a novel time-decaying online learning algorithm derived from the state-of-the-art FTRL-proximal algorithm, called Time-Decaying Adaptive Prediction (TDAP) algorithm. To scale Big Data, we further parallelize our algorithm following the data parallel scheme under both BSP and SSP consistency model. We experimentally evaluate our TDAP algorithm on real IPTV VoD datasets using two state-of-the-art distributed computing platforms, TDAP achieves good accuracy: it improves at least 5.6% in terms of prediction accuracy, compared to FTRL-proximal algorithm; and TDAP scales well: it runs 4 times faster when the number of machines increases from 2 to 10.",2016,Knowledge Discovery and Data Mining,theoretical computer science;data mining;machine learning;simulation;computer science;
Big Data Needs Big Dreamers: Lessons from Successful Big Data Investors,Evangelos Simoudis (IBM);Mark Gorenberg;Tim Guleri;Matt Ocko;Greg Sands;,"4129615,2179178191,2508595895,2515246776,2652982569",-,2016,Knowledge Discovery and Data Mining,computer science;
Subjectively Interesting Component Analysis: Data Projections that Contrast with Prior Expectations,Bo Kang (Ghent University);Jefrey Lijffijt (Ghent University);Raúl Santos-Rodríguez (University of Bristol);Tijl De Bie (Ghent University);,"2497045425,2509450284,2109855641,2522659473","Methods that find insightful low-dimensional projections are essential to effectively explore high-dimensional data. Principal Component Analysis is used pervasively to find low-dimensional projections, not only because it is straightforward to use, but it is also often effective, because the variance in data is often dominated by relevant structure. However, even if the projections highlight real structure in the data, not all structure is interesting to every user. If a user is already aware of, or not interested in the dominant structure, Principal Component Analysis is less effective for finding interesting components. We introduce a new method called Subjectively Interesting Component Analysis (SICA), designed to find data projections that are subjectively interesting , i.e, projections that truly surprise the end-user. It is rooted in information theory and employs an explicit model of a user's prior expectations about the data. The corresponding optimization problem is a simple eigenvalue problem, and the result is a trade-off between explained variance and novelty. We present five case studies on synthetic data, images, time-series, and spatial data, to illustrate how SICA enables users to find (subjectively) interesting projections.",2016,Knowledge Discovery and Data Mining,dimensionality reduction;information theory;data mining;pattern recognition;machine learning;statistics;computer science;mathematics;
Predicting Socio-Economic Indicators using News Events,Sunandan Chakraborty (New York University);Ashwin Venkataraman (New York University);Srikanth Jagabathula (New York University);Lakshminarayanan Subramanian (New York University);,"2168126670,2114574273,233778557,2102680016","Many socio-economic indicators are sensitive to real-world events. Proper characterization of the events can help to identify the relevant events that drive fluctuations in these indicators. In this paper, we propose a novel generative model of real-world events and employ it to extract events from a large corpus of news articles. We introduce the notion of an event class, which is an abstract grouping of similarly themed events. These event classes are manifested in news articles in the form of event triggers which are specific words that describe the actions or incidents reported in any article. We use the extracted events to predict fluctuations in different socio-economic indicators. Specifically, we focus on food prices and predict the price of 12 different crops based on real-world events that potentially influence food price volatility, such as transport strikes, festivals etc. Our experiments demonstrate that incorporating event information in the prediction tasks reduces the root mean square error (RMSE) of prediction by 22% compared to the standard ARIMA model. We also predict sudden increases in the food prices (i.e. spikes) using events as features, and achieve an average 5-10% increase in accuracy compared to baseline models, including an LDA topic-model based predictive model.",2016,Knowledge Discovery and Data Mining,news analytics;data science;data mining;
Unified Point-of-Interest Recommendation with Temporal Interval Assessment,Yanchi Liu (Rutgers–Newark);Chuanren Liu (Drexel University);Bin Liu (Rutgers–Newark);Meng Qu (Rutgers–Newark);Hui Xiong (Rutgers–Newark);,"2159798580,2169554947,2428181972,2223048303,2153710278","Point-of-interest (POI) recommendation, which helps mobile users explore new places, has become an important location-based service. Existing approaches for POI recommendation have been mainly focused on exploiting the information about user preferences, social influence, and geographical influence. However, these approaches cannot handle the scenario where users are expecting to have POI recommendation for a specific time period. To this end, in this paper, we propose a unified recommender system, named the 'Where and When to gO' (WWO) recommender system, to integrate the user interests and their evolving sequential preferences with temporal interval assessment. As a result, the WWO system can make recommendations dynamically for a specific time period and the traditional POI recommender system can be treated as the special case of the WWO system by setting this time period long enough. Specifically, to quantify users' sequential preferences, we consider the distributions of the temporal intervals between dependent POIs in the historical check-in sequences. Then, to estimate the distributions with only sparse observations, we develop the low-rank graph construction model, which identifies a set of bi-weighted graph bases so as to learn the static user preferences and the dynamic sequential preferences in a coherent way. Finally, we evaluate the proposed approach using real-world data sets from several location-based social networks (LBSNs). The experimental results show that our method outperforms the state-of-the-art approaches for POI recommendation in terms of various metrics, such as F-measure and NDCG, with a significant margin.",2016,Knowledge Discovery and Data Mining,world wide web;information retrieval;data mining;computer science;
Lossless Separation of Web Pages into Layout Code and Data,Adi Omari (Technion – Israel Institute of Technology);Benny Kimelfeld (Technion – Israel Institute of Technology);Eran Yahav (Technion – Israel Institute of Technology);Sharon Shoham (Tel Aviv University);,"2480507036,2676503258,2194672974,2144636783","A modern web page is often served by running layout code on data, producing an HTML document that enhances the data with front/back matters and layout/style operations. In this paper, we consider the opposite task: separating a given web page into a data component and a layout program. This separation has various important applications: page encoding may be significantly more compact (reducing web traffic), data representation is normalized across web designs (facilitating wrapping, retrieval and extraction), and repetitions are diminished (expediting site updates and redesign). We present a framework for defining the separation task, and devise an algorithm for synthesizing layout code from a web page while distilling its data in a lossless manner. The main idea is to synthesize layout code hierarchically for parts of the page, and use a combined program-data representation cost to decide whether to align intermediate programs. When intermediate programs are aligned, they are transformed into a single program, possibly with loops and conditionals. At the same time, differences between the aligned programs are captured by the data component such that executing the layout code on the data results in the original page. We have implemented our approach and conducted a thorough experimental study of its effectiveness. Our experiments show that our approach features state of the art (and higher) performance in both size compression and record extraction.",2016,Knowledge Discovery and Data Mining,comprehensive layout;lossless compression;theoretical computer science;data mining;database;machine learning;computer science;
Boosted Decision Tree Regression Adjustment for Variance Reduction in Online Controlled Experiments,Alexey Poyarkov (Yandex);Alexey Drutsa (Yandex);Andrey Khalyavin (Yandex);Gleb Gusev (Yandex);Pavel Serdyukov (Yandex);,"2563653331,2229408502,2565024728,2005728791,2130450538","Nowadays, the development of most leading web services is controlled by online experiments that qualify and quantify the steady stream of their updates achieving more than a thousand concurrent experiments per day. Despite the increasing need for running more experiments, these services are limited in their user traffic. This situation leads to the problem of finding a new or improving existing key performance metric with a higher sensitivity and lower variance. We focus on the problem of variance reduction for engagement metrics of user loyalty that are widely used in A/B testing of web services. We develop a general framework that is based on evaluation of the mean difference between the actual and the approximated values of the key performance metric (instead of the mean of this metric). On the one hand, it allows us to incorporate the state-of-the-art techniques widely used in randomized experiments of clinical and social research, but limitedly used in online evaluation. On the other hand, we propose a new class of methods based on advanced machine learning algorithms, including ensembles of decision trees, that, to the best of our knowledge, have not been applied earlier to the problem of variance reduction. We validate the variance reduction approaches on a very large set of real large-scale A/B experiments run at Yandex for different engagement metrics of user loyalty. Our best approach demonstrates $63\%$ average variance reduction (which is equivalent to 63% saved user traffic) and detects the treatment effect in $2$ times more A/B experiments.",2016,Knowledge Discovery and Data Mining,variance reduction;prediction;data mining;machine learning;simulation;statistics;computer science;
Keeping it Short and Simple: Summarising Complex Event Sequences with Multivariate Patterns,Roel Bertens (Utrecht University);Jilles Vreeken (Max Planck Society);Arno Siebes (Utrecht University);,"2116895963,1971070670,1988376837","We study how to obtain concise descriptions of discrete multivariate sequential data. In particular, how to do so in terms of rich multivariate sequential patterns that can capture potentially highly interesting (cor)relations between sequences. To this end we allow our pattern language to span over the domains (alphabets) of all sequences, allow patterns to overlap temporally, as well as allow for gaps in their occurrences. We formalise our goal by the Minimum Description Length principle, by which our objective is to discover the set of patterns that provides the most succinct description of the data. To discover high-quality pattern sets directly from data, we introduce Ditto, a highly efficient algorithm that approximates the ideal result very well. Experiments show that Ditto correctly discovers the patterns planted in synthetic data. Moreover, it scales favourably with the length of the data, the number of attributes, the alphabet sizes. On real data, ranging from sensor networks to annotated text, Ditto discovers easily interpretable summaries that provide clear insight in both the univariate and multivariate structure.",2016,Knowledge Discovery and Data Mining,data mining;pattern recognition;machine learning;statistics;algorithm;mathematics;
Reconstructing an Epidemic Over Time,Polina Rozenshtein (Aalto University);Aristides Gionis (Aalto University);B. Aditya Prakash (Virginia Tech);Jilles Vreeken (Saarland University);,"285511381,737311942,2124002246,1971070670","We consider the problem of reconstructing an epidemic over time, or, more general, reconstructing the propagation of an activity in a network. Our input consists of a temporal network , which contains information about when two nodes interacted, and a sample of nodes that have been reported as infected. The goal is to recover the flow of the spread, including discovering the starting nodes, and identifying other likely-infected nodes that are not reported. The problem we consider has multiple applications, from public health to social media and viral marketing purposes. Previous work explicitly factor-in many unrealistic assumptions: it is assumed that (a) the underlying network does not change;(b) we have access to perfect noise-free data; or (c) we know the exact propagation model. In contrast, we avoid these simplifications: we take into account the temporal network, we require only a small sample of reported infections, and we do not make any restrictive assumptions about the propagation model. We develop CulT, a scalable and effective algorithm to reconstruct epidemics that is also suited for online settings. CulT works by formulating the problem as that of a temporal Steiner-tree computation, for which we design a fast algorithm leveraging the specific problem structure. We demonstrate the efficacy of the proposed approach through extensive experiments on diverse datasets.",2016,Knowledge Discovery and Data Mining,approximation algorithm;theoretical computer science;data mining;artificial intelligence;machine learning;statistics;computer science;mathematics;
Skinny-dip: Clustering in a Sea of Noise,Samuel Maurus (Technische Universität München);Claudia Plant (University of Vienna);,"1989291604,2590664453","Can we find heterogeneous clusters hidden in data sets with 80% noise? Although such settings occur in the real-world, we struggle to find methods from the abundance of clustering techniques that perform well with noise at this level. Indeed, perhaps this is enough of a departure from classical clustering to warrant its study as a separate problem. In this paper we present SkinnyDip which, based on Hartigan's elegant dip test of unimodality, represents an intriguing approach to clustering with an attractive set of properties. Specifically, SkinnyDip is highly noise-robust, practically parameter-free and completely deterministic. SkinnyDip never performs multivariate distance calculations, but rather employs insightful recursion based on ""dips"" into univariate projections of the data. It is able to detect a range of cluster shapes and densities, assuming only that each cluster admits a unimodal shape. Practically, its run-time grows linearly with the data. Finally, for high-dimensional data, continuity properties of the dip enable SkinnyDip to exploit multimodal projection pursuit in order to find an appropriate basis for clustering. Although not without its limitations, SkinnyDip compares favorably to a variety of clustering approaches on synthetic and real data, particularly in high-noise settings.",2016,Knowledge Discovery and Data Mining,flame clustering;brown clustering;canopy clustering algorithm;correlation clustering;constrained clustering;cure data clustering algorithm;affinity propagation;fuzzy clustering;clustering high dimensional data;cluster analysis;consensus clustering;data mining;machine learning;statistics;computer science;mathematics;
Diversified Temporal Subgraph Pattern Mining,Yi Yang (Fudan University);Da Yan (The Chinese University of Hong Kong);Huanhuan Wu (The Chinese University of Hong Kong);James Cheng (The Chinese University of Hong Kong);Shuigeng Zhou (Fudan University);John C.S. Lui (The Chinese University of Hong Kong);,"2662026395,2146437384,2165269316,2304873892,2157277864,2045404162","Many graphs in real-world applications, such as telecommunications networks, social-interaction graphs and co-authorship graphs, contain temporal information. However, existing graph mining algorithms fail to exploit these temporal information and the resulting subgraph patterns do not contain any temporal attribute. In this paper, we study the problem of mining a set of diversified temporal subgraph patterns from a temporal graph, where each subgraph is associated with the time interval that the pattern spans. This problem motivates important applications such as finding social trends in social networks, or detecting temporal hotspots in telecommunications networks. We propose a divide-and-conquer algorithm along with effective pruning techniques, and our approach runs 2 to 3 orders of magnitude faster than a baseline algorithm and obtains high-quality temporal subgraph patterns in real temporal graphs.",2016,Knowledge Discovery and Data Mining,degeneracy;distance hereditary graph;induced subgraph isomorphism problem;subgraph isomorphism problem;combinatorics;data mining;machine learning;mathematics;
Bayesian Optimization and Embedded Learning Systems,Jeff Schneider (Carnegie Mellon University);,2165347984,"An important property of embedded learning systems is the ever-changing environment they create for all algorithms operating in the system. Optimizing the performance of those algorithms becomes a perpetual on-line activity rather than a one-off task. I will review some of these challenges in autonomous vehicles. I will discuss Bayesian optimization methods and their application in robotics and scientific applications, focusing on scaling up the dimensionality and managing multi-fidelity evaluations. I will finish with lessons learned and thoughts on future directions as these methods move into embedded systems.",2016,Knowledge Discovery and Data Mining,robotics;artificial intelligence;machine learning;simulation;computer science;
MANTRA: A Scalable Approach to Mining Temporally Anomalous Sub-trajectories,Prithu Banerjee (University of British Columbia);Pranali Yawalkar (Indian Institute of Technology Madras);Sayan Ranu (Indian Institute of Technology Madras);,"2185325292,2703137274,2096541091","In this paper, we study the problem of mining temporally anomalous sub-trajectory patterns from an input trajectory in a scalable manner. Given the prevailing road conditions, a sub-trajectory is temporally anomalous if its travel time deviates significantly from the expected time. Mining these patterns requires us to delve into the sub-trajectory space, which is not scalable for real-time analytics. To overcome this scalability challenge, we design a technique called MANTRA . We study the properties unique to anomalous sub-trajectories and utilize them in MANTRA to iteratively refine the search space into a disjoint set of sub-trajectory islands . The expensive enumeration of all possible sub-trajectories is performed only on the islands to compute the answer set of maximal anomalous sub-trajectories . Extensive experiments on both real and synthetic datasets establish MANTRA as more than 3 orders of magnitude faster than baseline techniques. Moreover, through trajectory classification and segmentation, we demonstrate that the proposed model conforms to human intuition.",2016,Knowledge Discovery and Data Mining,global positioning system;data mining;artificial intelligence;simulation;computer science;
Fast Component Pursuit for Large-Scale Inverse Covariance Estimation,Lei Han (Rutgers University);Yu Zhang (Hong Kong University of Science and Technology);Tong Zhang (Rutgers University);,"2531318857,2648094648,2510858842","The maximum likelihood estimation (MLE) for the Gaussian graphical model, which is also known as the inverse covariance estimation problem, has gained increasing interest recently. Most existing works assume that inverse covariance estimators contain sparse structure and then construct models with the l 1 regularization. In this paper, different from existing works, we study the inverse covariance estimation problem from another perspective by efficiently modeling the low-rank structure in the inverse covariance, which is assumed to be a combination of a low-rank part and a diagonal matrix. One motivation for this assumption is that the low-rank structure is common in many applications including the climate and financial analysis, and another one is that such assumption can reduce the computational complexity when computing its inverse. Specifically, we propose an efficient COmponent Pursuit (COP) method to obtain the low-rank part, where each component can be sparse. For optimization, the COP method greedily learns a rank-one component in each iteration by maximizing the log-likelihood. Moreover, the COP algorithm enjoys several appealing properties including the existence of an efficient solution in each iteration and the theoretical guarantee on the convergence of this greedy approach. Experiments on large-scale synthetic and real-world datasets including thousands of millions variables show that the COP method is faster than the state-of-the-art techniques for the inverse covariance estimation problem when achieving comparable log-likelihood on test data.",2016,Knowledge Discovery and Data Mining,covariance intersection;covariance function;covariance;greedy algorithm;estimation of covariance matrices;econometrics;machine learning;mathematical optimization;statistics;computer science;mathematics;
A Text Clustering Algorithm Using an Online Clustering Scheme for Initialization,Jianhua Yin (Tsinghua University);Jianyong Wang (Tsinghua University);,"2145412373,2105625159","In this paper, we propose a text clustering algorithm using an online clustering scheme for initialization called FGSDMM+. FGSDMM+ assumes that there are at most K max clusters in the corpus, and regards these K max potential clusters as one large potential cluster at the beginning. During initialization, FGSDMM+ processes the documents one by one in an online clustering scheme. The first document will choose the potential cluster, and FGSDMM+ will create a new cluster to store this document. Later documents will choose one of the non-empty clusters or the potential cluster with probabilities derived from the Dirichlet multinomial mixture model. Each time a document chooses the potential cluster, FGSDMM+ will create a new cluster to store that document and decrease the probability of later documents choosing the potential cluster. After initialization, FGSDMM+ will run a collapsed Gibbs sampling algorithm several times to obtain the final clustering result. Our extensive experimental study shows that FGSDMM+ can achieve better performance than three other clustering methods on both short and long text datasets.",2016,Knowledge Discovery and Data Mining,flame clustering;k medians clustering;canopy clustering algorithm;complete linkage clustering;correlation clustering;constrained clustering;data stream clustering;cure data clustering algorithm;single linkage clustering;fuzzy clustering;clustering high dimensional data;gibbs sampling;cluster analysis;document clustering;data mining;pattern recognition;machine learning;computer science;
Regime Shifts in Streams: Real-time Forecasting of Co-evolving Time Sequences,Yasuko Matsubara (Kumamoto University);Yasushi Sakurai (Kumamoto University);,"2106416586,1989929707","Given a large, online stream of multiple co-evolving event sequences, such as sensor data and Web-click logs, that contains various types of non-linear dynamic evolving patterns of different durations, how can we efficiently and effectively capture important patterns? How do we go about forecasting long-term future events? In this paper, we present REGIMECAST, an efficient and effective method for forecasting co-evolving data streams. REGIMECAST is designed as an adaptive non-linear dynamical system, which is inspired by the concept of ""regime shifts"" in natural dynamical systems. Our method has the following properties: (a) Effective: it operates on large data streams, captures important patterns and performs long-term forecasting; (b) Adaptive: it automatically and incrementally recognizes the latent trends and dynamic evolution patterns (i.e., regimes) that are unknown in advance; (c) Scalable: it is fast and the computation cost does not depend on the length of data streams; (d) Any-time: it provides a response at any time and generates long-range future events. Extensive experiments on real datasets demonstrate that REGIMECAST does indeed make long-range forecasts, and it outperforms state-of-the-art competitors as regards accuracy and speed.",2016,Knowledge Discovery and Data Mining,time series;data mining;real time computing;simulation;statistics;computer science;
Identifying Decision Makers from Professional Social Networks,Shipeng Yu (LinkedIn);Evangelia Christakopoulou (University of Minnesota);Abhishek Gupta (LinkedIn);,"2723179103,77291933,2101578767","Sales professionals help organizations win clients for products and services. Generating new clients starts with identifying the right decision makers at the target organization. For the past decade, online professional networks have collected tremendous amount of data on people's identity, their network and behavior data of buyers and sellers building relationships with each other for a variety of use-cases. Sales professionals are increasingly relying on these networks to research, identify and reach out to potential prospects, but it is often hard to find the right people effectively and efficiently. In this paper we present LDMS, the LinkedIn Decision Maker Score, to quantify the ability of making a sales decision for each of the 400M+ LinkedIn members. It is the key data-driven technology underlying Sales Navigator, a proprietary LinkedIn product that is designed for sales professionals. We will specifically discuss the modeling challenges of LDMS, and present two graph-based approaches to tackle this problem by leveraging the professional network data at LinkedIn. Both approaches are able to leverage both the graph information and the contextual information on the vertices, deal with small amount of labels on the graph, and handle heterogeneous graphs among different types of vertices. We will show some offline evaluations of LDMS on historical data, and also discuss its online usage in multiple applications in live production systems as well as future use cases within the LinkedIn ecosystem.",2016,Knowledge Discovery and Data Mining,management science;data mining;machine learning;
Predictors without Borders: Behavioral Modeling of Product Adoption in Three Developing Countries,Muhammad Raza Khan (University of Washington);Joshua E. Blumenstock (University of Washington);,"2529948456,2501829080","Billions of people around the world live without access to banks or other formal financial institutions. In the past several years, many mobile operators have launched ""Mobile Money"" platforms that deliver basic financial services over the mobile phone network. While many believe that these services can improve the lives of the poor, in many countries adoption of Mobile Money still remains anemic. In this paper, we develop a predictive model of Mobile Money adoption that uses billions of mobile phone communications records to understand the behavioral determinants of adoption. We describe a novel approach to feature engineering that uses a Deterministic Finite Automaton to construct thousands of behavioral metrics of phone use from a concise set of recursive rules. These features provide the foundation for a predictive model that is tested on mobile phone operators logs from Ghana, Pakistan, and Zambia, three very different developing-country contexts. The results highlight the key correlates of Mobile Money use in each country, as well as the potential for such methods to predict and drive adoption. More generally, our analysis provides insight into the extent to which homogenized supervised learning methods can generalize across geographic contexts. We find that without careful tuning, a model that performs very well in one country frequently does not generalize to another.",2016,Knowledge Discovery and Data Mining,gradient boosting;mobile payment;feature;supervised learning;data mining;machine learning;simulation;computer science;
Graph Wavelets via Sparse Cuts,"Arlei Lopes da Silva (University of California, Santa Barbara);Xuan-Hong Dang (University of California, Santa Barbara);Prithwish Basu (BBN Technologies);Ambuj Singh (University of California, Santa Barbara);Ananthram Swami (United States Army Research Laboratory);","2166555088,1964285635,2111459030,2099219664,2059211748","Modeling information that resides on vertices of large graphs is a key problem in several real-life applications, ranging from social networks to the Internet-of-things. Signal Processing on Graphs and, in particular, graph wavelets can exploit the intrinsic smoothness of these datasets in order to represent them in a compact and accurate manner. However, how to discover wavelet bases that capture the geometry of the data with respect to the signal as well as the graph structure remains an open problem. In this paper, we study the problem of computing graph wavelet bases via sparse cuts in order to produce low-dimensional encodings of data-driven bases. This problem is connected to known hard problems in graph theory (e.g. multiway cuts) and thus requires an efficient heuristic. We formulate the basis discovery task as a relaxation of a vector optimization problem, which leads to an elegant solution as a regularized eigenvalue computation. Moreover, we propose several strategies in order to scale our algorithm to large graphs. Experimental results show that the proposed algorithm can effectively encode both the graph structure and signal, producing compressed and accurate representations for vertex values in a wide range of datasets (e.g. sensor and gene networks) and significantly outperforming the best baseline.",2016,Knowledge Discovery and Data Mining,graph cuts in computer vision;distance hereditary graph;strength of a graph;voltage graph;complement graph;graph bandwidth;graph power;comparability graph;lattice graph;level structure;null graph;clique width;graph property;modular decomposition;graph;spectral theory;wavelet;combinatorics;machine learning;mathematical optimization;mathematics;
Online Context-Aware Recommendation with Time Varying Multi-Armed Bandit,Chunqiu Zeng (Florida International University);Qing Wang (Florida International University);Shekoofeh Mokhtari (Florida International University);Tao Li (Florida International University);,"2155150411,2564420227,2515803648,2472069284","Contextual multi-armed bandit problems have gained increasing popularity and attention in recent years due to their capability of leveraging contextual information to deliver online personalized recommendation services (e.g., online advertising and news article selection). To predict the reward of each arm given a particular context, existing relevant research studies for contextual multi-armed bandit problems often assume the existence of a fixed yet unknown reward mapping function. However, this assumption rarely holds in practice, since real-world problems often involve underlying processes that are dynamically evolving over time. In this paper, we study the time varying contextual multi-armed problem where the reward mapping function changes over time. In particular, we propose a dynamical context drift model based on particle learning. In the proposed model, the drift on the reward mapping function is explicitly modeled as a set of random walk particles, where good fitted particles are selected to learn the mapping dynamically. Taking advantage of the fully adaptive inference strategy of particle learning, our model is able to effectively capture the context change and learn the latent parameters. In addition, those learnt parameters can be naturally integrated into existing multi-arm selection strategies such as LinUCB and Thompson sampling . Empirical studies on two real-world applications, including online personalized advertising and news recommendation, demonstrate the effectiveness of our proposed approach. The experimental results also show that our algorithm can dynamically track the changing reward over time and consequently improve the click-through rate.",2016,Knowledge Discovery and Data Mining,personalization;recommender system;data mining;artificial intelligence;machine learning;simulation;computer science;
Domain Adaptation in the Absence of Source Domain Data,Boris Chidlovskii (Xerox);Stephane Clinchant (Xerox);Gabriela Csurka (Xerox);,"36378520,288365595,2124675631","The overwhelming majority of existing domain adaptation methods makes an assumption of freely available source domain data. An equal access to both source and target data makes it possible to measure the discrepancy between their distributions and to build representations common to both target and source domains. In reality, such a simplifying assumption rarely holds, since source data are routinely a subject of legal and contractual constraints between data owners and data customers. When source domain data can not be accessed, decision making procedures are often available for adaptation nevertheless. These procedures are often presented in the form of classification, identification, ranking etc. rules trained on source data and made ready for a direct deployment and later reuse. In other cases, the owner of a source data is allowed to share a few representative examples such as class means. In this paper we address the domain adaptation problem in real world applications, where the reuse of source domain data is limited to classification rules or a few representative examples. We extend the recent techniques of feature corruption and their marginalization , both in supervised and unsupervised settings. We test and compare them on private and publicly available source datasets and show that significant performance gains can be achieved despite the absence of source data and shortage of labeled target data.",2016,Knowledge Discovery and Data Mining,social exclusion;biological classification;data science;data mining;machine learning;statistics;computer science;
Sampling of Attributed Networks from Hierarchical Generative Models,Pablo Robles (Purdue University);Sebastian Moreno (Adolfo Ibáñez University);Jennifer Neville (Purdue University);,"2480430435,2104332529,2124572662","Network sampling is a widely used procedure in social network analysis where a random network is sampled from a generative network model (GNM). Recently proposed GNMs, allow generation of networks with more realistic structural characteristics than earlier ones. This facilitates tasks such as hypothesis testing and sensitivity analysis. However, sampling of networks with correlated vertex attributes remains a challenging problem. While the recent work of \cite{Pfeiffer:14} has provided a promising approach for attributed-network sampling, the approach was developed for use with relatively simple GNMs and does not work well with more complex hierarchical GNMs (which can model the range of characteristics and variation observed in real world networks more accurately). In contrast to simple GNMs where the probability mass is spread throughout the space of edges more evenly, hierarchical GNMs concentrate the mass to smaller regions of the space to reflect dependencies among edges in the network---this produces more realistic network characteristics, but also makes it more difficult to identify candidate networks from the sampling space. In this paper, we propose a novel sampling method, CSAG, to sample from hierarchical GNMs and generate networks with correlated attributes. CSAG constrains every step of the sampling process to consider the structure of the GNM---in order to bias the search to regions of the space with higher likelihood. We implemented CSAG using mixed Kronecker Product Graph Models and evaluated our approach on three real-world datasets. The results show that CSAG jointly models the correlation and structure of the networks better than the state of the art. Specifically, CSAG maintains the variability of the underlying GNM while providing a ≥ 5X reduction in attribute correlation error.",2016,Knowledge Discovery and Data Mining,complex network;data mining;machine learning;statistics;computer science;mathematics;
Latent Space Model for Road Networks to Predict Time-Varying Traffic,Dingxiong Deng (University of Southern California);Cyrus Shahabi (University of Southern California);Ugur Demiryurek (University of Southern California);Linhong Zhu (Information Sciences Institute);Rose Yu (University of Southern California);Yan Liu (University of Southern California);,"2109638660,240820708,319304351,2114458094,2107161032,2240541904","Real-time traffic prediction from high-fidelity spatiotemporal traffic sensor datasets is an important problem for intelligent transportation systems and sustainability. However, it is challenging due to the complex topological dependencies and high dynamism associated with changing road conditions. In this paper, we propose a Latent Space Model for Road Networks (LSM-RN) to address these challenges holistically. In particular, given a series of road network snapshots, we learn the attributes of vertices in latent spaces which capture both topological and temporal properties. As these latent attributes are time-dependent, they can estimate how traffic patterns form and evolve. In addition, we present an incremental online algorithm which sequentially and adaptively learns the latent attributes from the temporal graph changes. Our framework enables real-time traffic prediction by 1) exploiting real-time sensor readings to adjust/update the existing latent spaces, and 2) training as data arrives and making predictions on-the-fly. By conducting extensive experiments with a large volume of real-world traffic sensor data, we demonstrate the superiority of our framework for real-time traffic prediction on large road networks over competitors as well as baseline graph-based LSM's.",2016,Knowledge Discovery and Data Mining,data mining;machine learning;simulation;
Multi-Task Feature Interaction Learning,Kaixiang Lin (Michigan State University);Jianpeng Xu (Michigan State University);Inci M. Baytas (Michigan State University);Shuiwang Ji (Washington State University);Jiayu Zhou (Michigan State University);,"2644838670,2228111156,2520407735,2149659377,2629643112","One major limitation of linear models is the lack of capability to capture predictive information from interactions between features. While introducing high-order feature interaction terms can overcome this limitation, this approach tremendously increases the model complexity and imposes significant challenges in the learning against overfitting. In this paper, we proposed a novel Multi-Task feature Interaction Learning~(MTIL) framework to exploit the task relatedness from high-order feature interactions, which provides better generalization performance by inductive transfer among tasks via shared representations of feature interactions. We formulate two concrete approaches under this framework and provide efficient algorithms: the shared interaction approach and the embedded interaction approach. The former assumes tasks share the same set of interactions, and the latter assumes feature interactions from multiple tasks come from a shared subspace. We have provided efficient algorithms for solving the two approaches. Extensive empirical studies on both synthetic and real datasets have demonstrated the effectiveness of the proposed framework.",2016,Knowledge Discovery and Data Mining,multi task learning;data mining;pattern recognition;machine learning;computer science;
Generalized Hierarchical Sparse Model for Arbitrary-Order Interactive Antigenic Sites Identification in Flu Virus Data,Lei Han (Rutgers University);Yu Zhang (Hong Kong University of Science and Technology);Xiu-Feng Wan (Mississippi State University);Tong Zhang (Rutgers University);,"2531318857,2648094648,2718011926,2510858842","Recent statistical evidence has shown that a regression model by incorporating the interactions among the original covariates (features) can significantly improve the interpretability for biological data. One major challenge is the exponentially expanded feature space when adding high-order feature interactions to the model. To tackle the huge dimensionality, Hierarchical Sparse Models (HSM) are developed by enforcing sparsity under heredity structures in the interactions among the covariates. However, existing methods only consider pairwise interactions, making the discovery of important high-order interactions a non-trivial open problem. In this paper, we propose a Generalized Hierarchical Sparse Model (GHSM) as a generalization of the HSM models to learn arbitrary-order interactions. The GHSM applies the l 1 penalty to all the model coefficients under a constraint that given any covariate, if none of its associated k th-order interactions contribute to the regression model, then neither do its associated higher-order interactions. The resulting objective function is non-convex with a challenge lying in the coupled variables appearing in the arbitrary-order hierarchical constraints and we devise an efficient optimization algorithm to directly solve it. Specifically, we decouple the variables in the constraints via both the GIST and ADMM methods into three subproblems, each of which is proved to admit an efficiently analytical solution. We evaluate the GHSM method in both synthetic problem and the antigenic sites identification problem for the flu virus data, where we expand the feature space up to the 5th-order interactions. Empirical results demonstrate the effectiveness and efficiency of the proposed method and the learned high-order interactions have meaningful synergistic covariate patterns in the virus antigenicity.",2016,Knowledge Discovery and Data Mining,econometrics;data mining;machine learning;statistics;mathematics;
FASCINATE: Fast Cross-Layer Dependency Inference on Multi-layered Networks,Chen Chen (Arizona State University);Hanghang Tong (Arizona State University);Lei Xie (City University of New York);Lei Ying (Arizona State University);Qing He (University at Buffalo);,"2311808467,2667261544,2580828074,2686501891,2251990706","Multi-layered networks have recently emerged as a new network model, which naturally finds itself in many high-impact application domains, ranging from critical inter-dependent infrastructure networks, biological systems, organization-level collaborations, to cross-platform e-commerce, etc. Cross-layer dependency, which describes the dependencies or the associations between nodes across different layers/networks, often plays a central role in many data mining tasks on such multi-layered networks. Yet, it remains a daunting task to accurately know the cross-layer dependency a prior. In this paper, we address the problem of inferring the missing cross-layer dependencies on multi-layered networks. The key idea behind our method is to view it as a collective collaborative filtering problem. By formulating the problem into a regularized optimization model, we propose an effective algorithm to find the local optima with linear complexity. Furthermore, we derive an online algorithm to accommodate newly arrived nodes, whose complexity is just linear wrt the size of the neighborhood of the new node. We perform extensive empirical evaluations to demonstrate the effectiveness and the efficiency of the proposed methods.",2016,Knowledge Discovery and Data Mining,dependency theory;distributed computing;data mining;machine learning;computer science;
EMBERS at 4 years: Experiences operating an Open Source Indicators Forecasting System,"Sathappan Muthiah (Virginia Tech);Patrick Butler (Virginia Tech);Rupinder Paul Khandpur (Virginia Tech);Parang Saraf (Virginia Tech);Nathan Self (Virginia Tech);Alla Rozovskaya (Virginia Tech);Liang Zhao (Virginia Tech);Jose Cadena (Virginia Tech);Chang-Tien Lu (Virginia Tech);Anil Vullikanti (Virginia Tech);Achla Marathe (Virginia Tech);Kristen Maria Summers (IBM);Graham Katz (CACI);Andy Doyle (CACI);Jaime Arredondo (University of California, San Diego);Dipak K. Gupta (San Diego State University);David Mares (University of California, San Diego);Naren Ramakrishnan (Virginia Tech);","1509108369,2293049679,2054069009,2160730744,2093704914,2518199447,2619584304,2153027628,2112878203,393896382,2111312453,2175894886,2104271048,2238455168,2135686031,2142889206,2224701853,2199255697","EMBERS is an anticipatory intelligence system forecasting population-level events in multiple countries of Latin America. A deployed system from 2012, EMBERS has been generating alerts 24x7 by ingesting a broad range of data sources including news, blogs, tweets, machine coded events,currency rates, and food prices. In this paper, we describe our experiences operating EMBERS continuously for nearly 4 years, with specific attention to the discoveries it has enabled, correct as well as missed forecasts, lessons learnt from participating in a forecasting tournament, and our perspectives on the limits of forecasting including ethical considerations.",2016,Knowledge Discovery and Data Mining,operations research;world wide web;telecommunications;management;computer security;data mining;artificial intelligence;simulation;
Efficient Processing of Network Proximity Queries via Chebyshev Acceleration,Mustafa Coskun (Case Western Reserve University);Ananth Grama (Purdue University);Mehmet Koyuturk (Case Western Reserve University);,"2280576955,245679212,128763826","Network proximity is at the heart of a large class of network analytics and information retrieval techniques, including node/ edge rankings, network alignment, and randomwalk based proximity queries, among many others. Owing to its importance, significant effort has been devoted to accelerating iterative processes underlying network proximity computations. These techniques rely on numerical properties of power iterations, as well as structural properties of the networks to reduce the run time of iterative algorithms. In this paper, we present an alternate approach to acceleration of network proximity queries using Chebyshev polynomials. We show that our approach, called CHOPPER, yields asymptotically faster convergence in theory, and significantly reduced convergence times in practice. We also show that other existing acceleration techniques can be used in conjunction with Chopper to further reduce runtime. Using a number of large real-world networks, and top- k proximity queries as the benchmark problem, we show that CHOPPER outperforms existing methods for wide ranges of parameter values. CHOPPER is implemented in Matlab and is freely available at http://compbio.case.edu/chopper/.",2016,Knowledge Discovery and Data Mining,chebyshev polynomials;theoretical computer science;data mining;real time computing;machine learning;mathematical optimization;statistics;computer science;
From Online Behaviors to Offline Retailing,Ping Luo (Chinese Academy of Sciences);Su Yan (Chinese Academy of Sciences);Zhiqiang Liu (Baidu);Zhiyong Shen (Baidu);Shengwen Yang (Baidu);Qing He (Chinese Academy of Sciences);,"2291210646,2599389837,2514157585,2096063433,2652284171,2167314737","To combat the ease of online shopping in pajamas, offline mall owners focus increasingly on driving satisfaction and improving retention by identifying customers' preferences. However, most of these studies are based on customers' offline consuming history only. Benefiting from the internet, we can also get customers' online behaviors, such as the search logs, web browsing logs, online shopping logs, and so on. Might these seemingly irrelevant information from two different modalities (i.e. online and offline) be somehow interrelated? How can we make use of the online behaviors and offline actions jointly to promote recommendation for offline retailing? In this study, we formulate this task as a cross-modality recommendation problem, and present its solution via a proposed probabilistic graphical model, called Online-to-Offline Topic Modeling (O2OTM). Specifically, this method explicitly models the relationships between online and offline topics so that the likelihood of both online and offline behaviors is maximized. Then, the recommendation is made only based on the pairs of online and offline topics, denoted by (t,l), with high values of lift , such that the existence of the online topic $t$ greatly increases the response on the corresponding offline topic $l$ compared with the average response for the population without the online topic t. Furthermore, we evaluate this solution in both live and retrospect experiments. The real-world deployment of this model for the anniversary promotion campaign of a famous shopping mall in Beijing shows that our approach increases the occurred customer purchases per promotion message by 29.75\% compared with the baseline. Also, our model finds some interesting interpretable relationships between the online search topics and offline brand topics.",2016,Knowledge Discovery and Data Mining,topic model;world wide web;data mining;machine learning;computer science;
Safe Pattern Pruning: An Efficient Approach for Predictive Pattern Mining,Kazuya Nakagawa (Nagoya Institute of Technology);Shinya Suzumura (Nagoya Institute of Technology);Masayuki Karasuyama (Nagoya Institute of Technology);Koji Tsuda (University of Tokyo);Ichiro Takeuchi (Nagoya Institute of Technology);,"2280734645,2090618807,1256560114,1858955830,2619145315","In this paper we study predictive pattern mining problems where the goal is to construct a predictive model based on a subset of predictive patterns in the database. Our main contribution is to introduce a novel method called safe pattern pruning (SPP ) for a class of predictive pattern mining problems. The SPP method allows us to efficiently find a superset of all the predictive patterns in the database that are needed for the optimal predictive model. The advantage of the SPP method over existing boosting-type method is that the former can find the superset by a single search over the database, while the latter requires multiple searches. The SPP method is inspired by recent development of safe feature screening. In order to extend the idea of safe feature screening into predictive pattern mining, we derive a novel pruning rule called safe pattern pruning (SPP) rule that can be used for searching over the tree defined among patterns in the database. The SPP rule has a property that, if a node corresponding to a pattern in the database is pruned out by the SPP rule, then it is guaranteed that all the patterns corresponding to its descendant nodes are never needed for the optimal predictive model. We apply the SPP method to graph mining and item-set mining problems, and demonstrate its computational advantage.",2016,Knowledge Discovery and Data Mining,convex optimization;data mining;pattern recognition;machine learning;computer science;mathematics;
"Leveraging Propagation for Data Mining: Models, Algorithms and Applications",B. Aditya Prakash (Virginia Tech);Naren Ramakrishnan (Virginia Tech);,"2124002246,2199255697","Can we infer if a user is sick from her tweet? How do opinions get formed in online forums? Which people should we immunize to prevent an epidemic as fast as possible? How do we quickly zoom out of a graph? Graphs---also known as networks---are powerful tools for modeling processes and situations of interest in real life domains of social systems, cyber-security, epidemiology, and biology. They are ubiquitous, from online social networks, gene-regulatory networks, to router graphs. This tutorial will cover recent and state-of-the-art research on how propagation-like processes can help big-data mining specifically involving large networks and time-series, algorithms behind network problems, and their practical applications in various diverse settings. Topics include diffusion and virus propagation in networks, anomaly and outbreak detection, event prediction and connections with work in public health, the web and online media, social sciences, humanities, and cyber-security.",2016,Knowledge Discovery and Data Mining,social media;diffusion;public health;data science;data mining;machine learning;simulation;computer science;
Rebalancing Bike Sharing Systems: A Multi-source Data Smart Optimization,Junming Liu (Rutgers–Newark);Leilei Sun (Dalian University of Technology);Weiwei Chen (Rutgers–Newark);Hui Xiong (Rutgers–Newark);,"2226988312,2098357117,2669133569,2153710278","Bike sharing systems, aiming at providing the missing links in public transportation systems, are becoming popular in urban cities. A key to success for a bike sharing systems is the effectiveness of rebalancing operations, that is, the efforts of restoring the number of bikes in each station to its target value by routing vehicles through pick-up and drop-off operations. There are two major issues for this bike rebalancing problem: the determination of station inventory target level and the large scale multiple capacitated vehicle routing optimization with outlier stations. The key challenges include demand prediction accuracy for inventory target level determination, and an effective optimizer for vehicle routing with hundreds of stations. To this end, in this paper, we develop a Meteorology Similarity Weighted K-Nearest-Neighbor (MSWK) regressor to predict the station pick-up demand based on large-scale historic trip records. Based on further analysis on the station network constructed by station-station connections and the trip duration, we propose an inter station bike transition (ISBT) model to predict the station drop-off demand. Then, we provide a mixed integer nonlinear programming (MINLP) formulation of multiple capacitated bike routing problem with the objective of minimizing total travel distance. To solve it, we propose an Adaptive Capacity Constrained K-centers Clustering (AdaCCKC) algorithm to separate outlier stations (the demands of these stations are very large and make the optimization infeasible) and group the rest stations into clusters within which one vehicle is scheduled to redistribute bikes between stations. In this way, the large scale multiple vehicle routing problem is reduced to inner cluster one vehicle routing problem with guaranteed feasible solutions. Finally, the extensive experimental results on the NYC Citi Bike system show the advantages of our approach for bike demand prediction and large-scale bike rebalancing optimization.",2016,Knowledge Discovery and Data Mining,cluster analysis;machine learning;simulation;computer science;
IoT Big Data Stream Mining,Gianmarco De Francisci Morales (Qatar Computing Research Institute);Albert Bifet (Université Paris-Saclay);Latifur Khan (University of Texas at Dallas);Joao Gama (University of Porto);Wei Fan (Baidu);,"2153118160,307521372,2155983610,2113857198,2422054197","The challenge of deriving insights from the Internet of Things (IoT) has been recognized as one of the most exciting and key opportunities for both academia and industry. Advanced analysis of big data streams from sensors and devices is bound to become a key area of data mining research as the number of applications requiring such processing increases. Dealing with the evolution over time of such data streams, i.e., with concepts that drift or change completely, is one of the core issues in IoT stream mining. This tutorial is a gentle introduction to mining IoT big data streams. The first part introduces data stream learners for classification, regression, clustering, and frequent pattern mining. The second part deals with scalability issues inherent in IoT applications, and discusses how to mine data streams on distributed engines such as Spark, Flink, Storm, and Samza.",2016,Knowledge Discovery and Data Mining,internet of things;data stream mining;big data;data science;world wide web;data mining;computer science;
Audience Expansion for Online Social Network Advertising,Haishan Liu (LinkedIn);David Pardoe (LinkedIn);Kun Liu (LinkedIn);Manoj Thakur (LinkedIn);Frank Cao (LinkedIn);Chongzhe Li (LinkedIn);,"2720207618,2512113778,2636583453,2531597640,2531042109,2531212888","Online social network advertising platforms, such as that provided by LinkedIn, generally allow marketers to specify targeting options so that their ads appear to a desired demographic. Audience Expansion is a technique developed at LinkedIn to simplify targeting and identify new audiences with similar attributes to the original target audience. We developed two methods to achieve Audience Expansion: campaign-agnostic expansion and campaign-aware expansion. In this paper, we describe the details of these methods, present in-depth analysis of their trade-offs, and demonstrate a hybrid strategy that possesses the combined strength of both methods. Through large scale online experiments, we show the effectiveness of the proposed approach, and as a result, the benefits it brings to the whole marketplace including both LinkedIn and advertisers. The achieved benefits can be characterized as: 1) simplified targeting process and increased reach for advertisers, and 2) better utilization of LinkedIn's ads inventory and higher and more efficient market participation.",2016,Knowledge Discovery and Data Mining,online advertising;computer science;
How to Compete Online for News Audience: Modeling Words that Attract Clicks,Joon Hee Kim (KAIST);Amin Mantrach (Yahoo!);Alejandro Jaimes (Yahoo!);Alice H. Oh (KAIST);,"2492617592,2027707398,2193615068,2647629047","Headlines are particularly important for online news outlets where there are many similar news stories competing for users' attention. Traditionally, journalists have followed rules-of-thumb and experience to master the art of crafting catchy headlines, but with the valuable resource of large-scale click-through data of online news articles, we can apply quantitative analysis and text mining techniques to acquire an in-depth understanding of headlines. In this paper, we conduct a large-scale analysis and modeling of 150K news articles published over a period of four months on the Yahoo home page. We define a simple method to measure click-value of individual words, and analyze how temporal trends and linguistic attributes affect click-through rate (CTR). We then propose a novel generative model, headline click-based topic model (HCTM), that extends latent Dirichlet allocation (LDA) to reveal the effect of topical context on the click-value of words in headlines. HCTM leverages clicks in aggregate on previously published headlines to identify words for headlines that will generate more clicks in the future. We show that by jointly taking topics and clicks into account we can detect changes in user interests within topics. We evaluate HCTM in two different experimental settings and compare its performance with ALDA (adapted LDA), LDA, and TextRank. The first task, full headline, is to retrieve full headline used for a news article given the body of news article. The second task, good headline , is to specifically identify words in the headline that have high click values for current news audience. For full headline task, our model performs on par with ALDA, a state-of-the art web-page summarization method that utilizes click-through information. For good headline task, which is of more practical importance to both individual journalists and online news outlets, our model significantly outperforms all other comparative methods.",2016,Knowledge Discovery and Data Mining,click through rate;multimedia;world wide web;data mining;computer science;
Just One More: Modeling Binge Watching Behavior,William Trouleau (École Polytechnique Fédérale de Lausanne);Azin Ashkan (Technicolor);Weicong Ding (Technicolor);Brian Eriksson (Technicolor);,"2513603576,2075598693,2674195122,2147262100","Easy accessibility can often lead to over-consumption, as seen in food and alcohol habits. On video on-demand (VOD) services, this has recently been referred to as binge watching, where potentially entire seasons of TV shows are consumed in a single viewing session. While a user viewership model may reveal this binging behavior, creating an accurate model has several challenges, including censored data, deviations in the population, and the need to consider external influences on consumption habits. In this paper, we introduce a novel statistical mixture model that incorporates these factors and presents a first of its kind characterization of viewer consumption behavior using a real-world dataset that includes playback data from a VOD service. From our modeling, we tackle various predictive tasks to infer the consumption decisions of a user in a viewing session, including estimating the number of episodes they watch and classifying if they continue watching another episode. Using these insights, we then identify binge watching sessions based on deviation from normal viewing behavior. We observe different types of binging behavior, that binge watchers often view certain content out-of-order, and that binge watching is not a consistent behavior among our users. These insights and our findings have application in VOD revenue generation, consumer health applications, and customer retention analysis.",2016,Knowledge Discovery and Data Mining,mixture model;multimedia;machine learning;simulation;computer science;
Efficient Frequent Directions Algorithm for Sparse Matrices,Mina Ghashami (University of Utah);Edo Liberty (Yahoo!);Jeff M. Phillips (University of Utah);,"2010898942,1215165747,2162012049","This paper describes Sparse Frequent Directions, a variant of Frequent Directions for sketching sparse matrices. It resembles the original algorithm in many ways: both receive the rows of an input matrix A n x d one by one in the streaming setting and compute a small sketch B ∈ R l x d . Both share the same strong (provably optimal) asymptotic guarantees with respect to the space-accuracy tradeoff in the streaming setting. However, unlike Frequent Directions which runs in O ( ndl ) time regardless of the sparsity of the input matrix A , Sparse Frequent Directions runs in O (nnz( A ) l + nl 2 ) time. Our analysis loosens the dependence on computing the Singular Value Decomposition (SVD) as a black box within the Frequent Directions algorithm. Our bounds require recent results on the properties of fast approximate SVD computations. Finally, we empirically demonstrate that these asymptotic improvements are practical and significant on real and synthetic data.",2016,Knowledge Discovery and Data Mining,sparse matrix;theoretical computer science;combinatorics;mathematical optimization;algorithm;computer science;mathematics;
Improving the Sensitivity of Online Controlled Experiments: Case Studies at Netflix,Huizhi Xie (Netflix);Juliette Aurisset (Netflix);,"2521220520,2225021794","Controlled experiments are widely regarded as the most scientific way to establish a true causal relationship between product changes and their impact on business metrics. Many technology companies rely on such experiments as their main data-driven decision-making tool. The sensitivity of a controlled experiment refers to its ability to detect differences in business metrics due to product changes. At Netflix, with tens of millions of users, increasing the sensitivity of controlled experiments is critical as failure to detect a small effect, either positive or negative, can have a substantial revenue impact. This paper focuses on methods to increase sensitivity by reducing the sampling variance of business metrics. We define Netflix business metrics and share context around the critical need for improved sensitivity. We review popular variance reduction techniques that are broadly applicable to any type of controlled experiment and metric. We describe an innovative implementation of stratified sampling at Netflix where users are assigned to experiments in real time and discuss some surprising challenges with the implementation. We conduct case studies to compare these variance reduction techniques on a few Netflix datasets. Based on the empirical results, we recommend to use post-assignment variance reduction techniques such as post stratification and CUPED instead of at-assignment variance reduction techniques such as stratified sampling in large-scale controlled experiments.",2016,Knowledge Discovery and Data Mining,randomized experiment;variance reduction;sensitivity;econometrics;data mining;simulation;statistics;
How to Get Them a Dream Job?: Entity-Aware Features for Personalized Job Search Ranking,Jia Li (University of Illinois at Chicago);Dhruv Arya (LinkedIn);Viet Ha-Thuc (LinkedIn);Shakti Sinha (LinkedIn);,"2710729114,2231240302,2310324677,2155323900","This paper proposes an approach to applying standardized entity data to improve job search quality and to make search results more personalized. Specifically, we explore three types of entity-aware features and incorporate them into the job search ranking function. The first is query-job matching features which extract and standardize entities mentioned in queries and documents, then semantically match them based on these entities. The second type, searcher-job expertise homophily, aims to capture the fact that job searchers tend to be interested in the jobs requiring similar expertise as theirs. To measure the similarity, we use standardized skills in job descriptions and searchers' profiles as well as skills that we infer searchers might have but not explicitly list in their profiles. Third, we propose a concept of entity-faceted historical click-through-rates (CTRs) to capture job document quality. Faceting jobs by their standardized companies, titles, locations, etc., and computing historical CTRs at the facet level instead of individual job level alleviate sparseness issue in historical action data. This is particularly important in job search where job lifetime is typically short. Both offline and online experiments confirm the effectiveness of the features. In offline experiment, using the entity-aware features gives improvements of +20%, +12.1% and +8.3% on Precision@1, MRR and NDCG@25, respectively. Online A/B test shows that a new model with these features is +11.3% and +5.3% better than the baseline in terms of click-through-rate and apply rate.",2016,Knowledge Discovery and Data Mining,job analysis;personalization;world wide web;information retrieval;data mining;machine learning;computer science;
Scalable Learning of Graphical Models,Francois Petitjean (Monash University);Geoffrey I. Webb (Monash University);,"2709762332,2126304162","From understanding the structure of data, to classification and topic modeling, graphical models are core tools in machine learning and data mining. They combine probability and graph theories to form a compact representation of probability distributions. In the last decade, as data stores became larger and higher-dimensional, traditional algorithms for learning graphical models from data, with their lack of scalability, became less and less usable, thus directly decreasing the potential benefits of this core technology. To scale graphical modeling techniques to the size and dimensionality of most modern data stores, data science researchers and practitioners now have to meld the most recent advances in numerous specialized fields including graph theory, statistics, pattern mining and graphical modeling. This tutorial covers the core building blocks that are necessary to build and use scalable graphical modeling technologies on large and high-dimensional data.",2016,Knowledge Discovery and Data Mining,clustering high dimensional data;graphical model;data science;theoretical computer science;data mining;machine learning;statistics;computer science;
Distributing the Stochastic Gradient Sampler for Large-Scale LDA,Yuan Yang (Beihang University);Jianfei Chen (Tsinghua University);Jun Zhu (Tsinghua University);,"2435984540,2694226565,2305755055","Learning large-scale Latent Dirichlet Allocation (LDA) models is beneficial for many applications that involve large collections of documents.Recent work has been focusing on developing distributed algorithms in the batch setting, while leaving stochastic methods behind, which can effectively explore statistical redundancy in big data and thereby are complementary to distributed computing.The distributed stochastic gradient Langevin dynamics (DSGLD) represents one attempt to combine stochastic sampling and distributed computing, but it suffers from drawbacks such as excessive communications and sensitivity to partitioning of datasets across nodes. DSGLD is typically limited to learn small models that have about 10 3 topics and $10^3$ vocabulary size. In this paper, we present embarrassingly parallel SGLD (EPSGLD), a novel distributed stochastic gradient sampling method for topic models. Our sampler is built upon a divide-and-conquer architecture which enables us to produce robust and asymptotically exact samples with less communication overhead than DSGLD. We further propose several techniques to reduce the overhead in I/O and memory usage. Experiments on Wikipedia and ClueWeb12 documents demonstrate that, EPSGLD can scale up to large models with 10 10 parameters (i.e., 10 5 topics, 10 5 vocabulary size), four orders of magnitude larger than DSGLD, and converge faster.",2016,Knowledge Discovery and Data Mining,theoretical computer science;data mining;machine learning;statistics;computer science;
A Real Linear and Parallel Multiple Longest Common Subsequences (MLCS) Algorithm,Yanni Li (Xidian University);Hui Li (Xidian University);Tihua Duan (Shanghai Finance University);Sheng Wang;Zhi Wang (Xidian University);Yang Cheng (Xidian University);,"2668838690,2617926336,2512054100,2623250925,2512744947,2632713803","Information in various applications is often expressed as character sequences over a finite alphabet ( e.g. , DNA or protein sequences). In Big Data era, the lengths and sizes of these sequences are growing explosively, leading to grand challenges for the classical NP-hard problem, namely searching for the Multiple Longest Common Subsequences ( MLCS ) from multiple sequences. In this paper, we first unveil the fact that the state-of-the-art MLCS algorithms are unable to be applied to long and large-scale sequences alignments. To overcome their defects and tackle the longer and large-scale or even big sequences alignments, based on the proposed novel problem-solving model and various strategies, e.g. , parallel topological sorting, optimal calculating, reuse of intermediate results, subsection calculation and serialization, etc., we present a novel parallel MLCS algorithm. Exhaustive experiments on the datasets of both synthetic and real-world biological sequences demonstrate that both the time and space of the proposed algorithm are only linear in the number of dominants from aligned sequences, and the proposed algorithm significantly outperforms the state-of-the-art MLCS algorithms, being applicable to longer and large-scale sequences alignments.",2016,Knowledge Discovery and Data Mining,topological sorting;discrete mathematics;combinatorics;algorithm;computer science;mathematics;
QUINT: On Query-Specific Optimal Networks,Liangyue Li (Arizona State University);Yuan Yao (Nanjing University);Jie Tang (Tsinghua University);Wei Fan (Baidu);Hanghang Tong (Arizona State University);,"2144246580,2617797049,2158012360,2422054197,2676574370","Measuring node proximity on large scale networks is a fundamental building block in many application domains, ranging from computer vision, e-commerce, social networks, software engineering, disaster management to biology and epidemiology. The state of the art (e.g., random walk based methods) typically assumes the input network is given a priori, with the known network topology and the associated edge weights. A few recent works aim to further infer the optimal edge weights based on the side information. This paper generalizes the challenge in multiple dimensions, aiming to learn optimal networks for node proximity measures. First ( optimization scope ), our proposed formulation explores a much larger parameter space, so that it is able to simultaneously infer the optimal network topology and the associated edge weights. This is important as a noisy or missing edge could greatly mislead the network node proximity measures. Second ( optimization granularity ), while all the existing works assume one common optimal network, be it given as the input or learned by the algorithms, exists for all queries, our method performs optimization at a much finer granularity, essentially being able to infer an optimal network that is specific to a given query. Third ( optimization efficiency ), we carefully design our algorithms with a linear complexity wrt the neighborhood size of the user preference set. We perform extensive empirical evaluations on a diverse set of 10+ real networks, which show that the proposed algorithms (1) consistently outperform the existing methods on all six commonly used metrics; (2) empirically scale sub-linearly to billion-scale networks and (3) respond in a fraction of a second.",2016,Knowledge Discovery and Data Mining,data mining;machine learning;mathematical optimization;statistics;computer science;mathematics;
Privacy-preserving Class Ratio Estimation,Arun Shankar Iyer (Indian Institute of Technology Bombay);J. Saketha Nath (Indian Institute of Technology Bombay);Sunita Sarawagi (Indian Institute of Technology Bombay);,"2128888966,2137193185,156875573","In this paper we present learning models for the class ratio estimation problem, which takes as input an unlabeled set of instances and predicts the proportions of instances in the set belonging to the different classes. This problem has applications in social and commercial data analysis. Existing models for class-ratio estimation however require instance-level supervision. Whereas in domains like politics, and demography, set-level supervision is more common. We present a new method for directly estimating class-ratios using set-level supervision. Another serious limitation in applying these techniques to sensitive domains like health is data privacy. We propose a novel label privacy-preserving mechanism that is well-suited for supervised class ratio estimation and has guarantees for achieving efficient differential privacy, provided the per-class counts are large enough. We derive learning bounds for the estimation with and without privacy constraints, which lead to important insights for the data-publisher. Extensive empirical evaluation shows that our model is more accurate than existing methods and that the proposed privacy mechanism and learning model are well-suited for each other.",2016,Knowledge Discovery and Data Mining,differential privacy;kernel method;learning theory;econometrics;data mining;machine learning;statistics;computer science;mathematics;
Ranking Universities Based on Career Outcomes of Graduates,Navneet Kapur;Nikita I. Lytkin (LinkedIn);Bee-Chung Chen (LinkedIn);Deepak Agarwal (LinkedIn);Igor Perisic (LinkedIn);,"2592458651,2530163099,2152441490,2116605949,2234655699","Every year, millions of new students enter higher educational programs. Publicly available rankings of academic programs play a key role in prospective students' decisions regarding which universities to apply to and enroll in. While surveys indicate that majority of freshmen enter college to get good jobs after graduation, established methodologies for ranking universities rely on indirect indicators of career outcomes such as reputational assessments of the universities among academic peers, acceptance and graduation rates, learning environment, and availability of research funding. In addition, many of these methodologies rely on arbitrary choices of weighting factors for the different ranking indicators, and suffer from lack of analyses of statistical stability. In this paper, we addresses these challenges holistically by developing a novel methodology for ranking and recommending universities for different professions on the basis of career outcomes of professionals who graduated from those schools. Our methodology incorporates a number of techniques for achieving statistical stability, and represents a step towards personalized educational recommendations based on interests and ambitions of individuals. We have applied this methodology on LinkedIn's Economic Graph data of over 400 million professional from around the world. The resulting university rankings have been made available to the public and demonstrate that there are valuable insights to be gleaned from professional career data on LinkedIn.",2016,Knowledge Discovery and Data Mining,statistics;
Lightweight Monitoring of Distributed Streams,Arnon Lazerson (Technion – Israel Institute of Technology);Daniel Keren (University of Haifa);Assaf Schuster (Technion – Israel Institute of Technology);,"2288078023,2061587065,2135728993","As data becomes dynamic, large, and distributed, there is increasing demand for what have become known as distributed stream algorithms . Since continuously collecting the data to a central server and processing it there incurs very high communication and computation complexities, it is advantageous to define local conditions at the nodes, such that -- as long as they are maintained -- some desirable global condition holds. A generic algorithm which proved very useful for reducing communication in distributed streaming environments is geometric monitoring (GM). Alas, applying GM to many important tasks is computationally very demanding, as it requires solving a notoriously difficult problem -- computing the distance between a point and a surface, which is often very time-consuming even in low dimensions. Thus, while useful for reducing communication, GM often suffers from exceedingly heavy computational burden at the nodes, which renders it very problematic to apply, especially for ``thin'', battery-operated sensors, which are prevalent in numerous applications, including the ``Internet of Things'' paradigm. Here we propose a very different approach, designated CB (for Convex/Concave Bounds). CB is based on directly bounding the monitored function by suitably chosen convex and concave functions, that naturally enable monitoring distributed streams. These functions can be checked on the fly, yielding far simpler local conditions than those applied by GM. CB's superiority over GM is demonstrated in reducing computational complexity, by several orders of magnitude in some cases. As an added bonus, CB also reduced communication overhead in all application scenarios we tested.",2016,Knowledge Discovery and Data Mining,distributed computing;data mining;real time computing;machine learning;simulation;computer science;
Online Asymmetric Active Learning with Imbalanced Data,Xiaoxuan Zhang (University of Iowa);Tianbao Yang (University of Iowa);Padmini Srinivasan (University of Iowa);,"2508508812,2151859571,2237621063","This paper considers online learning with imbalanced streaming data under a query budget, where the act of querying for labels is constrained to a budget limit. We study different active querying strategies for classification. In particular, we propose an asymmetric active querying strategy that assigns different probabilities for query to examples predicted as positive and negative. To corroborate the proposed asymmetric query model, we provide a theoretical analysis on a weighted mistake bound. We conduct extensive evaluations of the proposed asymmetric active querying strategy in comparison with several baseline querying strategies and with previous online learning algorithms for imbalanced data. In particular, we perform two types of evaluations according to which examples appear as ``positive""/``negative''. In push evaluation only the positive predictions given to the user are taken into account; in push and query evaluation the decision to query is also considered for evaluation. The push and query evaluation strategy is particularly suited for a recommendation setting because the items selected for querying for labels may go to the end-user to enable customization and personalization. These would not be shown any differently to the end-user compared to recommended content (i.e., the examples predicated as positive). Additionally, given our interest in imbalanced data we measure F -score instead of accuracy that is traditionally considered by online classification algorithms. We also compare the querying strategies on five classification tasks from different domains, and show that the probabilistic query strategy achieves higher F -scores on both types of evaluation than deterministic strategy, especially when the budget is small, and the asymmetric query model further improves performance. When compared to the state-of-the-art cost-sensitive online learning algorithm under a budget, our online classification algorithm with asymmetric querying achieves a higher F -score on four of the five tasks, especially on the push evaluation.",2016,Knowledge Discovery and Data Mining,f1 score;web query classification;information retrieval;data mining;machine learning;computer science;
GLMix: Generalized Linear Mixed Models For Large-Scale Response Prediction,XianXing Zhang (LinkedIn);Yitong Zhou (LinkedIn);Yiming Ma (LinkedIn);Bee-Chung Chen (LinkedIn);Liang Zhang (LinkedIn);Deepak Agarwal (LinkedIn);,"2550559646,2294316712,2237551533,2152441490,2306956169,2116605949","Generalized linear model (GLM) is a widely used class of models for statistical inference and response prediction problems. For instance, in order to recommend relevant content to a user or optimize for revenue, many web companies use logistic regression models to predict the probability of the user's clicking on an item (e.g., ad, news article, job). In scenarios where the data is abundant, having a more fine-grained model at the user or item level would potentially lead to more accurate prediction, as the user's personal preferences on items and the item's specific attraction for users can be better captured. One common approach is to introduce ID-level regression coefficients in addition to the global regression coefficients in a GLM setting, and such models are called generalized linear mixed models (GLMix) in the statistical literature. However, for big data sets with a large number of ID-level coefficients, fitting a GLMix model can be computationally challenging. In this paper, we report how we successfully overcame the scalability bottleneck by applying parallelized block coordinate descent under the Bulk Synchronous Parallel (BSP) paradigm. We deployed the model in the LinkedIn job recommender system, and generated 20% to 40% more job applications for job seekers on LinkedIn.",2016,Knowledge Discovery and Data Mining,spark;statistical model;big data;data science;data mining;machine learning;statistics;computer science;
An Empirical Study on Recommendation with Multiple Types of Feedback,Liang Tang (LinkedIn);Bo Long (Yahoo!);Bee-Chung Chen (LinkedIn);Deepak Agarwal (LinkedIn);,"2520481619,2512786853,2152441490,2116605949","User feedback like clicks and ratings on recommended items provides important information for recommender systems to predict users' interests in unseen items. Most systems rely on models trained using a single type of feedback, e.g., ratings for movie recommendation and clicks for online news recommendation. However, in addition to the primary feedback, many systems also allow users to provide other types of feedback, e.g., liking or sharing an article, or hiding all articles from a source. These additional feedback potentially provides extra information for the recommendation models. To optimize user experience and business objectives, it is important for a recommender system to use both the primary feedback and additional feedback. This paper presents an empirical study on various training methods for incorporating multiple user feedback types based on LinkedIn recommendation products. We study three important problems that we face at LinkedIn: (1) Whether to send an email based on clicks and complaints, (2) how to rank updates in LinkedIn feeds based on clicks and hides and (3) how jointly optimize for viral actions and clicks in LinkedIn feeds. Extensive offline experiments on historical data show the effectiveness of these methods in different situations. Online A/B testing results further demonstrate the impact of these methods on LinkedIn production systems.",2016,Knowledge Discovery and Data Mining,multi objective optimization;recommender system;multimedia;world wide web;data mining;machine learning;computer science;
Email Volume Optimization at LinkedIn,Rupesh Gupta (LinkedIn);Guanfeng Liang (LinkedIn);Hsiao-Ping Tseng (LinkedIn);Ravi Kiran Holur Vijay (LinkedIn);Xiaoyu Chen (LinkedIn);Romer Rosales (LinkedIn);,"2299393997,2709775982,2224987450,2528836707,2509170365,2117656073","Online social networking services distribute various types of messages to their members. Common types of messages include news, connection requests, membership notifications, promotions and event notifications. Such communication, if used judiciously, can provide an enormous value to members thereby keeping them engaged. However sending a message for every instance of news, connection request, or the like can result in an overwhelming number of messages in a member's mailbox. This may result in reduced effectiveness of communication if the messages are not sufficiently relevant to the member's interests. It may also result in a poor brand perception of the networking service. In this paper we discuss our strategy and experience with regard to the problem of email volume optimization at LinkedIn. In particular, we present a cost-benefit analysis of sending emails, the key factors to administer an effective volume optimization, our algorithm for volume optimization, the architecture of the supporting system and experimental results from online A/B tests.",2016,Knowledge Discovery and Data Mining,internet privacy;multimedia;world wide web;data mining;machine learning;computer science;
Text Mining in Clinical Domain: Dealing with Noise,Hoang Nguyen (Commonwealth Scientific and Industrial Research Organisation);Jon Patrick (University of Sydney);,"2697665893,2630747136","Text mining in clinical domain is usually more difficult than general domains (e.g. newswire reports and scientific literature) because of the high level of noise in both the corpus and training data for machine learning (ML). A large number of unknown word, non-word and poor grammatical sentences made up the noise in the clinical corpus. Unknown words are usually complex medical vocabularies, misspellings, acronyms and abbreviations where unknown non-words are generally the clinical patterns including scores and measures. This noise produces obstacles in the initial lexical processing step as well as subsequent semantic analysis. Furthermore, the labelled data used to build ML models is very costly to obtain because it requires intensive clinical knowledge from the annotators. And even created by experts, the training examples usually contain errors and inconsistencies due to the variations in human annotators' attentiveness. Clinical domain also suffers from the nature of the imbalanced data distribution problem. These kinds of noise are very popular and potentially affect the overall information extraction performance but they were not carefully investigated in most presented health informatics systems. This paper introduces a general clinical data mining architecture which is potential of addressing all of these challenges using: automatic proof-reading process, trainable finite state pattern recogniser, iterative model development and active learning. The reportability classifier based on this architecture achieved 98.25% sensitivity and 96.14% specificity on an Australian cancer registry's held-out test set and up to 92% of training data provided for supervised ML was saved by active learning.",2016,Knowledge Discovery and Data Mining,active learning;natural language processing;speech recognition;data mining;machine learning;computer science;
FLASH: Fast Bayesian Optimization for Data Analytic Pipelines,Yuyu Zhang (Georgia Institute of Technology);Mohammad Taha Bahadori (Georgia Institute of Technology);Hang Su (Georgia Institute of Technology);Jimeng Sun (Georgia Institute of Technology);,"2525312675,2032867848,2627558300,2110385854","Modern data science relies on data analytic pipelines to organize interdependent computational steps. Such analytic pipelines often involve different algorithms across multiple steps, each with its own hyperparameters. To achieve the best performance, it is often critical to select optimal algorithms and to set appropriate hyperparameters, which requires large computational efforts. Bayesian optimization provides a principled way for searching optimal hyperparameters for a single algorithm. However, many challenges remain in solving pipeline optimization problems with high-dimensional and highly conditional search space. In this work, we propose Fast LineAr SearcH (FLASH), an efficient method for tuning analytic pipelines. FLASH is a two-layer Bayesian optimization framework, which firstly uses a parametric model to select promising algorithms, then computes a nonparametric model to fine-tune hyperparameters of the promising algorithms. FLASH also includes an effective caching algorithm which can further accelerate the search process. Extensive experiments on a number of benchmark datasets have demonstrated that FLASH significantly outperforms previous state-of-the-art methods in both search speed and accuracy. Using 50% of the time budget, FLASH achieves up to 20% improvement on test error rate compared to the baselines. FLASH also yields state-of-the-art performance on a real-world application for healthcare predictive modeling.",2016,Knowledge Discovery and Data Mining,data science;data mining;machine learning;statistics;computer science;
NetCycle: Collective Evolution Inference in Heterogeneous Information Networks,Yizhou Zhang (Fudan University);Yun Xiong (Fudan University);Xiangnan Kong (Worcester Polytechnic Institute);Yangyong Zhu (Fudan University);,"2512584869,2638681803,2204127537,2661458478","Collective inference has attracted considerable attention in the last decade, where the response variables within a group of instances are correlated and should be inferred collectively, instead of independently. Previous works on collective inference mainly focus on exploiting the autocorrelation among instances in a static network during the inference process. There are also approaches on time series prediction, which mainly exploit the autocorrelation within an instance at different time points during the inference process. However, in many real-world applications, the response variables of related instances can co-evolve over time and their evolutions are not following a static correlation across time, but are following an internal life cycle. In this paper, we study the problem of collective evolution inference, where the goal is to predict the values of the response variables for a group of related instances at the end of their life cycles. This problem is extremely important for various applications, e.g., predicting fund-raising results in crowd-funding and predicting gene-expression levels in bioinformatics. This problem is also highly challenging because different instances in the network can co-evolve over time and they can be at different stages of their life cycles and thus have different evolving patterns. Moreover, the instances in collective evolution inference problems are usually connected through heterogeneous information networks, which involve complex relationships among the instances interconnected by multiple types of links. We propose an approach, called NetCycle, by incorporating information from both the correlation among related instances and their life cycles. We compared our approach with existing methods of collective inference and time series analysis on two real-world networks. The results demonstrate that our proposed approach can improve the inference performance by considering the autocorrelation through networks and the life cycles of the instances.",2016,Knowledge Discovery and Data Mining,data mining;artificial intelligence;machine learning;statistics;computer science;mathematics;
When Recommendation Goes Wrong: Anomalous Link Discovery in Recommendation Networks,Bryan Perozzi (Stony Brook University);Michael Schueppert (Google);Jack Saalweachter (Google);Mayur Thakur (Missouri University of Science and Technology);,"1983756286,2227686788,2513242119,2235422528","We present a secondary ranking system to find and remove erroneous suggestions from a geospatial recommendation system. We discover such anomalous links by ""double checking"" the recommendation system's output to ensure that it is both structurally cohesive, and semantically consistent. Our approach is designed for the Google Related Places Graph, a geographic recommendation system which provides results for hundreds of millions of queries a day. We model the quality of a recommendation between two geographic entities as a function of their structure in the Related Places Graph, and their semantic relationship in the Google Knowledge Graph. To evaluate our approach, we perform a large scale human evaluation of such an anomalous link detection system. For the long tail of unpopular entities, our models can predict the recommendations users will consider poor with up to 42\% higher mean precision (29 raw points) than the live system. Results from our study reveal that structural and semantic features capture different facets of relatedness to human judges. We characterize our performance with a qualitative analysis detailing the categories of real-world anomalies our system is able to detect, and provide a discussion of additional applications of our method.",2016,Knowledge Discovery and Data Mining,anomaly detection;world wide web;information retrieval;data mining;machine learning;computer science;
Streaming Analytics,Ashish Gupta (LinkedIn);Neera Agarwal;,"2512098453,2688643513","Recently we have seen emergence and huge adoption of social media, internet of things for home, industrial internet of things, mobile applications and online transactions. These systems generate streaming data at very large scale. Building technologies and distributed systems that can capture, process and analyze this streaming data in real time is very important for gaining real time insights. Real-time analysis of streaming data can be used for applications as diverse as fraud detection, in-session targeting and recommendations, control systems for transportation systems and smarter cities, earthquake prediction and control of autonomous vehicles. This tutorial will provide overview of streaming systems and hands on tutorial on building streaming analytics systems using open source technologies.",2016,Knowledge Discovery and Data Mining,analytics;internet of things;stream;data science;world wide web;data mining;machine learning;computer science;
Partial Label Learning via Feature-Aware Disambiguation,Min-Ling Zhang (Southeast University);Bin-Bin Zhou (Southeast University);Xu-Ying Liu (Southeast University);,"2310402581,2517136083,2635414314","Partial label learning deals with the problem where each training example is represented by a feature vector while associated with a set of candidate labels, among which only one label is valid. To learn from such ambiguous labeling information, the key is to try to disambiguate the candidate label sets of partial label training examples. Existing disambiguation strategies work by either identifying the ground-truth label iteratively or treating each candidate label equally. Nonetheless, the disambiguation process is generally conducted by focusing on manipulating the label space, and thus ignores making full use of potentially useful information from the feature space. In this paper, a novel two-stage approach is proposed to learning from partial label examples based on feature-aware disambiguation. In the first stage, the manifold structure of feature space is utilized to generate normalized labeling confidences over candidate label set. In the second stage, the predictive model is learned by performing regularized multi-output regression over the generated labeling confidences. Extensive experiments on artificial as well as real-world partial label data sets clearly validate the superiority of the proposed feature-aware disambiguation approach.",2016,Knowledge Discovery and Data Mining,manifold;data mining;pattern recognition;machine learning;mathematics;
Learning Sparse Models at Scale,Ralf Herbrich (Amazon.com);,2227627541,"Recently, learning deep models from dense data has received a lot of attention in tasks such as object recognition and signal processing. However, when dealing with non-sensory data about real-world entities, data is often sparse; for example people interaction with products in e-Commerce, people interacting with each other in social networks or word sequences in natural language. In this talk, I will share lessons learned over the past 10 years when learning predictive models based on sparse data: 1) how to scale the inference algorithms to distributed data setting, 2) how to automate the learning process by reducing the amount of hyper-parameters to zero, 3) how to deal with Zipf distributions when learning resource-constrained models, and 4) how to combine dense and sparse-learning algorithms. The talk will be drawing from many real-world experiences I gathered over the past decade in applications of the techniques in gaming, search, advertising and recommendations of systems developed at Microsoft, Facebook and Amazon.",2016,Knowledge Discovery and Data Mining,zipf s law;data science;data mining;machine learning;statistics;computer science;
Data-driven Automatic Treatment Regimen Development and Recommendation,Leilei Sun (Dalian University of Technology);Chuanren Liu (Drexel University);Chonghui Guo (Dalian University of Technology);Hui Xiong (Rutgers–Newark);Yanming Xie;,"2098357117,2169554947,2719303095,2153710278,2642302339","The analysis of large-scale Electrical Medical Records (EMRs) has the potential to develop and optimize clinical treatment regimens. A treatment regimen usually includes a series of doctor orders containing rich temporal and heterogeneous information. However, in many existing studies, a doctor order is simplified as an event code and a treatment record is simplified as a code sequence. Thus, the information inherent in doctor orders is not fully used for in-depth analysis. In this paper, we aim at exploiting the rich information in doctor orders and developing data-driven approaches for improving clinical treatments. To this end, we first propose a novel method to measure the similarities between treatment records with consideration of sequential and multifaceted information in doctor orders. Then, we propose an efficient density-based clustering algorithm to summarize large-scale treatment records, and extract a semantic representation of each treatment cluster. Finally, we develop a unified framework to evaluate the discovered treatment regimens, and find the most effective treatment regimen for new patients. In the empirical study, we validate our methods with EMRs of 27,678 patients from 14 hospitals. The results show that: 1) Our method can successfully extract typical treatment regimens from large-scale treatment records. The extracted treatment regimens are intuitive and provide managerial implications for treatment regimen design and optimization. 2) By recommending the most effective treatment regimens, the total cure rate in our data improves from 19.89% to 21.28%, and the effective rate increases up to 98.29%.",2016,Knowledge Discovery and Data Mining,data mining;
Ranking Relevance in Yahoo Search,Dawei Yin (Yahoo!);Yuening Hu (Yahoo!);Jiliang Tang (Yahoo!);Tim Daly (Yahoo!);Mianwei Zhou (Yahoo!);Hua Ouyang (Yahoo!);Jianhui Chen (Yahoo!);Changsung Kang (Yahoo!);Hongbo Deng (Yahoo!);Chikashi Nobata (Yahoo!);Jean-Marc Langlois (Yahoo!);Yi Chang (Yahoo!);,"2170531144,2134853736,2147392410,2478637130,2528719982,2489973799,2486916609,2158414383,2682826105,2295249286,2529055905,2168000538","Search engines play a crucial role in our daily lives. Relevance is the core problem of a commercial search engine. It has attracted thousands of researchers from both academia and industry and has been studied for decades. Relevance in a modern search engine has gone far beyond text matching, and now involves tremendous challenges. The semantic gap between queries and URLs is the main barrier for improving base relevance. Clicks help provide hints to improve relevance, but unfortunately for most tail queries, the click information is too sparse, noisy, or missing entirely. For comprehensive relevance, the recency and location sensitivity of results is also critical. In this paper, we give an overview of the solutions for relevance in the Yahoo search engine. We introduce three key techniques for base relevance -- ranking functions, semantic matching features and query rewriting. We also describe solutions for recency sensitive relevance and location sensitive relevance. This work builds upon 20 years of existing efforts on Yahoo search, summarizes the most recent advances and provides a series of practical relevance solutions. The performance reported is based on Yahoo's commercial search engine, where tens of billions of urls are indexed and served by the ranking system.",2016,Knowledge Discovery and Data Mining,ranking;search engine;deep learning;learning to rank;world wide web;information retrieval;data mining;machine learning;computer science;
Question Independent Grading using Machine Learning: The Case of Computer Program Grading,Gursimran Singh;Shashank Srikant;Varun Aggarwal;,"2666593215,2226147892,2608931570","Learning supervised models to grade open-ended responses is an expensive process. A model has to be trained for every prompt/question separately, which in turn requires graded samples. In automatic programming evaluation specifically, the focus of this work, this issue is amplified. The models have to be trained not only for every question but also for every language the question is offered in. Moreover, the availability and time taken by experts to create a labeled set of programs for each question is a major bottleneck in scaling such a system. We address this issue by presenting a method to grade computer programs which requires no manually assigned labeled samples for grading responses to a new, unseen question. We extend our previous work [25] wherein we introduced a grammar of features to learn question specific models. In this work, we propose a method to transform those features into a set of features that maintain their structural relation with the labels across questions. Using these features we learn one supervised model, across questions for a given language, which can then be applied to an ungraded response to an unseen question. We show that our method rivals the performance of both, question specific models and the consensus among human experts while substantially outperforming extant ways of evaluating codes. We demonstrate the system single s value by deploying it to grade programs in a high stakes assessment. The learning from this work is transferable to other grading tasks such as math question grading and also provides a new variation to the supervised learning approach.",2016,Knowledge Discovery and Data Mining,feature;supervised learning;natural language processing;data mining;artificial intelligence;machine learning;computer science;
Evaluating Mobile Apps with A/B and Quasi A/B Tests,Ya Xu (LinkedIn);Nanyu Chen (LinkedIn);,"2310280520,2170586657","We have seen an explosive growth of mobile usage, particularly on mobile apps. It is more important than ever to be able to properly evaluate mobile app release. A/B testing is a standard framework to evaluate new ideas. We have seen much of its applications in the online world across the industry [9,10,12]. Running A/B tests on mobile apps turns out to be quite different, and much of it is attributed to the fact that we cannot ship code easily to mobile apps other than going through a lengthy build, review and release process. Mobile infrastructure and user behavior differences also contribute to how A/B tests are conducted differently on mobile apps, which will be discussed in details in this paper. In addition to measuring features individually in the new app version through randomized A/B tests, we have a unique opportunity to evaluate the mobile app as a whole using the quasi-experimental framework [21]. Not all features can be A/B tested due to infrastructure changes and wholistic product redesign. We propose and establish quasi-experimental techniques for measuring impact from mobile app release, with results shared from a recent major app launch at LinkedIn.",2016,Knowledge Discovery and Data Mining,mobile deep linking;mobile search;quasi experiment;causal inference;mobile technology;world wide web;computer security;data mining;simulation;computer science;
Towards Optimal Cardinality Estimation of Unions and Intersections with Sketches,Daniel Ting (Facebook);,2230432365,Estimating the cardinality of unions and intersections of sets is a problem of interest in OLAP. Large data applications often require the use of approximate methods based on small sketches of the data. We give new estimators for the cardinality of unions and intersection and show they approximate an optimal estimation procedure. These estimators enable the improved accuracy of the streaming MinCount sketch to be exploited in distributed settings. Both theoretical and empirical results demonstrate substantial improvements over existing methods.,2016,Knowledge Discovery and Data Mining,randomized algorithm;discrete mathematics;combinatorics;mathematical optimization;computer science;mathematics;
Accelerating the Race to Autonomous Cars,Danny Shapiro (Nvidia);,2515414339,"Every automaker is working on driver assistance systems and self-driving cars. Conventional computer vision used for ADAS is reaching its threshold because it is impossible to write code for every possible scenario as a vehicle navigates. In order to develop a truly autonomous car, deep learning and artificial intelligence are required. With deep learning, the vehicle can be trained to have super human levels of perception, driving safer than anyone on the road. An end-to-end artificial intelligence platform based on supercomputers in the cloud and in the vehicle enables cars to get smarter and smarter. Coupled with an extensive software development kit with vision and AI libraries and software modules, automakers, tier 1s, and startups can build scalable systems from ADAS to full autonomy.",2016,Knowledge Discovery and Data Mining,scalability;deep learning;embedded system;artificial intelligence;machine learning;simulation;computer science;
Business Applications of Predictive Modeling at Scale,Qiang Zhu (LinkedIn);Songtao Guo (LinkedIn);Paul Ogilvie (LinkedIn);Yan Liu (LinkedIn);,"2685659162,2700533644,2530390680,2516943954","Predictive modeling is the art of building statistical models that forecast probabilities and trends of future events. It has broad applications in industry across different domains. Some popular examples include user intention predictions, lead scoring, churn analysis, etc. In this tutorial, we will focus on the best practice of predictive modeling in the big data era and its applications in industry, with motivating examples across a range of business tasks and relevance products. We will start with an overview of how predictive modeling helps power and drive various key business use cases. We will introduce the essential concepts and state of the art in building end-to-end predictive modeling solutions, and discuss the challenges, key technologies, and lessons learned from our practice, including case studies of LinkedIn feed relevance and a platform for email response prediction. Moreover, we will discuss some practical solutions of building predictive modeling platform to scale the modeling efforts for data scientists and analysts, along with an overview of popular tools and platforms used across the industry.",2016,Knowledge Discovery and Data Mining,predictive analytics;business analytics;management science;data science;data mining;machine learning;computer science;
A VC View of Investing in ML,Greg Papadopoulos;,2643010249,"We are seeing a remarkable watershed in the application of data science across markets and industries. A trifecta of advances in algorithms, cheap cycles, and the capture of networked data from everywhere are no doubt the catalysts. The results for many are continuous improvements in efficiencies, and for some are a fundamental re-imagination and disruption of just about every industry. This talk will give examples we are seeing (and funding!) for the latter, and then focus on our views of the ecosystem of value-from-data infrastructure and end-application companies. A big question is whether the enormous collective advances in tools, techniques and education are in-fact converting would-be differentiated products into democratized features used everywhere. We'll follow the value and make our own predictions on future as ML as a business.",2016,Knowledge Discovery and Data Mining,investment;data mining;machine learning;computer science;
Joint Optimization of Multiple Performance Metrics in Online Video Advertising,Sahin Cem Geyik;Sergey Faleev;Jianqiang Shen;Sean O'Donnell;Santanu Kolay (Yahoo!);,"2704095071,2630348086,2507557475,2618978856,2062334970","The field of online advertising, in essence, deals with the problem of presenting ads to online users in the most appropriate contexts to achieve a multitude of advertiser goals. A vast amount of work in online advertising has been focused on optimizing banner display advertising campaigns where the main goal lies in direct response metrics, often as clicks or conversions. In this paper, we explore the newly popularized space of online video advertising, where brand recognition is the key focus. We propose a framework based on a feedback mechanism where we optimize multiple video specific performance indicators while making sure the delivery constraints (budget and user reach) of advertisers are satisfied. While our main focus is on improving metrics such as engagement (amount of view time), and viewability (whether a campaign is within eyesight of a user), we also discuss the possibilities of expanding to other metrics. We demonstrate the benefit of our framework via empirical results in multiple real-world advertising campaigns. To the best of our knowledge, this is the first paper that deals with the unique challenges arising from the nature of online video advertising.",2016,Knowledge Discovery and Data Mining,share of voice;native advertising;informative advertising;advertising campaign;online advertising;multimedia;computer science;
Minimizing Legal Exposure of High-Tech Companies through Collaborative Filtering Methods,Bo Jin (Dalian University of Technology);Chao Che (Dalian University of Technology);Kuifei Yu;Yue Qu (Dalian University of Technology);Li Guo (Dalian University of Technology);Cuili Yao (Dalian University of Technology);Ruiyun Yu (Northeastern University);Qiang Zhang (Dalian University of Technology);,"2434583827,2632277346,2657291978,2515567699,2230458316,2499006483,2691302245,2684549118","Patent litigation not only covers legal and technical issues, it is also a key consideration for managers of high-technology (high-tech) companies when making strategic decisions. Patent litigation influences the market value of high-tech companies. However, this raises unique challenges. To this end, in this paper, we develop a novel recommendation framework to solve the problem of litigation risk prediction. We will introduce a specific type of patent-related litigation, that is, Section 337 investigations, which prohibit all acts of unfair competition, or any unfair trade practices, when exporting products to the United States. To build this recommendation framework, we collect and exploit a large amount of published information related to almost all Section 337 investigation cases. This study has two aims: (1) to predict the litigation risk in a specific industry category for high-tech companies and (2) to predict the litigation risk from competitors for high-tech companies. These aims can be achieved by mining historical investigation cases and related patents. Specifically, we propose two methods to meet the needs of both aims: a proximal slope one predictor and a time-aware predictor. Several factors are considered in the proposed methods, including the litigation risk if a company wants to enter a new market and the risk that a potential competitor would file a lawsuit against the new entrant. Comparative experiments using real-world data demonstrate that the proposed methods outperform several baselines with a significant margin.",2016,Knowledge Discovery and Data Mining,litigation risk analysis;collaborative filtering;actuarial science;data mining;machine learning;computer science;
Structured Doubly Stochastic Matrix for Graph Based Clustering: Structured Doubly Stochastic Matrix,Xiaoqian Wang (University of Texas at Arlington);Feiping Nie (University of Texas at Arlington);Heng Huang (University of Texas at Arlington);,"2149602155,2245267964,2137533801","As one of the most significant machine learning topics, clustering has been extensively employed in various kinds of area. Its prevalent application in scientific research as well as industrial practice has drawn high attention in this day and age. A multitude of clustering methods have been developed, among which the graph based clustering method using the affinity matrix has been laid great emphasis on. Recent research work used the doubly stochastic matrix to normalize the input affinity matrix and enhance the graph based clustering models. Although the doubly stochastic matrix can improve the clustering performance, the clustering structure in the doubly stochastic matrix is not clear as expected. Thus, post processing step is required to extract the final clustering results, which may not be optimal. To address this problem, in this paper, we propose a novel convex model to learn the structured doubly stochastic matrix by imposing low-rank constraint on the graph Laplacian matrix. Our new structured doubly stochastic matrix can explicitly uncover the clustering structure and encode the probabilities of pair-wise data points to be connected, such that the clustering results are enhanced. An efficient optimization algorithm is derived to solve our new objective. Also, we provide theoretical discussions that when the input differs, our method possesses interesting connections with K -means and spectral graph cut models respectively. We conduct experiments on both synthetic and benchmark datasets to validate the performance of our proposed method. The empirical results demonstrate that our model provides an approach to better solving the K -mean clustering problem. By using the cluster indicator provided by our model as initialization, K -means converges to a smaller objective function value with better clustering performance. Moreover, we compare the clustering performance of our model with spectral clustering and related double stochastic model. On all datasets, our method performs equally or better than the related methods.",2016,Knowledge Discovery and Data Mining,flame clustering;k medians clustering;brown clustering;canopy clustering algorithm;dbscan;correlation clustering;constrained clustering;data stream clustering;cure data clustering algorithm;doubly stochastic matrix;affinity propagation;spectral clustering;fuzzy clustering;laplacian matrix;k means clustering;clustering high dimensional data;cluster analysis;consensus clustering;biclustering;combinatorics;data mining;machine learning;mathematical optimization;statistics;computer science;mathematics;
Towards Robust and Versatile Causal Discovery for Business Applications,Giorgos Borboudakis (University of Crete);Ioannis Tsamardinos (University of Crete);,"2028001986,2619192764","Causal discovery algorithms can induce some of the causal relations from the data, commonly in the form of a causal network such as a causal Bayesian network. Arguably however, all such algorithms lack far behind what is necessary for a true business application. We develop an initial version of a new, general causal discovery algorithm called ETIO with many features suitable for business applications. These include (a) ability to accept prior causal knowledge (e.g., taking senior driving courses improves driving skills), (b) admitting the presence of latent confounding factors, (c) admitting the possibility of (a certain type of) selection bias in the data (e.g., clients sampled mostly from a given region), (d) ability to analyze data with missing-by-design (i.e., not planned to measure) values (e.g., if two companies merge and their databases measure different attributes), and (e) ability to analyze data from different interventions (e.g., prior and posterior to an advertisement campaign). ETIO is an instance of the logical approach to integrative causal discovery that has been relatively recently introduced and enables the solution of complex reverse-engineering problems in causal discovery. ETIO is compared against the state-of-the-art and is shown to be more effective in terms of speed, with only a slight degradation in terms of learning accuracy, while incorporating all the features above. The code is available on the mensxmachina.org website.",2016,Knowledge Discovery and Data Mining,answer set programming;latent variable;selection bias;data science;data mining;artificial intelligence;machine learning;statistics;computer science;
Recruitment Market Trend Analysis with Sequential Latent Variable Models,Chen Zhu (Baidu);Hengshu Zhu (Baidu);Hui Xiong (Rutgers–Newark);Pengliang Ding (Baidu);Fang Xie (Baidu);,"2529879468,2098414524,2153710278,2515451369,2507864332","Recruitment market analysis provides valuable understanding of industry-specific economic growth and plays an important role for both employers and job seekers. With the rapid development of online recruitment services, massive recruitment data have been accumulated and enable a new paradigm for recruitment market analysis. However, traditional methods for recruitment market analysis largely rely on the knowledge of domain experts and classic statistical models, which are usually too general to model large-scale dynamic recruitment data, and have difficulties to capture the fine-grained market trends. To this end, in this paper, we propose a new research paradigm for recruitment market analysis by leveraging unsupervised learning techniques for automatically discovering recruitment market trends based on large-scale recruitment data. Specifically, we develop a novel sequential latent variable model, named MTLVM, which is designed for capturing the sequential dependencies of corporate recruitment states and is able to automatically learn the latent recruitment topics within a Bayesian generative framework. In particular, to capture the variability of recruitment topics over time, we design hierarchical dirichlet processes for MTLVM. These processes allow to dynamically generate the evolving recruitment topics. Finally, we implement a prototype system to empirically evaluate our approach based on real-world recruitment data in China. Indeed, by visualizing the results from MTLVM, we can successfully reveal many interesting findings, such as the popularity of LBS related jobs reached the peak in the 2nd half of 2014, and decreased in 2015.",2016,Knowledge Discovery and Data Mining,latent variable model;trend analysis;data mining;simulation;statistics;computer science;
Robust and Effective Metric Learning Using Capped Trace Norm: Metric Learning via Capped Trace Norm,Zhouyuan Huo (University of Texas at Arlington);Feiping Nie (University of Texas at Arlington);Heng Huang (University of Texas at Arlington);,"2668104805,2245267964,2137533801","Metric learning aims at automatically learning a metric from pair or triplet based constraints in data, and it can be potentially beneficial whenever the notion of metric between instances plays a nontrivial role. In Mahalanobis distance metric learning, distance matrix M is in symmetric positive semi-definite cone, and in order to avoid overfitting and to learn a better Mahalanobis distance from weakly supervised constraints, the low-rank regularization has been often imposed on matrix M to learn the correlations between features and samples. As the approximations of the rank minimization function, the trace norm and Fantope have been utilized to regularize the metric learning objectives and achieve good performance. However, these low-rank regularization models are either not tight enough to approximate rank minimization or time-consuming to tune an optimal rank. In this paper, we introduce a novel metric learning model using the capped trace norm based regularization, which uses a singular value threshold to constraint the metric matrix M as low-rank explicitly such that the rank of matrix M is stable when the large singular values vary. The capped trace norm regularization can also be viewed as the adaptive Fantope regularization. We minimize singular values which are less than threshold value and the rank of M is not necessary to be k , thus our method is more stable and applicable in practice when we do not know the optimal rank of matrix M . We derive an efficient optimization algorithm to solve the proposed new model and the algorithm convergence proof is also provided in this paper. We evaluate our method on a variety of challenging benchmarks, such as LFW and Pubfig datasets. Face verification experiments are performed and results show that our method consistently outperforms the state-of-the-art metric learning algorithms.",2016,Knowledge Discovery and Data Mining,convex metric space;intrinsic metric;low rank approximation;metric;discrete mathematics;combinatorics;machine learning;mathematical optimization;computer science;mathematics;
CompanyDepot: Employer Name Normalization in the Online Recruitment Industry,Qiaoling Liu;Faizan Javed;Matt Mcnair;,"2486156090,2627345740,2124723780","Entity linking links entity mentions in text to the corresponding entities in a knowledge base (KB) and has many applications in both open domain and specific domains. For example, in the recruitment domain, linking employer names in job postings or resumes to entities in an employer KB is very important to many business applications. In this paper, we focus on this employer name normalization task, which has several unique challenges: handling employer names from both job postings and resumes, leveraging the corresponding location context, and handling name variations, irrelevant input data, and noises in the KB. We present a system called CompanyDepot which contains a machine learning based approach CompanyDepot-ML and a heuristic approach CompanyDepot-H to address these challenges in three steps: (1) searching for candidate entities based on a customized search engine for the KB; (2) ranking the candidate entities using learning-to-rank methods or heuristics; and (3) validating the top-ranked entity via binary classification or heuristics. While CompanyDepot-ML shows better extendability and flexibility, CompanyDepot-H serves as a strong baseline and useful way to collect training data for CompanyDepot-ML. The proposed system achieves 2.5%-21.4% higher coverage at the same precision level compared to an existing system used at CareerBuilder over multiple real-world datasets. Applying the system to a similar task of academic institution name normalization further shows the generalization ability of the method.",2016,Knowledge Discovery and Data Mining,entity linking;world wide web;information retrieval;data mining;machine learning;computer science;
Structural Neighborhood Based Classification of Nodes in a Network,Sharad Nandanwar (Indian Institute of Science);Musti Narasimha Murty (Indian Institute of Science);,"2514386424,2222915647","Classification of entities based on the underlying network structure is an important problem. Networks encountered in practice are sparse and have many missing and noisy links. Statistical learning techniques have been used in intra-network classification; however, they typically exploit only the local neighborhood, so may not perform well. In this paper, we propose a novel structural neighborhood-based classifier learning using a random walk. For classifying a node, we take a random walk from the node and make a decision based on how nodes in the respective k^th-level neighborhood are labeled. We observe that random walks of short length are helpful in classification. Emphasizing role of longer random walks may cause the underlying Markov chain to converge to a stationary distribution. Considering this, we take a lazy random walk based approach with variable termination probability for each node, based on the node's structural properties including its degree. Our experimental study on real world datasets demonstrates the superiority of the proposed approach over the existing state-of-the-art approaches.",2016,Knowledge Discovery and Data Mining,statistical relational learning;data mining;pattern recognition;machine learning;computer science;mathematics;
Fast Memory-efficient Anomaly Detection in Streaming Heterogeneous Graphs,Emaad A. Manzoor (Stony Brook University);Sadegh M. Milajerdi (University of Illinois at Chicago);Leman Akoglu (Stony Brook University);,"1532370067,2511109482,2288278917","Given a stream of heterogeneous graphs containing different types of nodes and edges, how can we spot anomalous ones in real-time while consuming bounded memory? This problem is motivated by and generalizes from its application in security to host-level advanced persistent threat (APT) detection. We propose StreamSpot, a clustering based anomaly detection approach that addresses challenges in two key fronts: (1) heterogeneity, and (2) streaming nature. We introduce a new similarity function for heterogeneous graphs that compares two graphs based on their relative frequency of local substructures, represented as short strings. This function lends itself to a vector representation of a graph, which is (a) fast to compute, and (b) amenable to a sketched version with bounded size that preserves similarity. StreamSpot exhibits desirable properties that a streaming application requires: it is (i) fully-streaming; processing the stream one edge at a time as it arrives, (ii) memory-efficient; requiring constant space for the sketches and the clustering, (iii) fast; taking constant time to update the graph sketches and the cluster summaries that can process over 100,000 edges per second, and (iv) online; scoring and flagging anomalies in real time. Experiments on datasets containing simulated system-call flow graphs from normal browser activity and various attack scenarios (ground truth) show that StreamSpot is high-performance; achieving above 95% detection accuracy with small delay, as well as competitive time and memory usage.",2016,Knowledge Discovery and Data Mining,indifference graph;hopcroft karp algorithm;1 planar graph;graph product;modular decomposition;maximal independent set;pathwidth;longest path problem;anomaly detection;theoretical computer science;combinatorics;world wide web;data mining;machine learning;computer science;mathematics;
The Wisdom of Crowds: Best Practices for Data Prep & Machine Learning Derived from Millions of Data Science Workflows,Ingo Mierswa (Technical University of Dortmund);,278133066,"With hundreds of thousands of users, RapidMiner is the most frequently used visual workflow platform for machine learning. It covers the full spectrum of analytics from data preparation to machine learning and model validation. In this presentation, I will take you on a tour of machine learning which spans the last 15 years of research and industry applications and share key insights with you about how data scientists perform their daily analysis tasks. These patterns are extracted from mining millions of analytical workflows that have been created with RapidMiner over the past years. This talk will address important questions around the data mining process such as: What are the most frequently used solutions for typical data quality problems? How often are analysts using decision trees or neural networks? And does this behavior change over time or depend on the users experience level?",2016,Knowledge Discovery and Data Mining,analytics;data visualization;data science;world wide web;data mining;machine learning;computer science;
Streaming-LDA: A Copula-based Approach to Modeling Topic Dependencies in Document Streams,Hesam Amoualian (University of Grenoble);Marianne Clausel (University of Grenoble);Eric Gaussier (University of Grenoble);Massih-Reza Amini (University of Grenoble);,"2512811048,2031349992,2667237248,2687710440","We propose in this paper two new models for modeling topic and word-topic dependencies between consecutive documents in document streams. The first model is a direct extension of Latent Dirichlet Allocation model (LDA) and makes use of a Dirichlet distribution to balance the influence of the LDA prior parameters wrt to topic and word-topic distribution of the previous document. The second extension makes use of copulas, which constitute a generic tools to model dependencies between random variables. We rely here on Archimedean copulas, and more precisely on Franck copulas, as they are symmetric and associative and are thus appropriate for exchangeable random variables. Our experiments, conducted on three standard collections that have been used in several studies on topic modeling, show that our proposals outperform previous ones (as dynamic topic models and temporal \LDA), both in terms of perplexity and for tracking similar topics in a document stream.",2016,Knowledge Discovery and Data Mining,dynamic topic model;copula;latent dirichlet allocation;data mining;pattern recognition;machine learning;computer science;
Talent Circle Detection in Job Transition Networks,Huang Xu (Northwestern Polytechnical University);Zhiwen Yu (Northwestern Polytechnical University);Jingyuan Yang (Rutgers–Newark);Hui Xiong (Rutgers–Newark);Hengshu Zhu (Baidu);,"2132199107,2118377178,2521081551,2153710278,2098414524","With the high mobility of talent, it becomes critical for the recruitment team to find the right talent from the right source in an efficient manner. The prevalence of Online Professional Networks (OPNs), such as LinkedIn, enables the new paradigm for talent recruitment and job search. However, the dynamic and complex nature of such talent information imposes significant challenges to identify prospective talent sources from large-scale professional networks. Therefore, in this paper, we propose to create a job transition network where vertices stand for organizations and a directed edge represents the talent flow between two organizations for a time period. By analyzing this job transition network, it is able to extract talent circles in a way such that every circle includes the organizations with similar talent exchange patterns. Then, the characteristics of these talent circles can be used for talent recruitment and job search. To this end, we develop a talent circle detection model and design the corresponding learning method by maximizing the Normalized Discounted Cumulative Gain (NDCG) of inferred probability for the edge existence based on edge weights. Then, the identified circles will be labeled by the representative organizations as well as keywords in job descriptions. Moreover, based on these identified circles, we develop a talent exchange prediction method for talent recommendation. Finally, we have performed extensive experiments on real-world data. The results show that, our method can achieve much higher modularity when comparing to the benchmark approaches, as well as high precision and recall for talent exchange prediction.",2016,Knowledge Discovery and Data Mining,simulation;computer science;
Compact and Scalable Graph Neighborhood Sketching,Takuya Akiba (National Institute of Informatics);Yosuke Yano (National Institute of Informatics);,"2700554244,2507740877","The all-distances sketch (ADS) has recently emerged as a promising paradigm of graph neighborhood sketching. An ADS is a probabilistic data structure that is defined for each vertex of a graph. ADSs facilitate accurate estimation of many useful indicators for network analysis with the guarantee of accuracy, and the ADSs for all the vertices in a graph can be computed in near-linear time. Because of these useful properties, ADS has attracted considerable attention. However, a critical drawback of ADS is its space requirement, which tends to be much larger than that of the graph itself. In the present study, we address this issue by designing a new graph sketching scheme, namely, sketch retrieval shortcuts (SRS). Although SRSs are more space-efficient than ADSs by an order of magnitude, an ADS of any vertex can be quickly retrieved from the SRSs. The retrieved ADSs can be used to estimate the aforementioned indicators in exactly the same manner as with plain ADSs, inheriting the same accuracy guarantee. Our experiments on real-world networks demonstrate the usefulness of SRSs as a practical back-end of large-scale graph data mining.",2016,Knowledge Discovery and Data Mining,graph;theoretical computer science;discrete mathematics;combinatorics;mathematics;
Compute Job Memory Recommender System Using Machine Learning,Taraneh Taghavi (Qualcomm);Maria Lupetini (Qualcomm);Yaron Kretchmer (Qualcomm);,"2635545676,2511213601,2650071953","This paper presents a machine learning approach to predict the amount of compute memory needed by jobs which are submitted to Load Sharing Facility (LSF® ) with a high level of accuracy. LSF® is the compute resource manager and job scheduler for Qualcomm chip design process. It schedules the jobs based on available resources: CPU, memory, storage, and software licenses. Memory is one of the key resources and its proper utilization leads to a substantial improvement in saving machine resources which in turn results in a significant reduction in overall job pending time. In addition, efficient memory utilization helps to reduce the operations cost by decreasing the number of servers needed for the end-to-end design process. In this paper, we explored a suite of statistical and machine learning techniques to develop a Compute Memory Recommender System for the Qualcomm chip design process with over 90% accuracy in predicting the amount of memory a job needs. Moreover, it demonstrates the potential to significantly reduce job pending time.",2016,Knowledge Discovery and Data Mining,computing with memory;flat memory model;uniform memory access;shared memory;memory management;grid computing;theoretical computer science;data mining;real time computing;machine learning;simulation;computer science;
Burstiness Scale: A Parsimonious Model for Characterizing Random Series of Events,Rodrigo Augusto da Silva Alves (Centro Federal de Educação Tecnológica de Minas Gerais);Renato Martins Assuncao (Universidade Federal de Minas Gerais);Pedro Olmo Stancioli Vaz de Melo (Universidade Federal de Minas Gerais);,"2683066055,2001700347,2162838672","The problem to accurately and parsimoniously characterize random series of events (RSEs) seen in the Web, such as Yelp reviews or Twitter hashtags, is not trivial. Reports found in the literature reveal two apparent conflicting visions of how RSEs should be modeled. From one side, the Poissonian processes, of which consecutive events follow each other at a relatively regular time and should not be correlated. On the other side, the self-exciting processes, which are able to generate bursts of correlated events. The existence of many and sometimes conflicting approaches to model RSEs is a consequence of the unpredictability of the aggregated dynamics of our individual and routine activities, which sometimes show simple patterns, but sometimes results in irregular rising and falling trends. In this paper we propose a parsimonious way to characterize general RSEs, namely the Burstiness Scale ( BuSca ) model. BuSca views each RSE as a mix of two independent process: a Poissonian and a self-exciting one. Here we describe a fast method to extract the two parameters of BuSca that, together, gives the burstiness scale ψ, which represents how much of the RSE is due to bursty and viral effects. We validated our method in eight diverse and large datasets containing real random series of events seen in Twitter, Yelp, e-mail conversations, Digg, and online forums. Results showed that, even using only two parameters, BuSca is able to accurately describe RSEs seen in these diverse systems, what can leverage many applications.",2016,Knowledge Discovery and Data Mining,social media;data mining;simulation;statistics;computer science;
Temporal Order-based First-Take-All Hashing for Fast Attention-Deficit-Hyperactive-Disorder Detection,Hao Hu (University of Central Florida);Joey Velez-Ginorio (University of Central Florida);Guo-Jun Qi (University of Central Florida);,"2431064401,2512677353,2237849324","Attention Deficit Hyperactive Disorder (ADHD) is one of the most common childhood disorders and can continue through adolescence and adulthood. Although the root cause of the problem still remains unknown, recent advancements in brain imaging technology reveal there exists differences between neural activities of Typically Developing Children (TDC) and ADHD subjects. Inspired by this, we propose a novel First-Take-All (FTA) hashing framework to investigate the problem of fast ADHD subjects detection through the fMRI time-series of neuron activities. By hashing time courses from regions of interests (ROIs) in the brain into fixed-size hash codes, FTA can compactly encode the temporal order differences between the neural activity patterns that are key to distinguish TDC and ADHD subjects. Such patterns can be directly learned via minimizing the training loss incurred by the generated FTA codes. By conducting similarity search on the resultant FTA codes, data-driven ADHD detection can be achieved in an efficient fashion. The experiments' results on real-world ADHD detection benchmarks demonstrate the FTA can outperform the state-of-the-art baselines using only neural activity time series without any phenotypic information.",2016,Knowledge Discovery and Data Mining,speech recognition;machine learning;simulation;computer science;
Optimal Reserve Prices in Upstream Auctions: Empirical Application on Online Video Advertising,Miguel Angel Alcobendas Lisbona (Yahoo!);Sheide Chammas (Yahoo!);Kuang-chih Lee (Yahoo!);,"2508079860,2513521846,2696605660","We consider optimal reserve prices in BrightRoll Video Exchange when the inventory opportunity comes from other exchanges (downstream marketplaces). We show that the existence of downstream auctions impacts the optimal floor. Moreover, it renders the classical derivation of the floor set by a monopolist inadequate and suboptimal. We derive the new downstream-corrected reserve price and compare its performance with respect to existing floors and the classical optimal monopoly price. In our application, the downstream-corrected reserve price proves superior to both. The proposed model also deals with data challenges commonly faced by exchanges: limited number of logged bids in an auction, and uncertainty regarding the bidding behavior in other exchanges. The relevance of this study transcends its particular context and is applicable to a wide range of scenarios where sequential auctions exist, and where marketplaces interact with each other.",2016,Knowledge Discovery and Data Mining,reservation price;common value auction;online advertising;
"The Limits of Popularity-Based Recommendations, and the Role of Social Ties",Marco Bressan (Sapienza University of Rome);Stefano Leucci (Sapienza University of Rome);Alessandro Panconesi (Sapienza University of Rome);Prabhakar Raghavan (Google);Erisa Terolli (Sapienza University of Rome);,"2516098459,2290155465,2193688032,2326868227,2545383834","In this paper we introduce a mathematical model that captures some of the salient features of recommender systems that are based on popularity and that try to exploit social ties among the users. We show that, under very general conditions, the market always converges to a steady state, for which we are able to give an explicit form. Thanks to this we can tell rather precisely how much a market is altered by a recommendation system, and determine the power of users to influence others. Our theoretical results are complemented by experiments with real world social networks showing that social graphs prevent large market distortions in spite of the presence of highly influential users.",2016,Knowledge Discovery and Data Mining,shapley value;social influence;interpersonal ties;recommender system;social psychology;world wide web;computer science;
Convex Optimization for Linear Query Processing under Approximate Differential Privacy,Ganzhao Yuan (South China University of Technology);Yin Yang (Khalifa University);Zhenjie Zhang (National University of Singapore);Zhifeng Hao (Foshan University);,"2118188494,2280962535,2161417483,2684555521","Differential privacy enables organizations to collect accurate aggregates over sensitive data with strong, rigorous guarantees on individuals' privacy. Previous work has found that under differential privacy, computing multiple correlated aggregates as a batch, using an appropriate strategy , may yield higher accuracy than computing each of them independently. However, finding the best strategy that maximizes result accuracy is non-trivial, as it involves solving a complex constrained optimization program that appears to be non-convex. Hence, in the past much effort has been devoted in solving this non-convex optimization program. Existing approaches include various sophisticated heuristics and expensive numerical solutions. None of them, however, guarantees to find the optimal solution of this optimization problem. This paper points out that under (e, ཬ)-differential privacy, the optimal solution of the above constrained optimization problem in search of a suitable strategy can be found, rather surprisingly, by solving a simple and elegant convex optimization program. Then, we propose an efficient algorithm based on Newton's method, which we prove to always converge to the optimal solution with linear global convergence rate and quadratic local convergence rate. Empirical evaluations demonstrate the accuracy and efficiency of the proposed solution.",2016,Knowledge Discovery and Data Mining,meta optimization;differential privacy;quadratically constrained quadratic program;convex optimization;theoretical computer science;mathematical optimization;computer science;
Modeling Precursors for Event Forecasting via Nested Multi-Instance Learning,Yue Ning (Virginia Tech);Sathappan Muthiah (Virginia Tech);Huzefa Rangwala (George Mason University);Naren Ramakrishnan (Virginia Tech);,"2493066754,1509108369,2096698710,2199255697","Forecasting large-scale societal events like civil unrest movements, disease outbreaks, and elections is an important and challenging problem. From the perspective of human analysts and policy makers, forecasting algorithms must not only make accurate predictions but must also provide supporting evidence, e.g., the causal factors related to the event of interest. We develop a novel multiple instance learning based approach that jointly tackles the problem of identifying evidence-based precursors and forecasts events into the future. Specifically, given a collection of streaming news articles from multiple sources we develop a nested multiple instance learning approach to forecast significant societal events such as protests. Using data from three countries in Latin America, we demonstrate how our approach is able to consistently identify news articles considered as precursors for protests. Our empirical evaluation demonstrates the strengths of our proposed approach in filtering candidate precursors, in forecasting the occurrence of events with a lead time advantage and in accurately predicting the characteristics of civil unrest events.",2016,Knowledge Discovery and Data Mining,text mining;data science;data mining;machine learning;simulation;computer science;
Online Dual Decomposition for Performance and Delivery-Based Distributed Ad Allocation,Jim C. Huang (Amazon.com);Rodolphe Jenatton (Amazon.com);Cedric Archambeau (Amazon.com);,"2518843193,90224947,2006974593","Online optimization is central to display advertising, where we must sequentially allocate ad impressions to maximize the total welfare among advertisers, while respecting various advertiser-specified long-term constraints (e.g., total amount of the ad's budget that is consumed at the end of the campaign). In this paper, we present the online dual decomposition (ODD) framework for large-scale, online, distributed ad allocation, which combines dual decomposition and online convex optimization. ODD allows us to account for the distributed and the online nature of the ad allocation problem and is extensible to a variety of ad allocation problems arising in real-world display advertising systems. Moreover, ODD does not require assumptions about auction dynamics, stochastic or adversarial feedback, or any other characteristics of the ad marketplace. We further provide guarantees for the online solution as measured by bounds on cumulative regret. The regret analysis accounts for the impact of having to estimate constraints in an online setting before they are observed and for the dependence on the smoothness with which constraints and constraint violations are generated. We provide an extensive set of results from a large-scale production advertising system at Amazon to validate the framework and compare its behavior to various ad allocation algorithms.",2016,Knowledge Discovery and Data Mining,simulation;
Can You Teach the Elephant to Dance? AKA: Culture Eats Data Science for Breakfast,Jonathan D. Becher;,2508866078,"In the past 20 years, the practical examples of KDD/data mining have become so ubiquitous that it's almost impossible to imagine a new venture that isn't based on data science. Uber, Facebook, 23andMe, Tesla -- they aren't just technology companies; they are data companies. And yet the reality is that these companies are still anomalies. Large, successful companies usually still treat KDD as either an afterthought or as an experiment. It's not core to how they run the business. As practitioners we compound this problem by concentrating our efforts on valuable business problems; but ones which are usually on the periphery of the business. We do this because changing the heart of how a company operates requires more than just process or technology changes. It requires cultural changes. And these cultural changes usually trigger corporate antibodies adverse to anything new. This talk will review some practical realities of instituting data-driven decisions in a very large multi-national company.",2016,Knowledge Discovery and Data Mining,enterprise software;analytics;mobile technology;data mining;simulation;computer science;
CaSMoS: A Framework for Learning Candidate Selection Models over Structured Queries and Documents,Fedor Borisyuk (LinkedIn);Krishnaram Kenthapadi (LinkedIn);David Stein (LinkedIn);Bo Zhao (LinkedIn);,"2531515047,2530367615,2507570935,2674375462","User experience at social media and web platforms such as LinkedIn is heavily dependent on the performance and scalability of its products. Applications such as personalized search and recommendations require real-time scoring of millions of structured candidate documents associated with each query, with strict latency constraints. In such applications, the query incorporates the context of the user (in addition to search keywords if present), and hence can become very large, comprising of thousands of Boolean clauses over hundreds of document attributes. Consequently, candidate selection techniques need to be applied since it is infeasible to retrieve and score all matching documents from the underlying inverted index. We propose CaSMoS, a machine learned candidate selection framework that makes use of Weighted AND (WAND) query. Our framework is designed to prune irrelevant documents and retrieve documents that are likely to be part of the top-k results for the query. We apply a constrained feature selection algorithm to learn positive weights for feature combinations that are used as part of the weighted candidate selection query. We have implemented and deployed this system to be executed in real time using LinkedIn's Galene search platform. We perform extensive evaluation with different training data approaches and parameter settings, and investigate the scalability of the proposed candidate selection model. Our deployment of this system as part of LinkedIn's job recommendation engine has resulted in significant reduction in latency (up to 25%) without sacrificing the quality of the retrieved results, thereby paving the way for more sophisticated scoring models.",2016,Knowledge Discovery and Data Mining,web search query;query expansion;world wide web;information retrieval;data mining;machine learning;computer science;
Large-Scale Machine Learning at Verizon: Theory and Applications,Ashok Srivastava (Verizon Communications);,2529005834,"This talk will cover recent innovations in large-scale machine learning and their applications on massive, real-world data sets at Verizon. These applications power new revenue generating products and services for the company and are hosted on a massive computing and storage platform known as Orion. We will discuss the architecture of Orion and the underlying algorithmic framework. We will also cover some of the real-world aspects of building a new organization dedicated to creating new product lines based on data science.",2016,Knowledge Discovery and Data Mining,data science;theoretical computer science;data mining;machine learning;simulation;computer science;
How Machine Learning has Finally Solved Wanamaker's Dilemma,Oliver Downs;,2674567224,"It has become a cliche to talk about Wanamaker's dilemma, his famous quote that ""Half the money I spend on advertising is wasted; the trouble is I don't know which half"" is well known. So why talk about it today? Well, I'll show you that some of the best targeted consumer marketing still suffers exactly from this problem, and tell you why -- it's not humanly possible to solve it! At Amplero we've finally solved it and made that solution accessible to marketers, using machine learning in combination with the revolution in online experimentation that the advent of the multi-armed bandit has brought about.",2016,Knowledge Discovery and Data Mining,operations research;artificial intelligence;
A Survey on Social Media Anomaly Detection,Rose Yu (University of Southern California);Huida Qiu (University of Southern California);Zhen Wen (IBM);Ching-Yung Lin (IBM);Yan Liu (University of Southern California);,"2107161032,2646186367,2129596114,2134425921,2240541904","Social media anomaly detection is of critical importance to prevent malicious activities such as bullying, terrorist attack planning, and fraud information dissemination. With the recent popularity of social media, new types of anomalous behaviors arise, causing concerns from various parties. While a large amount of work have been dedicated to traditional anomaly detection problems, we observe a surge of research interests in the new realm of social media anomaly detection. In this paper, we present a survey on existing approaches to address this problem. We focus on the new type of anomalous phenomena in the social media and review the recent developed techniques to detect those special types of anomalies. We provide a general overview of the problem domain, common formulations, existing methodologies and potential directions. With this work, we hope to call out the attention from the research community on this challenging problem and open up new directions that we can contribute in the future",2016,Knowledge Discovery and Data Mining,computer security;
Current and Future Challenges in Mining Large Networks: Report on the Second SDM Workshop on Mining Networks and Graphs,Lawrence B. Holder (Washington State University);Rajmonda Caceres (Massachusetts Institute of Technology);David F. Gleich (Purdue University);E. Jason Riedy (Georgia Institute of Technology);Maleq Khan (Virginia Tech);Nitesh V. Chawla (University of Notre Dame);Ravi Kumar 0001 (Google);Yinghui Wu (Washington State University);Christine Klymko (Lawrence Livermore National Laboratory);Tina Eliassi-Rad (Rutgers University);B. Aditya Prakash (Virginia Tech);,"639663078,2102944021,2148810670,2278635082,2257365121,1979796846,2232709231,2134127457,2528138532,218538652,2124002246","We report on the Second Workshop on Mining Networks and Graphs held at the 2015 SIAM International Conference on Data Mining. This half-day workshop consisted of a keynote talk, four technical paper presentations, one demonstration, and a panel on future challenges in mining large networks. We summarize the main highlights of the workshop, including expanded written summaries of the future challenges provided by the panelists. The current and future challenges discussed at the workshop and elaborated here provide valuable guidance for future research in the field",2016,Knowledge Discovery and Data Mining,big data;data science;data mining;computer science;
On the effect of endpoints on dynamic time warping,"Diego Furtado Silva (Spanish National Research Council);Gustavo Enrique de Almeida Prado Alves Batista (University of São Paulo);Eamonn Keogh (University of California, Riverside);","2138204294,2165222361,2170070822",-,2016,Knowledge Discovery and Data Mining,dynamic time warping;time series;statistics;computer science;
The Internet of Things: Opportunities and Challenges for Distributed Data Analysis,Marco Stolpe (Technical University of Dortmund);,2159003941,"Nowadays, data is created by humans as well as automatically collected by physical things, which embed electronics, software, sensors and network connectivity. Together, these entities constitute the Internet of Things (IoT). The automated analysis of its data can provide insights into previously unknown relationships between things, their environment and their users, facilitating an optimization of their behavior. Especially the real-time analysis of data, embedded into physical systems, can enable new forms of autonomous control. These in turn may lead to more sustainable applications, reducing waste and saving resources IoT's distributed and dynamic nature, resource constraints of sensors and embedded devices as well as the amounts of generated data are challenging even the most advanced automated data analysis methods known today. In particular, the IoT requires a new generation of distributed analysis methods. Many existing surveys have strongly focused on the centralization of data in the cloud and big data analysis, which follows the paradigm of parallel high-performance computing. However, bandwidth and energy can be too limited for the transmission of raw data, or it is prohibited due to privacy constraints. Such communication-constrained scenarios require decentralized analysis algorithms which at least partly work directly on the generating devices. After listing data-driven IoT applications, in contrast to existing surveys, we highlight the differences between cloudbased and decentralized analysis from an algorithmic perspective. We present the opportunities and challenges of research on communication-efficient decentralized analysis algorithms. Here, the focus is on the difficult scenario of vertically partitioned data, which covers common IoT use cases. The comprehensive bibliography aims at providing readers with a good starting point for their own work",2016,Knowledge Discovery and Data Mining,world wide web;data mining;simulation;
Strategy discovery in professional soccer match data,Jan Van Haaren (Katholieke Universiteit Leuven);Siebe Hannosset (Katholieke Universiteit Leuven);Jesse Davis (Katholieke Universiteit Leuven);,"2036917247,2241357928,2144802550",-,2016,Knowledge Discovery and Data Mining,-
"MultiClust 2013: Multiple Clusterings, Multiview Data, and Multisource Knowledgedriven Clustering: [Workshop Report]",Ira Assent (Aarhus University);Carlotta Domeniconi (George Mason University);Francesco Gullo (Yahoo!);Andrea Tagarelli (University of Calabria);Arthur Zimek (Ludwig Maximilian University of Munich);,"145164693,45678088,1979201319,273425128,242745652","In this workshop report, we give a summary of the Multi-Clust workshop held in Chicago in conjunction with KDD 2013. We provide an overview on the history of this workshop series and the general topics covered. Furthermore, we provide summaries of the invited talks and of the contributed papers.",2016,Knowledge Discovery and Data Mining,data science;information retrieval;data mining;computer science;
Catching Synchronized Behaviors in Large Networks: A Graph Mining Approach,Meng Jiang (Tsinghua University);Peng Cui (Tsinghua University);Alex Beutel (Carnegie Mellon University);Christos Faloutsos (Carnegie Mellon University);Shiqiang Yang (Tsinghua University);,"2115305989,2113115369,2045447989,2198983026,2127183023","Given a directed graph of millions of nodes, how can we automatically spot anomalous, suspicious nodes judging only from their connectivity patterns? Suspicious graph patterns show up in many applications, from Twitter users who buy fake followers, manipulating the social network, to botnet members performing distributed denial of service attacks, disturbing the network traffic graph. We propose a fast and effective method, C atch S ync , which exploits two of the tell-tale signs left in graphs by fraudsters: (a) synchronized behavior: suspicious nodes have extremely similar behavior patterns because they are often required to perform some task together (such as follow the same user); and (b) rare behavior: their connectivity patterns are very different from the majority. We introduce novel measures to quantify both concepts (“synchronicity” and “normality”) and we propose a parameter-free algorithm that works on the resulting synchronicity-normality plots. Thanks to careful design, C atch S ync has the following desirable properties: (a) it is scalable to large datasets, being linear in the graph size; (b) it is parameter free ; and (c) it is side-information-oblivious : it can operate using only the topology, without needing labeled data, nor timing information, and the like., while still capable of using side information if available. We applied C atch S ync on three large, real datasets, 1-billion-edge Twitter social graph, 3-billion-edge, and 12-billion-edge Tencent Weibo social graphs, and several synthetic ones; C atch S ync consistently outperforms existing competitors, both in detection accuracy by 36p on Twitter and 20p on Tencent Weibo, as well as in speed.",2016,Knowledge Discovery and Data Mining,anomaly detection;world wide web;data mining;machine learning;computer science;
Less is More: Building Selective Anomaly Ensembles,Shebuti Rayana (Stony Brook University);Leman Akoglu (Stony Brook University);,"1722036351,2288278917","Ensemble learning for anomaly detection has been barely studied, due to difficulty in acquiring ground truth and the lack of inherent objective functions. In contrast, ensemble approaches for classification and clustering have been studied and effectively used for long. Our work taps into this gap and builds a new ensemble approach for anomaly detection, with application to event detection in temporal graphs as well as outlier detection in no-graph settings. It handles and combines multiple heterogeneous detectors to yield improved and robust performance. Importantly, trusting results from all the constituent detectors may deteriorate the overall performance of the ensemble, as some detectors could provide inaccurate results depending on the type of data in hand and the underlying assumptions of a detector. This suggests that combining the detectors selectively is key to building effective anomaly ensembles—hence “less is more”. In this paper we propose a novel ensemble approach called SELECT for anomaly detection, which automatically and systematically selects the results from constituent detectors to combine in a fully unsupervised fashion. We apply our method to event detection in temporal graphs and outlier detection in multi-dimensional point data (no-graph), where SELECT successfully utilizes five base detectors and seven consensus methods under a unified ensemble framework. We provide extensive quantitative evaluation of our approach for event detection on five real-world datasets (four with ground truth events), including Enron email communications, RealityMining SMS and phone call records, New York Times news corpus, and World Cup 2014 Twitter news feed. We also provide results for outlier detection on seven real-world multi-dimensional point datasets from UCI Machine Learning Repository. Thanks to its selection mechanism, SELECT yields superior performance compared to the individual detectors alone, the full ensemble (naively combining all results), an existing diversity-based ensemble, and an existing weighted ensemble approach.",2016,Knowledge Discovery and Data Mining,ensemble learning;unsupervised learning;data mining;pattern recognition;machine learning;computer science;
Eigen-Optimization on Large Graphs by Edge Manipulation,Chen Chen (Arizona State University);Hanghang Tong (Arizona State University);B. Aditya Prakash (Virginia Tech);Tina Eliassi-Rad (Rutgers University);Michalis Faloutsos (University of New Mexico);Christos Faloutsos (Carnegie Mellon University);,"2311808467,2667261544,2124002246,218538652,2703613186,2198983026","Large graphs are prevalent in many applications and enable a variety of information dissemination processes, e.g., meme, virus, and influence propagation. How can we optimize the underlying graph structure to affect the outcome of such dissemination processes in a desired way (e.g., stop a virus propagation, facilitate the propagation of a piece of good idea, etc)? Existing research suggests that the leading eigenvalue of the underlying graph is the key metric in determining the so-called epidemic threshold for a variety of dissemination models. In this paper, we study the problem of how to optimally place a set of edges (e.g., edge deletion and edge addition) to optimize the leading eigenvalue of the underlying graph, so that we can guide the dissemination process in a desired way. We propose effective, scalable algorithms for edge deletion and edge addition, respectively. In addition, we reveal the intrinsic relationship between edge deletion and node deletion problems. Experimental results validate the effectiveness and efficiency of the proposed algorithms.",2016,Knowledge Discovery and Data Mining,scalability;theoretical computer science;combinatorics;distributed computing;computer science;mathematics;
Spatial-Proximity Optimization for Rapid Task Group Deployment,Chih-Ya Shen (Academia Sinica);De-Nian Yang (Academia Sinica);Wang-Chien Lee (Pennsylvania State University);Ming-Syan Chen (National Taiwan University);,"2122520210,2096343151,2143778659,2122365371","Spatial proximity is one of the most important factors for the quick deployment of the task groups in various time-sensitive missions. This article proposes a new spatial query, Spatio-Social Team Query (SSTQ) , that forms a strong task group by considering (1) the group’s spatial distance (i.e., transportation time), (2) skills of the candidate group members, and (3) social rapport among the candidates. Efficient processing of SSTQ is very challenging, because the aforementioned spatial, skill, and social factors need to be carefully examined. In this article, therefore, we first formulate two subproblems of SSTQ, namely Hop-Constrained Team Problem (HCTP) and Connection-Oriented Team Query (COTQ) . HCTP is a decision problem that considers only social and skill dimensions. We prove that HCTP is NP-Complete. Moreover, based on the hardness of HCTP, we prove that SSTQ is NP-Hard and inapproximable within any factor . On the other hand, COTQ is a special case of SSTQ that relaxes the social constraint. We prove that COTQ is NP-Hard and propose an approximation algorithm for COTQ, namely COTprox . Furthermore, based on the observations on COTprox, we devise an approximation algorithm, SSTprox , with a guaranteed error bound for SSTQ. Finally, to efficiently obtain the optimal solution to SSTQ for small instances, we design two efficient algorithms, SpatialFirst and SkillFirst , with different scenarios in mind. These two algorithms incorporate various effective ordering and pruning techniques to reduce the search space for answering SSTQ. Experimental results on real datasets indicate that the proposed algorithms can efficiently answer SSTQ under various parameter settings.",2016,Knowledge Discovery and Data Mining,spatial database;social network;data mining;machine learning;simulation;computer science;mathematics;
Heterogeneous Translated Hashing: A Scalable Solution Towards Multi-Modal Similarity Search,Ying Wei (Hong Kong University of Science and Technology);Yangqiu Song (University of Illinois at Urbana–Champaign);Yi Zhen (Georgia Institute of Technology);Bo Liu (Hong Kong University of Science and Technology);Qiang Yang (Hong Kong University of Science and Technology);,"2235263654,2099747503,2706898368,2423441494,2109031554","Multi-modal similarity search has attracted considerable attention to meet the need of information retrieval across different types of media. To enable efficient multi-modal similarity search in large-scale databases recently, researchers start to study multi-modal hashing. Most of the existing methods are applied to search across multi-views among which explicit correspondence is provided. Given a multi-modal similarity search task, we observe that abundant multi-view data can be found on the Web which can serve as an auxiliary bridge. In this paper, we propose a Heterogeneous Translated Hashing (HTH) method with such auxiliary bridge incorporated not only to improve current multi-view search but also to enable similarity search across heterogeneous media which have no direct correspondence. HTH provides more flexible and discriminative ability by embedding heterogeneous media into different Hamming spaces, compared to almost all existing methods that map heterogeneous data in a common Hamming space. We formulate a joint optimization model to learn hash functions embedding heterogeneous media into different Hamming spaces, and a translator aligning different Hamming spaces. The extensive experiments on two real-world datasets, one publicly available dataset of Flickr, and the other MIRFLICKR-Yahoo Answers dataset, highlight the effectiveness and efficiency of our algorithm.",2016,Knowledge Discovery and Data Mining,locality sensitive hashing;scalability;world wide web;information retrieval;data mining;machine learning;computer science;
Co-Clustering Structural Temporal Data with Applications to Semiconductor Manufacturing,Yada Zhu (IBM);Jingrui He (Arizona State University);,"2712115745,2693123770","Recent years have witnessed data explosion in semiconductor manufacturing due to advances in instrumentation and storage techniques. The large amount of data associated with process variables monitored over time form a rich reservoir of information, which can be used for a variety of purposes, such as anomaly detection, quality control, and fault diagnostics. In particular, following the same recipe for a certain Integrated Circuit device, multiple tools and chambers can be deployed for the production of this device, during which multiple time series can be collected, such as temperature, impedance, gas flow, electric bias, etc. These time series naturally fit into a two-dimensional array (matrix), i.e., each element in this array corresponds to a time series for one process variable from one chamber. To leverage the rich structural information in such temporal data, in this article, we propose a novel framework named C-Struts to simultaneously cluster on the two dimensions of this array. In this framework, we interpret the structural information as a set of constraints on the cluster membership, introduce an auxiliary probability distribution accordingly, and design an iterative algorithm to assign each time series to a certain cluster on each dimension. Furthermore, we establish the equivalence between C-Struts and a generic optimization problem, which is able to accommodate various distance functions. Extensive experiments on synthetic, benchmark, as well as manufacturing datasets demonstrate the effectiveness of the proposed method.",2016,Knowledge Discovery and Data Mining,structure;semiconductor;biclustering;data mining;machine learning;simulation;statistics;computer science;
Introduction to the Special Issue of Best Papers in ACM SIGKDD 2014,"Wei Wang (University of California, Los Angeles);Jure Leskovec (Stanford University);","2315689540,1878631932",-,2016,Knowledge Discovery and Data Mining,-
CGC: A Flexible and Robust Approach to Integrating Co-Regularized Multi-Domain Graph for Clustering,"Wei Cheng (University of North Carolina at Chapel Hill);Zhishan Guo (University of North Carolina at Chapel Hill);Xiang Zhang (Case Western Reserve University);Wei Wang (University of California, Los Angeles);","2620045292,2165841276,2553248206,2315689540","Multi-view graph clustering aims to enhance clustering performance by integrating heterogeneous information collected in different domains. Each domain provides a different view of the data instances. Leveraging cross-domain information has been demonstrated an effective way to achieve better clustering results. Despite the previous success, existing multi-view graph clustering methods usually assume that different views are available for the same set of instances. Thus, instances in different domains can be treated as having strict one-to-one relationship. In many real-life applications, however, data instances in one domain may correspond to multiple instances in another domain. Moreover, relationships between instances in different domains may be associated with weights based on prior (partial) knowledge. In this article, we propose a flexible and robust framework, Co-regularized Graph Clustering (CGC), based on non-negative matrix factorization (NMF), to tackle these challenges. CGC has several advantages over the existing methods. First, it supports many-to-many cross-domain instance relationship. Second, it incorporates weight on cross-domain relationship. Third, it allows partial cross-domain mapping so that graphs in different domains may have different sizes. Finally, it provides users with the extent to which the cross-domain instance relationship violates the in-domain clustering structure, and thus enables users to re-evaluate the consistency of the relationship. We develop an efficient optimization method that guarantees to find the global optimal solution with a given confidence requirement. The proposed method can automatically identify noisy domains and assign smaller weights to them. This helps to obtain optimal graph partition for the focused domain. Extensive experimental results on UCI benchmark datasets, newsgroup datasets, and biological interaction networks demonstrate the effectiveness of our approach.",2016,Knowledge Discovery and Data Mining,correlation clustering;constrained clustering;clustering coefficient;fuzzy clustering;non negative matrix factorization;cluster analysis;data mining;machine learning;mathematical optimization;computer science;mathematics;
Jointly Modeling Label and Feature Heterogeneity in Medical Informatics,Pei Yang (Arizona State University);Hongxia Yang (IBM);Haoda Fu (Eli Lilly and Company);Dawei Zhou (Arizona State University);Jieping Ye (University of Michigan);Theodoros Lappas (Stevens Institute of Technology);Jingrui He (Arizona State University);,"2309210939,2309208648,2683856079,2303436841,2305258894,2028397797,2693123770","Multiple types of heterogeneity including label heterogeneity and feature heterogeneity often co-exist in many real-world data mining applications, such as diabetes treatment classification, gene functionality prediction, and brain image analysis. To effectively leverage such heterogeneity, in this article, we propose a novel graph-based model for Learning with both Label and Feature heterogeneity, namely L 2 F . It models the label correlation by requiring that any two label-specific classifiers behave similarly on the same views if the associated labels are similar, and imposes the view consistency by requiring that view-based classifiers generate similar predictions on the same examples. The objective function for L 2 F is jointly convex. To solve the optimization problem, we propose an iterative algorithm, which is guaranteed to converge to the global optimum. One appealing feature of L 2 F is that it is capable of handling data with missing views and labels. Furthermore, we analyze its generalization performance based on Rademacher complexity, which sheds light on the benefits of jointly modeling the label and feature heterogeneity. Experimental results on various biomedical datasets show the effectiveness of the proposed approach.",2016,Knowledge Discovery and Data Mining,health informatics;data science;data mining;machine learning;statistics;computer science;
"Mining Dual Networks: Models, Algorithms, and Applications",Yubao Wu (Case Western Reserve University);Xiaofeng Zhu (Case Western Reserve University);Li Li (Case Western Reserve University);Wei Fan (Baidu);Ruoming Jin (Kent State University);Xiang Zhang (Case Western Reserve University);,"2481087172,2212746422,2694743028,2422054197,2119237514,2553248206","Finding the densest subgraph in a single graph is a fundamental problem that has been extensively studied. In many emerging applications, there exist dual networks. For example, in genetics, it is important to use protein interactions to interpret genetic interactions. In this application, one network represents physical interactions among nodes, for example, protein--protein interactions, and another network represents conceptual interactions, for example, genetic interactions. Edges in the conceptual network are usually derived based on certain correlation measure or statistical test measuring the strength of the interaction. Two nodes with strong conceptual interaction may not have direct physical interaction. In this article, we propose the novel dual-network model and investigate the problem of finding the densest connected subgraph (DCS), which has the largest density in the conceptual network and is also connected in the physical network. Density in the conceptual network represents the average strength of the measured interacting signals among the set of nodes. Connectivity in the physical network shows how they interact physically. Such pattern cannot be identified using the existing algorithms for a single network. We show that even though finding the densest subgraph in a single network is polynomial time solvable, the DCS problem is NP-hard. We develop a two-step approach to solve the DCS problem. In the first step, we effectively prune the dual networks, while guarantee that the optimal solution is contained in the remaining networks. For the second step, we develop two efficient greedy methods based on different search strategies to find the DCS. Different variations of the DCS problem are also studied. We perform extensive experiments on a variety of real and synthetic dual networks to evaluate the effectiveness and efficiency of the developed methods.",2016,Knowledge Discovery and Data Mining,interaction network;combinatorics;machine learning;algorithm;computer science;mathematics;
Biomedical Ontology Quality Assurance Using a Big Data Approach,Licong Cui (University of Kentucky);Shiqiang Tao (University of Kentucky);Guo Qiang Zhang (University of Kentucky);,"2147120483,2119318853,2588264867","This article presents recent progresses made in using scalable cloud computing environment, Hadoop and MapReduce, to perform ontology quality assurance (OQA), and points to areas of future opportunity. The standard sequential approach used for implementing OQA methods can take weeks if not months for exhaustive analyses for large biomedical ontological systems. With OQA methods newly implemented using massively parallel algorithms in the MapReduce framework, several orders of magnitude in speed-up can be achieved (e.g., from three months to three hours). Such dramatically reduced time makes it feasible not only to perform exhaustive structural analysis of large ontological hierarchies, but also to systematically track structural changes between versions for evolutional analysis. As an exemplar, progress is reported in using MapReduce to perform evolutional analysis and visualization on the Systemized Nomenclature of Medicine—Clinical Terms (SNOMED CT), a prominent clinical terminology system. Future opportunities in three areas are described: one is to extend the scope of MapReduce-based approach to existing OQA methods, especially for automated exhaustive structural analysis. The second is to apply our proposed MapReduce Pipeline for Lattice-based Evaluation (MaPLE) approach, demonstrated as an exemplar method for SNOMED CT, to other biomedical ontologies. The third area is to develop interfaces for reviewing results obtained by OQA methods and for visualizing ontological alignment and evolution, which can also take advantage of cloud computing technology to systematically pre-compute computationally intensive jobs in order to increase performance during user interactions with the visualization interface. Advances in these directions are expected to better support the ontological engineering lifecycle.",2016,Knowledge Discovery and Data Mining,snomed ct;lattice;data science;data mining;database;computer science;
Inferring Dynamic Diffusion Networks in Online Media,Maryam Tahani (Sharif University of Technology);Ali Mohammad Afshin Hemmatyar (Sharif University of Technology);Hamid R. Rabiee (Sharif University of Technology);Maryam Ramezani (Sharif University of Technology);,"2499910170,1906074235,2032985511,2493360359","Online media play an important role in information societies by providing a convenient infrastructure for different processes. Information diffusion that is a fundamental process taking place on social and information networks has been investigated in many studies. Research on information diffusion in these networks faces two main challenges: (1) In most cases, diffusion takes place on an underlying network, which is latent and its structure is unknown. (2) This latent network is not fixed and changes over time. In this article, we investigate the diffusion network extraction (DNE) problem when the underlying network is dynamic and latent. We model the diffusion behavior (existence probability) of each edge as a stochastic process and utilize the Hidden Markov Model (HMM) to discover the most probable diffusion links according to the current observation of the diffusion process, which is the infection time of nodes and the past diffusion behavior of links. We evaluate the performance of our Dynamic Diffusion Network Extraction (DDNE) method, on both synthetic and real datasets. Experimental results show that the performance of the proposed method is independent of the cascade transmission model and outperforms the state of art method in terms of F-measure.",2016,Knowledge Discovery and Data Mining,dynamic network analysis;digital media;hidden markov model;artificial intelligence;machine learning;simulation;computer science;
Guest Editorial: Special Issue on Connected Health at Big Data Era (BigChat): A TKDD Special Issue,Hanghang Tong (Arizona State University);Fei Wang (University of Connecticut);Munmun De Choudhury (Georgia Institute of Technology);Zoran Obradovic (Temple University);,"2667261544,2465953593,2123618928,2029694244",-,2016,Knowledge Discovery and Data Mining,universal hashing;scalability;data science;theoretical computer science;information retrieval;computer science;
"Featuring, Detecting, and Visualizing Human Sentiment in Chinese Micro-Blog",Zhiwen Yu (Northwestern Polytechnical University);Zhitao Wang (Northwestern Polytechnical University);Liming Chen (De Montfort University);Bin Guo (Northwestern Polytechnical University);Wenjie Li (Hong Kong Polytechnic University);,"2118377178,2226468868,2155463703,2105384808,2139868060","Micro-blog has been increasingly used for the public to express their opinions, and for organizations to detect public sentiment about social events or public policies. In this article, we examine and identify the key problems of this field, focusing particularly on the characteristics of innovative words, multi-media elements, and hierarchical structure of Chinese “Weibo.” Based on the analysis, we propose a novel approach and develop associated theoretical and technological methods to address these problems. These include a new sentiment word mining method based on three wording metrics and point-wise information, a rule set model for analyzing sentiment features of different linguistic components, and the corresponding methodology for calculating sentiment on multi-granularity considering emoticon elements as auxiliary affective factors. We evaluate our new word discovery and sentiment detection methods on a real-life Chinese micro-blog dataset. Initial results show that our new diction can improve sentiment detection, and they demonstrate that our multi-level rule set method is more effective, with the average accuracy being 10.2p and 1.5p higher than two existing methods for Chinese micro-blog sentiment analysis. In addition, we exploit visualization techniques to study the relationships between online sentiment and real life. The visualization of detected sentiment can help depict temporal patterns and spatial discrepancy.",2016,Knowledge Discovery and Data Mining,visualization;sentiment analysis;data science;world wide web;data mining;computer science;
Kernelized Information-Theoretic Metric Learning for Cancer Diagnosis Using High-Dimensional Molecular Profiling Data,Feiyu Xiong (Drexel University);Moshe Kam (New Jersey Institute of Technology);Leonid Hrebien (Drexel University);Beilun Wang (University of Virginia);Yanjun Qi (University of Virginia);,"2131398225,2678317542,221359174,2357637447,2255911127","With the advancement of genome-wide monitoring technologies, molecular expression data have become widely used for diagnosing cancer through tumor or blood samples. When mining molecular signature data, the process of comparing samples through an adaptive distance function is fundamental but difficult, as such datasets are normally heterogeneous and high dimensional. In this article, we present kernelized information-theoretic metric learning (KITML) algorithms that optimize a distance function to tackle the cancer diagnosis problem and scale to high dimensionality. By learning a nonlinear transformation in the input space implicitly through kernelization, KITML permits efficient optimization, low storage, and improved learning of distance metric. We propose two novel applications of KITML for diagnosing cancer using high-dimensional molecular profiling data: (1) for sample-level cancer diagnosis, the learned metric is used to improve the performance of k -nearest neighbor classification; and (2) for estimating the severity level or stage of a group of samples, we propose a novel set-based ranking approach to extend KITML. For the sample-level cancer classification task, we have evaluated on 14 cancer gene microarray datasets and compared with eight other state-of-the-art approaches. The results show that our approach achieves the best overall performance for the task of molecular-expression-driven cancer sample diagnosis. For the group-level cancer stage estimation, we test the proposed set-KITML approach using three multi-stage cancer microarray datasets, and correctly estimated the stages of sample groups for all three studies.",2016,Knowledge Discovery and Data Mining,clustering high dimensional data;bioinformatics;data mining;machine learning;computer science;mathematics;
Unsupervised Rare Pattern Mining: A Survey,Yun Sing Koh (University of Auckland);Sri Devi Ravana (University of Malaya);,"1987110578,2557241765","Association rule mining was first introduced to examine patterns among frequent items. The original motivation for seeking these rules arose from need to examine customer purchasing behaviour in supermarket transaction data. It seeks to identify combinations of items or itemsets, whose presence in a transaction affects the likelihood of the presence of another specific item or itemsets. In recent years, there has been an increasing demand for rare association rule mining. Detecting rare patterns in data is a vital task, with numerous high-impact applications including medical, finance, and security. This survey aims to provide a general, comprehensive, and structured overview of the state-of-the-art methods for rare pattern mining. We investigate the problems in finding rare rules using traditional association rule mining. As rare association rule mining has not been well explored, there is still specific groundwork that needs to be established. We will discuss some of the major issues in rare association rule mining and also look at current algorithms. As a contribution, we give a general framework for categorizing algorithms: Apriori and Tree based. We highlight the differences between these methods. Finally, we present several real-world application using rare pattern mining in diverse domains. We conclude our survey with a discussion on open and practical challenges in the field.",2016,Knowledge Discovery and Data Mining,k optimal pattern discovery;association rule learning;data science;operations research;data mining;machine learning;computer science;
"Matrices, Compression, Learning Curves: Formulation, and the GroupNteach Algorithms",Bryan Hooi (Carnegie Mellon University);Hyun Ah Song (Carnegie Mellon University);Evangelos E. Papalexakis (Carnegie Mellon University);Rakesh Agrawal (IBM);Christos Faloutsos (Carnegie Mellon University);,"1755863881,2158947101,1418764031,2138427228,2198983026","Suppose you are a teacher, and have to convey a set of object-property pairs 'lions eat meat'. A good teacher will convey a lot of information, with little effort on the student side. What is the best and most intuitive way to convey this information to the student, without the student being overwhelmed? A related, harder problem is: how can we assign a numerical score to each lesson plan i.e., way of conveying information? Here, we give a formal definition of this problem of forming learning units and we provide a metric for comparing different approaches based on information theory. We also design an algorithm, groupNteach, for this problem. Our proposed groupNteach is scalable near-linear in the dataset size; it is effective, achieving excellent results on real data, both with respect to our proposed metric, but also with respect to encoding length; and it is intuitive, conforming to well-known educational principles. Experiments on real and synthetic datasets demonstrate the effectiveness of groupNteach.",2016,Knowledge Discovery and Data Mining,theoretical computer science;data mining;artificial intelligence;machine learning;simulation;statistics;computer science;mathematics;
Hashing-Based Distributed Multi-party Blocking for Privacy-Preserving Record Linkage,Thilina Ranbaduge (Australian National University);Dinusha Vatsalan (Australian National University);Peter Christen (Australian National University);Vassilios S. Verykios (Hellenic Open University);,"993935970,145048928,2023765750,77221159","In many application domains organizations require information from multiple sources to be integrated. Due to privacy and confidentiality concerns often these organizations are not willing or allowed to reveal their sensitive and personal data to other database owners, and to any external party. This has led to the emerging research discipline of privacy-preserving record linkage PPRL. We propose a novel blocking approach for multi-party PPRL to efficiently and effectively prune the record sets that are unlikely to match. Our approach allows each database owner to perform blocking independently except for the initial agreement of parameter settings and a final central hashing-based clustering. We provide an analysis of our technique in terms of complexity, quality, and privacy, and conduct an empirical study with large datasets. The results show that our approach is scalable with the size of the datasets and the number of parties, while providing better quality and privacy than previous multi-party private blocking approaches.",2016,Knowledge Discovery and Data Mining,locality sensitive hashing;bloom filter;cluster analysis;world wide web;data mining;database;machine learning;computer science;
Dual Similarity Regularization for Recommendation,Jing Zheng (Beijing University of Posts and Telecommunications);Jian Liu (Beijing University of Posts and Telecommunications);Chuan Shi (Beijing University of Posts and Telecommunications);Fuzhen Zhuang (Chinese Academy of Sciences);Jingzhi Li (Southern University and A&M College);Bin Wu (Beijing University of Posts and Telecommunications);,"2624298923,2501726924,2252461150,2050314250,2656297484,2464938123","Recently, social recommendation becomes a hot research direction, which leverages social relations among users to alleviate data sparsity and cold-start problems in recommender systems. The social recommendation methods usually employ simple similarity information of users as social regularization on users. Unfortunately, the widely used social regularization may suffer from several aspects: 1 the similarity information of users only stems from users' social relations; 2 it only has constraint on users; 3 it may not work well for users with low similarity. In order to overcome the shortcomings of social regularization, we propose a new dual similarity regularization to impose the constraint on users and items with high and low similarities simultaneously. With the dual similarity regularization, we design an optimization function to integrate the similarity information of users and items, and a gradient descend solution is derived to optimize the objective function. Experiments on two real datasets validate the effectiveness of the proposed solution.",2016,Knowledge Discovery and Data Mining,regularization perspectives on support vector machines;regularization;information retrieval;data mining;machine learning;computer science;mathematics;
Indoor Positioning System for Smart Homes Based on Decision Trees and Passive RFID,Frédéric Bergeron (Université de Sherbrooke);Kevin Bouchard (Université de Sherbrooke);Sébastien Gaboury (Université du Québec);Sylvain Giroux (Université de Sherbrooke);Bruno Bouchard (Université du Québec);,"2224459328,2132358045,2011809654,2146759314,2147421945","This paper presents a novel Indoor Positioning System IPS for objects of daily life equipped with passive RFID tags. The goal is to provide a simple to use, yet accurate, qualitative IPS for housing enhanced with technology sensors, effectors, etc.. With such a service, the housing, namely called smart home, could enable a wide range of services by being able to better understand the context and the current progression of activities of daily living. The paper shows that classical data mining techniques can be applied to raw data from RFID readers and passive tags. In particular, it explains how we built several datasets using a tagged object in a real smart home infrastructure. Our method was proven very effective as most algorithms result in high accuracy for the majority of the smart home.",2016,Knowledge Discovery and Data Mining,random forest;radio frequency identification;decision tree;embedded system;telecommunications;computer security;machine learning;computer science;
Social Group Based Video Recommendation Addressing the Cold-Start Problem,Chunfeng Yang (The Chinese University of Hong Kong);Yipeng Zhou (Shenzhen University);Liang Chen (Shenzhen University);Xiaopeng Zhang (Tencent);Dah Ming Chiu (The Chinese University of Hong Kong);,"2237381698,2096849978,2714252513,2520293188,2144653979","Video recommendation has become an essential part of online video services. Cold start, a problem relatively common in the practical online video recommendation service, occurs when the user who needs video recommendation has no viewing history Cold start consists of the new-user problem and the new-item problem. In this paper, we discuss the new-user one. A promising approach to resolve this problem is to capitalize on information in online social networks OSNs: Videos viewed by a user's friends may be good candidates for recommendation. However, in practice, this information is also quite limited, either because of insufficient friends or lack of abundant viewing history of friends. In this work, we utilize social groups with richer information to recommend videos. It is common that users may be affiliated with multiple groups in OSNs. Through members within the same group, we can reach a considerably larger set of users, hence more candidate videos for recommendation. In this paper, by collaborating with Tencent Video, we propose a social-group-based algorithm to produce personalized video recommendations by ranking candidate videos from the groups a user is affiliated with. This algorithm was implemented and tested in the Tencent Video service system. Compared with two state-of-the-art methods, the proposed algorithm not only improves the click-through rate, but also recommends more diverse videos.",2016,Knowledge Discovery and Data Mining,internet privacy;multimedia;world wide web;data mining;computer science;
Incremental Hierarchical Clustering ofźStochastic Pattern-Based Symbolic Data,Xin Xu;Jiaheng Lu (University of Helsinki);Wei Wang (Nanjing University);,"2717337900,2695000131,2658432775","Classic data analysis techniques generally assume that variables have single values only. However, the data complexity during the age of big data has gone beyond the classic framework such that variable values probably take the form of a set of stochastic measurements instead. We refer to the above case as the stochastic pattern-based symbolic data where each measurement set is an instance of an underlying stochastic pattern. In such a case, non existing classic data analysis approaches, such as the crystal item or fuzzy region ones, could apply yet. For this reason, we put forward a novel Incremental Hierarchical Clustering algorithm for stochastic Pattern-based Symbolic Data IHCPSD. IHCPSD is robust to overlapping and missing measurements and well adapted for incremental learning. Experiments on synthetic and application on real-life emitter parameter data have validated its effectiveness.",2016,Knowledge Discovery and Data Mining,symbolic data analysis;hierarchical clustering;data mining;pattern recognition;machine learning;statistics;computer science;
Robust Multi-view Manifold Ranking for Image Retrieval,Jun Wu (Beijing Jiaotong University);Jianbo Yuan (University of Rochester);Jiebo Luo (University of Rochester);,"2639188716,2128154613,2059910451","Graph-based similarity ranking plays a key role in improving image retrieval performance. Its current trend is to fuse the ranking results from multiple feature sets, including textual feature, visual feature and query log feature, to elevate the retrieval effectiveness. The primary challenge is how to effectively exploit the complementary properties of different features. Another tough issue is the highly noisy features contributed by users, such as textual tags and query logs, which makes the exploration of such complementary properties difficult. This paper proposes a Multi-view Manifold Ranking M2R framework, in which multiple graphs built on different features are integrated to simultaneously encode the similarity ranking. To deal with the high noise issue inherent in the user-contributed features, a data cleaning solution based on visual-neighbor voting is embedded into M2R, thus called Robust M2R RM2R. Experimental results show that the proposed method significantly outperforms the existing approaches, especially when the user-contributed features are highly noisy.",2016,Knowledge Discovery and Data Mining,ranking;ranking svm;visual word;data cleansing;image retrieval;information retrieval;data mining;pattern recognition;machine learning;computer science;
DeepCare: A Deep Dynamic Memory Model forźPredictive Medicine,Trang Pham (Deakin University);Truyen Tran 0001 (Deakin University);Dinh Q. Phung (Deakin University);Svetha Venkatesh (Deakin University);,"2503142456,2157392948,2314522249,2146461601","Personalized predictive medicine necessitates modeling of patient illness and care processes, which inherently have long-term temporal dependencies. Healthcare observations, recorded in electronic medical records, are episodic and irregular in time. We introduce DeepCare, a deep dynamic neural network that reads medical records and predicts future medical outcomes. At the data level, DeepCare models patient health state trajectories with explicit memory of illness. Built on Long Short-Term Memory LSTM, DeepCare introduces time parameterizations to handle irregular timing by moderating the forgetting and consolidation of illness memory. DeepCare also incorporates medical interventions that change the course of illness and shape future medical risk. Moving upi¾źto the health state level, historical and present health states are then aggregated through multiscale temporal pooling, before passing through a neural network that estimates future outcomes. We demonstrate the efficacy of DeepCare for disease progression modeling and readmission prediction in diabetes, a chronic disease with large economic burden. The results show improved modeling and risk prediction accuracy.",2016,Knowledge Discovery and Data Mining,data science;data mining;machine learning;simulation;computer science;mathematics;
Joint Recognition and Segmentation of Actions via Probabilistic Integration of Spatio-Temporal Fisher Vectors,Johanna Carvajal (University of Queensland);Chris McCool (Queensland University of Technology);Brian C. Lovell (University of Queensland);Conrad Sanderson (University of Queensland);,"2284811930,2307648760,2689104063,2090845954","We propose a hierarchical approach to multi-action recognition that performs joint classification and segmentation. Ai¾?given video containing several consecutive actions is processed via a sequence of overlapping temporal windows. Each frame in a temporal window is represented through selective low-level spatio-temporal features which efficiently capture relevant local dynamics. Features from each window are represented as a Fisher vector, which captures first and second order statistics. Instead of directly classifying each Fisher vector, it is converted into a vector of class probabilities. The final classification decision for each frame is then obtained by integrating the class probabilities at the frame level, which exploits the overlapping of the temporal windows. Experiments were performed on two datasets: s-KTH ai¾?stitched version of the KTH dataset to simulate multi-actions, and the challenging CMU-MMAC dataset. On s-KTH, the proposed approach achieves an accuracy of 85.0i¾?%, significantly outperforming two recent approaches based on GMMs and HMMs which obtained 78.3i¾?% and 71.2i¾?%, respectively. On CMU-MMAC, the proposed approach achieves an accuracy of 40.9i¾?%, outperforming the GMM and HMM approaches which obtained 33.7i¾?% and 38.4i¾?%, respectively. Furthermore, the proposed system is on average 40 times faster than the GMM based approach.",2016,Knowledge Discovery and Data Mining,segmentation;speech recognition;pattern recognition;machine learning;statistics;computer science;
Frequent Pattern Outlier Detection Without Exhaustive Mining,Arnaud Giacometti (François Rabelais University);Arnaud Soulet (François Rabelais University);,"2129444726,2470748226","Outlier detection consists in detecting anomalous observations from data. During the past decade, pattern-based outlier detection methods have proposed to mine all frequent patterns in order to compute the outlier factor of each transaction. This approach remains too expensive despite recent progress in pattern mining field. In this paper, we provide exact and approximate methods for calculating the frequent pattern outlier factor FPOF without extracting any pattern or by extracting a small sample. We propose an algorithm that returns the exact FPOF without mining any pattern. Surprisingly, it works in polynomial time on the size of the dataset. We also present an approximate method where the end-user controls the maximum error on the estimated FPOF. Experiments show the interest of both methods for very large datasets where exhaustive mining fails to provide the exact solution. The accuracy of our approximate method outperforms the baseline approach for a same budget in time or number of patterns.",2016,Knowledge Discovery and Data Mining,sampling;anomaly detection;data mining;pattern recognition;statistics;computer science;mathematics;
Normalized Cross-Match: Pattern Discovery Algorithm from Biofeedback Signals,Xueyuan Gong (University of Macau);Simon Fong (University of Macau);Yain-Whar Si (University of Macau);Robert P. Biuk-Agha (University of Macau);Raymond K. Wong (University of New South Wales);Athanasios V. Vasilakos (Luleå University of Technology);,"2109812359,2132140839,2137049483,2530744559,2290077475,2567820109","Biofeedback signals are important elements in critical care applications, such as monitoring ECG data of a patient, discovering patterns from large amount of ECG data sets, detecting outliers from ECG data, etc. Because the signal data update continuously and the sampling rates may be different, time-series data stream is harder to be dealt with compared to traditional historical time-series data. For the pattern discovery problem on time-series streams, Toyoda proposed the CrossMatch CM approach to discover the patterns between two time-series data streams sequences, which requires only On time per data update, where n is the length of one sequence. CM, however, does not support normalization, which is required for some kinds of sequences e.g. EEG data, ECG data. Therefore, we propose a normalized-CrossMatch approach NCM that extends CM to enforce normalization while maintaining the same performance capabilities.",2016,Knowledge Discovery and Data Mining,data science;data mining;database;computer science;
Unsupervised Parameter Estimation for One-Class Support Vector Machines,Zahra Ghafoori (University of Melbourne);Sutharshan Rajasegarar (Deakin University);Sarah M. Erfani (University of Melbourne);Shanika Karunasekera (University of Melbourne);Christopher A. Leckie (University of Melbourne);,"2537358011,249221098,1965944821,1271724398,2111831791","Although the hyper-plane based One-Class Support Vector Machine OCSVM and the hyper-spherical based Support Vector Data Description SVDD algorithms have been shown to be very effective in detecting outliers, their performance on noisy and unlabeled training data has not been widely studied. Moreover, only a few heuristic approaches have been proposed to set the different parameters of these methods in an unsupervised manner. In this paper, we propose two unsupervised methods for estimating the optimal parameter settings to train OCSVM and SVDD models, based on analysing the structure of the data. We show that our heuristic is substantially faster than existing parameter estimation approaches while its accuracy is comparable with supervised parameter learning methods, such as grid-search with cross-validation on labeled data. In addition, our proposed approaches can be used to prepare a labeled data set for a OCSVM or a SVDD from unlabeled data.",2016,Knowledge Discovery and Data Mining,estimation theory;anomaly detection;data mining;pattern recognition;machine learning;statistics;computer science;
Enhanced SVD for Collaborative Filtering,Xin Guan (University of Warwick);Chang-Tsun Li (University of Warwick);Yu Guan (Newcastle University);,"2551562102,2130874684,2099546537","Matrix factorization is one of the most popular techniques for prediction problems in the fields of intelligent systems and data mining. It has shown its effectiveness in many real-world applications such as recommender systems. As a collaborative filtering method, it gives users recommendations based on their previous preferences or ratings. Due to the extreme sparseness of the ratings matrix, active learning is used for eliciting ratings for a user to get better recommendations. In this paper, we propose a new matrix factorization model called Enhanced SVD ESVD which combines the classic matrix factorization method with a specific rating elicitation strategy. We evaluate the proposed ESVD method on the Movielens data set, and the experimental results suggest its effectiveness in terms of both accuracy and efficiency, when compared with traditional matrix factorization methods and active learning methods.",2016,Knowledge Discovery and Data Mining,non negative matrix factorization;matrix decomposition;recommender system;information retrieval;data mining;machine learning;computer science;
Multiple Seeds Based Evolutionary Algorithm for Mining Boolean Association Rules,Mir Md. Jahangir Kabir (University of Tasmania);Shuxiang Xu (University of Tasmania);Byeong Ho Kang (University of Tasmania);Zongyuan Zhao (University of Tasmania);,"2139298541,2101723723,2162029091,2095713079","Most of the association rule mining algorithms use a single seed for initializing a population without paying attention to the effectiveness of an initial population in an evolutionary learning. Recently, researchers show that an initial population has significant effects on producing good solutions over several generations of a genetic algorithm. There are two significant challenges raised by single seed based genetic algorithms for real world applications: 1 solutions of a genetic algorithm are varied, since different seeds generate different initial populations, 2 it is a hard process to define an effective seed for a specific application. To avoid these problems, in this paper we propose a new multiple seeds based genetic algorithm MSGA which generates multiple seeds from different domains of a solution space to discover high quality rules from a large data set. This approach introduces m-domain model and m-seeds selection process through which the whole solution space is subdivided into m-number of same size domains and from each domain it selects a seed. By using these seeds, this method generates an effective initial population to perform an evolutionary learning of the fitness value of each rule. As a result, this method obtains strong searching efficiency at the beginning of the evolution and achieves fast convergence along with the evolution. MSGA is tested with different mutation and crossover operators for mining interesting Boolean association rules from different real world data sets and compared the results with different single seeds based genetic algorithms.",2016,Knowledge Discovery and Data Mining,population based incremental learning;cultural algorithm;conditional probability;genetic algorithm;bioinformatics;data mining;machine learning;statistics;computer science;mathematics;
Matching Product Offers of E-Shops,Andrea Horch (Fraunhofer Society);Holger Kett (Fraunhofer Society);Anette Weisbecker (Fraunhofer Society);,"2229710468,2014678682,1976770254","E-commerce is a continuously growing and competitive market. There are several motivations for e-shoppers, sellers and manufacturers to require an automated approach for matching product offers from various online sources referring to the same or a similar real-world product. Currently, there are several approaches for the assignment of identical and similar product offers. These existing approaches are not sufficient for performing a precise comparison as they only return a similarity value for two compared products but do not give any information for further calculations and analyses. The contribution of this paper is a novel approach and an algorithm for matching identical and very similar product offers based on the pairwise comparison of the product names. For this purpose the approach uses different similarity values which are based on an existing string similarity measure. The approach is independent from a specific product domain or data source.",2016,Knowledge Discovery and Data Mining,web mining;data mining;computer science;
Mirror on the Wall: Finding Similar Questions with Deep Structured Topic Modeling,"Arpita Das (International Institute of Information Technology, Hyderabad);Manish Shrivastava (International Institute of Information Technology, Hyderabad);Manoj Kumar Chinnakotla (Microsoft);","2489785726,2251125838,2250862700","Internet users today prefer getting precise answers to their questions rather than sifting through a bunch of relevant documents provided by search engines. This has led to the huge popularity of Community Question Answering cQA services like Yahoo! Answers, Baidu Zhidao, Quora, StackOverflowetc., where forum users respond to questions with precise answers. Over time, such cQA archives become rich repositories of knowledge encoded in the form of questions and user generated answers. In cQA archives, retrieval of similar questions, which have already been answered in some form, is important for improving the effectiveness of such forums. The main challenge while retrieving similar questions is the ""lexico-syntactic"" gap between the user query and the questions already present in the forum. In this paper, we propose a novel approach called ""Deep Structured Topic Model DSTM"" to bridge the lexico-syntactic gap between the question posed by the user and forum questions. DSTM employs a two-step process consisting of initially retrieving similar questions that lie in the vicinity of the query and latent topic vector space and then re-ranking them using a deep layered semantic model. Experiments on large scale real-life cQA dataset show that our approach outperforms the state-of-the-art translation and topic based baseline approaches.",2016,Knowledge Discovery and Data Mining,world wide web;information retrieval;data mining;machine learning;computer science;
Collaborative Deep Ranking: A Hybrid Pair-Wise Recommendation Algorithm with Implicit Feedback,Haochao Ying (Zhejiang University);Liang Chen (RMIT University);Yuwen Xiong (Zhejiang University);Jian Wu (Zhejiang University);,"2146051823,2685275606,2436419321,2525822525","Collaborative Filtering with Implicit Feedbacks e.g., browsing or clicking records, named as CF-IF, is demonstrated to be an effective way in recommender systems. Existing works of CF-IF can be mainly classified into two categories, i.e., point-wise regression based and pair-wise ranking based, where the latter one relaxes assumption and usually obtains better performance in empirical studies. In real applications, implicit feedback is often very sparse, causing CF-IF based methods to degrade significantly in recommendation performance. In this case, side information e.g., item content is usually introduced and utilized to address the data sparsity problem. Nevertheless, the latent feature representation learned from side information by topic model may not be very effective when the data is too sparse. To address this problem, we propose collaborative deep ranking CDR, a hybrid pair-wise approach with implicit feedback, which leverages deep feature representation of item content into Bayesian framework of pair-wise ranking model in this paper. The experimental analysis on a real-world dataset shows CDR outperforms three state-of-art methods in terms of recall metric under different sparsity level.",2016,Knowledge Discovery and Data Mining,information retrieval;data mining;machine learning;computer science;
Image Segmentation with Superpixel Based Covariance Descriptor,Xianbin Gu (University of Otago);Martin K. Purvis (University of Otago);,"2224368314,2010549645","This paper investigates the problem of image segmentation using superpixels. We propose two approaches to enhance the discriminative ability of the superpixel's covariance descriptors. In the first one, we employ the Log-Euclidean distance as the metric on the covariance manifolds, and then use the RBF kernel to measure the similarities between covariance descriptors. The second method is focused on extracting the subspace structure of the set of covariance descriptors by extending a low rank representation algorithm on to the covariance manifolds. Experiments are carried out with the Berkly Segmentation Dataset, and compared with the state-of-the-art segmentation algorithms, both methods are competitive.",2016,Knowledge Discovery and Data Mining,scale space segmentation;covariance intersection;manifold;image segmentation;covariance matrix;computer vision;pattern recognition;machine learning;statistics;computer science;mathematics;
A Music Recommendation System Based on Acoustic Features and User Personalities,Rui Cheng (University of Arizona);Boyang Tang (Delft University of Technology);,"2544609326,2545145396","Music recommendation attracts great attention for music providers to improve their services as the volume of new music increases quickly. It is a great challenge for users to find their interested songs from such a large size of collections. In the previous studies, common strategies can be categorized into content-based music recommendation and collaborative music filtering. Content-based recommendation systems predict users' preferences in terms of the music content. Collaborative filtering systems predict users' ratings based on the preferences of the friends of the targeting user. In this study, we proposed a hybrid approach to provide personalized music recommendations. This is achieved by extracting audio features of songs and integrating these features and user personalities for context-aware recommendation using the state-of-the-art support vector machines SVM. Our experiments show the effectiveness of this proposed approach for personalized music recommendation.",2016,Knowledge Discovery and Data Mining,support vector machine;multimedia;world wide web;machine learning;computer science;
A Greedy Algorithm to Construct L1 Graph with Ranked Dictionary,Shuchu Han (Stony Brook University);Hong Qin (Stony Brook University);,"2276591736,2157485457","$$\mathcal {L}_1$$ graph is an effective way to represent data samples in many graph-oriented machine learning applications. Its original construction algorithm is nonparametric, and the graphs it generates may have high sparsity. Meanwhile, the construction algorithm also requires many iterative convex optimization calculations and is very time-consuming. Such characteristics would severely limit the application scope of $$\mathcal {L}_1$$ graph in many real-world tasks. In this paper, we design a greedy algorithm to speed up the construction of $$\mathcal {L}_1$$ graph. Moreover, we introduce the concept of ""Ranked Dictionary"" for $$\mathcal {L}_1$$ minimization. This ranked dictionary not only preserves the locality but also removes the randomness of neighborhood selection during the process of graph construction. To demonstrate the effectiveness of our proposed algorithm, we present our experimental results on several commonly-used datasets using two different ranking strategies: one is based on Euclidean metric, and another is based on diffusion metric.",2016,Knowledge Discovery and Data Mining,factor critical graph;distance hereditary graph;simplex graph;strength of a graph;voltage graph;complement graph;graph bandwidth;graph power;butterfly graph;string graph;lattice graph;null graph;clique width;dense graph;graph property;cubic graph;distance regular graph;line graph;graph;cluster analysis;combinatorics;pattern recognition;machine learning;computer science;mathematics;
Ensembles of Interesting Subgroups for Discovering High Potential Employees,Girish Keshav Palshikar (Tata Consultancy Services);Kuleshwar Sahu (Tata Consultancy Services);Rajiv Srivastava (Tata Consultancy Services);,"2602291868,2406347466,2709677202","We propose a new method for building a classifier ensemble, based on subgroup discovery techniques in data mining. We apply subgroup discovery techniques to a labeled training dataset to discover interesting subsets, characterized by a conjuctive logical expression rule, where such subset has an unusually high dominance of one class. Treating these rules as base classifiers, we propose several simple ensemble methods to construct a single classifier. Another novel aspect of the paper is that it applies these ensemble methods, along with standard anomaly detection and classification, to automatically identify high potential HIPO employees - an important problem in management. HIPO employees are critical for future-proofing the organization in the face of attrition, economic uncertainties and business challenges. Current HR processes for HIPO identification are manual and suffer from subjectivity, bias and disagreements. Proposed data-driven analytics algorithms address some of these issues. We show that the new ensemble methods perform better than other methods, including other ensemble methods on a real-life case-study dataset of a large multinational IT services company.",2016,Knowledge Discovery and Data Mining,data science;data mining;machine learning;statistics;computer science;
Reusing Extracted Knowledge in Genetic Programming to Solve Complex Texture Image Classification Problems,Muhammad Iqbal (Victoria University of Wellington);Bing Xue (Victoria University of Wellington);Mengjie Zhang (Victoria University of Wellington);,"2106323235,2167531144,2097529606","Transfer learning is a process to transfer knowledge learned in one or more source tasks to a related but more complex, unseen target task, in an effort to facilitate learning in the target task. Genetic programming GP is an evolutionary approach to generating computer programs for solving a given problem automatically. Transfer learning in GP has been investigated in complex Boolean and symbolic regression problems, but not much in image classification. In this paper, we propose a novel approach to use transfer learning in GP for image classification problems. Specifically, the proposed novel approach extends an existing state-of-the-art GP method by incorporating the ability to extract useful knowledge from simpler problems of a domain and reuse the extracted knowledge to solve complex problems of the domain. The proposed system has been compared with the baseline system i.e., GP without using transfer learning on multi-class texture classification problems from three widely-used texture datasets with different rotations and different levels of noise. The experimental results showed that the ability to reuse the extracted knowledge in the proposed GP method helps achieve better classification accuracy than the baseline GP method.",2016,Knowledge Discovery and Data Mining,transfer of learning;genetic programming;data mining;artificial intelligence;machine learning;computer science;
Learning Multi-faceted Activities from Heterogeneous Data with the Product Space Hierarchical Dirichlet Processes,Thanh-Binh Nguyen (Deakin University);Vu Nguyen (Deakin University);Svetha Venkatesh (Deakin University);Dinh Q. Phung (Deakin University);,"2568150942,2097300135,2146461601,2314522249","Hierarchical Dirichlet processes HDP was originally designed and experimented for a single data channel. In this paper we enhanced its ability to model heterogeneous data using a richer structure for the base measure being a product-space. The enhanced model, called Product Space HDP PS-HDP, can 1 simultaneously model heterogeneous data from multiple sources in a Bayesian nonparametric framework and 2 discover multilevel latent structures from data to result in different types of topics/latent structures that can be explained jointly. We experimented with the MDC dataset, a large and real-world data collected from mobile phones. Our goal was to discover identity---location---time a.k.a who-where-when patterns at different levels globally for all groups and locally for each group. We provided analysis on the activities and patterns learned from our model, visualized, compared and contrasted with the ground-truth to demonstrate the merit of the proposed framework. We further quantitatively evaluated and reported its performance using standard metrics including F1-score, NMI, RI, and purity. We also compared the performance of the PS-HDP model with those of popular existing clustering methods including K-Means, NNMF, GMM, DP-Means, and AP. Lastly, we demonstrate the ability of the model in learning activities with missing data, a common problem encountered in pervasive and ubiquitous computing applications.",2016,Knowledge Discovery and Data Mining,health informatics;data science;data mining;machine learning;statistics;computer science;
Deep Feature Extraction from Trajectories forźTransportation Mode Estimation,Yuki Endo (University of Tsukuba);Hiroyuki Toda (Nippon Telegraph and Telephone);Kyosuke Nishida (Nippon Telegraph and Telephone);Akihisa Kawanobe;,"2126580388,2028551331,2308334310,2560733909","This paper addresses the problem of feature extraction for estimating users' transportation modes from their movement trajectories. Previous studies have adopted supervised learning approaches and used engineers' skills to find effective features for accurate estimation. However, such hand-crafted features cannot always work well because human behaviors are diverse and trajectories include noise due to measurement error. To compensate for the shortcomings of hand-crafted features, we propose a method that automatically extracts additional features using a deep neural network DNN. In order that a DNN can easily handle input trajectories, our method converts a raw trajectory data structure into an image data structure while maintaining effective spatio-temporal information. A classification model is constructed in a supervised manner using both of the deep features and hand-crafted features. We demonstrate the effectiveness of the proposed method through several experiments using two real datasets, such as accuracy comparisons with previous methods and feature visualization.",2016,Knowledge Discovery and Data Mining,deep learning;computer vision;data mining;pattern recognition;machine learning;computer science;
Modeling Adversarial Learning as Nested Stackelberg Games,Yan Zhou (University of Texas at Dallas);Murat Kantarcioglu (University of Texas at Dallas);,"2620242970,332400322","Many data mining applications potentially operate in an adversarial environment where adversaries adapt their behavior to evade detection. Typically adversaries alter data under their control to cause a large divergence of distribution between training and test data. Existing state-of-the-art adversarial learning techniques try to address this problem in which there is only a single type of adversary. In practice, a learner often has to face multiple types of adversaries that may employ different attack tactics. In this paper, we tackle the challenges of multiple types of adversaries with a nested Stackelberg game framework. We demonstrate the effectiveness of our framework with extensive empirical results on both synthetic and real data sets. Our results demonstrate that the nested game framework offers more reliable defense against multiple types of attackers.",2016,Knowledge Discovery and Data Mining,distributed computing;computer security;simulation;computer science;
Secure k-NN Query on Encrypted Cloud Data with Limited Key-Disclosure and Offline Data Owner,Youwen Zhu (Nanjing University of Aeronautics and Astronautics);Zhikuan Wang (Nanjing University of Aeronautics and Astronautics);Yue Zhang (Nanjing University of Aeronautics and Astronautics);,"2645164142,2506532745,2423907867","Recently, many schemes have been proposed to support k-nearest neighbors k-NN query on encrypted cloud data. However, existing approaches either assume query users are fully-trusted, or require data owner to be online all the time. Query users in fully-trusted assumption can access the key to encrypt/decrypt outsourced data, thus, untrusted cloud server can completely break the data upon obtaining the key from any untrustworthy query user. The online requirement introduces much cost to data owner. This paper presents a new scheme to support k-NN query on encrypted cloud database while preserving the privacy of database and query points. Our proposed approach only discloses limited information about the key to query users, and does not require an online data owner. Theoretical analysis and extensive experiments confirm the security and efficiency of our scheme.",2016,Knowledge Discovery and Data Mining,sargable;online aggregation;web search query;web query classification;view;query by example;query expansion;query optimization;cloud computing;privacy;k nearest neighbors algorithm;internet privacy;world wide web;data mining;database;machine learning;computer science;
Efficient Page-Level Data Extraction via Schema Induction and Verification,Chia-Hui Chang (National Central University);Tian-Sheng Chen (National Central University);Ming-Chuan Chen (National Central University);Jhung-Li Ding (National Central University);,"2144182650,2528017490,2528246021,2529607081","Page-level data extraction provides a complete solution for all kinds of information requirement, however very few researches focus on this task because of the difficulties and complexities in the problem. On the other hands, previous page-level systems focus on how to achieve unsupervised data extraction and pay less attention on schema/wrapper generation and verification. In this paper, we emphasize the importance of schema verification for large-scale extraction tasks. Given a large amount of web pages for data extraction, the system uses part of the input pages for training the schema without supervision, and then extracts data from the rest of the input pages through schema verification. To speed up the processing, we utilize leaf nodes of the DOM trees as the processing units and dynamically adjust the encoding for better alignment. The proposed system works better than other page-level extraction systems in terms of schema correctness and extraction efficiency. Overall, the extraction efficiency is 2.7 times faster than state-of-the-art unsupervised approaches that extract data page by page without schema verification.",2016,Knowledge Discovery and Data Mining,theoretical computer science;data mining;database;machine learning;computer science;
"Comparative Evaluation of Action Recognition Methods via Riemannian Manifolds, Fisher Vectors and GMMs: Ideal and Challenging Conditions",Johanna Carvajal (University of Queensland);Arnold Wiliem (University of Queensland);Chris McCool (Queensland University of Technology);Brian C. Lovell (University of Queensland);Conrad Sanderson (University of Queensland);,"2284811930,2230955268,2307648760,2689104063,2090845954","We present a comparative evaluation of various techniques for action recognition while keeping as many variables as possible controlled. We employ two categories of Riemannian manifolds: symmetric positive definite matrices and linear subspaces. For both categories we use their corresponding nearest neighbour classifiers, kernels, and recent kernelised sparse representations. We compare against traditional action recognition techniques based on Gaussian mixture models and Fisher vectors FVs. We evaluate these action recognition techniques under ideal conditions, as well as their sensitivity in more challenging conditions variations in scale and translation. Despite recent advancements for handling manifolds, manifold based techniques obtain the lowest performance and their kernel representations are more unstable in the presence of challenging conditions. The FV approach obtains the highest accuracy under ideal conditions. Moreover, FV best deals with moderate scale and translation changes.",2016,Knowledge Discovery and Data Mining,combinatorics;geometry;pattern recognition;machine learning;mathematics;
Neural Choice by Elimination via Highway Networks,Truyen Tran 0001 (Deakin University);Dinh Q. Phung (Deakin University);Svetha Venkatesh (Deakin University);,"2157392948,2314522249,2146461601","We introduce Neural Choice by Elimination, a new framework that integrates deep neural networks into probabilistic sequential choice models for learning to rank. Given a set of items to chose from, the elimination strategy starts with the whole item set and iteratively eliminates the least worthy item in the remaining subset. We prove that the choice by elimination is equivalent to marginalizing out the random Gompertz latent utilities. Coupled with the choice model is the recently introduced Neural Highway Networks for approximating arbitrarily complex rank functions. We evaluate the proposed framework on a large-scale public dataset with over 425K items, drawn from the Yahoo! learning to rank challenge. It is demonstrated that the proposed method is competitive against state-of-the-art learning to rank methods.",2016,Knowledge Discovery and Data Mining,data mining;artificial intelligence;machine learning;statistics;computer science;
Dynamic Grouped Mixture Models for Intermittent Multivariate Sensor Data,Naoya Takeishi (University of Tokyo);Takehisa Yairi (University of Tokyo);Naoki Nishimura;Yuta Nakajima;Noboru Takata;,"2032596288,226036427,2571734071,2649398116,2650874421","For secure and efficient operation of engineering systems, it is of great importance to watch daily logs generated by them, which mainly consist of multivariate time-series obtained with many sensors. This work focuses on challenges in practical analyses of those sensor data: temporal unevenness and sparseness. To handle the unevenly and sparsely spaced multivariate time-series, this work presents a novel method, which roughly models temporal information that still remains in the data. The proposed model is a mixture model with dynamic hierarchical structure that considers dependency between temporally close batches of observations, instead of every single observation. We conducted experiments with synthetic and real dataset, and confirmed validity of the proposed model quantitatively and qualitatively.",2016,Knowledge Discovery and Data Mining,mixture model;econometrics;data mining;machine learning;statistics;computer science;
A Clustering-Based Framework for Incrementally Repairing Entity Resolution,Qing Wang (Australian National University);Jingyi Gao (Australian National University);Peter Christen (Australian National University);,"2287519903,2556752487,2023765750","Although entity resolution ER is known to be an important problem that has wide-spread applications in many areas, including e-commerce, health-care, social science, and crime and fraud detection, one aspect that has largely been neglected is to monitor the quality of entity resolution and repair erroneous matching decisions over time. In this paper we develop an efficient method for incrementally repairing ER, i.e., fix detected erroneous matches and non-matches. Our method is based on an efficient clustering algorithm that eliminates inconsistencies among matching decisions, and an efficient provenance indexing data structure that allows us to trace the evidence of clustering for supporting ER repairing. We have evaluated our method over real-world databases, and our experimental results show that the quality of entity resolution can be significantly improved through repairing over time.",2016,Knowledge Discovery and Data Mining,data deduplication;record linkage;world wide web;data mining;database;computer science;
Parallel Discord Discovery,"Tian Huang (Shanghai Jiao Tong University);Yongxin Zhu (Shanghai Jiao Tong University);Yishu Mao (Shanghai Jiao Tong University);Xinyang Li (Shanghai Jiao Tong University);Mengyun Liu (Shanghai Jiao Tong University);Yafei Wu (Shanghai Jiao Tong University);Yajun Ha (Agency for Science, Technology and Research);Gillian Dobbie (University of Auckland);","2127525933,2147820062,2246571419,2405658016,2475405144,2637191053,2690893769,2002900660","Discords are the most unusual subsequences of a time series. Sequential discovery of discords is time consuming. As the scale of datasets increases unceasingly, datasets have to be kept on hard disk, which degrades the utilization of computing resources. Furthermore, the results discovered from segmentations of a time series are non-combinable, which makes discord discovery hard to parallelize. In this paper, we propose Parallel Discord Discovery PDD, which divides the discord discovery problem in a combinable manner and solves its sub-problems in parallel. PDD accelerates discord discovery with multiple computing nodes and guarantees the correctness of the results. PDD stores large time series in distributed memory and takes advantage of in-memory computing to improve the utilization of computing resources. Experiments show that given 10 computing nodes, PDD is seven times faster than the sequential method HOTSAX. PDD is able to handle larger datasets than HOTSAX does. PDD achieves over 90i¾ź% utilization of computing resources, nearly twice as much as the disk-aware method does.",2016,Knowledge Discovery and Data Mining,in memory processing;parallel;theoretical computer science;parallel computing;distributed computing;computer science;
Active Learning Based Entity Resolution Using Markov Logic,Jeffrey Fisher (Australian National University);Peter Christen (Australian National University);Qing Wang (Australian National University);,"2148966539,2023765750,2287519903","Entity resolution is a common data cleaning and data integration problem that involves determining which records in one or more data sets refer to the same real-world entities. It has numerous applications for commercial, academic and government organisations. For most practical entity resolution applications, training data does not exist which limits the type of classification models that can be applied. This also prevents complex techniques such as Markov logic networks from being used on real-world problems. In this paper we apply an active learning based technique to generate training data for a Markov logic network based entity resolution model and learn the weights for the formulae in a Markov logic network. We evaluate our technique on real-world data sets and show that we can generate balanced training data and learn and also learn approximate weights for the formulae in the Markov logic network.",2016,Knowledge Discovery and Data Mining,markov model;data mining;machine learning;statistics;algorithm;computer science;
Image Representation Optimization Based on Locally Aggregated Descriptors,Shijiang Chen (Tsinghua University);Guiguang Ding (Tsinghua University);Chenxiao Li (Tsinghua University);Yuchen Guo (Tsinghua University);,"2582827174,2697274257,2265967718,2664008308","Aggregating local descriptors into super vectors achives excellent performance in image classification and retrieval tasks. Vector of locally aggregated descriptorsVLAD, which indexes images to compact representations by aggregating the residuals of descriptors and visual words, is a popular super vector encoding method among this kind. This paper will focus on the biggest difficulty of VLAD, the ""visual burstiness"", reviste the basic assumptions and solutions along this line, then make modifications to two key steps of the initial VLAD process. The main contributions are twofold. Firstly, we start from local coordinate systemLCS and propose the aggregated versionaggrLCS, which changes the objective and timing of coordinate rotation, for better captures of bursts. Secondly, an adaptive power-law normalization method is adopted to magnify the positive effect of power-law normalization by weighting each dimension respectively. Experiments on image retrieval tasks demonstrate that the proposed modifications show superior performance over the original and several variants of VLAD.",2016,Knowledge Discovery and Data Mining,visual word;power law;image retrieval;computer vision;data mining;pattern recognition;machine learning;computer science;mathematics;
Dboost: A Fast Algorithm for DBSCAN-based Clustering on High Dimensional Data,Yuxiao Zhang (Peking University);Xiaorong Wang (Strategy&);Bingyang Li (University of International Relations);Wei Chen (Peking University);Tengjiao Wang (Peking University);Kai Lei (Peking University);,"2251366251,2561341371,2566690164,2527738285,2144373700,2262537035","DBSCAN is a classic density-based clustering technique, which is well known in discovering clusters of arbitrary shapes and handling noise. However, it is very time-consuming in density calculation when facing high dimensional data, which makes it inefficient in many areas, such as multi-document summarization, product recommendation, etc. Therefore, how to efficiently calculate the density on high dimensional data becomes one key issue for DBSCAN-based clustering technique. In this paper, we propose a fast algorithm for DBSCAN-based clustering on high dimensional data, named Dboost. In our algorithm, a ranked retrieval technique adaption named $$WAND^\#$$ is novelly applied to improving the density calculations without accuracy loss, and we further improve this acceleration by reducing the invoking times of $$WAND^\#$$. Experiments were conducted on wire voltage data, Netflix dataset and microblog corpora. The results showed that an acceleration of over 50 times were achieved on wire voltage data and Netflix dataset, and 100 more times can be expected on microblog data.",2016,Knowledge Discovery and Data Mining,subclu;optics algorithm;dbscan;clustering high dimensional data;cluster analysis;world wide web;data mining;database;machine learning;computer science;
Adaptive Seeding for Gaussian Mixture Models,Johannes Blömer (University of Paderborn);Kathrin Bujna (University of Paderborn);,"138736257,101389126","We present new initialization methods for the expectation-maximization algorithm for multivariate Gaussian mixture models. Our methods are adaptions of the well-known K-means++ initialization and the Gonzalez algorithm. Thereby we aim to close the gap between simple random, e.g. uniform, and complex methods, that crucially depend on the right choice of hyperparameters. Our extensive experiments indicate the usefulness of our methods compared to common techniques and methods, which e.g. apply the original K-means++ and Gonzalez directly, with respect to artificial as well as real-world data sets.",2016,Knowledge Discovery and Data Mining,econometrics;machine learning;statistics;computer science;
Constraint Based Subspace Clustering for High Dimensional Uncertain Data,Xianchao Zhang (Dalian University of Technology);Lu Gao (Dalian University of Technology);Hong Yu (Dalian University of Technology);,"2132650836,2565296157,2250407094","Both uncertain data and high-dimensional data pose huge challenges to traditional clustering algorithms. It is even more challenging for clustering high dimensional uncertain data and there are few such algorithms. In this paper, based on the classical FINDIT subspace clustering algorithm for high dimensional data, we propose a constraint based semi-supervised subspace clustering algorithm for high dimensional uncertain data, UFINDIT. We extend both the distance functions and dimension voting rules of FINDIT to deal with high dimensional uncertain data. Since the soundness criteria of FINDIT fails for uncertain data, we introduce constraints to solve the problem. We also use the constraints to improve FINDIT in eliminating parameters' effect on the process of merging medoids. Furthermore, we propose some methods such as sampling to get an more efficient algorithm. Experimental results on synthetic and real data sets show that our proposed UFINDIT algorithm outperforms the existing subspace clustering algorithm for uncertain data.",2016,Knowledge Discovery and Data Mining,canopy clustering algorithm;correlation clustering;constrained clustering;data stream clustering;cure data clustering algorithm;fuzzy clustering;cluster analysis;data mining;machine learning;mathematical optimization;mathematics;
Personal Credit Profiling via Latent User Behavior Dimensions on Social Media,Guangming Guo (University of Science and Technology of China);Feida Zhu (Singapore Management University);Enhong Chen (University of Science and Technology of China);Le Wu (University of Science and Technology of China);Qi Liu (University of Science and Technology of China);Yingling Liu (University of Science and Technology of China);Minghui Qiu (Singapore Management University);,"2165049579,2160602068,2136372366,2181678313,2420624292,2439722024,2158082241","Consumer credit scoring and credit risk management have been the core research problem in financial industry for decades. In this paper, we target at inferring this particular user attribute called credit, i.e., whether a user is of the good credit class or not, from online social data. However, existing credit scoring methods, mainly relying on financial data, face severe challenges when tackling the heterogeneous social data. Moreover, social data only contains extremely weak signals about users' credit label. To that end, we put forward a Latent User Behavior Dimension based Credit Modeli¾źLUBD-CM to capture these small signals for personal credit profiling. LUBD-CM learns users' hidden behavior habits and topic distributions simultaneously, and represents each user at a much finer granularity. Specifically, we take a real-world Sina Weibo dataset as the testbed for personal credit profiling evaluation. Experiments conducted on the dataset demonstrate the effectiveness of our approach: 1 User credit label can be predicted using LUBD-CM with a considerable performance improvement over state-of-the-art baselines; 2 The latent behavior dimensions have very good interpretability in personal credit profiling.",2016,Knowledge Discovery and Data Mining,social network;computer security;data mining;computer science;
A Precise and Robust Clustering Approach Using Homophilic Degrees of Graph Kernel,Haolin Yang (Tsinghua University);Deli Zhao (HTC);Lele Cao (Tsinghua University);Fuchun Sun (Tsinghua University);,"2154347452,2729263704,2113194918,2118495735","To address the difficulties of ""data noise sensitivity"" and ""cluster center variance"" in mainstream clustering algorithms, we propose a novel robust approach for identifying cluster centers unambiguously from data contaminated with noise; it incorporates the strength of homophilic degrees and graph kernel. Exploiting that in-degrees can breed the homophilic distribution if ordered by their associated sorted out-degrees, it is easy to separate clusters from noise. Then we apply the diffusion kernel to the graph formed by clusters so as to obtain graph kernel matrix, which is treated as the measurement of global similarities. Based on local data densities and global similarities, the proposed approach manages to identify cluster centers precisely. Experiments on various synthetic and real-world databases verify the superiority of our algorithm in comparison with state-of-the-art algorithms.",2016,Knowledge Discovery and Data Mining,theoretical computer science;data mining;machine learning;statistics;computer science;
Attribute Selection and Classification of Prostate Cancer Gene Expression Data Using Artificial Neural Networks,Sreenivas Sremath Tirumala (Auckland University of Technology);A. Narayanan (Auckland University of Technology);,"2304086066,2179871709","Artificial Intelligence AI approaches for medical diagnosis and prediction of cancer are important and ever growing areas of research. Artificial Neural Networks ANN is one such approach that have been successfully applied in these areas. Various types of clinical datasets have been used in intelligent decision making systems for medical diagnosis, especially cancer for over three decades. However, gene expression datasets are complex with large numbers of attributes which make it more difficult for AI approaches to classification and prediction. Prostate Cancer dataset is one such dataset with 12600 attributes and only 102 samples. In this paper, we propose an extended ANN based approach for classification and prediction of prostate cancer using gene expression data. Firstly, we use four attribute selection approaches, namely Sequential Floating Forward Selection SFFS, RELIEFF, Sequential Backward Feature Section SFBS and Significant Attribute Evaluation SAE to identify the most influential attributes among 12600. We use ANNs and Naive Bayes for classification with complete sets of attributes as well as various sets obtained from attribute selection methods. Experimental results show that ANN outperformed Naive Bayes by achieving a classification accuracy of 98.2i¾?% compared to 62.74i¾?% with the full set of attributes. Further, with 21 selected attributes obtained with SFFS, ANNs achieved better accuracy 100i¾?% for classification compared to Naive Bayes. For prediction using ANNs, SFFS was able achieve best results with 92.31i¾?% of accuracy by correctly predicting 24 out of 26 samples provided for independent sample testing. Moreover, some of the gene selected by SFFS are identified to have a direct reference to cancer and tumour. Our results indicate that a combination of standard feature selection methods in conjunction with ANNs provide the most impressive results.",2016,Knowledge Discovery and Data Mining,artificial neural network;data mining;pattern recognition;machine learning;computer science;
Denoising Time Series by Way of a Flexible Model for Phase Space Reconstruction,Minhazul Islam Sk (University of Florida);Arunava Banerjee (University of Florida);,"2501234868,2119802459","We present a denoising technique in the domain of time series data that presumes a model for the uncorrupted underlying signal rather than a model for noise. Specifically, we show how the non-linear reconstruction of the underlying dynamical system by way of time delay embedding yields a new solution for denoising where the underlying dynamics is assumed to be highly non-linear yet low-dimensional. The model for the underlying data is recovered using a non-parametric Bayesian approach and is therefore very flexible. The proposed technique first clusters the reconstructed phase space through a Dirichlet Process Mixture of Exponential density, an infinite mixture model. Phase Space Reconstruction is accomplished by time delay embedding in the framework of Taken's Embedding Theorem with the underlying dimension being determined by the False Neighborhood method. Next, an Infinite Mixtures of Linear Regression via Dirichlet Process is used to non-linearly map the phase space data points to their respective temporally subsequent points in the phase space. Finally, a convex optimization based approach is used to restructure the dynamics by perturbing the phase space points to create the new denoised time series. We find that this method yields significantly better performance in noise reduction, power spectrum analysis and prediction accuracy of the phase space.",2016,Knowledge Discovery and Data Mining,machine learning;mathematical optimization;statistics;mathematics;
Predicting Phone Usage Behaviors with Sensory Data Using a Hierarchical Generative Model,Chuankai An (Dartmouth College);Dan Rockmore (Dartmouth College);,"2693990751,53499937","Using a sizable set of sensory data and related usage records on Android devices, we are able to give a reasonable prediction of three imporant aspects of phone usage: messages, phone calls and cellular data. We solve the problem via an estimation of a user's daily routine, on which we can train a hierarchical generative model on phone usages in all time slots of a day. The model generates phone usage behaviors in terms of three kinds of data: the state of user-phone interaction, occurrence times of an activity and the duration of the activity in each occurrence. We apply the model on a dataset with 107 frequent users, and find the prediction error of generative model is the smallest when compare with several other baseline methods. In addition, CDF curves illustrate the availability of generative model for most users with the distribution of prediction error for all test cases. We also explore the effects of time slots in a day, as well as size of training and test sets. The results suggest several interesting directions for further research.",2016,Knowledge Discovery and Data Mining,generative model;speech recognition;data mining;machine learning;computer science;
Linear Upper Confidence Bound Algorithm forźContextual Bandit Problem with Piled Rewards,Kuan-Hao Huang (National Taiwan University);Hsuan-Tien Lin (National Taiwan University);,"2113276713,2127632057","We study the contextual bandit problem with linear payoff function. In the traditional contextual bandit problem, the algorithm iteratively chooses an action based on the observed context, and immediately receives a reward for the chosen action. Motivated by a practical need in many applications, we study the design of algorithms under the piled-reward setting, where the rewards are received as a pile instead of immediately. We present how the Linear Upper Confidence Bound LinUCB algorithm for the traditional problem can be naively applied under the piled-reward setting, and prove its regret bound. Then, we extend LinUCB to a novel algorithm, called Linear Upper Confidence Bound with Pseudo Reward LinUCBPR, which digests the observed contexts to choose actions more strategically before the piled rewards are received. We prove that LinUCBPR can match LinUCB in the regret bound under the piled-reward setting. Experiments on the artificial and real-world datasets demonstrate the strong performance of LinUCBPR in practice.",2016,Knowledge Discovery and Data Mining,artificial intelligence;machine learning;
Distributed Sequential Pattern Mining in Large Scale Uncertain Databases,Jiaqi Ge (Indiana University – Purdue University Indianapolis);Yuni Xia (Indiana University – Purdue University Indianapolis);,"2107884066,2334154954","While sequential pattern mining SPM is an import application in uncertain databases, it is challenging in efficiency and scalability. In this paper, we develop a dynamic programming DP approach to mine probabilistic frequent sequential patterns in distributed computing platform Spark. Directly applying the DP method to Spark is impractical because its memory-consuming characteristic may cause heavy JVM garbage collection overhead in Spark. Therefore, we design a memory-efficient distributed DP approach and use an extended prefix-tree to save intermediate results efficiently. The extensive experimental results in various scales prove that our method is orders of magnitude faster than straight-forward approaches.",2016,Knowledge Discovery and Data Mining,sequential pattern mining;data mining;database;real time computing;computer science;
Privacy Aware K-Means Clustering with High Utility,Thanh Dai Nguyen (Deakin University);Sunil Kumar Gupta 0001 (Deakin University);Santu Rana (Deakin University);Svetha Venkatesh (Deakin University);,"2549737843,2119406083,2142238370,2146461601","Privacy-preserving data mining aims to keep data safe, yet useful. But algorithms providing strong guarantees often end up with low utility. We propose a novel privacy preserving framework that thwarts an adversary from inferring an unknown data point by ensuring that the estimation error is almost invariant to the inclusion/exclusion of the data point. By focusing directly on the estimation error of the data point, our framework is able to significantly lower the perturbation required. We use this framework to propose a new privacy aware K-means clustering algorithm. Using both synthetic and real datasets, we demonstrate that the utility of this algorithm is almost equal to that of the unperturbed K-means, and at strict privacy levels, almost twice as good as compared to the differential privacy counterpart.",2016,Knowledge Discovery and Data Mining,internet privacy;computer security;data mining;computer science;
Online Learning for Accurate Real-Time Map Matching,Biwei Liang (Peking University);Tengjiao Wang (Peking University);Shun Li (University of International Relations);Wei Chen (Peking University);Hongyan Li (Peking University);Kai Lei (Peking University);,"2538735692,2144373700,2397239839,2527738285,2430329963,2262537035","For the reason that deviation exists between GPS traces obtained by real-time positioning system and actual paths, real-time map matching which identifies the correct traveling road segment, becomes increasingly important. In order to effectively improve map matching accuracy, most state-of-art real-time map matching algorithms use machine learning which calls for time-consuming human labeling in advance. We propose an accurate real-time map matching method using online learning called OLMM. It takes into account a small piece of trajectory data and their matching result to support the subsequent matching process. We evaluate the effectiveness of the proposed approach using ground truth data. The results demonstrate that our approach can obtain more accurate matching results than existing methods without any human labeling beforehand.",2016,Knowledge Discovery and Data Mining,optimal matching;computer vision;data mining;machine learning;computer science;
Transfer-Learning Based Model for Reciprocal Recommendation,Chia-Hsin Ting (National Taiwan University);Hung-Yi Lo (National Taiwan University);Shou-De Lin (National Taiwan University);,"2559197646,2156980939,2114357324","This paper tackles the reciprocal recommendation task which has various applications such as online dating, employee recruitment and mentor-mentee matching. The major difference between traditional recommender systems and reciprocal recommender systems is that a reciprocal recommender has to satisfy the preference on both directions. This paper proposes a simple yet novel regularization term, the Mutual-Attraction Indicator, to model the mutual preferences of both parties. Given such indicator, we design a transfer-learning based CF model for reciprocal recommender. The experiments are based on two real world tasks, online dating and human resource matching, showing significantly improved performance over the original factorization model and state-of-the-art reciprocal recommenders.",2016,Knowledge Discovery and Data Mining,transfer of learning;multimedia;artificial intelligence;machine learning;computer science;
Towards a New Evolutionary Subsampling Technique for Heuristic Optimisation of Load Disaggregators,Michael Mayo (University of Waikato);Sara Omranian (University of Waikato);,"2166211886,2530592766","In this paper we present some preliminary work towards the development of a new evolutionary subsampling technique for solving the non-intrusive load monitoring NILM problem. The NILM problem concerns using predictive algorithms to analyse whole-house energy usage measurements, so that individual appliance energy usages can be disaggregated. The motivation is to educate home owners about their energy usage. However, by their very nature, the datasets used in this research are massively imbalanced in their target value distributions. Consequently standard machine learning techniques, which often rely on optimising for root mean squared error RMSE, typically fail. We therefore propose the target-weighted RMSE TW-RMSE metric as an alternative fitness function for optimising load disaggregators, and show in a simple initial study in which random search is utilised that TW-RMSE is a metric that can be optimised, and therefore has the potential to be included in a larger evolutionary subsampling-based solution to this problem.",2016,Knowledge Discovery and Data Mining,fitness function;data mining;artificial intelligence;machine learning;statistics;computer science;
Fast and Semantic Measurements on Collaborative Tagging Quality,Yuqing Sun (Shandong University);Haiqi Sun (Shandong University);Reynold Cheng (University of Hong Kong);,"2706175349,2639631723,2138267588","This paper focuses on the problem of tagging quality evaluation in collaborative tagging systems. By investigating the dynamics of tagging process, we find that high frequency tags almost cover the main aspects of a resource content and can be determined stable much earlier than a whole tag set. Motivated by this finding, we design the swapping index and smart moving index on tagging quality. We also study the correlations in tag usage and propose the semantic measurement on tagging quality. The proposed methods are evaluated against real datasets and the results show that they are more efficient than previous methods, which are appropriate for a large number of web resources. The effectiveness is justified by the results in tag based applications. The light weight metrics bring a little loss on the performance, while the semantic metric is better than current methods.",2016,Knowledge Discovery and Data Mining,world wide web;information retrieval;data mining;computer science;
Phishing Detection on Twitter Streams,Se Yeong Jeong (University of Auckland);Yun Sing Koh (University of Auckland);Gillian Dobbie (University of Auckland);,"2531645972,1987110578,2002900660","With the prevalence of cutting-edge technology, the social media network is gaining popularity and is becoming a worldwide phenomenon. Twitter is one of the most widely used social media sites, with over 500 million users all around the world. Along with its rapidly growing number of users, it has also attracted unwanted users such as scammers, spammers and phishers. Research has already been conducted to prevent such issues using network or contextual features with supervised learning. However, these methods are not robust to changes, such as temporal changes or changes in phishing trends. Current techniques also use additional network information. However, these techniques cannot be used before spammers form a particular number of user relationships. We propose an unsupervised technique that detects phishing in Twitter using a 2-phase unsupervised learning algorithm called PDT Phishing Detector for Twitter. From the experiments we show that our technique has high accuracy ranging between 0.88 and 0.99.",2016,Knowledge Discovery and Data Mining,dbscan;internet privacy;world wide web;computer security;machine learning;computer science;
Multi-hypergraph Incidence Consistent Sparse Coding for Image Data Clustering,Xiaodong Feng (University of Electronic Science and Technology of China);Sen Wu (University of Science and Technology Beijing);Wenjun Zhou (University of Tennessee);Zhiwei Tang (University of Electronic Science and Technology of China);,"2437673952,2706952931,2097769770,2492638385","Sparse representation has been a powerful technique for modeling image data and thus enhance the performance of image clustering. Sparse coding, as an unsupervised way to extract sparse representation, learns a dictionary that represents high-level semantics and the new representations on the dictionary. Though existing sparse coding schemes are considering local manifold structure of the data with graph/hypergraph regularization, more from the manifold should be exploited to utilize intrinsic manifold characteristics in the data. In this paper, we firstly propose a Hypergraph Incidence Consistency regularization term by minimizing the reconstruction error of the hypergraph incidence matrix with sparse codes to further regulate the learned sparse codes with hypergraph-based manifold. Moreover, a multi-hypergraph learning framework to automatically select the optimal manifold structure is integrated into the objective of sparse coding learning, resulting in multi-hypergraph incidence Consistent Sparse Coding MultiCSC. We show that the MultiCSC objective function can be optimized efficiently, and that several existing sparse coding methods are special cases of MultiCSC. Extensive experimental results on image clustering demonstrate the effectiveness of our proposed method.",2016,Knowledge Discovery and Data Mining,k svd;neural coding;sparse approximation;theoretical computer science;pattern recognition;machine learning;mathematics;
An Improved Self-Structuring Neural Network,Rami M. Mohammad (University of Huddersfield);Fadi A. Thabtah (Nelson Marlborough Institute of Technology);Lee McCluskey (University of Huddersfield);,"2153773975,2511251656,2094120944","Creating a neural network based classification model is traditionally accomplished using the trial and error technique. However, the trial and error structuring method nornally suffers from several difficulties including overtraining. In this article, a new algorithm that simplifies structuring neural network classification models has been proposed. It aims at creating a large structure to derive classifiers from the training dataset that have generally good predictive accuracy performance on domain applications. The proposed algorithm tunes crucial NN model thresholds during the training phase in order to cope with dynamic behavior of the learning process. This indeed may reduce the chance of overfitting the training dataset or early convergence of the model. Several experiments using our algorithm as well as other classification algorithms, have been conducted against a number of datasets from University of California Irvine UCI repository. The experiments' are performed to assess the pros and cons of our proposed NN method. The derived results show that our algorithm outperformed the compared classification algorithms with respect to several performance measures.",2016,Knowledge Discovery and Data Mining,phishing;pruning;structure;biological classification;artificial neural network;data mining;artificial intelligence;machine learning;statistics;computer science;
A Hierarchical Beta Process Approach for Financial Time Series Trend Prediction,Mojgan Ghanavati (University of New South Wales);Raymond K. Wong (University of New South Wales);Fang Chen (University of New South Wales);Yang Wang (NICTA);Joe Lee;,"2153149482,2290077475,2656583616,2675046882,2677831969","An automatic stock market categorization system would be invaluable to investors and financial experts, providing them with the opportunity to predict a stock price changes with respect to the other stocks. In recent years, clustering all companies in the stock markets based on their similarities in shape of the stock market has increasingly become popular. However, existing approaches may not be practical because the stock price data are high-dimensional data and the changes in the stock price usually occur with shift, which makes the categorization more complex. In this paper, a hierarchical beta process HBP based approach is proposed for stock market trend prediction. Preliminary results show that the approach is promising and outperforms other popular approaches.",2016,Knowledge Discovery and Data Mining,actuarial science;
A Rule Based Open Information Extraction Method Using Cascaded Finite-State Transducer,Hailun Lin (Chinese Academy of Sciences);Yuanzhuo Wang (Chinese Academy of Sciences);Peng Zhang (Chinese Academy of Sciences);Weiping Wang (Chinese Academy of Sciences);Yinliang Yue (Chinese Academy of Sciences);Zheng Lin (Chinese Academy of Sciences);,"2103889526,2120380447,2120503182,2597937528,2717669663,2085002608","In this paper, we present R-OpenIE, a rule based open information extraction method using cascaded finite-state transducer. R-OpenIE defines contextual constraint declarative rules to generate relation extraction templates, which frees from the influence of syntactic parser errors, and it uses cascaded finite-state transducer model to match the satisfied relational tuples. It is noted that R-OpenIE creates inverted index for each matched state during the matching process of cascaded finite-state transducer, which improves the efficiency of pattern matching. The experimental results have shown that our R-OpenIE can achieve good adaptability and efficiency for open information extraction.",2016,Knowledge Discovery and Data Mining,relationship extraction;data mining;pattern recognition;machine learning;computer science;
Computing Hierarchical Summary of the Data Streams,Zubair Shah (University of New South Wales);Abdun Naser Mahmood (University of New South Wales);Michael Barlow (University of New South Wales);,"2609482565,2095913062,2134092914","Data stream processing is an important function in many online applications such as network traffic analysis, web applications, and financial data analysis. Computing summaries of data stream is challenging since streaming data is generally unbounded, and cannot be permanently stored or accessed more than once. In this paper, we have proposed two counter based hierarchical CHS $$\epsilon $$---approximation algorithms to create hierarchical summaries of one dimensional data. CHS maintains a data structure, where each entry contains the incoming data item and an associated counter to store its frequency. Since every item in streaming data cannot be stored, CHS only maintains frequent items known as hierarchical heavy hitters at various levels of generalization hierarchy by exploiting the natural hierarchy of the data. The algorithm guarantees accuracy of count within an $$\epsilon $$ bound. Furthermore, using aperiodic CHS-A and periodic CHS-P compression strategy the proposed technique offers improved space complexities of $$O\frac{\eta }{\epsilon }$$ and $$O\frac{\eta }{\epsilon }\log \epsilon N$$, respectively. We provide theoretical proofs for both space and time requirements of CHS algorithm. We have also experimentally compared the proposed algorithm with the existing benchmark techniques. Experimental results show that the proposed algorithm requires fewer updates per element of data, and uses a moderate amount of bounded memory. Moreover, precision-recall analysis demonstrates that CHS algorithm provides a high quality output compared to existing benchmark techniques. For the experimental validation, we have used both synthetic data derived from an open source generator, and real benchmark data sets from an international Internet Service Provider.",2016,Knowledge Discovery and Data Mining,data stream mining;world wide web;data mining;database;machine learning;statistics;computer science;
Clinical Decision Support for Stroke Using Multi---view Learning Based Models for NIHSS Scores,"Vaibhav Rajan (Xerox);Sakyajit Bhattacharya (Xerox);Ranjan Shetty (Kasturba Medical College, Manipal);Amith Sitaram;G. Vivek (Kasturba Medical College, Manipal);","1988331176,2112986011,2107658649,2521828683,2063671572","Cerebral stroke is a leading cause of physical disability and death in the world. The severity of a stroke is assessed by a neurological examination using a scale known as the NIH stroke scale NIHSS. As a measure of stroke severity, the NIHSS score is widely adopted and has been found to also be useful in outcome prediction, rehabilitation planning and treatment planning. In many applications, such as in patient triage in under---resourced primary health care centres and in automated clinical decision support tools, it would be valuable to obtain the severity of stroke with minimal human intervention using simple parameters like age, past conditions and blood investigations. In this paper we propose a new model for predicting NIHSS scores which, to our knowledge, is the first statistical model for stroke severity. Our multi---view learning approach can handle data from heterogeneous sources with mixed data distributions binary, categorical and numerical and is robust against missing values --- strengths that many other modeling techniques lack. In our experiments we achieve better predictive accuracy than other commonly used methods.",2016,Knowledge Discovery and Data Mining,data mining;
Enabling Hierarchical Dirichlet Processes to Work Better for Short Texts at Large Scale,Khai Mai (Hanoi University of Science and Technology);Sang Mai (Hanoi University of Science and Technology);Anh Nguyen (Hanoi University of Science and Technology);Ngo Van Linh (Hanoi University of Science and Technology);Khoat Than (Hanoi University of Science and Technology);,"2510839171,2512314896,2103572204,2102101224,2116469936","Analyzing texts from social media often encounters many challenges, including shortness, dynamic, and huge size. Short texts do not provide enough information so that statistical models often fail to work. In this paper, we present a very simple approach namely, bag-of-biterms that helps statistical models such as Hierarchical Dirichlet Processes HDP to work well with short texts. By using both terms words and biterms to represent documents, bag-of-biterms BoB provides significant benefits: 1 it naturally lengthens representation and thus helps us reduce bad effects of shortness; 2 it enables the posterior inference in a large class of probabilistic models including HDP to be less intractable; 3 no modification of existing models/methods is necessary, and thus BoB can be easily employed in a wide class of statistical models. To evaluate those benefits of BoB, we take Online HDP into account in that it can deal with dynamic and massive text collections, and we do experiments on three large corpora of short texts which are crawled from Twitter, Yahoo Q&A, and New York Times. Extensive experiments show that BoB can help HDP work significantly better in both predictiveness and quality.",2016,Knowledge Discovery and Data Mining,data science;data mining;artificial intelligence;machine learning;statistics;computer science;
Imbalanced ELM Based on Normal Density Estimation for Binary-Class Classification,Yulin He (Shenzhen University);Rana Aamir Raza Ashfaq (Shenzhen University);Joshua Zhexue Huang (Shenzhen University);Xizhao Wang (Shenzhen University);,"2559741520,2297205250,2699884321,2617279703","The imbalanced Extreme Learning Machine based on kernel density estimation imELM-kde is a latest classification algorithm for handling the imbalanced binary-class classification. By adjusting the real outputs of training data with intersection point of two probability density functions p.d.f.s corresponding to the predictive outputs of majority and minority classes, imELM-kde updates ELM which is trained based on the original training data and thus improves the performance of ELM-based imbalanced classifier. In this paper, we analyze the shortcomings of imELM-kde and then propose an improved version of imELM-kde. The Parzen window method used in imELM-kde leads to multiple intersection points between p.d.f.s of majority and minority classes. In addition, it is unreasonable to update the real outputs with intersection point, because the p.d.f.s are estimated based on the predictive outputs. Thus, in order to improve the shortcomings of imELM-kde, an imbalanced ELM based on normal density estimation imELM-nde is proposed in this paper. In imELM-nde, the p.d.f.s of predictive outputs corresponding to majority and minority classes are computed with normal density estimation and the intersection point is used to update the predictive outputs instead of real outputs. This makes the training of probability density estimation-based imbalanced ELM simpler and more feasible. The comparative results show that our proposed imELM-nde performs better than unweighted ELM and imELM-kde for imbalanced binary-class classification problem.",2016,Knowledge Discovery and Data Mining,kernel density estimation;probability density function;data mining;pattern recognition;machine learning;statistics;mathematics;
FeRoSA: A Faceted Recommendation System for Scientific Articles,Tanmoy Chakraborty (Indian Institute of Technology Kharagpur);Amrith Krishna (Indian Institute of Technology Kharagpur);Mayank Singh (Indian Institute of Technology Kharagpur);Niloy Ganguly (Indian Institute of Technology Kharagpur);Pawan Goyal (Indian Institute of Technology Kharagpur);Animesh Mukherjee (Indian Institute of Technology Kharagpur);,"2471175502,2223599978,2322263662,2097625090,2556932677,2134540012","The overwhelming number of scientific articles over the years calls for smart automatic tools to facilitate the process of literature review. Here, we propose for the first time a framework of faceted recommendation for scientific articles abbreviated as FeRoSA which apart from ensuring quality retrieval of scientific articles for a query paper, also efficiently arranges the recommended papers into different facets categories. Providing users with an interface which enables the filtering of recommendations across multiple facets can increase users' control over how the recommendation system behaves. FeRoSA is precisely built on a random walk based framework on an induced subnetwork consisting of nodes related to the query paper in terms of either citations or content similarity. Rigorous analysis based an experts' judgment shows that FeRoSA outperforms two baseline systems in terms of faceted recommendations overall precision of 0.65. Further, we show that the faceted results of FeRoSA can be appropriately combined to design a better flat recommendation system as well. An experimental version of FeRoSA is publicly available at www.ferosa.org receiving as many as 170 hits within the first 15 days of launch.",2016,Knowledge Discovery and Data Mining,world wide web;information retrieval;data mining;computer science;
Query-Focused Multi-document Summarization Based on Concept Importance,Hai-Tao Zheng (Tsinghua University);Ji-Min Guo (Tsinghua University);Yong Jiang (Tsinghua University);Shu-Tao Xia (Tsinghua University);,"2128052405,2644077339,2584127598,2142362674","With the exponential growth of the web documents and the requirement of limited bandwidth for mobile devices, it becomes more and more difficult for users to get information they look forward to from the vast amount of information. Query-focused summarization gets more attention from both the research and engineering area in recent years. However, existing query-focused summarization methods don't consider the conceptual relation and the concept importance that make up the sentences, a concept is the title of a wikipedia article and can express an entity or action. In this article. We propose a novel method called Query-focused Multi-document Summarization based on Concept Importance QMSCI. We first map sentence to concepts and get ranked weighted concepts by reinforcement between the concepts of sentences and concepts of the query in a bipartite graph, then we use the ranked weighted concepts to help to rank the sentences in a hyper-graph model, sentences that contain important concepts, related with the query and also central among sentences are ranked higher and comprise the summary. We experiment on the DUC datasets, the experimental result demonstrates the effectiveness of our proposed method compared to the state-of-art methods.",2016,Knowledge Discovery and Data Mining,multi document summarization;automatic summarization;natural language processing;information retrieval;data mining;machine learning;computer science;
Keystroke Biometric Recognition on Chinese Long Text Input,Xiaodong Li (Southwestern University of Finance and Economics);Jiafen Liu (Southwestern University of Finance and Economics);,"2545172367,2627148494","Keystroke Biometric is useful in distinguishing legal users from perpetrators in online activities. Most previous keystroke studies focus on short text, however short text keystroke can only be used in limited scenarios such as user name and password input and provide one-time authentication. In this paper, we concentrate on how to detect whether current user is the legal one during the whole activity, such as writing an E-mail and chat online. We developed a JAVA applet to collect raw data, and then extracted features and constructed 4 classifiers. In the experiment, we required 30 users to choose a topic randomly and then type in a text about 400 Chinese characters on it. This experiment repeated 9 times in different days under the same typing environment. The accuracy of different methods shifts from 94.07i¾?% to 98.15i¾?%, the FAR reaches to 0.74i¾?% and FRR to 1.15i¾?%. In summary, Chinese free long text keystroke biometric recognition can be used to authenticate users during the whole online activity with satisfactory precision.",2016,Knowledge Discovery and Data Mining,keystroke dynamics;authentication;internet privacy;world wide web;speech recognition;computer science;
Event Prediction in Healthcare Analytics: Beyond Prediction Accuracy,Lina Fu (PARC);Faming Li (PARC);Jing Zhou (PARC);Xuejin Wen (PARC);Jinhui Yao (PARC);Michael Shepherd (PARC);,"2252213683,2521841085,2432412799,2535590692,2628471814,2599164936","During the recent few years, the United States healthcare industry is under unprecedented pressure to improve outcome and reduce cost. Many healthcare organizations are leveraging healthcare analytics, especially predictive analytics in moving towards these goals and bringing better value to the patients. While many existing event prediction models provide helpful predictions in terms of accuracy, their use are typically limited to prioritizing individual patients for care management at fixed time points. In this paper we explore Enhanced Modeling approaches around two important aspects: 1 model interpretability; 2 flexible prediction window. Better interpretability of the model will guide us towards more effective intervention design. Flexible prediction window can provide a higher resolution picture of patients' risks of adverse events over time, and thereby enable timely interventions. We illustrate interpretation and insights from our Bayesian Hierarchical Model for readmission prediction, and demonstrate flexible prediction window with Random Survival Forests model for prediction of future emergency department visits.",2016,Knowledge Discovery and Data Mining,analytics;predictive analytics;bayesian hierarchical modeling;data science;data mining;machine learning;simulation;computer science;
Efficient Iris Image Segmentation for ATM Based Approach Through Fuzzy Entropy and Graph Cut,Shibai Yin (Southwestern University of Finance and Economics);Yibin Wang (Northwestern Polytechnical University);Tao Wang (Southwestern University of Finance and Economics);,"2643875572,2714109551,2695630697","In order to realize accurate personal identification in the ATMS, an efficient iris image segmentation approach based on the fuzzy 4-partition entropy and graph cut is presented which can not only yield noisy segmentation results but short the running time. In this paper, an iterative calculation scheme is presented for reducing redundant computations in fuzzy 4-entropy evaluation. Then the presented algorithm uses the probabilities of 4 fuzzy events to define the costs of 4 label assignments iris, pupil, background and eyelash for each region in the graph cut. The final segmentation result is computed using graph cut, which produces smooth segmentation result and yields noise. The experimental results demonstrate the presented iterative calculation scheme can greatly reduce the running time. Quantitative evaluations over 20 classic iris images also show that our algorithm outperforms existing iris image segmentation approaches.",2016,Knowledge Discovery and Data Mining,scale space segmentation;segmentation based object categorization;cut;image segmentation;computer vision;machine learning;mathematical optimization;mathematics;
A Social Spam Detection Framework via Semi-supervised Learning,Xianchao Zhang (Dalian University of Technology);Haijun Bai (Dalian University of Technology);Wenxin Liang (Dalian University of Technology);,"2132650836,2531861419,2103505816","With the increasing popularity of social networking websites such as Twitter, Facebook, Sina Weibo and MySpace, spammers on them are getting more and more rampant. Social spammers always create a mass of compromised or fake accounts to deceive users and lead them to access malicious websites which contain illegal, pornography or dangerous information. As we all know, most of the studies on social spam detection are based on supervised machine learning which requires plenty of annotated datasets. Unfortunately, labeling a large number of datasets manually is a complex, error-prone and tedious task which may costs a lot of human efforts and time. In this paper, we propose a novel semi-supervised classification framework for social spam detection, which combines co-training with k-medoids. First we utilize k-medoids clustering algorithm to acquire some informative and presentative samples for labelling as our initial seeds set. Then we take advantage of the content features and behavior features of users for our co-training classification framework. In order to illustrate the effectiveness of k-medoids, we compare the performance with random selecting strategy. Finally, we evaluate the effectiveness of our proposed detection framework compared with several classical supervised algorithms.",2016,Knowledge Discovery and Data Mining,spambot;social spam;k medoids;semi supervised learning;internet privacy;world wide web;data mining;machine learning;computer science;
Rigidly Self-Expressive Sparse Subspace Clustering,Linbo Qiao (National University of Defense Technology);Bofeng Zhang (National University of Defense Technology);Yipin Sun (National University of Defense Technology);Jinshu Su (National University of Defense Technology);,"2095838241,2136605010,2116260882,2109843016","Sparse subspace clustering is a well-known algorithm, and it is widely used in many research field nowadays, and a lot effort has been contributed to improve it. In this paper, we propose a novel approach to obtain the coefficient matrix. Compared with traditional sparse subspace clustering SSC approaches, the key advantage of our approach is that it provides a new perspective of the self-expressive property. We call it rigidly self-expressive RSE property. This new formulation captures the rigidly self-expressive property of the data points in the same subspace, and provides a new formulation for sparse subspace clustering. Extensions to traditional SSC could also be cooperating with this new formulation. We present a first-order algorithm to solve the nonconvex optimization, and further prove that it converges to a KKT point of the nonconvex problem under certain standard assumptions. Extensive experiments on the Extended Yale B dataset, the USPS digital images dataset, and the Columbia Object Image Library shows that for images with upi¾?to 30i¾?% missing pixels the clustering quality achieved by our approach outperforms the original SSC.",2016,Knowledge Discovery and Data Mining,cluster analysis;data mining;machine learning;mathematical optimization;statistics;mathematics;
Recommendation Algorithm Design in a Land Exchange Platform,Xubin Luo;Jiang Duan;,"2655618462,2648965982","In China the majority of the farmlands are small pieces, which should be circulated and aggregated to a larger scale and pave the way for modern farms. A Platform needs to be built to connect the small landowners and the potential new farmers or investors. This paper proposes an efficient recommendation algorithm that takes both the space attributes and other properties of farmland pieces into consideration and produce best selection for the intended potential new farmers or investors with customized object functions.",2016,Knowledge Discovery and Data Mining,data mining;simulation;
An Efficient Dynamic Programming Algorithm for STR-IC-STR-IC-LCS Problem,Daxin Zhu (Quanzhou Normal University);Yingjie Wu (Fuzhou University);Xiaodong Wang (Fujian University of Technology);,"2100681708,2291935952,2677196269","In this paper, we consider a generalized longest common subsequence problem, in which a constraining sequence of length s must be included as a substring and the other constraining sequence of length t must be included as a subsequence of two main sequences and the length of the result must be maximal. For the two input sequences X and Y of lengths n and m, and the given two constraining sequences of length s and t, we present an Onmst time dynamic programming algorithm for solving the new generalized longest common subsequence problem. The time complexity can be reduced further to cubic time in a more detailed analysis. The correctness of the new algorithm is proved.",2016,Knowledge Discovery and Data Mining,hunt mcilroy algorithm;longest alternating subsequence;longest repeated substring problem;longest common substring problem;longest increasing subsequence;substring;longest common subsequence problem;dynamic programming;time complexity;discrete mathematics;combinatorics;mathematical optimization;algorithm;computer science;mathematics;
A Survey on Truth Discovery,Yaliang Li (University at Buffalo);Jing Gao (University at Buffalo);Chuishi Meng (University at Buffalo);Qi Li (University at Buffalo);Lu Su (University at Buffalo);Bo Zhao (LinkedIn);Wei Fan (Baidu);Jiawei Han (University of Illinois at Urbana–Champaign);,"2116094297,2096731881,2116340933,2261907930,2148733542,2674375462,2422054197,2121939561","Thanks to information explosion, data for the objects of interest can be collected from increasingly more sources. However, for the same object, there usually exist conflicts among the collected multi-source information. To tackle this challenge, truth discovery, which integrates multi-source noisy information by estimating the reliability of each source, has emerged as a hot topic. Several truth discovery methods have been proposed for various scenarios, and they have been successfully applied in diverse application domains. In this survey, we focus on providing a comprehensive overview of truth discovery methods, and summarizing them from different aspects. We also discuss some future directions of truth discovery research. We hope that this survey will promote a better understanding of the current progress on truth discovery, and offer some guidelines on how to apply these approaches in application domains.",2016,Knowledge Discovery and Data Mining,data science;data mining;artificial intelligence;
An Interactive Data Repository with Visual Analytics,Ryan A. Rossi (PARC);Nesreen K. Ahmed (Intel);,"2060818872,2119838086","Scientific data repositories have historically made data widely accessible to the scientific community, and have led to better research through comparisons, reproducibility, as well as further discoveries and insights. Despite the growing importance and utilization of data repositories in many scientific disciplines, the design of existing data repositories has not changed for decades. In this paper, we revisit the current design and envision interactive data repositories, which not only make data accessible, but also provide techniques for interactive data exploration, mining, and visualization in an easy, intuitive, and free-flowing manner.",2016,Knowledge Discovery and Data Mining,information repository;network science;graph drawing;information visualization;visual analytics;
Shedding Light on the Performance of Solar Panels: A Data-Driven View,Sue A. Chen (IBM);Arun Vishwanath (IBM);Saket Sathe (IBM);Shivkumar Kalyanaraman (IBM);,"2111710552,2085402668,2478550524,1969120992","The significant adoption of solar photovoltaic (PV) systems in both commercial and residential sectors has spurred an interest in monitoring the performance of these systems. This is facilitated by the increasing availability of regularly logged PV performance data in recent years. In this paper, we present a data-driven framework to systematically characterise the relationship between performance of an existing photovoltaic (PV) system and various environmental factors. We demonstrate the efficacy of our proposed framework by applying it to a PV generation dataset from a building located in northern Australia. We show how, in light of limited site-specific weather information, this data set may be coupled with publicly available data to yield rich insights on the performance of the building's PV system.",2016,Knowledge Discovery and Data Mining,simulation;computer science;
Web Content Extraction: a MetaAnalysis of its Past and Thoughts on its Future,"Tim Weninger (University of Notre Dame);Rodrigo Palacios (California State University, Fresno);Valter Crescenzi (Roma Tre University);Thomas Gottron;Paolo Merialdo (Worcester Polytechnic Institute);","2037649753,2595255449,1986707317,2610112695,262536330","In this paper, we present a meta-analysis of several Web content extraction algorithms, and make recommendations for the future of content extraction on the Web. First, we find that nearly all Web content extractors do not consider a very large, and growing, portion of modernWeb pages. Second, it is well understood that wrapper induction extractors tend to break as theWeb changes; ; heuristic/ feature engineering extractors were thought to be immune to a Web site's evolution, but we find that this is not the case: heuristic content extractor performance also tends to degrade over time due to the evolution of Web site forms and practices. We conclude with recommendations for future work that address these and other findings.",2016,Knowledge Discovery and Data Mining,multimedia;world wide web;data mining;
Automated analytics: the organizational impact of analytics-as-a-service,Tine Van Calster (Katholieke Universiteit Leuven);Jasmien Lismont (Katholieke Universiteit Leuven);María Oskarsdottir (Katholieke Universiteit Leuven);Seppe vanden Broucke (Katholieke Universiteit Leuven);Jan Vanthienen (Katholieke Universiteit Leuven);,"2519675195,2512307785,2522590243,783869731,153925120,1218880500,2061851337",-,2016,Knowledge Discovery and Data Mining,software analytics;web analytics;semantic analytics;analytics;cloud computing;
Interactive Constrained Boolean Matrix Factorization,Nelson Mukuze (Max Planck Society);Pauli Miettinen (Max Planck Society);,"2586077000,2015634213",-,2016,Knowledge Discovery and Data Mining,standard boolean model;incomplete lu factorization;logical matrix;matrix decomposition;discrete mathematics;
SIDE : a web app for interactive visual data exploration with subjective feedback,Jefrey Lijffijt (University of Bristol);Bo Kang (Ghent University);Kai Puolamäki (Aalto University);Tijl De Bie (University of Bristol);,"115479936,2497045425,96415260,2080198120",-,2016,Knowledge Discovery and Data Mining,multimedia;world wide web;simulation;
"Advances in knowledge discovery and data mining: 20th pacific-asia conference, PAKDD 2016 Auckland, New Zealand, April 19-22, 2016 proceedings, part I",James Bailey (University of Melbourne);Latifur Khan (University of Texas at Dallas);Takashi Washio (Osaka University);Gillian Dobbie (University of Auckland);Joshua Zhexue Huang (Shenzhen University);,"2131557737,2155983610,1794038515,2002900660,2699884321,2162153024",Classification.- Machine learning.- Applications.- Novel methods and algorithms.- Opinion mining and sentiment analysis.- Clustering.- Feature extraction and pattern mining.- Graph and network data.- Spatiotemporal and image data.- Anomaly detection and clustering.- Novel models and algorithms.- Text mining and recommender systems.,2016,Knowledge Discovery and Data Mining,text mining;data science;information retrieval;data mining;
